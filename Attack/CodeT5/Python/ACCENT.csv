#LINE# #TAB# if c is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return c in boolstrs
"#LINE# #TAB# PL_x = ctypes.c_float() #LINE# #TAB# PL_y = ctypes.c_float() #LINE# #TAB# PL_z = ctypes.c_float() #LINE# #TAB# _Get_call(ctypes.c_void_p(p_state), PL_x, PL_y, ctypes.c_int(idx_image), #LINE# #TAB# #TAB# ctypes.c_int(idx_chain)) #LINE# #TAB# return PL_x, PL_y, PL_z"
"#LINE# #TAB# state = False #LINE# #TAB# if not argv: #LINE# #TAB# #TAB# return get_state_and_argv() #LINE# #TAB# else: #LINE# #TAB# #TAB# state = True #LINE# #TAB# #TAB# for arg in argv: #LINE# #TAB# #TAB# #TAB# if not arg.startswith('-h'): #LINE# #TAB# #TAB# #TAB# #TAB# val = arg[2:] #LINE# #TAB# #TAB# #TAB# #TAB# if val.startswith('-'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# val = val[1:] #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return state, argv"
"#LINE# #TAB# raw_len = len(raw) #LINE# #TAB# offset = 4 #LINE# #TAB# slip = raw[offset:raw_len + slip] #LINE# #TAB# if slip > 32: #LINE# #TAB# #TAB# offset = 32 #LINE# #TAB# #TAB# slip = raw[offset:offset + slip] #LINE# #TAB# return slip, raw_len"
"#LINE# #TAB# if gain > 100: #LINE# #TAB# #TAB# raise ValueError('Gain must be between 100 and 100') #LINE# #TAB# if gain < 0: #LINE# #TAB# #TAB# raise ValueError('Gain must be between 0 and 100') #LINE# #TAB# ret = '' #LINE# #TAB# for gain_val in gain: #LINE# #TAB# #TAB# if gain_val > 0: #LINE# #TAB# #TAB# #TAB# ret += posterior_encode(gain_val, peak) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret += posterior_encode(gain_val, peak) #LINE# #TAB# return ret"
"#LINE# #TAB# if isinstance(e, urllib.error.HTTPError) and e.code in ('503', '408', '500' #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# scheme = None #LINE# #TAB# for k, v in page.items(): #LINE# #TAB# #TAB# if k.lower() == 'content-type': #LINE# #TAB# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# #TAB# scheme = v #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# elif isinstance(v, bytes): #LINE# #TAB# #TAB# #TAB# #TAB# v = v.decode('utf-8') #LINE# #TAB# #TAB# #TAB# scheme = scheme.replace('\\""', '""') #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if scheme: #LINE# #TAB# #TAB# return scheme #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# float_value = float(value) #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# if cut: #LINE# #TAB# #TAB# #TAB# float_value = minimum #LINE# #TAB# #TAB# elif maximum: #LINE# #TAB# #TAB# #TAB# float_value = maximum #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# float_value = float(float(value)) #LINE# #TAB# if pad: #LINE# #TAB# #TAB# return round(float_value, 2) #LINE# #TAB# else: #LINE# #TAB# #TAB# return float_value"
#LINE# #TAB# name = dist.project_name #LINE# #TAB# for part in name.split('.'): #LINE# #TAB# #TAB# result = cls.extract_namespace(part) #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# yield result
"#LINE# #TAB# moments = np.linalg.moments(signal) #LINE# #TAB# center = np.mean(moments, axis=0) #LINE# #TAB# width_half_max = np.max(moments, axis=0) #LINE# #TAB# return center, width_half_max"
#LINE# #TAB# out_object = ctx.out_object #LINE# #TAB# return out_object
#LINE# #TAB# keystone_data = get_keystone_data() #LINE# #TAB# client = keystone_data['client'] #LINE# #TAB# admin_credential = get_admin_credential(trust_id) #LINE# #TAB# if not admin_credential: #LINE# #TAB# #TAB# return None #LINE# #TAB# client_pem = keystone_data.get('pem_path') or admin_credential.get('client_pem') #LINE# #TAB# if not client_pem: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# client = keystone_client.get_client('v2') #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None #LINE# #TAB# return client
#LINE# #TAB# rsplit = [] #LINE# #TAB# for t in txt: #LINE# #TAB# #TAB# if t.endswith(delims): #LINE# #TAB# #TAB# #TAB# rsplit.append(t[:-len(delims)]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# rsplit.append(t) #LINE# #TAB# return rsplit
"#LINE# #TAB# try: #LINE# #TAB# #TAB# boolean_expr = parse_bool(prompt) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""'{0}' did not match a boolean expression (true/false, yes/no, t/f, y/n)"" #LINE# #TAB# #TAB# #TAB#.format(prompt)) #LINE# #TAB# #TAB# return False #LINE# #TAB# return boolean_expr"
"#LINE# #TAB# opts = 'x' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# cmdlist = [cmd, opts, os.path.abspath(archive)] #LINE# #TAB# return cmdlist, {'cwd': outdir}"
#LINE# #TAB# dn_information = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# dn_information = pwd.getpwuid(os.getuid()) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return dn_information
"#LINE# #TAB# forest = get_minimum_spanning_forest(graph) #LINE# #TAB# list_of_subgraphs = [get_subgraph_from_edge_list(graph, edge_list) for edge_list in forest] #LINE# #TAB# return list_of_subgraphs"
#LINE# #TAB# ids = os.listdir(os.getcwd()) #LINE# #TAB# text_idx = [] #LINE# #TAB# for id in ids: #LINE# #TAB# #TAB# if name_only: #LINE# #TAB# #TAB# #TAB# text_idx.append(id) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# text_idx.append(id) #LINE# #TAB# return text_idx
"#LINE# #TAB# if not allow_none: #LINE# #TAB# #TAB# raise TypeError(""{} is not a {}"".format(name, type(objects).__name__)) #LINE# #TAB# t = type(objects) #LINE# #TAB# for o in objects: #LINE# #TAB# #TAB# if t is None: #LINE# #TAB# #TAB# #TAB# if o is not None: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# elif issubclass(t, allowed_type): #LINE# #TAB# #TAB# #TAB# #TAB# raise TypeError(""{} is not a {}"".format(name, o)) #LINE# #TAB# return t"
#LINE# #TAB# assert length == 288.0 #LINE# #TAB# if q == 0.0: #LINE# #TAB# #TAB# return -0.0065 #LINE# #TAB# elif q == 1.0: #LINE# #TAB# #TAB# return -0.024 #LINE# #TAB# elif q == 2.0: #LINE# #TAB# #TAB# return -0.036 #LINE# #TAB# else: #LINE# #TAB# #TAB# x = (q - 2.0) / 2.0 #LINE# #TAB# #TAB# y = (q - 4.0) / 2.0 #LINE# #TAB# #TAB# m = 1.0 / x * y * z #LINE# #TAB# #TAB# return m
#LINE# #TAB# apps = [] #LINE# #TAB# with open('dever_config.py') as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# if not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# apps.append(line.strip()) #LINE# #TAB# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# return apps
"#LINE# #TAB# with open(file_path, 'r') as data_file: #LINE# #TAB# #TAB# data = json.load(data_file) #LINE# #TAB# #TAB# return data"
"#LINE# #TAB# lookup_fields = lookup_path.split('__') #LINE# #TAB# if lookup_fields[-1] in QUERY_TERMS: #LINE# #TAB# #TAB# lookup_fields = lookup_fields[:-1] #LINE# #TAB# for field_name in lookup_fields: #LINE# #TAB# #TAB# field = opts.get_field(field_name) #LINE# #TAB# #TAB# if hasattr(field, 'get_path_info'): #LINE# #TAB# #TAB# #TAB# path_info = field.get_path_info() #LINE# #TAB# #TAB# #TAB# opts = path_info[-1].to_opts #LINE# #TAB# #TAB# #TAB# if any(path.m2m for path in path_info): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# global root_app #LINE# #TAB# root_app = app #LINE# #TAB# return root_app
#LINE# #TAB# if country not in COUNTRIES: #LINE# #TAB# #TAB# raise KeyError('Invalid country: {}'.format(country)) #LINE# #TAB# counts = {} #LINE# #TAB# for d in d: #LINE# #TAB# #TAB# if d.get(country) == country: #LINE# #TAB# #TAB# #TAB# spread = 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# spread = 0 #LINE# #TAB# #TAB# if d.get(country) == country: #LINE# #TAB# #TAB# #TAB# counts[d.get(country)][0] = spread #LINE# #TAB# return counts
#LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# if field in allowed_fields: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield field
#LINE# #TAB# if version: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# header_record = HeaderRecord() #LINE# #TAB# header_record.name = row[0] #LINE# #TAB# header_record.value = row[1] #LINE# #TAB# header_record.md5sum = md5(header_record.value).hexdigest() #LINE# #TAB# if file_name: #LINE# #TAB# #TAB# file_name = os.path.normpath(file_name) #LINE# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# reader = csv.reader(f) #LINE# #TAB# #TAB# next(reader, None) #LINE# #TAB# #TAB# for value in reader: #LINE# #TAB# #TAB# #TAB# if value!= '': #LINE# #TAB# #TAB# #TAB# #TAB# header_record.values.append(value) #LINE# #TAB# return header_record"
#LINE# #TAB# #TAB# col_names = row.keys() #LINE# #TAB# #TAB# for col_name in col_names: #LINE# #TAB# #TAB# #TAB# col = row[col_name] #LINE# #TAB# #TAB# #TAB# if tag in col.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# col = col[tag] #LINE# #TAB# #TAB# #TAB# #TAB# if type(col) == dict: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# col_values = col[tag] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# row[col_name] = col_values #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# del row[col_name] #LINE# #TAB# #TAB# return row
#LINE# #TAB# global zombie #LINE# #TAB# if zombie == None: #LINE# #TAB# #TAB# zombie = McQuillan.load_zombie() #LINE# #TAB# return zombie
"#LINE# #TAB# n = len(observations) #LINE# #TAB# index = int(np.ceil(n / lag)) #LINE# #TAB# new_observations = np.empty(n, dtype=observations.dtype) #LINE# #TAB# for i in range(n - lag): #LINE# #TAB# #TAB# new_observations[i] = observations[i + lag:i + stride] #LINE# #TAB# return new_observations"
"#LINE# #TAB# if call == 'action': #LINE# #TAB# #TAB# raise SaltCloudSystemExit( #LINE# #TAB# #TAB# #TAB# 'The extract_callback_args function must be called with'#LINE# #TAB# #TAB# #TAB# '-f or --function, or with the --list-callback option' #LINE# #TAB# #TAB# ) #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = get_session() #LINE# #TAB# pv_args = session.call('VBoxManage{0}'.format(name)) #LINE# #TAB# return pv_args"
#LINE# #TAB# user_db = Credentials.get_user_db() #LINE# #TAB# user = User.query.filter_by(username=username).first() #LINE# #TAB# if not user: #LINE# #TAB# #TAB# raise InvalidServiceKey('User does not exist in database') #LINE# #TAB# user = user_db.search(username=username).one() #LINE# #TAB# user_key = None #LINE# #TAB# if user_key: #LINE# #TAB# #TAB# user_key = user_db.search(password=password).one() #LINE# #TAB# #TAB# if user_key: #LINE# #TAB# #TAB# #TAB# return user_key['service_key'] #LINE# #TAB# return None
"#LINE# #TAB# if verb in ('and', 'or'): #LINE# #TAB# #TAB# time = bool(time) #LINE# #TAB# if not isinstance(time, np.ndarray) or not isinstance(signal, np.ndarray): #LINE# #TAB# #TAB# signal = bool(signal) #LINE# #TAB# if verb in ('and', 'or'): #LINE# #TAB# #TAB# return time and signal #LINE# #TAB# return False"
"#LINE# #TAB# if encoding is None: #LINE# #TAB# #TAB# encoding = get_default_encoding(doc) #LINE# #TAB# if etree is None: #LINE# #TAB# #TAB# etree = etree.Element( #LINE# #TAB# #TAB# #TAB# 'lxml', #LINE# #TAB# #TAB# #TAB# xml_declaration=True, #LINE# #TAB# #TAB# #TAB# encoding=encoding #LINE# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# etree = etree.Element( #LINE# #TAB# #TAB# #TAB# 'lxml', #LINE# #TAB# #TAB# #TAB# xml_declaration=True, #LINE# #TAB# #TAB# #TAB# encoding=encoding #LINE# #TAB# #TAB# ) #LINE# #TAB# return etree"
#LINE# #TAB# windowed = [] #LINE# #TAB# for sample in data: #LINE# #TAB# #TAB# windowed.append(sample) #LINE# #TAB# #TAB# if len(windowed) >= window_len: #LINE# #TAB# #TAB# #TAB# return windowed #LINE# #TAB# return windowed
#LINE# #TAB# percent_overlap = 100 - (max1 - min1) / (min2 - min1) #LINE# #TAB# return percent_overlap
"#LINE# #TAB# from os.path import join #LINE# #TAB# cache_file = join(directory, cache_file_path) #LINE# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# time.sleep(1) #LINE# #TAB# #TAB# file_path = join(directory, cache_file_path) #LINE# #TAB# #TAB# if os.path.isfile(file_path): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# elif os.path.isdir(directory): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = operator_name() #LINE# #TAB# names = get_operator_names() #LINE# #TAB# return names[name]
"#LINE# #TAB# return [Partition(key='name', value=partition_dict['name']) for #LINE# #TAB# #TAB# partition_dict in partition_dicts]"
#LINE# #TAB# js_nans = '' #LINE# #TAB# for i in range(len(array[idx_col])): #LINE# #TAB# #TAB# value = np.nan if array[idx_col][i] is not None else np.nan #LINE# #TAB# #TAB# js_nans += value #LINE# #TAB# return js_nans
#LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if path.startswith('.'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if path.endswith('.'): #LINE# #TAB# #TAB# #TAB# path = path[:-1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# res = run_as_root('score-subject -c {0}'.format(name)) #LINE# #TAB# #TAB# return res"
"#LINE# #TAB# system = None #LINE# #TAB# handler_path = handler_name.split('.')[0] #LINE# #TAB# system = os.path.dirname(system) #LINE# #TAB# logger.debug('Found system: %s', system) #LINE# #TAB# while system == sys.modules: #LINE# #TAB# #TAB# system = sys.modules[handler_path] #LINE# #TAB# logger.debug('Found system: %s', system) #LINE# #TAB# return system"
"#LINE# #TAB# tool_registry = _get_tool_registry() #LINE# #TAB# results = {} #LINE# #TAB# for name, tool in tool_registry.items(): #LINE# #TAB# #TAB# if name not in results: #LINE# #TAB# #TAB# #TAB# results[name] = tool #LINE# #TAB# return results"
#LINE# #TAB# store = find_store_for_field(field) #LINE# #TAB# if store is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return store
"#LINE# #TAB# if boatd is None: #LINE# #TAB# #TAB# boatd = Boatd() #LINE# #TAB# content = boatd.get('/waypoints') #LINE# #TAB# new_points = [] #LINE# #TAB# for point_name in content.get('waypoints', []): #LINE# #TAB# #TAB# point = Point(point_name[0], point_name[1], point_name[2]) #LINE# #TAB# #TAB# new_points.append(point) #LINE# #TAB# return new_points"
"#LINE# #TAB# test_path = Path(sys.modules[__name__].__file__) #LINE# #TAB# if not test_path.exists(): #LINE# #TAB# #TAB# test_path = Path(os.path.join(os.path.dirname(test_path), 'tests')) #LINE# #TAB# return test_path"
#LINE# #TAB# frag_lengths = {} #LINE# #TAB# with open(fastafile) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# seq = line.strip() #LINE# #TAB# #TAB# #TAB# if not seq: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# frag_lengths[seq.id] = len(seq.get_fragment_lengths()) #LINE# #TAB# return frag_lengths
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('devices_count', dtype='int32', direction=function.IN) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# result = [] #LINE# #TAB# for item in list(dict.keys()): #LINE# #TAB# #TAB# result.append(VnlNameValue(item, omniORB.any.to_any(dict[ #LINE# #TAB# #TAB# #TAB# item]))) #LINE# #TAB# return result"
#LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# return False #LINE# #TAB# if filename.endswith('.py'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# contrib_type = ""author non-byline"" #LINE# #TAB# contributors_ = contributors(soup, detail) #LINE# #TAB# indexes = [x for x in contributors_ if x.get(""type"") == contrib_type] #LINE# #TAB# position = 1 #LINE# #TAB# for index, author in enumerate(indexes): #LINE# #TAB# #TAB# if author[""type""] == contrib_type: #LINE# #TAB# #TAB# #TAB# indexes[index] = position #LINE# #TAB# #TAB# position = position + 1 #LINE# #TAB# return indexes"
"#LINE# #TAB# from.modules import babelify #LINE# #TAB# if babelify: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'babelify_tools': { #LINE# #TAB# #TAB# #TAB# #TAB# 'input': entry_point, #LINE# #TAB# #TAB# #TAB# #TAB# 'output': output_file, #LINE# #TAB# #TAB# #TAB# #TAB# 'output_file': export_as, #LINE# #TAB# #TAB# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'babelify': babelify, #LINE# #TAB# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# } #LINE# #TAB# return output_file + '.js'"
"#LINE# #TAB# combinations = [ #LINE# #TAB# #TAB# (c1, c2), #LINE# #TAB# #TAB# (c3, c4), #LINE# #TAB# #TAB# (c4, c5), #LINE# #TAB# #TAB# (c6, c7), #LINE# #TAB# #TAB# (c8, c9), #LINE# #TAB# #TAB# (c9, c10), #LINE# #TAB# #TAB# (c10, c11), #LINE# #TAB# #TAB# (c12, c13), #LINE# #TAB# #TAB# (c15, c16), #LINE# #TAB# #TAB# (c15, c16), #LINE# #TAB# ] #LINE# #TAB# return combinations"
"#LINE# #TAB# m = list(m) #LINE# #TAB# for i in range(len(m)): #LINE# #TAB# #TAB# for j in range(len(m[i])): #LINE# #TAB# #TAB# #TAB# m[i][j] = m[i][j][0] #LINE# #TAB# flowMatrix = m[0][0] #LINE# #TAB# dropMatrix = MarkovMatrix(m[0][0], m[0][1], m[1][1], m[1][2]) #LINE# #TAB# return dropMatrix"
"#LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.partyname, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# program ='mafft' #LINE# #TAB# args = [program] + file_to_align #LINE# #TAB# proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess #LINE# #TAB# #TAB#.PIPE) #LINE# #TAB# stdout, stderr = proc.communicate() #LINE# #TAB# if proc.returncode!= 0: #LINE# #TAB# #TAB# sys.stderr.write('Failed to run mafft: {0}\n'.format(stderr)) #LINE# #TAB# #TAB# return None #LINE# #TAB# return stdout[0]"
"#LINE# #TAB# encoded = value #LINE# #TAB# for idx, char in enumerate(encoded): #LINE# #TAB# #TAB# if char == '$': #LINE# #TAB# #TAB# #TAB# encoded = encoded.replace('$', '$') #LINE# #TAB# #TAB# elif char == ':': #LINE# #TAB# #TAB# #TAB# encoded = encoded.replace('$', '$') #LINE# #TAB# return encoded"
"#LINE# #TAB# requires_moderate = forum.get('requires_moderate') #LINE# #TAB# if not requires_moderate: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# topics_to_moderate = [] #LINE# #TAB# for token in forum.get('topics', []): #LINE# #TAB# #TAB# topic_id = token['topic_id'] #LINE# #TAB# #TAB# for token in token['topics']: #LINE# #TAB# #TAB# #TAB# if token['moderate']!= forum.get('forum_id'): #LINE# #TAB# #TAB# #TAB# #TAB# topics_to_moderate.append(topic_id) #LINE# #TAB# if len(topics_to_moderate) == 0: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"#LINE# #TAB# result = False #LINE# #TAB# if isinstance(array, list): #LINE# #TAB# #TAB# if len(array) == 1: #LINE# #TAB# #TAB# #TAB# result = array[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for i in range(0, len(array), 2): #LINE# #TAB# #TAB# #TAB# #TAB# result = True #LINE# #TAB# return result"
"#LINE# #TAB# return {'__kind__': kind_inst, 'class': fqname_for(v.__class__), 'args': #LINE# #TAB# #TAB# format_label_model_args(v)}"
"#LINE# #TAB# return {'jsonrpc': '2.0','version': '1.0.0', 'components': [{'name': #LINE# #TAB# #TAB# 'x', 'value': 0}, {'name': 'hff.info', 'value': 1}, {'name': 'hff.out', #LINE# #TAB# #TAB# 'value': None}]}"
"#LINE# #TAB# assert isinstance(repo, Repo), type(repo) #LINE# #TAB# check_repo(repo) #LINE# #TAB# if repo.git.rev_parse('--show-toplevel'): #LINE# #TAB# #TAB# return'master' #LINE# #TAB# elif repo.git.rev_parse('--show-toplevel'): #LINE# #TAB# #TAB# return'master' #LINE# #TAB# else: #LINE# #TAB# #TAB# return'master'"
"#LINE# #TAB# edges_dictionary = {} #LINE# #TAB# for node in graph_object: #LINE# #TAB# #TAB# edges = hff_get_node_edges(node, graph_identifier) #LINE# #TAB# #TAB# for edge_key, edge_value in edges.items(): #LINE# #TAB# #TAB# #TAB# edge_dict = { #LINE# #TAB# #TAB# #TAB# #TAB# edge_key: edge_value #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# #TAB# hff_write_edge(edge_dict, edge_key, edge_value) #LINE# #TAB# return edges_dictionary"
"#LINE# #TAB# for client in CLIENT_DESCRIPTORS: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# accept(client, descriptor) #LINE# #TAB# #TAB# #TAB# return client, descriptor.address #LINE# #TAB# #TAB# except Exception as ex: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return None"
"#LINE# #TAB# tuple_ = [(math.sin(i / 2.0) * math.cos(i * math.pi / 180.0), math.sin(i / #LINE# #TAB# #TAB# 2.0) * math.sin(i * math.pi / 180.0)) for i in range(3)] #LINE# #TAB# tuple_ = [(math.cos(i * math.pi / 180.0), math.sin(i * #LINE# #TAB# #TAB# math.pi / 180.0)), (math.sin(i * math.pi / 180.0), math.cos(i * #LINE# #TAB# #TAB# math.pi / 180.0)) for i in range(3)] #LINE# #TAB# return tuple_"
"#LINE# #TAB# if not blocks: #LINE# #TAB# #TAB# return None #LINE# #TAB# _blocks = blocks.copy() #LINE# #TAB# for b in blocks: #LINE# #TAB# #TAB# if isinstance(b, ndarray): #LINE# #TAB# #TAB# #TAB# _blocks[b] = encode_image(b) #LINE# #TAB# #TAB# elif isinstance(b, sparse.csr_matrix): #LINE# #TAB# #TAB# #TAB# _blocks[b] = b.tocsr() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _blocks[b] = b #LINE# #TAB# return _blocks"
"#LINE# #TAB# L = [] #LINE# #TAB# i = iter(a) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(i, list): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# L.append(logit_pretty(i)) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# i = next(i) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# L.append(i) #LINE# #TAB# #TAB# #TAB# #TAB# i = next(i) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return L"
#LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# if previous_object is not None: #LINE# #TAB# #TAB# #TAB# result = previous_object.update_description(attributes) #LINE# #TAB# #TAB# return result
"#LINE# #TAB# settings = request.registry.settings #LINE# #TAB# if settings.get('DEBUG', False): #LINE# #TAB# #TAB# settings['indent'] = 4 #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = make_response(dumped, code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
"#LINE# #TAB# subdomain_df.loc[:, (subdomain_df['Subdomain'].isnull())] = subdomain_df.loc[:, ( #LINE# #TAB# #TAB# subdomain_df['Subdomain'].isnull())] == 'true', axis=1 #LINE# #TAB# subdomain_df.loc[:, (subdomain_df['Subdomain'].isnull())] = 'false', axis=1 #LINE# #TAB# return subdomain_df"
"#LINE# #TAB# new_key = base64.b32decode(key) #LINE# #TAB# padding = int(new_key[-4:], 16) #LINE# #TAB# for i in range(0, len(new_key), 4): #LINE# #TAB# #TAB# new_key = new_key[i:i + padding] + new_key[i + padding:] #LINE# #TAB# return new_key"
"#LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
#LINE# #TAB# node = conn.create_node(name) #LINE# #TAB# while True: #LINE# #TAB# #TAB# opt = conn.describe_vm(name) #LINE# #TAB# #TAB# if opt['Target']: #LINE# #TAB# #TAB# #TAB# node.append(opt['Target']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return node
#LINE# #TAB# date = str(date) #LINE# #TAB# img = imgcol.get_date_image(date) #LINE# #TAB# if img is None: #LINE# #TAB# #TAB# raise ValueError('Could not find a valid Image for {}'.format(imgcol)) #LINE# #TAB# if validate and imgcol.validate_date(date): #LINE# #TAB# #TAB# img = img.validate_image(date) #LINE# #TAB# return img
#LINE# #TAB# l = line.split() #LINE# #TAB# if len(l) > 1 and l[-1].startswith('search'): #LINE# #TAB# #TAB# return True
#LINE# #TAB# has_color = False #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# has_color = False #LINE# #TAB# #TAB# for sequence in line[1:]: #LINE# #TAB# #TAB# #TAB# if sequence.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# sequence = '\\x' + sequence #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# sequence = '' #LINE# #TAB# #TAB# if has_color: #LINE# #TAB# #TAB# #TAB# line += [sequence] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# line += [sequence] #LINE# #TAB# return lines
#LINE# #TAB# if tz is None: #LINE# #TAB# #TAB# tz = pytz.timezone('UTC') #LINE# #TAB# dttm = datetime.timedelta(days=1) #LINE# #TAB# if relPeriod == 'today': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() #LINE# #TAB# elif relPeriod =='monthly': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() - timedelta(days=1) #LINE# #TAB# elif relPeriod == 'yesterday': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() - timedelta(days=1) #LINE# #TAB# return dttm
"#LINE# #TAB# with h5py.File(path, 'r') as h5f: #LINE# #TAB# #TAB# model = h5f['model'] #LINE# #TAB# weights = model['weights'] #LINE# #TAB# indices = model['indices'] #LINE# #TAB# indptr = model['indptr'] #LINE# #TAB# model['weights'] = np.array(weights) #LINE# #TAB# model['indices'] = indices #LINE# #TAB# model['indptr'] = indptr #LINE# #TAB# return model"
#LINE# #TAB# if colored_output: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# try: #LINE# #TAB# #TAB# int(s, 16) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if type(c)!= list: #LINE# #TAB# #TAB# c = [c] #LINE# #TAB# c = convert_color(c) #LINE# #TAB# p = 0 #LINE# #TAB# q = 1 #LINE# #TAB# if len(c) == 3: #LINE# #TAB# #TAB# s = c[0] #LINE# #TAB# #TAB# r = c[1] #LINE# #TAB# #TAB# g = c[2] #LINE# #TAB# #TAB# b = c[3] #LINE# #TAB# else: #LINE# #TAB# #TAB# s = c[0] #LINE# #TAB# #TAB# r = c[1] #LINE# #TAB# #TAB# g = c[2] #LINE# #TAB# #TAB# b = c[3] #LINE# #TAB# return p, q, g, b"
#LINE# #TAB# n = X.shape[0] #LINE# #TAB# assert w.min() > 0 and w.max() > 1 #LINE# #TAB# if X.shape[1] == 1: #LINE# #TAB# #TAB# return X[w[0]:(n + 1) * (W[1] - w[0])] #LINE# #TAB# elif X.shape[1] == 2: #LINE# #TAB# #TAB# return X[w[0]:(n + 1) * (W[1] - w[1])] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if dd.get_sample_name(data) == ""ensemble"": #LINE# #TAB# #TAB# vcf_ref = dd.get_sample_name(data) #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# if vcf_ref: #LINE# #TAB# #TAB# #TAB# out[""vcf_ref""] = dd.get_sample_name(data) #LINE# #TAB# #TAB# return out #LINE# #TAB# data = _expand_vcf_refs(data) #LINE# #TAB# if dd.get_sample_name(data) == ""batches"": #LINE# #TAB# #TAB# out[""vcf_ref""] = dd.get_sample_name(data) #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return data"
"#LINE# #TAB# assert analysis_fields.shape == (len(analysis_fields), len(data_frame)) #LINE# #TAB# socket_path = analysis_fields[0] + '_socket' #LINE# #TAB# socket = requests.post(socket_path, data=data_frame) #LINE# #TAB# return socket"
#LINE# #TAB# if key == 'enabled': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# radius = float(radius) / 100.0 #LINE# #TAB# #TAB# #TAB# url['enable'] = True #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# url['enable'] = False #LINE# #TAB# #TAB# #TAB# url['disable'] = False #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# radius = float(radius) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# url['enable'] = False #LINE# #TAB# #TAB# #TAB# url['disable'] = True #LINE# #TAB# return url
"#LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page, length in site.infer_reads_page(total=total): #LINE# #TAB# #TAB# yield page"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# username = getpass.getuser() #LINE# #TAB# #TAB# password = getpass.getpass('Enter password: ') #LINE# #TAB# #TAB# return username, password #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None, None"
"#LINE# #TAB# height, width, _ = image.size #LINE# #TAB# from PIL import ImageMath #LINE# #TAB# if image.mode == 'RGBA': #LINE# #TAB# #TAB# bands = ImageMath.cvtColor(image, cv2.COLOR_BGR2GRAY) #LINE# #TAB# #TAB# target_space = bands * 4 #LINE# #TAB# else: #LINE# #TAB# #TAB# target_space = ImageMath.cvtColor(image, cv2.COLOR_BGR2GRAY) #LINE# #TAB# entity_space = TargetSpace(target_space) #LINE# #TAB# return entity_space"
"#LINE# #TAB# if isinstance(e, Var): #LINE# #TAB# #TAB# return e.node #LINE# #TAB# if isinstance(e, Expr): #LINE# #TAB# #TAB# e = e.expr #LINE# #TAB# #TAB# if not e.is_atom(): #LINE# #TAB# #TAB# #TAB# return e.node #LINE# #TAB# #TAB# return e.source #LINE# #TAB# if isinstance(e, Expr): #LINE# #TAB# #TAB# return e.node #LINE# #TAB# for child in e.children: #LINE# #TAB# #TAB# norm_top_node(child) #LINE# #TAB# return e"
#LINE# #TAB# props = artist.generate_pep484_props(**kwargs) #LINE# #TAB# if 'title' in kwargs: #LINE# #TAB# #TAB# props['title'] = kwargs['title'] #LINE# #TAB# if 'artist_id' in kwargs: #LINE# #TAB# #TAB# props['artist_id'] = kwargs['artist_id'] #LINE# #TAB# return props
#LINE# #TAB# seen = set() #LINE# #TAB# out = [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if item not in seen: #LINE# #TAB# #TAB# #TAB# seen.add(item) #LINE# #TAB# #TAB# #TAB# out.append(item) #LINE# #TAB# return out
"#LINE# #TAB# ret = {} #LINE# #TAB# for lib in libs: #LINE# #TAB# #TAB# val = libs[lib].split(';') #LINE# #TAB# #TAB# if len(val) > 0: #LINE# #TAB# #TAB# #TAB# for k, v in val: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ret[k] = [(j, v) for j in v] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ret[k] = v #LINE# #TAB# return ret"
#LINE# #TAB# if value == True: #LINE# #TAB# #TAB# return 'True' #LINE# #TAB# elif value == False: #LINE# #TAB# #TAB# return 'False' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Unknown'
"#LINE# #TAB# if as_id.startswith('unix://'): #LINE# #TAB# #TAB# as_id = as_id[5:] #LINE# #TAB# parts = as_id.split(':', 1) #LINE# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# return parts[0], parts[1] #LINE# #TAB# return None, as_id"
"#LINE# #TAB# env = {} #LINE# #TAB# if opts is None: #LINE# #TAB# #TAB# opts = {} #LINE# #TAB# for ext in COMMANDS: #LINE# #TAB# #TAB# ext = ext[0].lower() #LINE# #TAB# #TAB# if ext == '.py': #LINE# #TAB# #TAB# #TAB# env[ext] = os.path.join(path, ext) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# env[ext] = opts.copy() #LINE# #TAB# for ext in COMMANDS: #LINE# #TAB# #TAB# ext = ext[0].lower() #LINE# #TAB# #TAB# env[ext] = os.path.join(path, ext) #LINE# #TAB# return env"
#LINE# #TAB# global _top_buffer_size #LINE# #TAB# _top_buffer_size = limit
#LINE# #TAB# api = get_api() #LINE# #TAB# user_org = api.get_user(orgrepo) #LINE# #TAB# org = api.get_org(orgrepo) #LINE# #TAB# click.echo('{0} is a valid org repo'.format(org)) #LINE# #TAB# if not user_org: #LINE# #TAB# #TAB# click.echo('{0} is not a valid org repo'.format(orgrepo)) #LINE# #TAB# #TAB# return False #LINE# #TAB# return org == org_org
"#LINE# #TAB# formula = find_formula(colorname) #LINE# #TAB# formula[0] = alpha #LINE# #TAB# formula[1] = alpha #LINE# #TAB# return [(formula[3], formula[2], formula[1] * 255), (formula[3], formula[2] * 255), #LINE# #TAB# #TAB# (formula[3], formula[0] * 255), (formula[3], formula[1] * 255), ( #LINE# #TAB# #TAB# formula[3], formula[1] * 255), (formula[2], formula[3] * 255)]"
"#LINE# #TAB# user_groups = clean_username_string_list(host_string, default_user, #LINE# #TAB# #TAB# default_port) #LINE# #TAB# if user_groups: #LINE# #TAB# #TAB# user = default_user #LINE# #TAB# #TAB# host = user_groups[0] #LINE# #TAB# if ':' in host: #LINE# #TAB# #TAB# host, port = host.split(':', 1) #LINE# #TAB# #TAB# port = int(port) #LINE# #TAB# return [(host, port, user)]"
"#LINE# #TAB# content = '' #LINE# #TAB# with open(f, 'r') as f_obj: #LINE# #TAB# #TAB# for line in f_obj: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# content += line #LINE# #TAB# return content"
"#LINE# #TAB# if f is None: #LINE# #TAB# #TAB# return False, False #LINE# #TAB# if f >= 0.5: #LINE# #TAB# #TAB# return float('-inf'), float('inf') #LINE# #TAB# else: #LINE# #TAB# #TAB# return float('inf'), float('-inf') #LINE# #TAB# else: #LINE# #TAB# #TAB# return True, False"
"#LINE# #TAB# if type(value)!= list: #LINE# #TAB# #TAB# return value #LINE# #TAB# for i, x in enumerate(value): #LINE# #TAB# #TAB# pixel_check = True #LINE# #TAB# #TAB# for j, y in enumerate(x): #LINE# #TAB# #TAB# #TAB# pixel_check = False #LINE# #TAB# #TAB# #TAB# if pixel_check: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# for x, y in enumerate(y): #LINE# #TAB# #TAB# #TAB# pixel_check = True #LINE# #TAB# #TAB# if pixel_check: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# result = None #LINE# #TAB# if p_date: #LINE# #TAB# #TAB# result = parse_date(p_date) #LINE# #TAB# return result
"#LINE# #TAB# dictionary = {} #LINE# #TAB# for i in range(len(sql_raw_list[0]), len(sql_raw_list[1])): #LINE# #TAB# #TAB# dictionary[sql_raw_list[0][i]] = sql_raw_list[1][i] #LINE# #TAB# return dictionary"
"#LINE# #TAB# if not bounds: #LINE# #TAB# #TAB# raise ValueError('bounds cannot be empty') #LINE# #TAB# if len(poly) < 2: #LINE# #TAB# #TAB# raise ValueError('poly must be a 2-tuple') #LINE# #TAB# area = 0 #LINE# #TAB# for b in reversed(poly): #LINE# #TAB# #TAB# sum_ = sum(b - bounds[0]) #LINE# #TAB# #TAB# area += sum_ * algorithm(b, bounds[1], algorithm) #LINE# #TAB# return area"
#LINE# #TAB# _top_runner_thread.enabled = True #LINE# #TAB# return True
#LINE# #TAB# for match in re_vertex.finditer(text): #LINE# #TAB# #TAB# yield match.group() #LINE# #TAB# yield text
#LINE# #TAB# with get_notebook_context(): #LINE# #TAB# #TAB# paths_lock = True #LINE# #TAB# #TAB# with get_notebook_context() as context: #LINE# #TAB# #TAB# #TAB# paths_lock = False #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# #TAB# if paths_lock: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# os.unlink(paths_lock) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# paths_lock = False
"#LINE# #TAB# Q = C.strip().splitlines() #LINE# #TAB# rB = B.strip().splitlines() #LINE# #TAB# rC = C.strip().splitlines() #LINE# #TAB# root_partition = np.zeros((rB.shape[0], rC.shape[0])) #LINE# #TAB# for i in range(rB.shape[0]): #LINE# #TAB# #TAB# x = rC[i] #LINE# #TAB# #TAB# if not x: #LINE# #TAB# #TAB# #TAB# if not B.shape[i]: #LINE# #TAB# #TAB# #TAB# #TAB# root_partition = np.zeros((rB.shape[0], rC.shape[0])) #LINE# #TAB# #TAB# #TAB# #TAB# B_partition[i] = x #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# C_partition[i] = x #LINE# #TAB# return root_partition"
"#LINE# #TAB# rms = cls.pix_rms_atom(atom_map, center_data) #LINE# #TAB# return rms"
"#LINE# #TAB# chunks = [] #LINE# #TAB# for child in tree: #LINE# #TAB# #TAB# for prop in child.keys(): #LINE# #TAB# #TAB# #TAB# if isinstance(prop, property): #LINE# #TAB# #TAB# #TAB# #TAB# prop = prop.value #LINE# #TAB# #TAB# #TAB# elif isinstance(prop, list): #LINE# #TAB# #TAB# #TAB# #TAB# prop = [chunk_db(x) for x in prop] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# prop = prop.value #LINE# #TAB# #TAB# #TAB# chunks.append((prop, child)) #LINE# #TAB# return chunks"
"#LINE# #TAB# fields = line.rstrip().split('\t') #LINE# #TAB# try: #LINE# #TAB# #TAB# fields = list(map(int, fields)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValidationError('tsv header is invalid: {}'.format(line)) #LINE# #TAB# columns = fields[:2] #LINE# #TAB# if len(columns)!= 2: #LINE# #TAB# #TAB# raise ValidationError('tsv header has wrong columns: {}'.format( #LINE# #TAB# #TAB# #TAB# columns)) #LINE# #TAB# column_indices = {} #LINE# #TAB# for i, name in enumerate(columns): #LINE# #TAB# #TAB# column_indices[name] = i #LINE# #TAB# return columns, column_indices"
#LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# if rule['type'] == 'comment': #LINE# #TAB# #TAB# #TAB# rules.remove(rule) #LINE# #TAB# return rules
#LINE# #TAB# for path in dl_paths: #LINE# #TAB# #TAB# for url in url_dict[path]: #LINE# #TAB# #TAB# #TAB# if url in path: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# df = pd.read_excel(df, sheet_name=station_name) #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return df"
"#LINE# #TAB# if isinstance(op, Operation): #LINE# #TAB# #TAB# result = op.result #LINE# #TAB# #TAB# if result is not None: #LINE# #TAB# #TAB# #TAB# return result #LINE# #TAB# #TAB# elif isinstance(op, ScalarOp): #LINE# #TAB# #TAB# #TAB# result = op.items #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result = op.item #LINE# #TAB# #TAB# #TAB# if isinstance(result, tuple): #LINE# #TAB# #TAB# #TAB# #TAB# result = tuple([(k, generate_preference(v, name)) for k, v in result]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return {k: generate_preference(v, name) for k, v in result.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return op"
"#LINE# #TAB# if not isinstance(module_names, (list, tuple)): #LINE# #TAB# #TAB# module_names = [module_names] #LINE# #TAB# modules = [] #LINE# #TAB# for module_name in module_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = importlib.import_module(module_name) #LINE# #TAB# #TAB# except ImportError as e: #LINE# #TAB# #TAB# #TAB# return False, str(e) #LINE# #TAB# #TAB# modules.append(module) #LINE# #TAB# try: #LINE# #TAB# #TAB# status = verify_status(modules) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# return False, str(e) #LINE# #TAB# return True, modules"
"#LINE# #TAB# data = {} #LINE# #TAB# for k, v in parameters.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# data[k] = download_kwarg(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data[k] = download_kwarg(v) #LINE# #TAB# return data"
"#LINE# #TAB# for path in _CONFIG_PATHS: #LINE# #TAB# #TAB# for root, dirs, files in os.walk(path): #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# full_path = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# path = os.path.relpath(full_path, path) #LINE# #TAB# #TAB# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# config = loads(f.read()) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield config"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# line = fd.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# cols = line.split() #LINE# #TAB# #TAB# n, t, x = cols[:2] #LINE# #TAB# #TAB# return n, t, x"
#LINE# #TAB# states = None #LINE# #TAB# try: #LINE# #TAB# #TAB# states = Decimal(str(value)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if states.hour > 0 and states.minute > 0: #LINE# #TAB# #TAB# states = int(states.hour) #LINE# #TAB# if states.hour == 0: #LINE# #TAB# #TAB# states = int(states.minute) #LINE# #TAB# if states.hour == 0: #LINE# #TAB# #TAB# states = int(states.hour) #LINE# #TAB# return states
#LINE# #TAB# backend = _valid_backend(name) #LINE# #TAB# if backend is not None: #LINE# #TAB# #TAB# backend.set_versions() #LINE# #TAB# return backend
#LINE# #TAB# client_by_item = [] #LINE# #TAB# for i in item: #LINE# #TAB# #TAB# if i == 'dimensions': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if i =='meta': #LINE# #TAB# #TAB# #TAB# client_by_item.append(save_client_by_meta(i)) #LINE# #TAB# #TAB# elif i == 'value_meta': #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# client_by_item.append(save_client_by_value_meta(i)) #LINE# #TAB# client_by_item.append(client_by_item) #LINE# #TAB# return client_by_item
#LINE# #TAB# global sched #LINE# #TAB# sched = _sched
"#LINE# #TAB# length = len(rlp) - start - 1 #LINE# #TAB# if length == 0: #LINE# #TAB# #TAB# return None, 1 #LINE# #TAB# value = rlp[start:start + length] #LINE# #TAB# return value, length"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return get_host_ip(hostname) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return fallback
#LINE# #TAB# cfg = ConfigParser() #LINE# #TAB# cfg_path = os.path.expanduser(file_path) #LINE# #TAB# if os.path.isfile(cfg_path) and force: #LINE# #TAB# #TAB# config_parser.read(cfg_path) #LINE# #TAB# else: #LINE# #TAB# #TAB# config_parser.read(cfg_path) #LINE# #TAB# return cfg
#LINE# #TAB# inverted_flux = flux_vec[biomass_index] / flux_vec[biomass_index] #LINE# #TAB# return inverted_flux
"#LINE# #TAB# image = image.astype(np.uint8) #LINE# #TAB# matrix = color_correlation_svd_sqrt * image.shape #LINE# #TAB# dest = np.zeros(image.shape, dtype=np.uint8) #LINE# #TAB# for i in range(image.shape[0]): #LINE# #TAB# #TAB# for j in range(image.shape[1]): #LINE# #TAB# #TAB# #TAB# matrix[i, j] = (image[i, j] - color_correlation_svd_sqrt[i, #LINE# #TAB# #TAB# #TAB# #TAB# i]) / color_correlation_svd_sqrt[j, i] #LINE# #TAB# return dest"
"#LINE# #TAB# path = path or [] #LINE# #TAB# array_name = _get_string_key(path) #LINE# #TAB# for key in store: #LINE# #TAB# #TAB# array_name = store[key] #LINE# #TAB# #TAB# if array_name in path: #LINE# #TAB# #TAB# #TAB# store[key] = store[key].replace(path[0], path[1]) #LINE# #TAB# #TAB# elif array_name in store: #LINE# #TAB# #TAB# #TAB# store.pop(key) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# arrays = [asarray(x) for x in scalars_or_arrays] #LINE# #TAB# out_type = dtypes.result_type(*arrays) #LINE# #TAB# return [x.astype(out_type, copy=False) for x in arrays]"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = getattr(request, 'heartbeat', None) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# value = ( #LINE# #TAB# #TAB# #TAB# request.META.get('HTTP_USER_AGENT', ''), #LINE# #TAB# #TAB# #TAB# request.META.get('HTTP_USER_AGENT', ''), #LINE# #TAB# #TAB# ) #LINE# #TAB# if value: #LINE# #TAB# #TAB# return {'status': value} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'status':'success', 'value': value}"
"#LINE# #TAB# if len(string) <= length: #LINE# #TAB# #TAB# return string #LINE# #TAB# new_string = """" #LINE# #TAB# for i in range(0, length): #LINE# #TAB# #TAB# line = string[i] #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# new_string += ""\n"" #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_string += line + ""\n"" #LINE# #TAB# return new_string"
"#LINE# #TAB# sock = nl_socket() #LINE# #TAB# if cb: #LINE# #TAB# #TAB# nl_socket_set_cb(sock, cb) #LINE# #TAB# _LOGGER.debug('Allocated new socket') #LINE# #TAB# return sock"
"#LINE# #TAB# start_response('404 NOT FOUND', [('Content-Type', 'text/plain')]) #LINE# #TAB# return [ #LINE# #TAB# #TAB# ]"
"#LINE# #TAB# with h5py.File(filename, 'r') as h5f: #LINE# #TAB# #TAB# json_data = h5f['settings'][()] #LINE# #TAB# new_settings = {} #LINE# #TAB# for key in json_data: #LINE# #TAB# #TAB# new_settings[key] = json_data[key][()] #LINE# #TAB# h5f.close() #LINE# #TAB# return new_settings"
"#LINE# #TAB# tot_actions = len(traces) #LINE# #TAB# assert dims >= 2 #LINE# #TAB# if weight_vec is not None: #LINE# #TAB# #TAB# weights = weight_vec #LINE# #TAB# else: #LINE# #TAB# #TAB# weights = np.ones(tot_actions, dtype=np.float64) #LINE# #TAB# temp = np.zeros(tot_actions, dtype=np.float64) #LINE# #TAB# for trace in traces: #LINE# #TAB# #TAB# new_trace = trace.copy() #LINE# #TAB# #TAB# new_trace[:, :dims] = new_trace #LINE# #TAB# #TAB# temp[dims:tot_actions] = new_trace #LINE# #TAB# return temp"
#LINE# #TAB# if uids: #LINE# #TAB# #TAB# for pif in pifs: #LINE# #TAB# #TAB# #TAB# if 'uids' in pif: #LINE# #TAB# #TAB# #TAB# #TAB# pif['uids'] = uids.split(':') #LINE# #TAB# else: #LINE# #TAB# #TAB# uids = {} #LINE# #TAB# return uids
#LINE# #TAB# values_structure = dict() #LINE# #TAB# absolute_values = [] #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# value = layer.get_complete_value(field) #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# if absolute_values: #LINE# #TAB# #TAB# #TAB# #TAB# absolute_values.append(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# absolute_values.append('') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# values_structure[field] = str(value) #LINE# #TAB# return values_structure
"#LINE# #TAB# try: #LINE# #TAB# #TAB# out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# #TAB# shell=True) #LINE# #TAB# except subprocess.CalledProcessError as err: #LINE# #TAB# #TAB# if err.returncode == 1 and err.output == '': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# out = out.decode('utf-8') #LINE# #TAB# return out"
"#LINE# #TAB# value = z_value(data, loperand, roperand, is_not) #LINE# #TAB# if value: #LINE# #TAB# #TAB# if extras: #LINE# #TAB# #TAB# #TAB# value = apply_extras(value, extras) #LINE# #TAB# #TAB# for key in value: #LINE# #TAB# #TAB# #TAB# yield key"
"#LINE# #TAB# value = int(value) #LINE# #TAB# value = value.replace('-', '') #LINE# #TAB# value = value.replace('.', '') #LINE# #TAB# value = value.replace('0', '') #LINE# #TAB# value = value.replace('-', '') #LINE# #TAB# value = value.replace('(', '') #LINE# #TAB# value = value.replace(')', '') #LINE# #TAB# try: #LINE# #TAB# #TAB# sign = int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if sign < 0: #LINE# #TAB# #TAB# sign = -sign #LINE# #TAB# value = '-' + value #LINE# #TAB# return sign * value"
"#LINE# #TAB# count = SDL_GetDisplayCount(buf) #LINE# #TAB# if count < sample_width: #LINE# #TAB# #TAB# pad = sample_width - count % sample_width #LINE# #TAB# #TAB# buf = (buf + pad) * sample_width #LINE# #TAB# return count, buf"
#LINE# #TAB# try: #LINE# #TAB# #TAB# netaddr.IPNetwork(arg) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# result = {} #LINE# #TAB# for attr in dir(obj): #LINE# #TAB# #TAB# value = getattr(obj, attr) #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# result[attr] = [as_array(value)] #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for key, value in value.items(): #LINE# #TAB# #TAB# #TAB# #TAB# result[key] = as_array(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[attr] = value #LINE# #TAB# return result"
#LINE# #TAB# filename = band['filename'] #LINE# #TAB# layer = band['layer'] #LINE# #TAB# if is_netcdf_file(filename): #LINE# #TAB# #TAB# return f'{filename}:{layer}' #LINE# #TAB# return f'{filename}:${layer}'
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return datetime.datetime.strptime(filename, '%Y-%m-%d').date() #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# key = _get_module_path(exc_type) #LINE# #TAB# if key in registry: #LINE# #TAB# #TAB# return registry[key] #LINE# #TAB# else: #LINE# #TAB# #TAB# instance = exc_type() #LINE# #TAB# #TAB# registry[key] = instance #LINE# #TAB# #TAB# return instance
"#LINE# #TAB# username = props.get('username', username) #LINE# #TAB# if 'email' in props: #LINE# #TAB# #TAB# username +='<%(email)s>' % props #LINE# #TAB# return username"
"#LINE# #TAB# logger.debug('Checking index for {}'.format(table_name)) #LINE# #TAB# if engine.dialect.has_table(table_name): #LINE# #TAB# #TAB# table = engine.execute(table_name) #LINE# #TAB# #TAB# if table.inspect(): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# index = engine.execute(index_name) #LINE# #TAB# #TAB# #TAB# #TAB# logger.debug('Found index {} for {}'.format(index_name, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# table_name)) #LINE# #TAB# #TAB# #TAB# #TAB# index.flush() #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# return index"
#LINE# #TAB# result = '' #LINE# #TAB# for c in chunk: #LINE# #TAB# #TAB# if c == b'\x00': #LINE# #TAB# #TAB# #TAB# result += '0' #LINE# #TAB# #TAB# elif c == b'\x01': #LINE# #TAB# #TAB# #TAB# result += '1' #LINE# #TAB# #TAB# elif c == b'\x03': #LINE# #TAB# #TAB# #TAB# result += '2' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result += c #LINE# #TAB# return result
#LINE# #TAB# if not installed_packages: #LINE# #TAB# #TAB# installed_packages = [] #LINE# #TAB# return package in installed_packages
"#LINE# #TAB# seconds = None #LINE# #TAB# if s.tzinfo is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# seconds = s.total_seconds() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# if seconds is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# seconds = int(s.total_seconds()) #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# return seconds"
#LINE# #TAB# descriptor_object = gl_gen_function(n) #LINE# #TAB# gl_gen_function.argtypes = [ctypes.c_int] #LINE# #TAB# descriptor_object.restype = ctypes.c_int #LINE# #TAB# return descriptor_object
#LINE# #TAB# for token in sentence: #LINE# #TAB# #TAB# if token.isspace(): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# soup = BeautifulSoup(html, 'lxml') #LINE# #TAB# for elem in soup.find_all(['div', 'div', 'a']): #LINE# #TAB# #TAB# urls = [url.get('href') for url in elem.find_all('a')] #LINE# #TAB# #TAB# urls = sorted(urls, key=lambda x: x.get('href')) #LINE# #TAB# return urls"
"#LINE# #TAB# spacing = np.array(spacing) #LINE# #TAB# if packed: #LINE# #TAB# #TAB# d = data.shape[2] #LINE# #TAB# #TAB# x, y = np.meshgrid(*[range(d) for d in data]) #LINE# #TAB# else: #LINE# #TAB# #TAB# x, y = data #LINE# #TAB# xmin, xmax = np.percentile(x, spacing) #LINE# #TAB# ymin, ymax = np.percentile(y, spacing) #LINE# #TAB# return xmin, xmax, ymin, ymax"
#LINE# #TAB# global _instance #LINE# #TAB# if not _instance: #LINE# #TAB# #TAB# logger.info('No torch device is connected to the remote machine.') #LINE# #TAB# #TAB# _instance = torch.device(socket.gethostname()) #LINE# #TAB# return _instance
"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# stageId = conn.get_stage(restApiId=restApiId, stageName=stageName) #LINE# #TAB# #TAB# pvCache = conn.get_auth_stage_cache(stageId) #LINE# #TAB# #TAB# for stage in pvCache: #LINE# #TAB# #TAB# #TAB# del pvCache[stage.stageId] #LINE# #TAB# #TAB# #TAB# conn.delete_auth_stage_cache(stageId) #LINE# #TAB# #TAB# return {'stageCache': True} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'error': __utils__['boto3.get_error'](e)}"
"#LINE# #TAB# mean = np.mean(time_series, axis=0) #LINE# #TAB# std = np.std(time_series, axis=0) #LINE# #TAB# return 1 - mean / std"
#LINE# #TAB# children = set() #LINE# #TAB# for child in cls.__subclasses__(): #LINE# #TAB# #TAB# children |= dump_right_tree(child) #LINE# #TAB# return children
#LINE# #TAB# grouped_models = [] #LINE# #TAB# for model in models: #LINE# #TAB# #TAB# grouped_models.append(model.groupby_desc()) #LINE# #TAB# return grouped_models
#LINE# #TAB# S = Omega.copy() #LINE# #TAB# S[0] = R[0] * Omega[0] + Omega[1] * R[1] #LINE# #TAB# S[1] = R[1] * Omega[1] - R[0] * Omega[0] #LINE# #TAB# return S
#LINE# #TAB# names = string.split(sep) #LINE# #TAB# if len(names) == 1: #LINE# #TAB# #TAB# return names[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# idx = names.index(max(names)) #LINE# #TAB# #TAB# return names[idx - 1]
#LINE# #TAB# url = equiv_url(url_parts) #LINE# #TAB# is_secure = not url.path.endswith('/api/') #LINE# #TAB# if not is_secure: #LINE# #TAB# #TAB# url_parts = url_parts[:-1] #LINE# #TAB# return url
#LINE# #TAB# soup = Trimesh() #LINE# #TAB# for i in range(face_count): #LINE# #TAB# #TAB# soup.add_face(random.choice(faces)) #LINE# #TAB# return soup
#LINE# #TAB# layer_name = os.path.splitext(movie_path)[0] #LINE# #TAB# image_path = movie_path + '.png' #LINE# #TAB# im = Image.open(image_path) #LINE# #TAB# size_pix = im.size #LINE# #TAB# im.close() #LINE# #TAB# os.remove(image_path) #LINE# #TAB# return size_pix
"#LINE# #TAB# ref_url = urlparse(cfg.get('CFG_SITE_SECURE_URL')) #LINE# #TAB# test_url = urlparse(urljoin(cfg.get('CFG_SITE_SECURE_URL'), target)) #LINE# #TAB# return test_url.netloc == ref_url.netloc"
#LINE# #TAB# try: #LINE# #TAB# #TAB# from multiprocessing import cpu_count #LINE# #TAB# #TAB# return cpu_count() #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return {}
"#LINE# #TAB# try: #LINE# #TAB# #TAB# f = open(os.path.join(os.path.dirname(__file__), 'info.txt') #LINE# #TAB# #TAB# #TAB# ).read() #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# pass #LINE# #TAB# path = f.read() #LINE# #TAB# f.close() #LINE# #TAB# return path"
#LINE# #TAB# fingerprints = [] #LINE# #TAB# for key in profile.keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fingerprint = profile[key] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if len(fingerprint) == 1: #LINE# #TAB# #TAB# #TAB# return fingerprint #LINE# #TAB# #TAB# elif len(fingerprint) == 2: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return fingerprint.split('-')[1] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# return None #LINE# #TAB# if not fingerprints: #LINE# #TAB# #TAB# return None #LINE# #TAB# return None
#LINE# #TAB# if not parent.has_children(): #LINE# #TAB# #TAB# return False #LINE# #TAB# for child_node in child.get_nodes(): #LINE# #TAB# #TAB# if parent_node.has_children(): #LINE# #TAB# #TAB# #TAB# for pred in child_node.get_preds(): #LINE# #TAB# #TAB# #TAB# #TAB# if pred in parent_node.get_children(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# for name, field in node._fields.items(): #LINE# #TAB# #TAB# if isinstance(field, (AST, _ast.AST)): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, (AST, _ast.AST)): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item"
#LINE# #TAB# if not is_valid_vin(vin): #LINE# #TAB# #TAB# return False #LINE# #TAB# past_vin = parse_datetime(vin) #LINE# #TAB# if past_vin.month > 12: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not is_valid_vin(past_vin): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# new_preferences = {} #LINE# #TAB# for preference in preferences: #LINE# #TAB# #TAB# key = preference[0] #LINE# #TAB# #TAB# if key not in new_preferences: #LINE# #TAB# #TAB# #TAB# new_preferences[key] = [] #LINE# #TAB# #TAB# new_preferences[key].append(file) #LINE# #TAB# files = open(file, 'r') #LINE# #TAB# for line in files: #LINE# #TAB# #TAB# line = line.rstrip('\n') #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lines = line.split('\t') #LINE# #TAB# #TAB# #TAB# new_preferences[key].append(lines[0]) #LINE# #TAB# return new_preferences"
"#LINE# #TAB# import re #LINE# #TAB# text = re.sub(r'<a [^>]+>(.+?)</a>', r'\1', text) #LINE# #TAB# text = re.sub(r'<.+?>', r'\1', text) #LINE# #TAB# return text"
"#LINE# #TAB# metadata = {} #LINE# #TAB# for attribute in request.metadata: #LINE# #TAB# #TAB# metadata[attribute] = getattr(request, attribute) #LINE# #TAB# return metadata"
"#LINE# #TAB# metadata = {} #LINE# #TAB# zip_file = zipfile.ZipFile(filename, 'r') #LINE# #TAB# jobs = talker.get_issues(issue_id) #LINE# #TAB# for job in jobs: #LINE# #TAB# #TAB# print(job) #LINE# #TAB# #TAB# if job['title']!= 'issue': #LINE# #TAB# #TAB# #TAB# print(job['title']) #LINE# #TAB# #TAB# #TAB# meta = json.loads(job['data']) #LINE# #TAB# #TAB# #TAB# metadata[job['title']] = meta #LINE# #TAB# return metadata"
"#LINE# #TAB# out = [] #LINE# #TAB# for contract in invariants: #LINE# #TAB# #TAB# d = { #LINE# #TAB# #TAB# #TAB# 'var': contract.var, #LINE# #TAB# #TAB# #TAB#'mean': contract.mean, #LINE# #TAB# #TAB# #TAB#'std': contract.std, #LINE# #TAB# #TAB# #TAB# 'value': contract.value #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# out.append(f'{d}\n{output}') #LINE# #TAB# return out"
"#LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# delta = unixtime(line) #LINE# #TAB# #TAB# if delta > 0: #LINE# #TAB# #TAB# #TAB# line = line.replace(delta, '') #LINE# #TAB# #TAB# yield line"
"#LINE# #TAB# if value == 0 or isinstance(value, basestring) and value.strip() == '0': #LINE# #TAB# #TAB# return None #LINE# #TAB# return value.sum() if isinstance(value, (int, float, np.integer)) else value"
"#LINE# #TAB# out = {} #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# out[k] = predict_reverse(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out[k] = predict_reverse(v) #LINE# #TAB# return out"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return original_audio_bin.shape[0] == decoded_audio_bin.shape[0] #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# cleaned = orig.lower() #LINE# #TAB# if cleaned.startswith(prefixes): #LINE# #TAB# #TAB# return prefixes[len(cleaned):] #LINE# #TAB# else: #LINE# #TAB# #TAB# return cleaned
#LINE# #TAB# logging.info('Applying _name_append_suffix generator:'#LINE# #TAB# #TAB# #TAB# #TAB# 'appending suffix'+ suffix) #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# yield record
#LINE# #TAB# for w in ws: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# w.remove(proj) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass
#LINE# #TAB# position = {} #LINE# #TAB# for token in token_lists: #LINE# #TAB# #TAB# for i in range(len(token)): #LINE# #TAB# #TAB# #TAB# topics = get_topics(token) #LINE# #TAB# #TAB# #TAB# if len(topics) < num_topics: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# position[i] = True #LINE# #TAB# return position
#LINE# arg_spec = _get_cached_arg_spec(fn) #LINE# args = arg_spec.args #LINE# if arg_spec.defaults: #LINE# #TAB# args = args[:-len(arg_spec.defaults)] #LINE# return args
"#LINE# #TAB# if using_cpu_only or job_name not in ('serving', 'learner'): #LINE# #TAB# #TAB# device_id = '' #LINE# #TAB# else: #LINE# #TAB# #TAB# device_id = '0:0' #LINE# #TAB# return device_id"
#LINE# #TAB# import distutils.spawn #LINE# #TAB# path = distutils.spawn.find_executable(executable) #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# raise ValueError('{} executable not found in PATH.'.format(executable)) #LINE# #TAB# os.environ['PATH'] = path
#LINE# #TAB# file_type = input('Enter the file type to use (default: {}).'.format( #LINE# #TAB# #TAB# default_file_type)) #LINE# #TAB# if not os.path.isdir(file_dir): #LINE# #TAB# #TAB# raise ValueError('Invalid recording directory directory: {}'. #LINE# #TAB# #TAB# #TAB# format(file_dir)) #LINE# #TAB# if not is_valid_filetype(file_type): #LINE# #TAB# #TAB# raise ValueError('Invalid recording directory type: {}'.format(file_type) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return file_type
"#LINE# #TAB# rule = re.match('^\\s*(?P<value>.*?)\\s*$', value) #LINE# #TAB# if rule: #LINE# #TAB# #TAB# return rule.group('value') #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
#LINE# #TAB# try: #LINE# #TAB# #TAB# get_app_engine() #LINE# #TAB# #TAB# return True #LINE# #TAB# except: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# code = zlib.compress(f.read(), definitions) #LINE# #TAB# return code"
"#LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode brave --opt-mode=optN --opt-bound=' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# models = clyngor.solve_from_grounded(grounding, options=options) #LINE# #TAB# best_model = models.most_common(1)[0][0] #LINE# #TAB# return best_model"
#LINE# #TAB# fields = text.split(';') #LINE# #TAB# form = base_class() #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# if '=' not in field: #LINE# #TAB# #TAB# #TAB# raise ValueError('invalid parameter: %s' % field) #LINE# #TAB# #TAB# form.parse(field) #LINE# #TAB# return form
#LINE# #TAB# if not wrap: #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(iterable) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# i += step if i < len(iterable) else 0 #LINE# #TAB# yield iterable
"#LINE# #TAB# ifupdater is None: #LINE# #TAB# #TAB# updater = in_daemon() #LINE# #TAB# if not updater: #LINE# #TAB# #TAB# updater = in_daemon() #LINE# #TAB# log.info('Removing daemon daemon %s', updater.__class__.__name__) #LINE# #TAB# updater._in_daemon = True #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# updater._in_daemon = False"
#LINE# #TAB# out = np.empty_like(data) #LINE# #TAB# out[mask] = data[mask] #LINE# #TAB# return out
"#LINE# #TAB# if filepath is None: #LINE# #TAB# #TAB# filepath = pkg_resources.resource_filename('fydarc', 'config.yaml') #LINE# #TAB# with open(filepath, 'w') as fd: #LINE# #TAB# #TAB# yaml.dump(DEFAULT, fd) #LINE# #TAB# module = imp.new_module('fydarc') #LINE# #TAB# return module"
#LINE# #TAB# vector = jdvector_from_underscores(string) #LINE# #TAB# return vector
"#LINE# #TAB# summary = [] #LINE# #TAB# for root, dirs, files in os.walk(dirname): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# summary.append(Card(f)) #LINE# #TAB# return summary"
#LINE# #TAB# for block in blocks: #LINE# #TAB# #TAB# block['path'] = block['path'][1:]
"#LINE# #TAB# if isinstance(data, Mapping): #LINE# #TAB# #TAB# yield dict(data) #LINE# #TAB# elif isinstance(data, list): #LINE# #TAB# #TAB# for item in data: #LINE# #TAB# #TAB# #TAB# for item_dict in cls.construct_help(item): #LINE# #TAB# #TAB# #TAB# #TAB# yield item_dict #LINE# #TAB# elif isinstance(data, dict): #LINE# #TAB# #TAB# for item_dict in data.values(): #LINE# #TAB# #TAB# #TAB# yield cls.construct_help(item_dict) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield data"
"#LINE# #TAB# for x in content_types: #LINE# #TAB# #TAB# yield '%s/%s' % (x[0], x[1]) #LINE# #TAB# yield 'application/json'"
"#LINE# #TAB#gateway_slots = [] #LINE# #TAB# parent_param_classes = [c for c in classlist(class_)[1:]] #LINE# #TAB# for c in parent_param_classes: #LINE# #TAB# #TAB# if hasattr(c, '__slots__'): #LINE# #TAB# #TAB# #TAB# slots = list(c.__slots__) #LINE# #TAB# #TAB# #TAB# for slot in slots: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(slot, str): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# gateway_slots.append(slot) #LINE# #TAB# return gateway_slots"
"#LINE# #TAB# score = [(int(score[1]), int(score[2])) for score in score] #LINE# #TAB# return score"
#LINE# #TAB# rendered = None #LINE# #TAB# if path: #LINE# #TAB# #TAB# rendered = path(args) #LINE# #TAB# return rendered
"#LINE# #TAB# base_dir = os.path.abspath(base_dir) #LINE# #TAB# df = DDF(ddf_id, base_dir=base_dir) #LINE# #TAB# return df"
#LINE# #TAB# global FD_CLOEXEC #LINE# #TAB# FD_CLOEXEC = True
#LINE# #TAB# has_variable = False #LINE# #TAB# try: #LINE# #TAB# #TAB# if variable in obj.variables: #LINE# #TAB# #TAB# #TAB# has_variable = True #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return has_variable
"#LINE# #TAB# methods = [] #LINE# #TAB# for attr_name, attr_type in cls.attributes.items(): #LINE# #TAB# #TAB# if hasattr(attr_type,'schema'): #LINE# #TAB# #TAB# #TAB# schema = attr_type.schema #LINE# #TAB# #TAB# #TAB# table_name = schema.table_name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# table_name = cls.table_name #LINE# #TAB# #TAB# methods.append((attr_name, table_name)) #LINE# #TAB# return methods"
"#LINE# #TAB# code3 = max(code1, ord('a')) #LINE# #TAB# code4 = min(code2, ord('z') + 1) #LINE# #TAB# if code3 < code4: #LINE# #TAB# #TAB# d = ord('A') - ord('a') #LINE# #TAB# #TAB# return code3 + d, code4 + d #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if type(list_) == tuple: #LINE# #TAB# #TAB# list_ = Field(*list_) #LINE# #TAB# elif type(list_) == list: #LINE# #TAB# #TAB# for x in list_: #LINE# #TAB# #TAB# #TAB# list_ = Field(*x) #LINE# #TAB# return list_
#LINE# #TAB# statements = statement.split(';') #LINE# #TAB# return [parse_statement(s) for s in statements]
"#LINE# #TAB# packages = pkg_resources.working_set #LINE# #TAB# for package in packages: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mod = importlib.import_module(package) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not hasattr(mod, '__all__'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return mod.__all__ #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return []"
"#LINE# #TAB# all_services = list_services() #LINE# #TAB# found = False #LINE# #TAB# for _service in all_services: #LINE# #TAB# #TAB# if _service['name'] == name: #LINE# #TAB# #TAB# #TAB# found = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if not found: #LINE# #TAB# #TAB# click.secho('Unable to fix service ""{}""'.format(name)) #LINE# #TAB# return found"
#LINE# #TAB# family = socket.AF_INET #LINE# #TAB# if address.startswith('[') and address.endswith(']'): #LINE# #TAB# #TAB# group = address.split('[')[1] #LINE# #TAB# #TAB# family = socket.AF_INET6 #LINE# #TAB# elif address.startswith('[') and address.endswith(']'): #LINE# #TAB# #TAB# group = address.split('[')[1] #LINE# #TAB# #TAB# family = socket.AF_INET #LINE# #TAB# return family
"#LINE# #TAB# if args is None: #LINE# #TAB# #TAB# args = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# table = getattr(builtins, method) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# table = getattr(builtins, method) #LINE# #TAB# _table = hashable(table) #LINE# #TAB# return _table"
"#LINE# #TAB# index = 0 #LINE# #TAB# with open(infile) as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# if line[0] == '$': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# result = parse_docx(line, index) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return result"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('open_url', dtype='string', direction= #LINE# #TAB# #TAB# function.OUT, description='The open url') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# Current value was retrieved #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# Could not retrieve location #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
#LINE# #TAB# t.value = float(t.value) #LINE# #TAB# return t
"#LINE# #TAB# if not isinstance(s, str): #LINE# #TAB# #TAB# return s #LINE# #TAB# if sys.version_info < (3, 0): #LINE# #TAB# #TAB# return s.decode('utf-8') #LINE# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(num) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# try: #LINE# #TAB# #TAB# num = float(num) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# if num < 0: #LINE# #TAB# #TAB# return default #LINE# #TAB# return num"
"#LINE# #TAB# val = int(val, 16) #LINE# #TAB# if val > 32767: #LINE# #TAB# #TAB# val -= 65536 #LINE# #TAB# return val"
"#LINE# #TAB# with open(ai_filename, 'wb') as ai_file: #LINE# #TAB# #TAB# for i_ol in ai_figure: #LINE# #TAB# #TAB# #TAB# ai_file.write('<br>') #LINE# #TAB# #TAB# #TAB# for one_line_or_arc in i_ol: #LINE# #TAB# #TAB# #TAB# #TAB# print('outline:', one_line_or_arc) #LINE# #TAB# #TAB# #TAB# #TAB# download_sample_in_svg(one_line_or_arc, ai_file) #LINE# #TAB# return 0"
#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cls.get_required_code_spec(template) #LINE# #TAB# #TAB# except InvalidCodeSpecException: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return True
"#LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# _, src = os.path.split(filename) #LINE# #TAB# #TAB# _, dst = os.path.split(dst) #LINE# #TAB# #TAB# if os.path.exists(src): #LINE# #TAB# #TAB# #TAB# os.unlink(src) #LINE# #TAB# #TAB# _, filename = os.path.split(filename) #LINE# #TAB# return"
"#LINE# #TAB# a = int(r / 2) #LINE# #TAB# b = np.zeros((a.shape[0], a.shape[1] * a.shape[2], 3), dtype=np.uint8) #LINE# #TAB# buf = np.zeros((a.shape[0], a.shape[1], 3), dtype=np.uint8) #LINE# #TAB# buf[a == 0] = 1 #LINE# #TAB# buf[r - 1] = 1 #LINE# #TAB# return buf"
#LINE# #TAB# global _install_dir #LINE# #TAB# _install_dir = Path(os.path.expanduser('~') + '/.bcml') #LINE# #TAB# return _install_dir
"#LINE# #TAB# if low_freq_cutoff < 0: #LINE# #TAB# #TAB# return FrequencySeries( #LINE# #TAB# #TAB# #TAB# data=[0], #LINE# #TAB# #TAB# #TAB# index=[0], #LINE# #TAB# #TAB# #TAB# delta_f=delta_f, #LINE# #TAB# #TAB# #TAB# low_freq_cutoff=low_freq_cutoff) #LINE# #TAB# else: #LINE# #TAB# #TAB# return FrequencySeries( #LINE# #TAB# #TAB# #TAB# data=[0], #LINE# #TAB# #TAB# #TAB# index=[0], #LINE# #TAB# #TAB# #TAB# delta_f=delta_f, #LINE# #TAB# #TAB# #TAB# low_freq_cutoff=low_freq_cutoff)[0]"
#LINE# #TAB# rate = ET.Element('fps') #LINE# #TAB# rate.text = fps #LINE# #TAB# return rate
"#LINE# #TAB# if not group_name: #LINE# #TAB# #TAB# return #LINE# #TAB# group_id = uuid.UUID(group_name) #LINE# #TAB# fields = [] #LINE# #TAB# for field_name, field in fields.items(): #LINE# #TAB# #TAB# if field.name!= group_id: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# fields.append({'name': field_name, 'group': group_id}) #LINE# #TAB# return fields"
"#LINE# #TAB# new_string = string #LINE# #TAB# for char in string: #LINE# #TAB# #TAB# new_string = new_string.replace(char, '_') #LINE# #TAB# #TAB# new_string = camelcase_to_snake_case(new_string) #LINE# #TAB# return new_string"
"#LINE# #TAB# if maya.cmds.objExists(geometry): #LINE# #TAB# #TAB# geo_shape = maya.cmds.listRelatives(geometry, s=True, pa=True) #LINE# #TAB# #TAB# if not geo_shape: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if 'Polygon' in geo_shape: #LINE# #TAB# #TAB# #TAB# return 'Polygon' #LINE# #TAB# #TAB# if 'MultiPolygon' in geo_shape: #LINE# #TAB# #TAB# #TAB# return 'MultiPolygon' #LINE# #TAB# return None"
#LINE# #TAB# data = data[0:batch_size] #LINE# #TAB# for i in range(num_epochs): #LINE# #TAB# #TAB# data = data[i:i + batch_size] #LINE# #TAB# #TAB# if len(data) > 0: #LINE# #TAB# #TAB# #TAB# yield data
"#LINE# #TAB# arr = [] #LINE# #TAB# for i in range(0, len(s), n): #LINE# #TAB# #TAB# arr.append(s[i:i + n]) #LINE# #TAB# return arr"
"#LINE# #TAB# config = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# #TAB# config = json.load(f) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if 'configs' not in config: #LINE# #TAB# #TAB# config['configs'] = {} #LINE# #TAB# config['configs'].update(config['rank']) #LINE# #TAB# return config"
#LINE# #TAB# if letter.isalpha(): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif letter == 'a': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(obj, type): #LINE# #TAB# #TAB# obj = copy.deepcopy(obj) #LINE# #TAB# if skip == {}: #LINE# #TAB# #TAB# for k, v in obj.items(): #LINE# #TAB# #TAB# #TAB# if k not in skip: #LINE# #TAB# #TAB# #TAB# #TAB# skip[k] = [] #LINE# #TAB# #TAB# #TAB# v = form_walk(v, skip) #LINE# #TAB# #TAB# #TAB# obj = v #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for i in obj: #LINE# #TAB# #TAB# #TAB# obj.append(form_walk(i, skip)) #LINE# #TAB# else: #LINE# #TAB# #TAB# obj = copy.deepcopy(obj) #LINE# #TAB# return obj"
"#LINE# #TAB# n = len(G) #LINE# #TAB# E = nx.DiGraph() #LINE# #TAB# for v in G: #LINE# #TAB# #TAB# E.add_edge(v, np.zeros(n)) #LINE# #TAB# E.add_edges_from(E) #LINE# #TAB# return E"
"#LINE# #TAB# result = [] #LINE# #TAB# for start, quantity in sorted(d.items(), key=lambda x: x[0] - x[1]): #LINE# #TAB# #TAB# result.append((start, start + width, quantity)) #LINE# #TAB# return result"
"#LINE# #TAB# if reposave == '' or reponame == '': #LINE# #TAB# #TAB# return {'path': path,'repo_name': reponame} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'path': path,'repo_name': reposave,'reponame': reponame, #LINE# #TAB# #TAB# #TAB#'repo_name': reposave}"
"#LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return tuple(normalize_lists_string(x) for x in value) #LINE# #TAB# return value"
#LINE# #TAB# if not 'description' in holder: #LINE# #TAB# #TAB# holder['description'] = Milestone.objects.get_or_create( #LINE# #TAB# #TAB# #TAB# id=holder['id']) #LINE# #TAB# return holder['description']
"#LINE# #TAB# for key in node.keys(): #LINE# #TAB# #TAB# if key.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if hasattr(node, key): #LINE# #TAB# #TAB# #TAB# trim_tag(node, key) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield node[key]"
"#LINE# #TAB# term = os.environ.get('TERM', '') #LINE# #TAB# if term in ('truecolor', '24bit'): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif term in ('brown','red', 'green'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# _module_name = 'ds_connectors.handlers.aws_s3_handlers' #LINE# #TAB# _profile_name = 'aws_profile_redis' #LINE# #TAB# return _module_name, _profile_name"
"#LINE# #TAB# if mod_name is None: #LINE# #TAB# #TAB# mod_name = filepath #LINE# #TAB# import imp #LINE# #TAB# fp, pathname, description = imp.find_module(mod_name, filepath) #LINE# #TAB# try: #LINE# #TAB# #TAB# exec(fp, pathname, description) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# raise e #LINE# #TAB# finally: #LINE# #TAB# #TAB# fp.close() #LINE# #TAB# spec = importlib.util.spec_from_file_location(fp, pathname) #LINE# #TAB# mod = importlib.util.module_from_spec(spec) #LINE# #TAB# spec.loader.exec_module(mod) #LINE# #TAB# return mod"
"#LINE# #TAB# output = {} #LINE# #TAB# for key in src: #LINE# #TAB# #TAB# if isinstance(key, bytes): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = key.decode('utf-8') #LINE# #TAB# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = key.decode('latin-1') #LINE# #TAB# return output"
"#LINE# #TAB# urls = [(subparser.parse_known_args(args)[-1]), (subparser.parse_args(args)[0], #LINE# #TAB# #TAB# args[-1])] #LINE# #TAB# urls.sort(key=lambda i: i[0]) #LINE# #TAB# for url in urls: #LINE# #TAB# #TAB# subparser.add_argument( #LINE# #TAB# #TAB# #TAB# '-o', '--output-directory', #LINE# #TAB# #TAB# #TAB# type=str, #LINE# #TAB# #TAB# #TAB# default=None, #LINE# #TAB# #TAB# #TAB# help='The output directory (default: current directory)', #LINE# #TAB# #TAB# #TAB# type=str, #LINE# #TAB# #TAB# ) #LINE# #TAB# return urls"
"#LINE# #TAB# ""Return a list of article-id data."" #LINE# #TAB# versions = [] #LINE# #TAB# for tag in raw_parser.part_version(soup): #LINE# #TAB# #TAB# versions.append(node_text(tag)) #LINE# #TAB# return versions"
#LINE# #TAB# LanguageTool = Pool().get('ir.language') #LINE# #TAB# if not LanguageTool: #LINE# #TAB# #TAB# return #LINE# #TAB# language_code = LanguageTool.get_language() #LINE# #TAB# if language_code: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return LanguageTool.objects.get(app_label=language_code).language #LINE# #TAB# #TAB# except LanguageTool.DoesNotExist: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# else: #LINE# #TAB# #TAB# return language_code
#LINE# #TAB# try: #LINE# #TAB# #TAB# winreg.OpenKey(key) #LINE# #TAB# except WindowsError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# try: #LINE# #TAB# #TAB# return request.query['access_token'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if isinstance(json, dict): #LINE# #TAB# #TAB# new_dict = {} #LINE# #TAB# #TAB# for key, value in json.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# new_dict[Link(key, Link(value))] = wrap_properties_with_links( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# new_dict[Link(key)] = value #LINE# #TAB# #TAB# return new_dict #LINE# #TAB# elif isinstance(json, list): #LINE# #TAB# #TAB# return [wrap_properties_with_links(item) for item in json] #LINE# #TAB# else: #LINE# #TAB# #TAB# return json"
#LINE# #TAB# if header in IGNORE_HEADERS: #LINE# #TAB# #TAB# return False #LINE# #TAB# if header.startswith('HTTP_') or header == 'CONTENT_TYPE': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if not isinstance(inp, numbers.Number): #LINE# #TAB# #TAB# check_date(inp) #LINE# #TAB# #TAB# raise error.BASICError(err) #LINE# #TAB# return inp"
#LINE# #TAB# label = '' #LINE# #TAB# if _is_cid_valid(cid): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# compose_obj = Entity.objects.get(id=cid) #LINE# #TAB# #TAB# except Entity.DoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if compose_obj.meta: #LINE# #TAB# #TAB# #TAB# #TAB# label = compose_obj.meta.get('label') #LINE# #TAB# if label: #LINE# #TAB# #TAB# return label #LINE# #TAB# return ''
"#LINE# #TAB# blob_path = find_bigdata(envkey) #LINE# #TAB# bigdataset_path = os.path.join(blob_path, 'bigdata') #LINE# #TAB# if os.path.exists(bigdataset_path): #LINE# #TAB# #TAB# return bigdataset_path #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# ner = cfg.ner_file if cfg.ner_file else None #LINE# #TAB# sequence_file = cfg.sequence_file if cfg.sequence_file else None #LINE# #TAB# if ner and seq_file: #LINE# #TAB# #TAB# assert sorted(cfg.label_files, key=lambda x: x.start) == sorted(cfg. #LINE# #TAB# #TAB# #TAB# label_files, reverse=True) #LINE# #TAB# #TAB# ner_file = None #LINE# #TAB# elif ner and seq_file: #LINE# #TAB# #TAB# seq_file = sorted(cfg.sequence_file, key=lambda x: x.start) #LINE# #TAB# #TAB# ner_file = seq_file[0] #LINE# #TAB# return ner_file, sequence_file"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# p = compile(source, name, 'eval') #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# p = compile(source, name, 'exec') #LINE# #TAB# #TAB# p.body = source #LINE# #TAB# try: #LINE# #TAB# #TAB# p.body = compile(source, name, 'eval') #LINE# #TAB# #TAB# return p.body #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# for error in find_spellcheck_errors(build_dir): #LINE# #TAB# #TAB# yield error
"#LINE# #TAB# s = s.replace('&', '&amp;') #LINE# #TAB# s = s.replace('<', '&lt;') #LINE# #TAB# s = s.replace('>', '&gt;') #LINE# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return tuple(calendar.monthrange(yw[0], yw[1])) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if registry is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# key = registry.CreateKey(registry.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Valve\\Steam') #LINE# #TAB# return registry.QueryValueEx(key, 'Path')[0]"
"#LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# el.value = "", "".join(value) #LINE# #TAB# #TAB# elif isinstance(value, tuple): #LINE# #TAB# #TAB# #TAB# el.value = "", "".join(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# el.value = value"
#LINE# #TAB# line_id = '' #LINE# #TAB# for node in program_ast.body: #LINE# #TAB# #TAB# if type(node) is ast.Assign: #LINE# #TAB# #TAB# #TAB# line_id += node.value.id #LINE# #TAB# #TAB# elif type(node) is ast.Name: #LINE# #TAB# #TAB# #TAB# line_id += node.name #LINE# #TAB# return line_id
"#LINE# #TAB# #TAB# consensus_hash = None #LINE# #TAB# #TAB# if prev_consensus_hashes and len(prev_consensus_hashes) == 0: #LINE# #TAB# #TAB# #TAB# consensus_hash = prev_consensus_hashes[0] #LINE# #TAB# #TAB# if record_root_hash is not None and len(record_root_hash) > 32: #LINE# #TAB# #TAB# #TAB# record_root_hash = record_root_hash[:32] #LINE# #TAB# #TAB# #TAB# consensus_hash = compute_consensus_hash(record_root_hash) #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'op_code': record_root_hash, #LINE# #TAB# #TAB# #TAB# 'consensus_hash': consensus_hash, #LINE# #TAB# #TAB# }"
#LINE# #TAB# a_names = set(a.arg_names) #LINE# #TAB# b_names = set(b.arg_names) #LINE# #TAB# if a_names!= b_names: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# with open(STORE_FILE_PATH, 'r') as store_file: #LINE# #TAB# #TAB# data = json.load(store_file) #LINE# #TAB# return data"
"#LINE# #TAB# button.layout = mara_page.response.ActionButton( #LINE# #TAB# #TAB# data=button.data, #LINE# #TAB# #TAB# disabled=button.disabled, #LINE# #TAB# #TAB# hover_text=button.hover_text, #LINE# #TAB# #TAB# hover_position=button.position, #LINE# #TAB# ) #LINE# #TAB# return button"
#LINE# #TAB# if CudaApp._apps is None: #LINE# #TAB# #TAB# CudaApp._apps = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# os.environ['CudaApp_' + name] = app #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# CudaApp._apps[name] = app
"#LINE# #TAB# ds_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# ds_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# ds_socket.bind(('', 0)) #LINE# #TAB# return ds_socket"
"#LINE# #TAB# regions = [] #LINE# #TAB# for chrom in data[0]: #LINE# #TAB# #TAB# start = data[chrom][0] #LINE# #TAB# #TAB# end = data[chrom][1] #LINE# #TAB# #TAB# if start == end: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# regions.append([chrom, start, end]) #LINE# #TAB# return regions"
#LINE# #TAB# global _byte_register_correlate_flip_bug #LINE# #TAB# if _byte_register_correlate_flip_bug is None: #LINE# #TAB# #TAB# byte_register_correlate_flip_bug = 1
"#LINE# #TAB# host_hexsha = 'unknown' #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.devnull, 'w') as devnull: #LINE# #TAB# #TAB# #TAB# host_hexsha = subprocess.check_output( #LINE# #TAB# #TAB# #TAB# #TAB# ['git','rev-parse', '--show-toplevel'], #LINE# #TAB# #TAB# #TAB# #TAB# stderr=devnull #LINE# #TAB# #TAB# #TAB# ).decode('utf-8') #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return host_hexsha"
#LINE# #TAB# out = [] #LINE# #TAB# for i in x: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# out.append(int(i)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return out
"#LINE# #TAB# header = [] #LINE# #TAB# if len(fileContent)!= 32: #LINE# #TAB# #TAB# raise Exception('Content must be 32 bytes long.') #LINE# #TAB# for i in range(0, len(fileContent), 8): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# header.append(fileContent[i:i + 8]) #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# raise Exception('Invalid file length for header: %d' % i) #LINE# #TAB# header.append(fileContent[i:i + 8]) #LINE# #TAB# header.append(b'\x00' * (8 - len(fileContent))) #LINE# #TAB# return header"
"#LINE# #TAB# import os #LINE# #TAB# import json #LINE# #TAB# filename = os.path.join(os.path.dirname(__file__), os.pardir, 'data', #LINE# #TAB# #TAB# 'pixel.json') #LINE# #TAB# with open(filename) as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# pixel = {} #LINE# #TAB# for x, y in data['data'].iterrows(): #LINE# #TAB# #TAB# pixel[x] = np.array(x[0], dtype=float) #LINE# #TAB# #TAB# pixel[y] = np.array(y[1], dtype=float) #LINE# #TAB# return pixel"
"#LINE# #TAB# klass = module.__dict__.get(class_string, None) #LINE# #TAB# if klass is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return klass"
"#LINE# #TAB# image = dict() #LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# info = pickle.load(f) #LINE# #TAB# #TAB# if grayscale: #LINE# #TAB# #TAB# #TAB# image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #LINE# #TAB# #TAB# if target_size: #LINE# #TAB# #TAB# #TAB# image = cv2.cvtColor(image, cv2.COLOR_RGB2RGB) #LINE# #TAB# #TAB# image = cv2.cvtColor(image, cv2.COLOR_RGB1) #LINE# #TAB# #TAB# info['path'] = path #LINE# #TAB# return image, info"
"#LINE# #TAB# data = yaml.load(open(input_file, 'r')) #LINE# #TAB# if not isinstance(data, dict): #LINE# #TAB# #TAB# raise ValueError('Configuration not a dictionary.') #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# if key.startswith('_'): #LINE# #TAB# #TAB# #TAB# del data[key] #LINE# #TAB# return data"
#LINE# #TAB# langs = {} #LINE# #TAB# for lang in get_language_list(): #LINE# #TAB# #TAB# lang_code = None #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# lang = Language.objects.get(**request.GET) #LINE# #TAB# #TAB# except Language.DoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if lang.code not in langs: #LINE# #TAB# #TAB# #TAB# langs[lang.code] = Language(lang_code=lang.code) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# langs[lang.code] = lang #LINE# #TAB# return langs
"#LINE# #TAB# acl_url = urljoin(_acl_url(), 'users/{}'.format(uid)) #LINE# #TAB# try: #LINE# #TAB# #TAB# r = http.get(acl_url) #LINE# #TAB# #TAB# return r.json() #LINE# #TAB# except DCOSHTTPException as e: #LINE# #TAB# #TAB# if e.response.status_code!= 409: #LINE# #TAB# #TAB# #TAB# raise"
"#LINE# #TAB# res = '' #LINE# #TAB# for char in correction: #LINE# #TAB# #TAB# if char in word: #LINE# #TAB# #TAB# #TAB# res += char #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# word = res[0] #LINE# #TAB# for i, char in enumerate(word): #LINE# #TAB# #TAB# if char in 'aeiou': #LINE# #TAB# #TAB# #TAB# res += char #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return word"
"#LINE# #TAB# cred_ids = set() #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# stat = os.stat(name) #LINE# #TAB# #TAB# if S_ISREG(stat.st_mode): #LINE# #TAB# #TAB# #TAB# if stat.S_ISDIR(stat.st_mode): #LINE# #TAB# #TAB# #TAB# #TAB# cred_ids.add(os.path.join('/tmp', name)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# cred_ids.add(os.path.join('/tmp', name)) #LINE# #TAB# return cred_ids"
"#LINE# #TAB# mi_doc = dict_of_dict #LINE# #TAB# for k, v in dict_of_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# mi_doc[k] = v #LINE# #TAB# return mi_doc"
"#LINE# #TAB# radar = radar[:] #LINE# #TAB# for i in range(1, len(radar)): #LINE# #TAB# #TAB# if sweep_is_sequential(radar[i], radar[i+1]): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return radar"
"#LINE# #TAB# mylist = np.asarray(mylist) #LINE# #TAB# if not isinstance(mylist, list): #LINE# #TAB# #TAB# raise TypeError('list type required') #LINE# #TAB# c_array = np.ndarray((len(mylist), len(mylist))) #LINE# #TAB# for i, v in enumerate(mylist): #LINE# #TAB# #TAB# c_array[i] = v #LINE# #TAB# return c_array"
#LINE# #TAB# for file in os.listdir(folder): #LINE# #TAB# #TAB# if file.endswith('.pyc') or file.endswith('.pyo'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# message = 'Invalid request.' #LINE# #TAB# print(message) #LINE# #TAB# print('Invalid request') #LINE# #TAB# return
"#LINE# #TAB# if s.ndim < 2: #LINE# #TAB# #TAB# return np.abs(s) #LINE# #TAB# a = s.shape[0] #LINE# #TAB# b = s.shape[1] #LINE# #TAB# C = s.shape[2] #LINE# #TAB# R = np.zeros((a, B, C)) #LINE# #TAB# for i in range(0, D): #LINE# #TAB# #TAB# for j in range(0, C): #LINE# #TAB# #TAB# #TAB# R[i, j] = radius_axis(s[i, j], s[j, i]) #LINE# #TAB# return R"
"#LINE# #TAB# scheme = sha256() #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file, 'rb') as f: #LINE# #TAB# #TAB# #TAB# scheme.update(f.read()) #LINE# #TAB# #TAB# return scheme #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# #TAB# obj = cls(name) #LINE# #TAB# #TAB# obj.exporter = 'update_schema_table' #LINE# #TAB# #TAB# obj.output_name = output_name #LINE# #TAB# #TAB# return obj
"#LINE# #TAB# h5f = h5py.File(fasta_name, 'r') #LINE# #TAB# data = list() #LINE# #TAB# for record in h5f: #LINE# #TAB# #TAB# data.append(record[0]) #LINE# #TAB# return data"
"#LINE# #TAB# if not isinstance(identifier, URIRef): #LINE# #TAB# #TAB# identifier = URIRef(identifier) #LINE# #TAB# store = DjangoStore(store_id) #LINE# #TAB# graph = TraceGraph(store, identifier=identifier) #LINE# #TAB# if graph.open(None, create=create)!= VALID_STORE: #LINE# #TAB# #TAB# raise ValueError(""The store identified by {0} is not a valid store"".format(store_id)) #LINE# #TAB# return graph"
"#LINE# #TAB# l1 = set(g1.labels) #LINE# #TAB# l2 = set(g2.labels) #LINE# #TAB# if not l1.issubset(l2): #LINE# #TAB# #TAB# return [('in first', 'in second'), ('in both', 'in first'), ('in second', #LINE# #TAB# #TAB# #TAB# 'in second')] #LINE# #TAB# return [('in first', 'in second'), ('in both', 'in second'), ('in both', 'in second')]"
#LINE# #TAB# merged = {} #LINE# #TAB# if previous_props and next_props: #LINE# #TAB# #TAB# for key in list(previous_props.keys()): #LINE# #TAB# #TAB# #TAB# if key in list(next_props.keys()): #LINE# #TAB# #TAB# #TAB# #TAB# if key not in merged: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# merged[key] = next_props[key] #LINE# #TAB# #TAB# #TAB# #TAB# elif key in list(next_props.keys()): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if key not in merged: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# merged[key] = next_props[key] #LINE# #TAB# return merged
"#LINE# #TAB# result = None #LINE# #TAB# if isinstance(message, str): #LINE# #TAB# #TAB# result = message.split(' ') #LINE# #TAB# for item in result: #LINE# #TAB# #TAB# if isinstance(item, list): #LINE# #TAB# #TAB# #TAB# item = list(item) #LINE# #TAB# #TAB# if isinstance(item, dict): #LINE# #TAB# #TAB# #TAB# for key, value in item: #LINE# #TAB# #TAB# #TAB# #TAB# item[key] = generate_resource_commit_message(value) #LINE# #TAB# #TAB# #TAB# result = result + '-' + key #LINE# #TAB# return result"
#LINE# #TAB# global _ALL_LANGUAGES #LINE# #TAB# if not _ALL_LANGUAGES: #LINE# #TAB# #TAB# _ALL_LANGUAGES = load_supported_languages() #LINE# #TAB# for language in _ALL_LANGUAGES: #LINE# #TAB# #TAB# if language.endswith(search_text): #LINE# #TAB# #TAB# #TAB# yield language
"#LINE# #TAB# value = value.replace('\\', '\\\\') #LINE# #TAB# value = re.sub('([_\\^\\[\\]\\{\\}\\])', '\\\\\\1', value) #LINE# #TAB# value = re.sub('([_\\^\\[\\]\\{\\}\\])', '\\\\\\1', value) #LINE# #TAB# return value"
#LINE# #TAB# if text.startswith('<'): #LINE# #TAB# #TAB# return text #LINE# #TAB# text = strip_xml_highlight(text) #LINE# #TAB# return text
#LINE# #TAB# try: #LINE# #TAB# #TAB# return copy.deepcopy(m) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return m
#LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib #LINE# #TAB# #TAB# return False #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return True
#LINE# #TAB# if igs is None: #LINE# #TAB# #TAB# igs = range(len(conns)) #LINE# #TAB# env = Envelope() #LINE# #TAB# env.name = name #LINE# #TAB# env.coors = coors #LINE# #TAB# env.ngroups = ngroups #LINE# #TAB# env.conns = conns #LINE# #TAB# env.mat_ids = mat_ids #LINE# #TAB# env.descs = descs #LINE# #TAB# if igs is not None: #LINE# #TAB# #TAB# env.igs = igs #LINE# #TAB# return env
"#LINE# #TAB# for obj in builder.get_objects(): #LINE# #TAB# #TAB# if isinstance(obj, gtk.Window): #LINE# #TAB# #TAB# #TAB# return obj"
"#LINE# #TAB# key = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = objects[key] #LINE# #TAB# #TAB# #TAB# key += obj.id #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# obj_cache = {} #LINE# #TAB# #TAB# for obj in objects: #LINE# #TAB# #TAB# #TAB# obj_cache[obj.id] = obj, summary #LINE# #TAB# #TAB# time.sleep(5) #LINE# #TAB# return key, obj_cache"
#LINE# #TAB# try: #LINE# #TAB# #TAB# import timemachine #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# raise ImportError('Time machine database not found.') #LINE# #TAB# return timemachine
"#LINE# #TAB# if isinstance(bgcolor, str): #LINE# #TAB# #TAB# bgcolor = (bgcolor, bg) #LINE# #TAB# if isinstance(img, tk.Image): #LINE# #TAB# #TAB# img = img.convert('RGB') #LINE# #TAB# else: #LINE# #TAB# #TAB# img = img.convert('RGB') #LINE# #TAB# return img"
"#LINE# #TAB# df_by_group = df.groupby(groupby) #LINE# #TAB# for row_index, row in df_by_group.iterrows(): #LINE# #TAB# #TAB# value_by_group = df_by_group[row_index].iloc[fld] #LINE# #TAB# #TAB# if value_by_group.count() > 1: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# log_format = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(levelname)s - %(filename)s - %(message)s', datefmt= #LINE# #TAB# #TAB# '%Y-%m-%d %H:%M:%S') #LINE# #TAB# handler = logging.StreamHandler(sys.stderr) #LINE# #TAB# handler.setFormatter(log_format) #LINE# #TAB# logger = logging.getLogger('refunc') #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# logger.setLevel(log_format.upper()) #LINE# #TAB# return logger"
"#LINE# #TAB# current_path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# name = name.lower() #LINE# #TAB# if name.startswith('salt-init'): #LINE# #TAB# #TAB# current_path = os.path.join(current_path,'salt-init.py') #LINE# #TAB# elif name =='salt': #LINE# #TAB# #TAB# current_path = os.path.join(current_path,'salt-init.py') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception('unknown service name: %s' % name) #LINE# #TAB# script_path = os.path.join(current_path, 'init-script.sh') #LINE# #TAB# with open(script_path, 'w') as fp: #LINE# #TAB# #TAB# fp.write(name.encode('utf-8')) #LINE# #TAB# return script_path"
"#LINE# #TAB# pop_no_diff = min(latest_config.get('diff', 0), current_config.get('diff', 0)) #LINE# #TAB# return len(pop_no_diff)!= len(current_config.get('diff', 0)) if pop_no_diff else 0"
"#LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB# 'In requests 3.0, get_array_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)' #LINE# #TAB# #TAB#, DeprecationWarning) #LINE# #TAB# tried_encodings = [] #LINE# #TAB# encoding = get_encoding_from_headers(r.headers) #LINE# #TAB# if encoding: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return r.content.decode(encoding) #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# tried_encodings.append(encoding) #LINE# #TAB# try: #LINE# #TAB# #TAB# return [i.encode('utf-8') for i in r.content] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return r.content"
"#LINE# #TAB# amount, stock = item #LINE# #TAB# if not amount: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# if amount >= max_amount: #LINE# #TAB# #TAB# return amount + 1, stock #LINE# #TAB# return amount, stock"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# f = pysam.AlignmentFile(bam_file, 'r') #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# raise #LINE# #TAB# try: #LINE# #TAB# #TAB# f.fetch() #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise #LINE# #TAB# try: #LINE# #TAB# #TAB# f.close() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# raise #LINE# #TAB# return True"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return DDB_CONDITIONS.get(name, None) #LINE# #TAB# else: #LINE# #TAB# #TAB# if name.lower() == 'true': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif name.lower() == 'false': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return None"
"#LINE# #TAB# del value, frame #LINE# #TAB# return []"
#LINE# #TAB# if 'incrementalMFD' in taglist: #LINE# #TAB# #TAB# mfd = node.getElementsByTagName('incrementalMFD')[0].getAttribute('incrementalMFD') #LINE# #TAB# #TAB# mfd = float(mfd) #LINE# #TAB# elif 'truncGutenbergRichterMFD' in taglist: #LINE# #TAB# #TAB# mfd = node.getElementsByTagName('truncGutenbergRichterMFD')[0].getAttribute( #LINE# #TAB# #TAB# #TAB# 'truncGutenbergRichterMFD')[0].getAttribute('truncGutenbergRichterMFD') #LINE# #TAB# #TAB# mfd = float(mfd) #LINE# #TAB# else: #LINE# #TAB# #TAB# mfd = None #LINE# #TAB# return mfd
#LINE# #TAB# s = str(string) #LINE# #TAB# n = 0 #LINE# #TAB# i = 0 #LINE# #TAB# while n < len(s): #LINE# #TAB# #TAB# if s[i] == '0': #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# while i < len(s): #LINE# #TAB# #TAB# s = s[i] #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# return s
"#LINE# #TAB# combined_headers = """" #LINE# #TAB# for name in fnames: #LINE# #TAB# #TAB# name = name.replace(""_"", ""-"") #LINE# #TAB# #TAB# header = _header_from_instrument(name) #LINE# #TAB# #TAB# if header and blend: #LINE# #TAB# #TAB# #TAB# if header!= """": #LINE# #TAB# #TAB# #TAB# #TAB# combined_headers += ""}\n"".join(header) #LINE# #TAB# #TAB# #TAB# combined_headers += ""\n"".join(header) #LINE# #TAB# if blend: #LINE# #TAB# #TAB# return ""\n"".join(combined_headers) #LINE# #TAB# return combined_headers"
"#LINE# #TAB# names = ['MD', 'MD2', 'MD3', 'MD4', 'MD5', 'B1', 'B2', 'B3', 'B4', 'B5', #LINE# #TAB# #TAB# 'B1', 'B2', 'B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'D1', 'D2', 'D3', 'D4', #LINE# #TAB# #TAB# 'D5', 'D6', 'D7', 'D8', 'D9', 'Da', 'F1', 'F2', 'F3', 'F4', 'F5'] #LINE# #TAB# return names"
"#LINE# #TAB# if model_mae.ndim!= 1: #LINE# #TAB# #TAB# raise ValueError('model_mae must be 1D') #LINE# #TAB# if naive_mae.ndim!= 1: #LINE# #TAB# #TAB# raise ValueError('naive_mae must be 1D') #LINE# #TAB# model_mae_trunc = model_mae - 1.0 #LINE# #TAB# naive_mae_trunc = naive_mae - 1.0 #LINE# #TAB# for i, score in enumerate(model_mae_trunc): #LINE# #TAB# #TAB# if score > naive_mae_trunc: #LINE# #TAB# #TAB# #TAB# model_mae_trunc[i] -= 1.0 #LINE# #TAB# return model_mae_trunc"
"#LINE# #TAB# filename_list = os.listdir(directory) #LINE# #TAB# cache = {} #LINE# #TAB# for filename in filename_list: #LINE# #TAB# #TAB# filename = os.path.join(directory, filename) #LINE# #TAB# #TAB# if filename.endswith('.xml'): #LINE# #TAB# #TAB# #TAB# xml = etree.fromstring(filename) #LINE# #TAB# #TAB# #TAB# if xml.text: #LINE# #TAB# #TAB# #TAB# #TAB# cache[filename] = xml #LINE# #TAB# return cache"
#LINE# #TAB# if entity.width == 'auto': #LINE# #TAB# #TAB# xy = xy[0] #LINE# #TAB# if entity.height == 'auto': #LINE# #TAB# #TAB# xy = xy[1] #LINE# #TAB# return xy
#LINE# #TAB# try: #LINE# #TAB# #TAB# return eval(obj) #LINE# #TAB# except: #LINE# #TAB# #TAB# return obj
#LINE# #TAB# forwarding = UwEmailForwarding() #LINE# #TAB# forwarding.mac_address = netid #LINE# #TAB# forwarding.save() #LINE# #TAB# return forwarding
"#LINE# #TAB# fields = [] #LINE# #TAB# for s in series: #LINE# #TAB# #TAB# if s in df.columns and isinstance(df[s], list): #LINE# #TAB# #TAB# #TAB# fields.append(df[s].iloc[0]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# fields.append(s) #LINE# #TAB# df = df[fields] #LINE# #TAB# del df[fields] #LINE# #TAB# return df"
"#LINE# #TAB# if isinstance(string, str): #LINE# #TAB# #TAB# string = string.encode('utf-8') #LINE# #TAB# tree = etree.fromstring(string) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# return root"
#LINE# #TAB# area = sum(float(p) * scale for p in polygon) #LINE# #TAB# return [polygon[i] * area + polygon[i + 1] * scale]
"#LINE# #TAB# while isinstance(data, xr.Dataset): #LINE# #TAB# #TAB# data = data.value #LINE# #TAB# return data"
#LINE# #TAB# annealing = {} #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# a = AAIndexObject() #LINE# #TAB# #TAB# a.record = record #LINE# #TAB# #TAB# annealing[record.id] = a #LINE# #TAB# return annealing
"#LINE# #TAB# s = unicodedata.normalize('NFC', u) #LINE# #TAB# s = s.replace('.', '_') #LINE# #TAB# s = s.replace('DFC', 'DFC_(') #LINE# #TAB# s = s.replace('DFC', 'DFC_(') #LINE# #TAB# s = s.replace('DFC', 'DFC_(') #LINE# #TAB# s = s.replace('DFC', 'DFC_(') #LINE# #TAB# return s"
"#LINE# #TAB# object_dict = {'id': object.id,'model': object.model, 'parameters': #LINE# #TAB# #TAB# object.parameters, 'target': object.target} #LINE# #TAB# for key, value in object_dict.items(): #LINE# #TAB# #TAB# if key.endswith('.json'): #LINE# #TAB# #TAB# #TAB# if value.endswith('.json'): #LINE# #TAB# #TAB# #TAB# #TAB# object_dict[key.replace('.json', '.json')] = value #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# object_dict[key.replace('.json', '.json')] = value #LINE# #TAB# return object_dict"
"#LINE# #TAB# min_val = numpy.min(matrix) #LINE# #TAB# max_val = numpy.max(matrix) #LINE# #TAB# matrix[shift:shift + scale, shift:max_val] = min_val #LINE# #TAB# return matrix"
"#LINE# #TAB# if mimetype is None: #LINE# #TAB# #TAB# return #LINE# #TAB# mimetype = mimetype.lower() #LINE# #TAB# if 'text/' not in mimetype: #LINE# #TAB# #TAB# return #LINE# #TAB# mimetype_parts = mimetype.split('/', 1) #LINE# #TAB# if len(mimetype_parts)!= 2: #LINE# #TAB# #TAB# return #LINE# #TAB# for part in mimetype_parts: #LINE# #TAB# #TAB# if part not in ('image', 'video'): #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# return"
#LINE# #TAB# stat = os.stat(mypath) #LINE# #TAB# if stat.S_ISDIR(stat.S_IRWXU): #LINE# #TAB# #TAB# symbol = 'directory' #LINE# #TAB# else: #LINE# #TAB# #TAB# symbol = 'file' #LINE# #TAB# return symbol
#LINE# #TAB# if response.status_code == 200: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif response.status_code == 401: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# #TAB# hdulist = {} #LINE# #TAB# #TAB# for entry in data: #LINE# #TAB# #TAB# #TAB# hdulist[entry['name']] = { #LINE# #TAB# #TAB# #TAB# #TAB# 'config': entry['config'], #LINE# #TAB# #TAB# #TAB# #TAB# 'type': entry['type'], #LINE# #TAB# #TAB# #TAB# #TAB# 'file': entry['file']} #LINE# #TAB# #TAB# return hdulist"
#LINE# #TAB# if exn is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# exn.stop() #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False
#LINE# #TAB# curve = cls(name) #LINE# #TAB# curve.validate() #LINE# #TAB# return curve
#LINE# #TAB# import tempfile #LINE# #TAB# output_file = tempfile.NamedTemporaryFile() #LINE# #TAB# for chunk in stream: #LINE# #TAB# #TAB# output_file.write(chunk) #LINE# #TAB# #TAB# output_file.flush() #LINE# #TAB# infile = NamedTemporaryFile(delete=False) #LINE# #TAB# infile.seek(0) #LINE# #TAB# output_file.write(b'\n') #LINE# #TAB# return output_file
"#LINE# #TAB# candidates = set() #LINE# #TAB# for char in model: #LINE# #TAB# #TAB# if predicate(char): #LINE# #TAB# #TAB# #TAB# if char not in candidates: #LINE# #TAB# #TAB# #TAB# #TAB# candidates.add(char) #LINE# #TAB# for i in range(len(model.atoms)): #LINE# #TAB# #TAB# character = model.atoms[i] #LINE# #TAB# #TAB# yield kH2CO3_character(character, 'X') #LINE# #TAB# #TAB# yield kH2CO3_character(character, 'Y') #LINE# #TAB# for i in range(len(model.atoms)): #LINE# #TAB# #TAB# candidate = model.atoms[i] #LINE# #TAB# #TAB# if candidate not in candidates: #LINE# #TAB# #TAB# #TAB# yield kH2CO3_character(candidate, 'Z') #LINE# #TAB# #TAB# #TAB# yield candidate"
#LINE# #TAB# A_magnitude = np.linalg.norm(A) #LINE# #TAB# B_magnitude = np.linalg.norm(B) #LINE# #TAB# return A_magnitude + B_magnitude
"#LINE# #TAB# if isinstance(structure, Mapping): #LINE# #TAB# #TAB# return OrderedDict((key, structure[key]) for key in structure.keys()) #LINE# #TAB# elif isinstance(structure, Sequence): #LINE# #TAB# #TAB# return [construct_ordered(element) for element in structure] #LINE# #TAB# else: #LINE# #TAB# #TAB# return structure"
"#LINE# #TAB# return {'id': obj.id, 'capakey': {'id': obj.capakey.id, 'naam': obj. #LINE# #TAB# #TAB# capakey.naam, 'definitie': obj.capakey.definitie},'status': {'id': obj.status. #LINE# #TAB# #TAB# id, 'naam': obj.status.naam, 'definitie': obj.status.definitie}}"
"#LINE# #TAB# t = None #LINE# #TAB# if path and os.path.isfile(path): #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t = datetime.datetime.strptime(f.read(), TIME_FORMAT) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return t"
"#LINE# #TAB# days, remainder = divmod(duration_in_seconds, 86400) #LINE# #TAB# hours, remainder = divmod(remainder, 3600) #LINE# #TAB# minutes, seconds = divmod(remainder, 60) #LINE# #TAB# return f'days:{days}:{hours:02d}:{minutes:02d}:{seconds:02d}'"
"#LINE# #TAB# yaml_file = os.path.join(os.path.dirname(__file__),'version.yaml') #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = 'v' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source = yaml_file #LINE# #TAB# return cfg"
"#LINE# #TAB# latList = [x.replace(' ', '') for x in searchList] #LINE# #TAB# lonList = [x.split(' ', 1)[0] for x in latList] #LINE# #TAB# lon = [x.split(' ', 1)[0] for x in lonList] #LINE# #TAB# latList = [x.split(' ', 1)[0] for x in latList] #LINE# #TAB# crs = [x.split(' ', 1)[0] for x in crsList] #LINE# #TAB# return lonList, latList, crs"
"#LINE# #TAB# desc = ""Root schema tag '{}'"".format(tag_name) #LINE# #TAB# result = run_command(desc, push=push) #LINE# #TAB# if not result: #LINE# #TAB# #TAB# return False #LINE# #TAB# return result"
#LINE# #TAB# megaman = Keymap() #LINE# #TAB# for mapping in raw_keymaps: #LINE# #TAB# #TAB# megaman.add_keymap(**mapping) #LINE# #TAB# return megaman
"#LINE# #TAB# directory_meta = list(DIR_ATTS) #LINE# #TAB# meta = get_dir_meta(fp, directory_meta) #LINE# #TAB# meta.update(get_fname_meta(fp)) #LINE# #TAB# return meta"
#LINE# #TAB# global dhc_enabled #LINE# #TAB# dhc_enabled = False
#LINE# #TAB# if not string: #LINE# #TAB# #TAB# return string #LINE# #TAB# string = string.split('/')[-1] #LINE# #TAB# return string
"#LINE# #TAB# if not regexps: #LINE# #TAB# #TAB# return content #LINE# #TAB# html = content[:] #LINE# #TAB# for regex, replace in regexps: #LINE# #TAB# #TAB# html = re.sub(regex, replace, html) #LINE# #TAB# return html"
"#LINE# #TAB# if id_cols is None: #LINE# #TAB# #TAB# id_cols = df.columns.values #LINE# #TAB# invalid_ids = [] #LINE# #TAB# for col in id_cols: #LINE# #TAB# #TAB# if df[col].isnull().any().any(): #LINE# #TAB# #TAB# #TAB# invalid_ids.append(col) #LINE# #TAB# out = df.dropna(axis=1, subset=invalid_ids) #LINE# #TAB# return out"
#LINE# #TAB# conf = configparser.ConfigParser() #LINE# #TAB# conf.read(CONF_FILE) #LINE# #TAB# return conf
"#LINE# #TAB# try: #LINE# #TAB# #TAB# files = [f for f in os.listdir(directory) if f.endswith('.pid')] #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# print('Error: pid file not found in directory:'+ directory) #LINE# #TAB# pid_list = [] #LINE# #TAB# with open(directory + os.sep + 'pid_list.txt', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# pid_list.append(line.rstrip('\n')) #LINE# #TAB# #TAB# pid_list.sort() #LINE# #TAB# return pid_list"
"#LINE# #TAB# if expiration_ms is None: #LINE# #TAB# #TAB# expiration_ms = settings.DB_EXPIRATION_MS #LINE# #TAB# date_part = field.split('__')[:-1] #LINE# #TAB# partitioned_table ='sqlite_date_' + field.split('__')[-1] #LINE# #TAB# if len(date_part) == 1 and date_part[0].isdigit(): #LINE# #TAB# #TAB# partitioned_table += '_date_' + field.split('__')[-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# date_part = field.split('__')[-1] #LINE# #TAB# #TAB# partitioned_table += '_date_' + field.split('__')[-1] #LINE# #TAB# return {'date': date_part, 'partitioned': partitioned_table}"
"#LINE# #TAB# done = set() #LINE# #TAB# for href, calendar in calendars: #LINE# #TAB# #TAB# done.add(href) #LINE# #TAB# #TAB# for component in calendar: #LINE# #TAB# #TAB# #TAB# vdays = component.days #LINE# #TAB# #TAB# #TAB# for vtodo in vdays: #LINE# #TAB# #TAB# #TAB# #TAB# if vtodo not in done: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield component"
"#LINE# #TAB# begidx = get_begidx(text, start) #LINE# #TAB# if begidx is not None: #LINE# #TAB# #TAB# return [(begidx, end)] #LINE# #TAB# endidx = get_endidx(text, end) #LINE# #TAB# return []"
#LINE# #TAB# print('Please enter your API key ( from'+ read_webapp_url() +'):') #LINE# #TAB# key = click.prompt('API key: ') #LINE# #TAB# return key
"#LINE# #TAB# assert theta_units in ['radians', 'degrees'],\ #LINE# #TAB# #TAB# ""kwarg theta_units must specified in radians or degrees"" #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.pi / 2 #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.pi / 2 #LINE# #TAB# x = x * np.cos(theta) - y * np.sin(theta) #LINE# #TAB# y = y * np.sin(theta) + x * np.cos(theta) #LINE# #TAB# return x, y"
"#LINE# #TAB# collection = resource.collection #LINE# #TAB# method = request.method.lower() #LINE# #TAB# params = request.GET.get_params(collection=collection, method=method) #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# set_index = cursor.execute(sql) #LINE# #TAB# #TAB# #TAB# rows = [row for row in cursor.fetchall()] #LINE# #TAB# #TAB# resource.collection.update(rows) #LINE# #TAB# return {'resource': resource, 'parent_resource': parent_resource,'method': #LINE# #TAB# #TAB# method, 'params': params}"
"#LINE# #TAB# if renderer is None: #LINE# #TAB# #TAB# renderer = 'agg' #LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib #LINE# #TAB# #TAB# renderer_module = importlib.import_module(renderer) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# renderer_module = renderer.split('.')[-1] #LINE# #TAB# normal_renderer = getattr(matplotlib, renderer_module) #LINE# #TAB# formatter_module = getattr(matplotlib, 'FuncFormatter') #LINE# #TAB# normal_formatter = getattr(matplotlib, formatter_module) #LINE# #TAB# formatter = FuncFormatter(formatter=formatter) #LINE# #TAB# return normal_renderer, formatter"
"#LINE# #TAB# x = x[(...), ::-1] #LINE# #TAB# x[..., 0] -= 103.939 #LINE# #TAB# x[..., 1] -= 116.779 #LINE# #TAB# x[..., 2] -= 123.68 #LINE# #TAB# return x"
#LINE# #TAB# mapping = {} #LINE# #TAB# for e in G: #LINE# #TAB# #TAB# for f in G[e]: #LINE# #TAB# #TAB# #TAB# if f not in mapping: #LINE# #TAB# #TAB# #TAB# #TAB# mapping[f] = e #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# old_value = mapping[f] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if old_value == e: #LINE# #TAB# #TAB# #TAB# del mapping[f] #LINE# #TAB# #TAB# #TAB# mapping[e] = old_value
"#LINE# #TAB# with wave.open(fn, 'rb') as f: #LINE# #TAB# #TAB# n_channels = wavfile.getnchannels() #LINE# #TAB# #TAB# if n_channels == 1: #LINE# #TAB# #TAB# #TAB# return float(f.getframerate()) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# secs = wavfile.getframerate() #LINE# #TAB# #TAB# #TAB# return float(secs) / n_channels #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# assert False"
"#LINE# #TAB# env = [] #LINE# #TAB# for arg in args: #LINE# #TAB# #TAB# if arg[0] in env: #LINE# #TAB# #TAB# #TAB# env.append((arg[0], arg[1])) #LINE# #TAB# #TAB# elif arg[1] in defaults: #LINE# #TAB# #TAB# #TAB# default = defaults[arg[0]] #LINE# #TAB# #TAB# #TAB# env.append((arg[0], default)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return [(arg[0], arg[1])] #LINE# #TAB# return env"
#LINE# #TAB# value = data.extracted #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if len(widget.children) == 1 and widget.children[0].value == value: #LINE# #TAB# #TAB# #TAB# #TAB# del widget.children[0] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# compound = Compound.objects.get(identifier=value) #LINE# #TAB# #TAB# #TAB# except Compound.DoesNotExist: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# yield compound #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# #TAB# ret = str(obj) #LINE# #TAB# #TAB# if pretty: #LINE# #TAB# #TAB# #TAB# ret = pretty_string(ret, indent) #LINE# #TAB# #TAB# return ret"
"#LINE# #TAB# cached_rows = [] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cached_rows.append(elastic_client.indices.get(index).open()) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# print('Failed to open index {} in index {}'.format(index, #LINE# #TAB# #TAB# #TAB# #TAB# indices)) #LINE# #TAB# return cached_rows"
#LINE# #TAB# mod = types.ModuleType(name) #LINE# #TAB# mod.__dict__.update(code or extract_code(name)) #LINE# #TAB# return mod
"#LINE# #TAB# data = [] #LINE# #TAB# for obj in object_list: #LINE# #TAB# #TAB# for key, value in obj.as_dict().items(): #LINE# #TAB# #TAB# #TAB# data.append(_text_convert_json(value)) #LINE# #TAB# return data"
#LINE# #TAB# t = angles % 360.0 #LINE# #TAB# if t > 180.0: #LINE# #TAB# #TAB# t = 360.0 - t #LINE# #TAB# return t
#LINE# #TAB# params['data_id'] = 0 #LINE# #TAB# return params
#LINE# #TAB# system_columns = [] #LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# system_columns.append(col) #LINE# #TAB# return system_columns
"#LINE# #TAB# poly = np.zeros(shape=(epoch, steps, factor)) #LINE# #TAB# for step in range(epoch + 1, steps): #LINE# #TAB# #TAB# step_lr = lr * factor ** step #LINE# #TAB# #TAB# poly[step_lr <= epoch] = 0 #LINE# #TAB# return poly"
"#LINE# #TAB# Tpm = np.array(tpm) #LINE# #TAB# N = Tpm.shape[0] #LINE# #TAB# connectivity = np.zeros((N, N)) #LINE# #TAB# idx = 0 #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# for j in range(N): #LINE# #TAB# #TAB# #TAB# connectivity[i, j] = 1.0 #LINE# #TAB# #TAB# #TAB# idx += 1 #LINE# #TAB# return connectivity"
"#LINE# #TAB# attributes = {} #LINE# #TAB# for key, value in six.iteritems(network): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# attributes[key] = generate_client_attributes(value) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# attributes[key] = generate_client_attributes(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# attributes[key] = value #LINE# #TAB# return attributes"
#LINE# #TAB# normalized_file_dict = {} #LINE# #TAB# for file in file_dict.values(): #LINE# #TAB# #TAB# normalized_file_dict[file[0]] = file[1] #LINE# #TAB# return normalized_file_dict
#LINE# #TAB# try: #LINE# #TAB# #TAB# vco = Metrics(sql) #LINE# #TAB# except ValueError as e: #LINE# #TAB# #TAB# raise ValueError('Invalid sql or table: %s' % sql) from e
#LINE# #TAB# model_name = ContentType.objects.get_for_model(model=ct_name) #LINE# #TAB# try: #LINE# #TAB# #TAB# return model_name._meta.model_class() #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# return None
#LINE# #TAB# T = transform.user_transform #LINE# #TAB# if T == 'array': #LINE# #TAB# #TAB# return namedb_restore(transform.inverse) #LINE# #TAB# elif T == 'affine': #LINE# #TAB# #TAB# return namedb_restore(transform.affine) #LINE# #TAB# elif T == 'row': #LINE# #TAB# #TAB# return namedb_restore(transform.row) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if isinstance(obj, p.Future): #LINE# #TAB# #TAB# return obj.result() #LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# return [dense_adjust(item) for item in obj] #LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# return {key: dense_adjust(value) for key, value in obj.items()} #LINE# #TAB# return obj"
"#LINE# #TAB# module_dir = os.path.dirname(os.path.abspath(module_name)) #LINE# #TAB# return [os.path.join(module_dir, file_name) for file_name in os.listdir( #LINE# #TAB# #TAB# module_dir) if os.path.isdir(os.path.join(module_dir, file_name)) and #LINE# #TAB# #TAB# not file_name.startswith('_')]"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(CACHE_PACKAGE_TAGS_PATH, 'r') as f: #LINE# #TAB# #TAB# #TAB# return json.load(f) #LINE# #TAB# except (IOError, ValueError): #LINE# #TAB# #TAB# return {}"
"#LINE# #TAB# for name, value in form.get(FIELD_NAME, {}).items(): #LINE# #TAB# #TAB# if name == key: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# if not element: #LINE# #TAB# #TAB# return {} #LINE# #TAB# o = dict() #LINE# #TAB# for e in element: #LINE# #TAB# #TAB# if e.tag!= 'diff': #LINE# #TAB# #TAB# #TAB# o[e.tag] = e.text #LINE# #TAB# #TAB# elif e.tag == 'file': #LINE# #TAB# #TAB# #TAB# o[e.tag] = _diff_file(e) #LINE# #TAB# #TAB# elif e.tag == 'date': #LINE# #TAB# #TAB# #TAB# o[o.tag] = _diff_date(e) #LINE# #TAB# #TAB# elif e.tag == 'image': #LINE# #TAB# #TAB# #TAB# o[o.tag] = _diff_image(e) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# o[o.tag] = _diff_text(e) #LINE# #TAB# return
"#LINE# #TAB# out = {} #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# line = line.rstrip().split(""\t"") #LINE# #TAB# #TAB# #TAB# if len(line)!= 2: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# ssms = line[0].split("" "") #LINE# #TAB# #TAB# #TAB# chrom, loc = ssms[1].split("" "") #LINE# #TAB# #TAB# #TAB# loc = [chrom, loc[0]] #LINE# #TAB# #TAB# #TAB# out[chrom] = loc #LINE# #TAB# return out"
"#LINE# #TAB# machine_id = hashlib.md5() #LINE# #TAB# with open(MACHINE_ID_FILE, 'r') as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('0x'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if not machine_id.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# machine_id = int(machine_id) #LINE# #TAB# return machine_id"
#LINE# #TAB# if not context: #LINE# #TAB# #TAB# return False #LINE# #TAB# if context.is_jsonrpc_request(): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# region = profile.get('region') #LINE# #TAB# if profile.get('aws_access_key_id'): #LINE# #TAB# #TAB# aws_access_key = profile['aws_access_key_id'] #LINE# #TAB# else: #LINE# #TAB# #TAB# aws_access_key = profile['aws_default_access_key'] #LINE# #TAB# if 'aws_secret_access_key' in profile: #LINE# #TAB# #TAB# aws_secret_access_key = profile['aws_secret_access_key'] #LINE# #TAB# else: #LINE# #TAB# #TAB# aws_secret_access_key = profile['aws_secret_access_key'] #LINE# #TAB# return {'account': aws_access_key, 'aws_secret_access_key': #LINE# #TAB# #TAB# aws_secret_access_key, 'aws_secret_access_key': aws_secret_access_key}"
"#LINE# #TAB# n_peaks = data.shape[0] #LINE# #TAB# peak_index = np.arange(n_peaks) #LINE# #TAB# temp = data.copy() #LINE# #TAB# temp[peak_index] = 0 #LINE# #TAB# for j in range(1, n_peaks): #LINE# #TAB# #TAB# temp = temp[j] #LINE# #TAB# signal = temp[peak_index] #LINE# #TAB# for k in range(1, n_peaks + 1): #LINE# #TAB# #TAB# signal = signal[:, (k)] #LINE# #TAB# #TAB# signal = signal[:, (k)] #LINE# #TAB# data = data.drop(signal.index, axis=1) #LINE# #TAB# data = data.rename(columns={'peak_index': peak_index}) #LINE# #TAB# return data"
"#LINE# #TAB# if operand_name_list is None: #LINE# #TAB# #TAB# operand_name_list = get_operand_name_list() #LINE# #TAB# title_prototype = [] #LINE# #TAB# for operand in operand_list: #LINE# #TAB# #TAB# if isinstance(operand, ArrayLike): #LINE# #TAB# #TAB# #TAB# title_prototype.extend(infer_title_prototype(operand, #LINE# #TAB# #TAB# #TAB# #TAB# operand_name_list)) #LINE# #TAB# #TAB# elif isinstance(operand, Symbol): #LINE# #TAB# #TAB# #TAB# title_prototype.append('symbol') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise TypeError('operand must be a Symbol or a list of operands') #LINE# #TAB# return title_prototype"
#LINE# #TAB# host = 'localhost' #LINE# #TAB# messages_host = '' #LINE# #TAB# while messages_host == '': #LINE# #TAB# #TAB# messages_host = 'localhost' #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# app_info = get_app_info() #LINE# #TAB# #TAB# #TAB# message_host = app_info['message']['domain'] + '.' + app_info[ #LINE# #TAB# #TAB# #TAB# #TAB#'message']['action'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return messages_host
"#LINE# #TAB# _, lc_input_data = load_cell_data(filename) #LINE# #TAB# return lc_input_data"
#LINE# #TAB# if not signature.startswith('(') and len(signature) > 1: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'Invalid function signature {signature}') #LINE# #TAB# if signature.startswith('(') and signature.endswith(')'): #LINE# #TAB# #TAB# return signature[1:-1] #LINE# #TAB# return signature
"#LINE# #TAB# collect_root = app.extensions['collect'].static_root #LINE# #TAB# weighted_removal = [] #LINE# #TAB# for blueprint in blueprints: #LINE# #TAB# #TAB# if hasattr(blueprint,'static_root'): #LINE# #TAB# #TAB# #TAB# if blueprint.static_root: #LINE# #TAB# #TAB# #TAB# #TAB# weighted_removal.append(blueprint.static_root) #LINE# #TAB# return weighted_removal"
"#LINE# #TAB# client = _get_client() #LINE# #TAB# table = client.get_table(table_name) #LINE# #TAB# output = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# startTime = table.get_startTime(utc_hour) #LINE# #TAB# #TAB# duration = table.getTraceDuration(utc_hour) #LINE# #TAB# #TAB# data = client.trace_bytes( #LINE# #TAB# #TAB# #TAB# table.name, #LINE# #TAB# #TAB# #TAB# startTime, #LINE# #TAB# #TAB# #TAB# duration #LINE# #TAB# #TAB# ) #LINE# #TAB# #TAB# if data: #LINE# #TAB# #TAB# #TAB# output.append(data) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return output"
#LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in fromDic and fromDic[key]!= toDic[key]: #LINE# #TAB# #TAB# #TAB# del fromDic[key]
"#LINE# #TAB# return [Source(url=line[0], title=line[1], start=line[2], end=line[3]) for line in #LINE# #TAB# #TAB# sourcelist]"
"#LINE# #TAB# for path, dirs, files in os.walk(top): #LINE# #TAB# #TAB# if '__init__.py' in files: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# with open(path) as f: #LINE# #TAB# #TAB# #TAB# name = os.path.basename(f) #LINE# #TAB# #TAB# #TAB# package_file = Path(name) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# if package_file.match(package_file): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# shutil.copyfile(f, path) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# os.unlink(path) #LINE# #TAB# #TAB# yield path, dirs, files"
"#LINE# #TAB# biopax = None #LINE# #TAB# if isinstance(model, BioPAXElement): #LINE# #TAB# #TAB# biopax = model #LINE# #TAB# elif isinstance(model, BioPAXText): #LINE# #TAB# #TAB# biopax = model #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError(""Unserializable object {} of type {}"".format(model, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# type(model))) #LINE# #TAB# if biopax is None: #LINE# #TAB# #TAB# logger.debug(""No BiopaxProcessor for model {}"".format(model)) #LINE# #TAB# return biopax"
#LINE# #TAB# opener = urllib.request.build_opener(*get_handlers()) #LINE# #TAB# return opener
"#LINE# #TAB# scores = {} #LINE# #TAB# exemplar_followers = set() #LINE# #TAB# for followers in exemplars.values(): #LINE# #TAB# #TAB# exemplar_followers |= followers #LINE# #TAB# for brand, followers in brands: #LINE# #TAB# #TAB# scores[brand] = _y_encode(followers, exemplar_followers) #LINE# #TAB# return scores"
#LINE# #TAB# z = year - 1 + EPOCH_GREGORIAN_YEAR #LINE# #TAB# if month!= 20: #LINE# #TAB# #TAB# z += 1 #LINE# #TAB# return z + day + EPOCH_GREGORIAN_MONTH - 1
"#LINE# #TAB# try: #LINE# #TAB# #TAB# data = response.get_json() #LINE# #TAB# #TAB# if ""error_description"" in data: #LINE# #TAB# #TAB# #TAB# return data['error_description'] #LINE# #TAB# #TAB# if ""error"" in data: #LINE# #TAB# #TAB# #TAB# return data['error'] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ""Unknown error"""
"#LINE# #TAB# lexer, ext = os.path.splitext(file_name) #LINE# #TAB# if ext == '': #LINE# #TAB# #TAB# return #LINE# #TAB# for lexer in _iter_lexers(text): #LINE# #TAB# #TAB# yield lexer"
#LINE# #TAB# try: #LINE# #TAB# #TAB# module = import_module(module) #LINE# #TAB# #TAB# return module.struct_api() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if isinstance(index, str): #LINE# #TAB# #TAB# index = len(index) #LINE# #TAB# index = pad_index(index, count) #LINE# #TAB# if index == 0: #LINE# #TAB# #TAB# return '_' * count #LINE# #TAB# return index"
"#LINE# #TAB# pub = rsa_key.RSAPrivateKey.load_pub_key(inp) #LINE# #TAB# pub_str = pub.public_key().public_bytes(encoding=serialization.Encoding. #LINE# #TAB# #TAB# PEM, format=serialization.PrivateFormat.TraditionalOpenSSL, #LINE# #TAB# #TAB# encryption_algorithm=serialization.NoEncryption()) #LINE# #TAB# pub_str = pub_str.public_bytes(encoding=serialization.Encoding #LINE# #TAB# #TAB#.OpenSSH, format=serialization.PrivateFormat.TraditionalOpenSSL, #LINE# #TAB# #TAB# encryption_algorithm=serialization.NoEncryption()) #LINE# #TAB# return pub_str"
"#LINE# #TAB# release, major, minor = version_string.split('.', 2) #LINE# #TAB# if major == 'alpha' and minor == 'beta': #LINE# #TAB# #TAB# return release, major, minor #LINE# #TAB# return release, major, minor"
"#LINE# #TAB# cores = min(multiprocessing.cpu_count(), cores) #LINE# #TAB# if six.PY3: #LINE# #TAB# #TAB# cores = 1 #LINE# #TAB# return cores"
#LINE# #TAB# args['out_migrate'] = 0 #LINE# #TAB# return args['out_migrate']
"#LINE# #TAB# update_dict = {} #LINE# #TAB# for key, value in dictionary.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# update_dict[key] = extract_expand_clause_from_dict(value, datetime_format) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# update_dict[key] = value #LINE# #TAB# return update_dict"
#LINE# #TAB# if line == '': #LINE# #TAB# #TAB# return line #LINE# #TAB# commands = line.split(';') #LINE# #TAB# if len(commands) == 1: #LINE# #TAB# #TAB# return commands[0] #LINE# #TAB# fixed = ';'.join(commands[1:]) #LINE# #TAB# return fixed
#LINE# #TAB# ddb_minute_degrees = float(nmea_degrees) / 60 #LINE# #TAB# return ddb_minute_degrees
"#LINE# #TAB# list_of_instances = get_all_instances(gameweek, team_id) #LINE# #TAB# picks = {} #LINE# #TAB# for i in list_of_instances: #LINE# #TAB# #TAB# picks[i] = {} #LINE# #TAB# #TAB# picks[i]['gameweek'] = gameweek #LINE# #TAB# #TAB# picks[i]['game_id'] = team_id #LINE# #TAB# return picks"
"#LINE# #TAB# if hasattr(obj,'mode_embedded'): #LINE# #TAB# #TAB# return obj.mode_embedded #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if 'Authorization' in request.headers: #LINE# #TAB# #TAB# auth = request.headers['Authorization'] #LINE# #TAB# #TAB# if not auth: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if ':' in auth: #LINE# #TAB# #TAB# #TAB# username, password = auth.split(':', 1) #LINE# #TAB# #TAB# #TAB# return 'username:%s' % username #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return auth #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# j = vehicle_journey_element #LINE# #TAB# if len(j) == 1: #LINE# #TAB# #TAB# base_limit = 0 #LINE# #TAB# elif len(j) == 2: #LINE# #TAB# #TAB# base_limit = (j[0] - J[1]) / 7 #LINE# #TAB# else: #LINE# #TAB# #TAB# base_limit = (j[0] - J[1]) / 7 #LINE# #TAB# return base_limit
"#LINE# #TAB# errorCode = c_uint() #LINE# #TAB# try: #LINE# #TAB# #TAB# cdb = conf.lib.clang_CompilationDatabase_getCache(buildDir, #LINE# #TAB# #TAB# #TAB# byref(errorCode)) #LINE# #TAB# finally: #LINE# #TAB# #TAB# conf.lib.clang_CompilationDatabase_free(cdb) #LINE# #TAB# return errorCode"
"#LINE# #TAB# folders = [os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# 'SteamLib')] #LINE# #TAB# return folders"
"#LINE# #TAB# client = boto3.client('apigateway', region_name=PRIMARY_REGION) #LINE# #TAB# try: #LINE# #TAB# #TAB# return client.get_gateway_tokens(tag) #LINE# #TAB# except ClientError as exc: #LINE# #TAB# #TAB# if exc.response.status_code == 404: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# #TAB# raise"
"#LINE# #TAB# global _actions #LINE# #TAB# key = '%s:%s' % (prefix, ns) #LINE# #TAB# _actions[key] = ns"
#LINE# #TAB# value = 'true' #LINE# #TAB# if value == 'false': #LINE# #TAB# #TAB# return 'false' #LINE# #TAB# elif value == 'true': #LINE# #TAB# #TAB# return 'true' #LINE# #TAB# return value
"#LINE# #TAB# ra = gl[0] #LINE# #TAB# dec = gl[1] #LINE# #TAB# ell = gl[2] #LINE# #TAB# eq_x = np.arctan2(ra, dec) #LINE# #TAB# eq_y = np.arctan2(dec, ell) #LINE# #TAB# if eq_x < 0: #LINE# #TAB# #TAB# eq_x = -gl[3] + eq_x #LINE# #TAB# if eq_y < 0: #LINE# #TAB# #TAB# eq_y = 0 #LINE# #TAB# eq_z = -gl[4] + eq_y #LINE# #TAB# return eq_x, eq_y, eq_z"
#LINE# #TAB# logger.setLevel(level) #LINE# #TAB# return logger
#LINE# #TAB# start_activities = set() #LINE# #TAB# for node in dfg: #LINE# #TAB# #TAB# if node.type =='start': #LINE# #TAB# #TAB# #TAB# for act in node.attributes: #LINE# #TAB# #TAB# #TAB# #TAB# if act not in start_activities: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# start_activities.add(act) #LINE# #TAB# return start_activities
#LINE# #TAB# extension = os.path.splitext(file_path)[1] #LINE# #TAB# return extension
"#LINE# #TAB# params = [] #LINE# #TAB# for name, param in inspect.signature(func).parameters.items(): #LINE# #TAB# #TAB# if ':' in name: #LINE# #TAB# #TAB# #TAB# param_name = name.split(':')[0] #LINE# #TAB# #TAB# #TAB# params.append((param_name, _get_descriptor_param(param))) #LINE# #TAB# return params"
"#LINE# #TAB# result = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# from uuid import uuid4 #LINE# #TAB# #TAB# result = str(uuid4()) #LINE# #TAB# #TAB# result = result.replace('-', '') #LINE# #TAB# #TAB# result = result.replace(' ', '-') #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return result"
#LINE# #TAB# if is_poly_ref(typ): #LINE# #TAB# #TAB# return typ #LINE# #TAB# elif is_reference(typ): #LINE# #TAB# #TAB# poly_typ = resolve_poly_ref(typ) #LINE# #TAB# #TAB# if poly_typ is not None: #LINE# #TAB# #TAB# #TAB# return poly_typ #LINE# #TAB# return typ
"#LINE# #TAB# try: #LINE# #TAB# #TAB# doi, value = value.split('/', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if '@' not in doi: #LINE# #TAB# #TAB# return None #LINE# #TAB# return doi, value"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if t.value.count('.') == 0: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = int(t.value) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('[%d]: Number %s is not valid!' % (t.lineno, t.value)) #LINE# #TAB# #TAB# t.value = 0 #LINE# #TAB# return t"
#LINE# #TAB# result = [] #LINE# #TAB# for i in _bytes: #LINE# #TAB# #TAB# result.append(uint16_vnl_int32(i)) #LINE# #TAB# return result
#LINE# #TAB# Tr = 298.15 #LINE# #TAB# return abc[0] + abc[1] * (T - Tr) + abc[2] * (T - Tr) ** 2
"#LINE# #TAB# code = str(code) #LINE# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# crs = None #LINE# #TAB# if proj4: #LINE# #TAB# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# #TAB# crs = utils.crscode_to_string(""esri"", code, ""crs"") #LINE# #TAB# return crs"
"#LINE# #TAB# cosa = np.sum(v1 * v2, axis=1) #LINE# #TAB# sina = np.sum(v1 * v2, axis=1) #LINE# #TAB# cosb = np.sum(v1 * v2, axis=1) #LINE# #TAB# sina = np.sqrt(cosa) #LINE# #TAB# cosb = np.sqrt(sina) #LINE# #TAB# ang = np.arctan2(sina, cosb) #LINE# #TAB# return ang"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return module.models #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# from bs4 import BeautifulSoup #LINE# #TAB# soup = BeautifulSoup(html, 'html.parser') #LINE# #TAB# for node in soup.find_all('a', href=True): #LINE# #TAB# #TAB# url = node.get('href') #LINE# #TAB# #TAB# if url: #LINE# #TAB# #TAB# #TAB# node['href'] = request.build_absolute_uri(url) #LINE# #TAB# return html"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# overlap = table[class_name].geometry_overlap_all() #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# raise LookupError( #LINE# #TAB# #TAB# #TAB# '{0} is not a geometry overlap function for {1}'.format(class_name, classes) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# overlap = TP[class_name] - TP[class_name]['geometry_overlap_first'] #LINE# #TAB# #TAB# if overlap is not None: #LINE# #TAB# #TAB# #TAB# return overlap #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# d = deepcopy(orig_dict) #LINE# #TAB# if keys is None: #LINE# #TAB# #TAB# keys = [] #LINE# #TAB# elif isinstance(keys, str): #LINE# #TAB# #TAB# keys = [keys] #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in d: #LINE# #TAB# #TAB# #TAB# d[key] = d[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# else: #LINE# #TAB# #TAB# if default is not None: #LINE# #TAB# #TAB# #TAB# d[key] = default #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d[key] = d[key] #LINE# #TAB# return d"
"#LINE# #TAB# return [q for q in concept if q in (min_c, min_z, max_c, max_z, min_neu) and (q[0] in #LINE# #TAB# #TAB# ['c', 'z', 'neu'] and q[1] >= min_c) and (q[0] <= max_c or q[0] >= #LINE# #TAB# #TAB# min_z and q[1] <= max_z) and (q[0] >= min_c or q[0] <= max_c) and (q[1] >= #LINE# #TAB# #TAB# min_neu or q[1] <= max_neu)]"
"#LINE# #TAB# bins = Bins() #LINE# #TAB# for i in range(num_bins): #LINE# #TAB# #TAB# p =probs[i] #LINE# #TAB# #TAB# diff = p - p[0] #LINE# #TAB# #TAB# bins.put(diff, p) #LINE# #TAB# return bins"
#LINE# #TAB# chunks = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# md5sum = hashlib.md5(f.read()).hexdigest() #LINE# #TAB# #TAB# chunks.append(md5sum) #LINE# #TAB# return chunks
#LINE# #TAB# if string is True: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif string is False: #LINE# #TAB# #TAB# return False #LINE# #TAB# string = str(string).lower() #LINE# #TAB# try: #LINE# #TAB# #TAB# ipaddress.ip_address(string) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# lang = request.META.get('HTTP_ACCEPT_LANGUAGE') #LINE# #TAB# if not lang: #LINE# #TAB# #TAB# return #LINE# #TAB# l1 = lang.split(',') #LINE# #TAB# if len(l1) == 2: #LINE# #TAB# #TAB# l1[0] = l1[0].lower() #LINE# #TAB# #TAB# l1[1] = l1[1].replace('-', '_') #LINE# #TAB# elif l1[0] == 'en': #LINE# #TAB# #TAB# l1[0] = 'en' #LINE# #TAB# return l1"
#LINE# #TAB# if len(array_one)!= len(array_two): #LINE# #TAB# #TAB# raise ValueError('arrays must be same length') #LINE# #TAB# exp_diff = 1.0 #LINE# #TAB# for i in range(len(array_one)): #LINE# #TAB# #TAB# exp_diff *= (array_one[i] - array_two[i]) / 2.0 #LINE# #TAB# return exp_diff
"#LINE# #TAB# y, x = rectangle #LINE# #TAB# row, col = np.nonzero(grid == y) #LINE# #TAB# len_grid = len(grid) #LINE# #TAB# if len_grid < 3: #LINE# #TAB# #TAB# return grid #LINE# #TAB# bin_x, bin_y = np.nonzero(grid == x) #LINE# #TAB# if len_grid < 3: #LINE# #TAB# #TAB# return grid[(bin_x), :] #LINE# #TAB# minimum = grid[(bin_x), :] - grid[(bin_y), :] #LINE# #TAB# median = grid[(bin_x + 1), :] - grid[(bin_y + 1), :] #LINE# #TAB# return median - minimum"
#LINE# #TAB# rst = '' #LINE# #TAB# if format: #LINE# #TAB# #TAB# format = format.lower() #LINE# #TAB# lines = [] #LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# return rst #LINE# #TAB# for line in open(filename): #LINE# #TAB# #TAB# rst += '#TAB#'+ '#TAB#'+ format_line[0:3] + '\n' #LINE# #TAB# #TAB# lines.append(line + '\n') #LINE# #TAB# return rst
#LINE# #TAB# return get_origin(annotation) is not None and get_destination(annotation #LINE# #TAB# #TAB# ) is not None
"#LINE# #TAB# template = event.get('template', {}) #LINE# #TAB# options = template.get('options', {}) #LINE# #TAB# options.update(event.get('options', {})) #LINE# #TAB# serialized_event = template.format(**options) #LINE# #TAB# serialized_event['uuid'] = uuidutils.generate_uuid() #LINE# #TAB# event_message = template.render( #LINE# #TAB# #TAB# event=serialized_event, #LINE# #TAB# #TAB# secret=secret #LINE# #TAB# ) #LINE# #TAB# return serialized_event"
#LINE# #TAB# password = None #LINE# #TAB# while not password and skipUserInput: #LINE# #TAB# #TAB# password = getpass.getpass('Enter password: ') #LINE# #TAB# return password
#LINE# #TAB# if name in row: #LINE# #TAB# #TAB# value = row[name] #LINE# #TAB# #TAB# if not value: #LINE# #TAB# #TAB# #TAB# errors.append(error_message) #LINE# #TAB# #TAB# return value #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# network, sent_list, failed_commands = [], [], [] #LINE# #TAB# if args['network']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if len(args['sent']) > 0: #LINE# #TAB# #TAB# #TAB# #TAB# sent_list.extend(args['sent']) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# failed_commands.append('') #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# if not sent_list: #LINE# #TAB# #TAB# #TAB# #TAB# protocol, sent_list = split_network(args) #LINE# #TAB# return network, sent_list, failed_commands"
"#LINE# #TAB# host = socket.gethostname() #LINE# #TAB# if platform.system() == 'Darwin': #LINE# #TAB# #TAB# for prefix in ('linux', 'linux2'): #LINE# #TAB# #TAB# #TAB# if host.startswith(prefix): #LINE# #TAB# #TAB# #TAB# #TAB# return prefix.replace(prefix, '', 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'en'"
#LINE# #TAB# clsvar = 'cuda' #LINE# #TAB# if sys.platform == 'darwin': #LINE# #TAB# #TAB# if sys.platform.startswith('linux'): #LINE# #TAB# #TAB# #TAB# clsvar ='mac' #LINE# #TAB# #TAB# elif sys.platform.startswith('win'): #LINE# #TAB# #TAB# #TAB# clsvar = 'win' #LINE# #TAB# #TAB# elif sys.platform.startswith('darwin'): #LINE# #TAB# #TAB# #TAB# clsvar ='mac' #LINE# #TAB# return clsvar
#LINE# #TAB# path = unicode_path(geo) #LINE# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# return [path] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# content_type = input #LINE# #TAB# try: #LINE# #TAB# #TAB# if hasattr(magic, 'loads'): #LINE# #TAB# #TAB# #TAB# content_type = magic.loads(content_type) #LINE# #TAB# #TAB# elif hasattr(magic, 'from_buffer'): #LINE# #TAB# #TAB# #TAB# content_type = magic.from_buffer(content_type) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# content_type = magic.from_buffer(content_type) #LINE# #TAB# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return content_type"
#LINE# #TAB# if text: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return DateTime(text) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# backend = Backend() #LINE# #TAB# backend.add_url_rule('/', view_func=show_url) #LINE# #TAB# return backend"
"#LINE# #TAB# url = url.strip('/') #LINE# #TAB# url = url.rstrip('/') #LINE# #TAB# assert url.endswith('/'), 'URL should end with /' #LINE# #TAB# parts = url.split('/') #LINE# #TAB# config = dict() #LINE# #TAB# config['protocol'] = parts[0] #LINE# #TAB# if len(parts) > 1: #LINE# #TAB# #TAB# config['path'] = '/' + parts[1] #LINE# #TAB# if len(parts) > 2: #LINE# #TAB# #TAB# config['path'] = '/' + parts[2] #LINE# #TAB# return config"
"#LINE# #TAB# if 'break_source' not in chromosome_plot_info: #LINE# #TAB# #TAB# chromosome_plot_info['break_source'] = ColumnDataSource(store, #LINE# #TAB# #TAB# #TAB# 'cpu', chromosome_plot_info['cpu']) #LINE# #TAB# if 'number' in chromosome_plot_info: #LINE# #TAB# #TAB# chromosome_plot_info['copy_number'] = ColumnDataSource(store, #LINE# #TAB# #TAB# #TAB# 'cpu', chromosome_plot_info['number']) #LINE# #TAB# if 'break_target' in chromosome_plot_info: #LINE# #TAB# #TAB# chromosome_plot_info['break_target'] = ColumnDataSource(store, #LINE# #TAB# #TAB# #TAB# 'cpu', chromosome_plot_info['break_target']) #LINE# #TAB# return chromosome_plot_info"
"#LINE# #TAB# if prameter_type == 'quadrature': #LINE# #TAB# #TAB# kwargs['quadrature'] = [flatten_backend_parameter(path, kwargs, param, prameter_type) #LINE# #TAB# #TAB# #TAB# ] #LINE# #TAB# else: #LINE# #TAB# #TAB# kwargs[param] = flatten_backend_parameter(path, kwargs, param, prameter_type) #LINE# #TAB# return kwargs"
#LINE# #TAB# hash_value = hashlib.md5(url.encode('utf-8')).hexdigest() #LINE# #TAB# filename = url.split('/')[-1] #LINE# #TAB# if postfix: #LINE# #TAB# #TAB# return filename + '.' + postfix #LINE# #TAB# else: #LINE# #TAB# #TAB# return filename
#LINE# #TAB# log.debug('set_owner: called') #LINE# #TAB# if entry: #LINE# #TAB# #TAB# log.debug('entry: {0}'.format(entry)) #LINE# #TAB# if username and not prompt: #LINE# #TAB# #TAB# username = getpass.getuser() #LINE# #TAB# if always_ask: #LINE# #TAB# #TAB# return entry #LINE# #TAB# password = getpass.getpass('Enter password: ') #LINE# #TAB# log.debug('set_owner: username: {0}'.format(username)) #LINE# #TAB# if not password: #LINE# #TAB# #TAB# return entry #LINE# #TAB# log.debug('set_owner: password: {0}'.format(password)) #LINE# #TAB# return password
"#LINE# #TAB# cmplp_fid = real_fid.to_bytes(4, byteorder='little') #LINE# #TAB# cmp_fid = cmp(cmpl_fid) #LINE# #TAB# cmp_med = cmp(cmpl_fid) #LINE# #TAB# if cmp_fid == -1: #LINE# #TAB# #TAB# return Decimal(0) #LINE# #TAB# elif cmp_fid == 0: #LINE# #TAB# #TAB# return Decimal(0) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1.0"
#LINE# #TAB# if version_info[0] < 3: #LINE# #TAB# #TAB# return value.decode('utf-8') #LINE# #TAB# return value
"#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.Ack(ImuMsgCode.DeleteTimeSet, AckCode.OK)) #LINE# #TAB# return packet"
#LINE# #TAB# if key not in _known_hosts: #LINE# #TAB# #TAB# raise KeyError('Unknown host: %s' % key) #LINE# #TAB# return _known_hosts[key]
"#LINE# #TAB# import re #LINE# #TAB# pattern = r'<(.*?)>' #LINE# #TAB# text = re.sub(pattern, '', text) #LINE# #TAB# match = re.search(pattern, text) #LINE# #TAB# if match: #LINE# #TAB# #TAB# text = match.group(1) #LINE# #TAB# if remove_url: #LINE# #TAB# #TAB# text = re.sub(pattern, '', text) #LINE# #TAB# return text"
"#LINE# #TAB# if isinstance(x, str) and re.match(MAC_PATTERN, x): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# ret = salt.utils.mac_utils.execute_return_result( #LINE# #TAB# #TAB#'systemsetup -extractprofilesonnetwork') #LINE# #TAB# return salt.utils.mac_utils.validate_enabled( #LINE# #TAB# #TAB# salt.utils.mac_utils.parse_return(ret)) == 'on'
"#LINE# #TAB# if hostname: #LINE# #TAB# #TAB# handler = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# #TAB# handler.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# handler.connect((hostname, 0)) #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return handler.getsockname()[0]"
"#LINE# #TAB# if not m: #LINE# #TAB# #TAB# raise ValueError('m must be non-empty') #LINE# #TAB# if g < 0: #LINE# #TAB# #TAB# raise ValueError('g must be a positive integer') #LINE# #TAB# if s > n: #LINE# #TAB# #TAB# raise ValueError('s must be > n') #LINE# #TAB# cipher = DamgardJurikEncrypter(m, g, s) #LINE# #TAB# else: #LINE# #TAB# #TAB# cipher = DamgardJurikEncrypter(m, g) #LINE# #TAB# return cipher.encrypt(m)[0:n]"
#LINE# #TAB# T = np.asarray(T) #LINE# #TAB# if T.ndim < 2: #LINE# #TAB# #TAB# T = T - T.sum(axis=1) #LINE# #TAB# return T
"#LINE# #TAB# my_scores = np.zeros((len(x2ys), n_trans)) #LINE# #TAB# for i in range(n_trans): #LINE# #TAB# #TAB# my_scores[i] = co_occurrences(x2ys[i], x2ys[i + 1], n_trans) #LINE# #TAB# return my_scores"
"#LINE# #TAB# y_pos = np.sum(sample_weight[y == 1]) #LINE# #TAB# y_neg = np.sum(sample_weight[y == -1]) #LINE# #TAB# intercept = np.mean(y_pos) #LINE# #TAB# tmp = -y / (1 + np.exp(-intercept * y)) #LINE# #TAB# tmp = _rescale_data_binomial(tmp, sample_weight) #LINE# #TAB# d_beta = safe_sparse_dot(X.T, tmp) / X.shape[0] #LINE# #TAB# G = len(np.unique(group_index)) #LINE# #TAB# tmle = 1 / np.max([np.linalg.norm(d_beta[np.where(group_index == g)[0]], #LINE# #TAB# #TAB# 2) for g in range(0, G)]) #LINE# #TAB# return intercept, d_beta, tmle"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ip = socket.gethostbyaddr(peer) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# return False #LINE# #TAB# if ip: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# d = {'name': colour.name, 'colour': colour.colour, 'w': colour.w, #LINE# #TAB# #TAB# 'h': colour.h} #LINE# #TAB# colours = [] #LINE# #TAB# for i in range(colour.ncols): #LINE# #TAB# #TAB# colours.append({'name': colours[i], 'colour': colour[i], #LINE# #TAB# #TAB# #TAB# 'w': colour[i + 1], 'h': colour[i + 2]}) #LINE# #TAB# return colours"
#LINE# #TAB# tag_model = get_model_for_model(item_type='tag') #LINE# #TAB# try: #LINE# #TAB# #TAB# item = tag_model.objects.get(pid=item_pid) #LINE# #TAB# except object_model.DoesNotExist: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# name = os.path.basename(filename) #LINE# #TAB# if name in ['.pyc', '.pyo']: #LINE# #TAB# #TAB# mtime = os.path.getmtime(filename) #LINE# #TAB# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# #TAB# mtime = os.path.getmtime(filename) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# elif name in ['.pyc', '.pyo']: #LINE# #TAB# #TAB# mtime = os.path.getmtime(filename) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None #LINE# #TAB# return mtime"
"#LINE# #TAB# curr_dir = os.path.dirname(nested_path) #LINE# #TAB# file_name = os.path.basename(curr_dir) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return run_func_in_dir(os.path.join(curr_dir, file_name)) #LINE# #TAB# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# #TAB# curr_dir = os.path.dirname(os.path.abspath(curr_dir)) #LINE# #TAB# #TAB# #TAB# raise"
"#LINE# #TAB# if isinstance(entry, dict): #LINE# #TAB# #TAB# used_by = dot_standard_by(remote, entry) #LINE# #TAB# elif isinstance(entry, list): #LINE# #TAB# #TAB# used_by = [dot_standard_by(remote, e) for e in entry] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Entry must be either a list, set, or a dict, not {}'. #LINE# #TAB# #TAB# #TAB# format(entry)) #LINE# #TAB# return {'used_by': used_by, 'by': entry}"
"#LINE# #TAB# masking_idx = np.where(m == masking_value, 0, masking_value) #LINE# #TAB# return masking_idx"
#LINE# #TAB# return p[0] >= rect[0] and p[1] <= rect[1] and p[0] <= rect[2] and p[1 #LINE# #TAB# #TAB# ] <= rect[3]
"#LINE# #TAB# return [reshape_child(x, pole, width) for x in np.array(vertices, dtype=np.float64 #LINE# #TAB# #TAB# )]"
"#LINE# #TAB# if is_module: #LINE# #TAB# #TAB# is_mod = False #LINE# #TAB# else: #LINE# #TAB# #TAB# is_mod = True #LINE# #TAB# good_word = word.lower() #LINE# #TAB# for bad_word in BAD_WORDS: #LINE# #TAB# #TAB# if word.find(bad_word) > -1: #LINE# #TAB# #TAB# #TAB# good_word = word.replace(bad_word, '_') #LINE# #TAB# if not good_word: #LINE# #TAB# #TAB# return word #LINE# #TAB# for bad_word in good_word: #LINE# #TAB# #TAB# word = word.replace(bad_word, '_') #LINE# #TAB# return word"
#LINE# #TAB# undeclared = set() #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# if name not in nodes: #LINE# #TAB# #TAB# #TAB# undeclared.add(name) #LINE# #TAB# #TAB# #TAB# nodes.remove(name) #LINE# #TAB# return undeclared
"#LINE# #TAB# if port == 443 or port == 80: #LINE# #TAB# #TAB# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return #LINE# #TAB# sleep(0.1) #LINE# #TAB# for i in range(logging.DEBUG): #LINE# #TAB# #TAB# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# time.sleep(0.1) #LINE# #TAB# return"
"#LINE# #TAB# ip_addresses = set() #LINE# #TAB# try: #LINE# #TAB# #TAB# resolved = dns.resolver.query(dns_name, dns.rdatatype.INET) #LINE# #TAB# #TAB# for address in resolved.values(): #LINE# #TAB# #TAB# #TAB# ip_addresses.add(address.address) #LINE# #TAB# except dns.resolver.NXDOMAIN: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ip_addresses"
#LINE# #TAB# if not options.height: #LINE# #TAB# #TAB# height = DEFAULT_HEIGHT #LINE# #TAB# elif options.height: #LINE# #TAB# #TAB# height = options.height #LINE# #TAB# return height
"#LINE# #TAB# additional = [] #LINE# #TAB# for seed in seeds: #LINE# #TAB# #TAB# if '|' in seed: #LINE# #TAB# #TAB# #TAB# user, url = seed.split('|') #LINE# #TAB# #TAB# #TAB# user = user.strip() #LINE# #TAB# #TAB# #TAB# if not user: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# additional.append(user) #LINE# #TAB# return additional"
"#LINE# #TAB# output = {} #LINE# #TAB# if os.path.exists(directory) and os.path.isfile(directory): #LINE# #TAB# #TAB# files = [os.path.join(directory, path) for path in os.listdir( #LINE# #TAB# #TAB# #TAB# directory) if os.path.isfile(os.path.join(directory, path))] #LINE# #TAB# #TAB# for filepath in files: #LINE# #TAB# #TAB# #TAB# sha256 = hashlib.sha256(open(filepath).read()).hexdigest() #LINE# #TAB# #TAB# #TAB# output.update({filepath: sha256}) #LINE# #TAB# return output"
#LINE# #TAB# if image.pixeltype!= 'unsigned char': #LINE# #TAB# #TAB# image = image.clone('unsigned char') #LINE# #TAB# idim = image.dimension #LINE# #TAB# vec = iio.ANTsVector(idim) #LINE# #TAB# vec.GetDataArray()[:] = image.data #LINE# #TAB# return vec
"#LINE# #TAB# try: #LINE# #TAB# #TAB# query = context.session.query(models.BNPSignature) #LINE# #TAB# #TAB# credential = query.filter_by(name=name).first() #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# LOG.info(_LI('no credential found with name: %s'), name) #LINE# #TAB# #TAB# return #LINE# #TAB# if not credential: #LINE# #TAB# #TAB# LOG.info(_LI('no credential found with name: %s'), name) #LINE# #TAB# #TAB# return #LINE# #TAB# return credential"
"#LINE# #TAB# out_dict = {} #LINE# #TAB# if base_dict: #LINE# #TAB# #TAB# out_dict.update(base_dict) #LINE# #TAB# if extra_dict: #LINE# #TAB# #TAB# for k, v in extra_dict.items(): #LINE# #TAB# #TAB# #TAB# out_dict[k] = encode_segment(v) #LINE# #TAB# return out_dict"
#LINE# #TAB# vault_ssh_client = vault_client.connect_to_vault() #LINE# #TAB# with vault_ssh_client.begin_nested(): #LINE# #TAB# #TAB# directives = vault_ssh_client.head_directives(opt) #LINE# #TAB# return directives
"#LINE# #TAB# id, version = get_id_n_version(ident_hash) #LINE# #TAB# stmt = _set_sql('set_input.sql') #LINE# #TAB# args = dict(id=id, version=version) #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# cursor.execute(stmt, args) #LINE# #TAB# #TAB# #TAB# db_conn.commit() #LINE# #TAB# return args"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# raw = get_raw_blast(pdb_id, chain_id, output_form) #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# return #LINE# #TAB# else: #LINE# #TAB# #TAB# blasts = parse_blast(raw, chain_id, output_form) #LINE# #TAB# #TAB# return blasts"
#LINE# #TAB# global _MATCH #LINE# #TAB# _MATCH = guess
#LINE# #TAB# data_stream.seek(start) #LINE# #TAB# for byte in data_stream: #LINE# #TAB# #TAB# if byte!= 128: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# fields = {'from': batch.sender, 'to': sorted(batch.recipients)} #LINE# #TAB# if batch.delivery_report: #LINE# #TAB# #TAB# fields['delivery_report'] = batch.delivery_report #LINE# #TAB# if batch.send_at: #LINE# #TAB# #TAB# fields['send_at'] = _write_datetime(batch.send_at) #LINE# #TAB# if batch.expire_at: #LINE# #TAB# #TAB# fields['expire_at'] = _write_datetime(batch.expire_at) #LINE# #TAB# if batch.tags: #LINE# #TAB# #TAB# fields['tags'] = sorted(batch.tags) #LINE# #TAB# return fields"
#LINE# #TAB# if power not in _kH2CO3_xml_map: #LINE# #TAB# #TAB# p = str(power) #LINE# #TAB# #TAB# if power in _kH2CO3_xml_map: #LINE# #TAB# #TAB# #TAB# return _kH2CO3_xml_map[power](wg) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# p = str(power) #LINE# #TAB# #TAB# #TAB# return _k2CO3_xml_map[power](wg) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0.001
"#LINE# #TAB# response_json = None #LINE# #TAB# if is_cached(url): #LINE# #TAB# #TAB# response = _get_cached(url) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# response_json = response.json() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# _write_cache(url, response_json) #LINE# #TAB# return response_json"
#LINE# #TAB# header = '' #LINE# #TAB# for key in dic: #LINE# #TAB# #TAB# header += str(key) +'' #LINE# #TAB# return header
"#LINE# #TAB# if module is None: #LINE# #TAB# #TAB# module = loadConfig() #LINE# #TAB# slot = loadSlot(name, module) #LINE# #TAB# matches = [] #LINE# #TAB# for slot in slot.slots: #LINE# #TAB# #TAB# if slot.name == name: #LINE# #TAB# #TAB# #TAB# matches.append(slot) #LINE# #TAB# return matches"
"#LINE# #TAB# body_backbone_names = [f.name for f in body_backbone_CNN] #LINE# #TAB# image_backbone_names = [f.name for f in image_backbone_CNN] #LINE# #TAB# weights_filenames = ['{}-weights.csv'.format(body_backbone_names[0]) for #LINE# #TAB# #TAB# body_backbone_name in body_backbone_names] #LINE# #TAB# csvlogger_filenames = ['{}-image-backbone-{}'.format(body_backbone_names[1], #LINE# #TAB# #TAB# image_backbone_names[0]) for image_backbone_name in image_backbone_names] #LINE# #TAB# return weights_filenames, csvlogger_filenames"
#LINE# #TAB# bytes_left = read_int(sock) #LINE# #TAB# bytes_right = read_int(sock) #LINE# #TAB# return bytes_left - bytes_right
"#LINE# #TAB# n, n = covmatrix.shape #LINE# #TAB# resid = np.linalg.norm(covmatrix, ord=2) #LINE# #TAB# corr = np.zeros((n, n)) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(n): #LINE# #TAB# #TAB# #TAB# resid[:, (i)] = resid[:, (j)] #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(n): #LINE# #TAB# #TAB# #TAB# resid[:, (j)] = resid[:, (i)] / np.linalg.norm(resid[:, (j)] #LINE# #TAB# #TAB# #TAB# #TAB# )[:, (i)] #LINE# #TAB# return corr"
#LINE# #TAB# out = {} #LINE# #TAB# for profile in pr_list: #LINE# #TAB# #TAB# out[profile] = profile_put(profile) #LINE# #TAB# return out
"#LINE# #TAB# default = default or _find_frame_property(entry, prop, raw=raw) #LINE# #TAB# if not default: #LINE# #TAB# #TAB# return default #LINE# #TAB# return entry[prop]"
"#LINE# #TAB# if datatype in (int, np.int8, np.uint8): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# #TAB# dtype = np.uint8 #LINE# #TAB# elif datatype in (float, np.float16, np.float32): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# #TAB# dtype = np.float32 #LINE# #TAB# elif datatype in (int, np.int16, np.int32): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# elif datatype in (np.float32, np.float64): #LINE# #TAB# #TAB# dbrcode = 2 #LINE# #TAB# #TAB# dtype = np.float64 #LINE# #TAB# return dbrcode, dtype"
"#LINE# #TAB# if mono: #LINE# #TAB# #TAB# n = int(np.ceil(n)) #LINE# #TAB# win = np.array([np.hanning(n) for i in range(n + 1)]) #LINE# #TAB# elif n == 0: #LINE# #TAB# #TAB# win = np.array([np.hanning(n) for i in range(0, n)]) #LINE# #TAB# else: #LINE# #TAB# #TAB# win = np.array([np.hanning(n) for i in range(0, n)]) #LINE# #TAB# return win"
#LINE# #TAB# if cls.lock_disabled: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, from_date=True, #LINE# #TAB# #TAB# archive=True, token_auth=False) #LINE# #TAB# group = parser.parser.add_argument_group('NNTP arguments') #LINE# #TAB# group.add_argument('-v', '--verbose', action='store_true', default=False) #LINE# #TAB# group.add_argument('--input-file', dest='input_file', help= #LINE# #TAB# #TAB# 'Path to the input file') #LINE# #TAB# group.add_argument('--output-file', dest='output_file', help= #LINE# #TAB# #TAB# 'Path to the output file') #LINE# #TAB# return parser"
#LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# yield sequence[index]
"#LINE# #TAB# global _VERSION #LINE# #TAB# if _VERSION is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), #LINE# #TAB# #TAB# #TAB# #TAB#'mOLGENIS_VERSION.txt'), 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# _VERSION = f.readline().strip() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return _VERSION"
"#LINE# #TAB# array = np.zeros((len(events), len(slots))) #LINE# #TAB# for row, event in enumerate(events): #LINE# #TAB# #TAB# for col, slot in enumerate(slots): #LINE# #TAB# #TAB# #TAB# array[row, col] = event.get_media_analysis(row, col).values #LINE# #TAB# return array"
"#LINE# #TAB# found = find_instance_load(name, import_) #LINE# #TAB# if found: #LINE# #TAB# #TAB# del found #LINE# #TAB# return found"
#LINE# #TAB# key = msg['msg_key'] #LINE# #TAB# try: #LINE# #TAB# #TAB# auth = cfg['profile']['auth'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return #LINE# #TAB# if key in cfg['profile']['auth']: #LINE# #TAB# #TAB# del cfg['profile']['auth'][key] #LINE# #TAB# try: #LINE# #TAB# #TAB# del cfg['profile']['auth'][key] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# path = os.path.join(_ROOT, 'items.json') #LINE# #TAB# items = read_json_file(path) #LINE# #TAB# version = items.get(item_name) #LINE# #TAB# if version: #LINE# #TAB# #TAB# return version #LINE# #TAB# else: #LINE# #TAB# #TAB# return '0.0'"
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# result_value = lower #LINE# #TAB# else: #LINE# #TAB# #TAB# if result_value > lower: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# ratio = (used / total) * 100 #LINE# #TAB# if round_ is not None: #LINE# #TAB# #TAB# ratio = round(ratio, round_) #LINE# #TAB# return ratio"
"#LINE# #TAB# if enabled: #LINE# #TAB# #TAB# return enabled #LINE# #TAB# if hasattr(sys, 'pypy_version_info') and sys.pypy_version_info < (2, 5 #LINE# #TAB# #TAB# ) and sys.pypy_version_info < (3, 5): #LINE# #TAB# #TAB# return False #LINE# #TAB# if enabled and sys.pypy_version_info < (3, 5): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not enabled and sys.pypy_version_info < (3, 5): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return next(modules) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# found_values = [] #LINE# #TAB# for key, value in six.iteritems(d): #LINE# #TAB# #TAB# if key == which_key: #LINE# #TAB# #TAB# #TAB# found_values.append(value) #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# found_values.extend(json_remove(value, which_key)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# found_values.append(value) #LINE# #TAB# return found_values"
#LINE# #TAB# meta = cls() #LINE# #TAB# meta.load(f) #LINE# #TAB# return meta
"#LINE# #TAB# u = get_current_user() #LINE# #TAB# if u is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif u.startswith('group:'): #LINE# #TAB# #TAB# return u.split(':', 1)[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# len_waves = len(waves) #LINE# #TAB# seq1 = waves[0] #LINE# #TAB# seq2 = waves[1:] #LINE# #TAB# for i in range(1, len_waves): #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# seq1 += gaps[i] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# seq2 += gaps[i] #LINE# #TAB# return seq1 - seq2"
#LINE# #TAB# dl = [] #LINE# #TAB# if not depends: #LINE# #TAB# #TAB# return data #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# org = depends.get(item) #LINE# #TAB# #TAB# if org: #LINE# #TAB# #TAB# #TAB# dl.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dl.append(item) #LINE# #TAB# return dl
"#LINE# #TAB# html = {} #LINE# #TAB# if ot is None: #LINE# #TAB# #TAB# ot = get_ot() #LINE# #TAB# params = {'search': search,'resultformat': resultformat, 'ot': ot} #LINE# #TAB# response = requests.get(INSPIRE_URL, params=params) #LINE# #TAB# if response.status_code == 200: #LINE# #TAB# #TAB# for entry in response.json(): #LINE# #TAB# #TAB# #TAB# html[entry['id']] = entry['text'] #LINE# #TAB# return html"
#LINE# #TAB# goea_sorted = goea_res[0] #LINE# #TAB# for goea_key in goea_res[1:]: #LINE# #TAB# #TAB# if goea_key < goea_res[2]: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if goea_res[2] > goea_res[1] and goea_res[2] < goea_res[3]: #LINE# #TAB# #TAB# #TAB# goea_sorted[0] = goea_res[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# goea_sorted[1] = goea_res[1] #LINE# #TAB# return goea_sorted
#LINE# #TAB# try: #LINE# #TAB# #TAB# response = client.list_buckets(Bucket=bucket_name) #LINE# #TAB# except botocore.exceptions.ClientError as e: #LINE# #TAB# #TAB# if e.response['Error']['Code'] == 'Forbidden': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
#LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# df['chars_' + col] = df[col] * 2 #LINE# #TAB# return df
"#LINE# #TAB# sdm = getsdm(sdmname) #LINE# #TAB# srcdict = {} #LINE# #TAB# decdict = {} #LINE# #TAB# for row in sdm['Field']: #LINE# #TAB# #TAB# src = str(row[1]) #LINE# #TAB# #TAB# dec = str(row[2]) #LINE# #TAB# #TAB# srcdict[src] = float(coord) #LINE# #TAB# #TAB# decdict[src] = float(coord) #LINE# #TAB# return srcdict, decdict"
"#LINE# #TAB# s = duration_to_string(duration) #LINE# #TAB# if s[0] in ('s','m', 'h'): #LINE# #TAB# #TAB# return '%02d' % s #LINE# #TAB# elif s[0] == 'd': #LINE# #TAB# #TAB# return '%02d' % s #LINE# #TAB# elif s[0] in ('w','m', 'h'): #LINE# #TAB# #TAB# return '%02d' % s #LINE# #TAB# elif s[0] =='s': #LINE# #TAB# #TAB# return '%02d' % s #LINE# #TAB# else: #LINE# #TAB# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# f = open(tags_dir(), 'r') #LINE# #TAB# #TAB# next(f) #LINE# #TAB# #TAB# return True #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# out = [] #LINE# #TAB# for x in s: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# out.append(int(x)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return out
"#LINE# #TAB# if not tea_slug: #LINE# #TAB# #TAB# tea_slug = settings.TEA_NAME #LINE# #TAB# for request in requested_additions: #LINE# #TAB# #TAB# if request in tea_slug.combinations: #LINE# #TAB# #TAB# #TAB# return ForbiddenCombinations( #LINE# #TAB# #TAB# #TAB# #TAB# request, tea_slug=tea_slug #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return None"
#LINE# #TAB# for path_item in sys.path: #LINE# #TAB# #TAB# if os.path.isdir(path_item): #LINE# #TAB# #TAB# #TAB# return 'runtime' #LINE# #TAB# #TAB# elif os.path.isfile(path_item): #LINE# #TAB# #TAB# #TAB# return 'original' #LINE# #TAB# #TAB# elif os.path.isfile(path_item): #LINE# #TAB# #TAB# #TAB# return 'frozen' #LINE# #TAB# return 'original'
"#LINE# #TAB# t0 = time.time() #LINE# #TAB# r = {} #LINE# #TAB# for ws in nb['worksheets']: #LINE# #TAB# #TAB# for cell in ws['cells']: #LINE# #TAB# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# output = cell.get('outputs', {}) #LINE# #TAB# #TAB# #TAB# #TAB# t0 = time.time() #LINE# #TAB# #TAB# #TAB# #TAB# r['outputs'] = output #LINE# #TAB# #TAB# #TAB# #TAB# t1 = time.time() #LINE# #TAB# #TAB# #TAB# #TAB# r['cells'].update({cell['cell_type']: {'source': 'code', 'target': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'code'}, 'timeout': t1}) #LINE# #TAB# return r"
"#LINE# #TAB# if not isinstance(dir_name, str): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not os.path.isdir(dir_name): #LINE# #TAB# #TAB# return False #LINE# #TAB# for f in os.listdir(dir_name): #LINE# #TAB# #TAB# if not f.startswith('/'): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# from..forms.converter import convert_form_field #LINE# #TAB# args = {} #LINE# #TAB# for name, filter_field in six.iteritems(filterset_class.base_filters): #LINE# #TAB# #TAB# field_type = convert_form_field(filter_field.field).Argument() #LINE# #TAB# #TAB# field_type.description = filter_field.label #LINE# #TAB# #TAB# args[name] = field_type #LINE# #TAB# return args"
#LINE# #TAB# default_value = [] #LINE# #TAB# for i in range(len(initialparameters)): #LINE# #TAB# #TAB# default_value.append(initialparameters[i]) #LINE# #TAB# return default_value
#LINE# #TAB# penn_tree = '' #LINE# #TAB# for element in tree.iter(): #LINE# #TAB# #TAB# if element.tag == 'P': #LINE# #TAB# #TAB# #TAB# penn_tree += '<P>' #LINE# #TAB# #TAB# elif element.tag == 'S': #LINE# #TAB# #TAB# #TAB# penn_tree += '<S>' #LINE# #TAB# #TAB# elif element.tag == 'K': #LINE# #TAB# #TAB# #TAB# penn_tree += '<K>' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# penn_tree += '<P>' #LINE# #TAB# return penn_tree
"#LINE# #TAB# for sLine in oLine.line.split(','): #LINE# #TAB# #TAB# if sLine.strip().startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# sLine = sLine.strip() #LINE# #TAB# #TAB# if '=' in sLine: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# ast.literal_eval(sLine.value) #LINE# #TAB# #TAB# #TAB# except (SyntaxError, ValueError): #LINE# #TAB# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# symbols = set() #LINE# #TAB# for paradigm in paradigms: #LINE# #TAB# #TAB# symbols.update(angular_encode_alphabet(paradigm)) #LINE# #TAB# return symbols
#LINE# #TAB# global _determine_thread #LINE# #TAB# if _determine_thread: #LINE# #TAB# #TAB# _determine_thread.kill() #LINE# #TAB# #TAB# _determine_thread = None
"#LINE# #TAB# result = { #LINE# #TAB# #TAB# 'current_step': current_step, #LINE# #TAB# #TAB#'steps': steps_per_epoch, #LINE# #TAB# #TAB#'steps_per_loop': steps_per_loop, #LINE# #TAB# } #LINE# #TAB# return result"
#LINE# #TAB# standard_tokens = [] #LINE# #TAB# for t in reserved_tokens: #LINE# #TAB# #TAB# if not t: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# parts = [t.strip()] #LINE# #TAB# #TAB# if not parts[0]: #LINE# #TAB# #TAB# #TAB# parts[0] =''.join(parts) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# regex = re.compile(t) #LINE# #TAB# #TAB# #TAB# standard_tokens.append(regex) #LINE# #TAB# return standard_tokens
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# path, tail = os.path.split(path) #LINE# #TAB# #TAB# new_path = os.path.join(path, tail) #LINE# #TAB# #TAB# if os.path.isdir(new_path): #LINE# #TAB# #TAB# #TAB# shutil.rmtree(new_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls._response_queue.get_latest(timeout=0) #LINE# #TAB# except Empty: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# normvectpart = np.sqrt(quat[0] ** 2 + quat[1] ** 2 + quat[2] ** 2 + #LINE# #TAB# #TAB# quat[3] ** 2) #LINE# #TAB# angle = np.arccos(quat[3] / normvectpart) * 2.0 #LINE# #TAB# unitvec = np.array(quat[:3]) / np.sin(angle / 2) / normvectpart #LINE# #TAB# return unitvec, angle"
"#LINE# #TAB# data = pd.read_table(filepath, names=['OTU', 'Taxonomy']) #LINE# #TAB# data = data.set_index('OTU') #LINE# #TAB# data = data.drop(['OTU', 'Taxonomy']) #LINE# #TAB# data.index = data.index.rename(None) #LINE# #TAB# data = data.sort_index() #LINE# #TAB# return data"
#LINE# #TAB# tags = [] #LINE# #TAB# for tag in ds.tags: #LINE# #TAB# #TAB# if tag[0] == 'PRIVATE': #LINE# #TAB# #TAB# #TAB# tags.append(tag[1]) #LINE# #TAB# return tags
#LINE# #TAB# swapped_contents = [] #LINE# #TAB# for o in oligo_dict.values(): #LINE# #TAB# #TAB# for line in contents: #LINE# #TAB# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# swapped_contents.append(oligo_dict[o]) #LINE# #TAB# for line in swapped_contents: #LINE# #TAB# #TAB# print(line) #LINE# #TAB# return swapped_contents
#LINE# #TAB# inverted = {} #LINE# #TAB# outer_keys = nested_dict.keys() #LINE# #TAB# inner_keys = nested_dict.keys() #LINE# #TAB# for k1 in outer_keys: #LINE# #TAB# #TAB# if k1 not in inner_keys: #LINE# #TAB# #TAB# #TAB# inverted[k1] = {} #LINE# #TAB# #TAB# #TAB# for k2 in inner_keys: #LINE# #TAB# #TAB# #TAB# #TAB# inverted[k2][k1] = inner_dict[k1][k2] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# inverted[k1] = nested_dict[k1] #LINE# #TAB# return inverted
#LINE# #TAB# data = get_events(game_id) #LINE# #TAB# for event in data: #LINE# #TAB# #TAB# event.disable() #LINE# #TAB# return data
#LINE# #TAB# status = data['headers']['status'] #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# Memory.leave_room = data['room']
"#LINE# #TAB# parser.add_argument('--os-image-api-version', metavar= #LINE# #TAB# #TAB# '<image-api-version>', default=utils.env('OS_IMAGE_API_VERSION' #LINE# #TAB# #TAB# ), help='Image API version, default=' + DEFAULT_API_VERSION + #LINE# #TAB# #TAB#'(Env: OS_IMAGE_API_VERSION)') #LINE# #TAB# return parser"
"#LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# box_list = pickle.load(f) #LINE# #TAB# #TAB# return box_list"
#LINE# #TAB# if value is not None: #LINE# #TAB# #TAB# value = value.strip() #LINE# #TAB# return value
"#LINE# #TAB# layer = {} #LINE# #TAB# with open(model_path, 'rb') as f: #LINE# #TAB# #TAB# model_data = pickle.load(f) #LINE# #TAB# for key in model_data.keys(): #LINE# #TAB# #TAB# layer[key] = model_data[key][key] #LINE# #TAB# return layer"
"#LINE# #TAB# col_names = archive_table.columns #LINE# #TAB# uid_clause = [ #LINE# #TAB# #TAB# sa.asc(getattr(archive_table, col_names[0]), col_names[1]) for col_names in col_names #LINE# #TAB# ] #LINE# #TAB# uid_clause.append(sa.asc(archive_table.version_id)) #LINE# #TAB# return uid_clause"
#LINE# #TAB# internal_categories = cls.internal_categories.all() #LINE# #TAB# for category in cls.internal_categories: #LINE# #TAB# #TAB# if category not in internal_categories: #LINE# #TAB# #TAB# #TAB# internal_categories.add(category) #LINE# #TAB# return internal_categories
"#LINE# #TAB# classes = {} #LINE# #TAB# fname = pkg_resources.resource_filename(__name__,'resources/Latitudes-Longitudes.csv') #LINE# #TAB# with open(fname, 'rU') as csvfile: #LINE# #TAB# #TAB# reader = csv.reader(csvfile, delimiter = ',') #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# if row[0]: #LINE# #TAB# #TAB# #TAB# #TAB# word = row[1] #LINE# #TAB# #TAB# #TAB# #TAB# if word not in classes: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# classes[word] = 1 #LINE# #TAB# return classes"
"#LINE# #TAB# keys, values = p #LINE# #TAB# new_dict = {} #LINE# #TAB# for k in keys: #LINE# #TAB# #TAB# new_dict[k] = combine_generalized_8(values, k) #LINE# #TAB# return new_dict"
"#LINE# #TAB# url = '{}scenarios/{}'.format(LIZARD_URL, scenario_uuid) #LINE# #TAB# headers = {'Content-Type': 'application/zip; charset=utf-8'} #LINE# #TAB# response = requests.put(url, headers=headers, data=dict(scenario_id= #LINE# #TAB# #TAB# scenario_uuid)) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# return response.headers['location']"
"#LINE# #TAB# assert len(params) == 1 #LINE# #TAB# params = params[0] #LINE# #TAB# params_ = {} #LINE# #TAB# for k, v in params.items(): #LINE# #TAB# #TAB# key = str(k).lower() #LINE# #TAB# #TAB# params_[key] = v #LINE# #TAB# params_str = '\n'.join(params_) #LINE# #TAB# doc_string = params_str.strip() #LINE# #TAB# if doc_string: #LINE# #TAB# #TAB# doc_string += '\n' #LINE# #TAB# return doc_string"
#LINE# #TAB# char_to_char = {} #LINE# #TAB# char_to_char['flagged'] = {} #LINE# #TAB# char_to_char['charge']['read'] = _align_charge #LINE# #TAB# char_to_char['flagged']['write'] = _align_charge #LINE# #TAB# char_to_char['char_to_char']['delete'] = _align_charge #LINE# #TAB# return char_to_char
"#LINE# #TAB# status, output = set_role_status(git_path) #LINE# #TAB# return output"
"#LINE# #TAB# bone_name, filename, extension = os.path.splitext(name) #LINE# #TAB# if topping == 0: #LINE# #TAB# #TAB# filename += '.txt' #LINE# #TAB# elif topping == 1: #LINE# #TAB# #TAB# filename += '.txt' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('%s is not a valid bone name.' % bone_name) #LINE# #TAB# return filename"
#LINE# #TAB# cls.verbose_session = True #LINE# #TAB# return cls
"#LINE# #TAB# if isinstance(attribute_set, str): #LINE# #TAB# #TAB# return attribute_set #LINE# #TAB# return attribute_set"
#LINE# #TAB# group = request.matchdict['group'] #LINE# #TAB# return group.name
#LINE# #TAB# p = request.json['action']['detailParameters'] #LINE# #TAB# return p
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# if use and isinstance(use, str): #LINE# #TAB# #TAB# use = [use] #LINE# #TAB# new_key = key.use(use, kid) #LINE# #TAB# new_key.kid = kid #LINE# #TAB# return new_key"
#LINE# #TAB# size = len(list1) #LINE# #TAB# total = 0.0 #LINE# #TAB# for i in range(size): #LINE# #TAB# #TAB# total += list1[i] * list2[i] #LINE# #TAB# return total
#LINE# #TAB# helper = {} #LINE# #TAB# for headerName in allHeadersContent: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# helper[headerName] = allHeadersContent[headerName]['interpret'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return True
"#LINE# #TAB# if cause_string: #LINE# #TAB# #TAB# cause_list = ','.join(cause_string.split(',')) #LINE# #TAB# #TAB# for cause in cause_list: #LINE# #TAB# #TAB# #TAB# queryset = queryset.filter(**{'%s__icontains' % cause: True}) #LINE# #TAB# return queryset"
"#LINE# #TAB# if row_content[-1] == row_content[0]: #LINE# #TAB# #TAB# start_idx = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# start_idx = len(row_content) - 1 #LINE# #TAB# #TAB# end_idx = len(row_content) - 2 #LINE# #TAB# #TAB# while start_idx < end_idx: #LINE# #TAB# #TAB# #TAB# if row_content[start_idx] == row_content[end_idx]: #LINE# #TAB# #TAB# #TAB# #TAB# start_idx = end_idx #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# end_idx = start_idx + 1 #LINE# #TAB# return start_idx, end_idx"
"#LINE# #TAB# reaction_id = REACTION_ID #LINE# #TAB# template = ( #LINE# #TAB# #TAB# ) #LINE# #TAB# template += ( #LINE# #TAB# #TAB# ) #LINE# #TAB# template += ( #LINE# #TAB# #TAB# ) #LINE# #TAB# template += ( #LINE# #TAB# #TAB# ) #LINE# #TAB# for fn in os.listdir(reaction_log_path): #LINE# #TAB# #TAB# path = os.path.join(reaction_log_path, fn) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.unlink(path) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# with cls._batcher_lock: #LINE# #TAB# #TAB# if not cls._versions_client: #LINE# #TAB# #TAB# #TAB# cls._versions_client = VersionClient() #LINE# #TAB# return cls._versions_client
"#LINE# #TAB# b0 = 0.0641 * 3 / 4 #LINE# #TAB# b1 = 2.255 * 3 / 4 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.0 * 3 / 2 ** (5 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Na'] * i2c['S2O3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
#LINE# #TAB# cmdline = [] #LINE# #TAB# for spec in intfspec: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cmdline.append(spec['name']) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return cmdline
#LINE# #TAB# try: #LINE# #TAB# #TAB# version = db.session.query(models.Version).filter_by(id=model_id).one()[0][0] #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# return None #LINE# #TAB# if len(version) == 1: #LINE# #TAB# #TAB# return version[0] #LINE# #TAB# return None
"#LINE# #TAB# number_of_elements = len(buffer) #LINE# #TAB# table = [] #LINE# #TAB# while number_of_elements > 0: #LINE# #TAB# #TAB# item_size = min(number_of_elements, 256) #LINE# #TAB# #TAB# field_size = min(number_of_elements, 256) #LINE# #TAB# #TAB# value = buffer[field_size] #LINE# #TAB# #TAB# if is_string: #LINE# #TAB# #TAB# #TAB# value = str(value) #LINE# #TAB# #TAB# table.append(value) #LINE# #TAB# #TAB# number_of_elements -= 1 #LINE# #TAB# return table"
"#LINE# #TAB# header = argparse.Namespace() #LINE# #TAB# header.add_argument('--debug', action='store_true', help= #LINE# #TAB# #TAB# 'enable debugging mode') #LINE# #TAB# header.add_argument('--loglevel', default='INFO', help= #LINE# #TAB# #TAB# 'log level for root') #LINE# #TAB# header.add_argument('--loglevel', default=None, help= #LINE# #TAB# #TAB# 'log level for root') #LINE# #TAB# arguments = header.add_arguments() #LINE# #TAB# for arg, value in arguments.items(): #LINE# #TAB# #TAB# if arg.startswith('-'): #LINE# #TAB# #TAB# #TAB# arg = '-' + arg #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# arg = arg.replace('_', '-') #LINE# #TAB# #TAB# arguments.add_argument(arg, value=value) #LINE# #TAB# return header"
"#LINE# #TAB# #TAB# checkpoint = os.path.join(checkpointDir,'model.checkpoint') #LINE# #TAB# #TAB# model = cls.readFromCheckpoint(checkpoint) #LINE# #TAB# #TAB# model.checkpoint = checkpoint #LINE# #TAB# #TAB# return model"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return os.kill(pid, 0) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# length = int(data.decode('utf-8'), 16) #LINE# #TAB# #TAB# return ProfileEncoder.encode(length) + command_data #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# raise"
#LINE# #TAB# indent = None #LINE# #TAB# for c in code.split('\n'): #LINE# #TAB# #TAB# if c == '\n': #LINE# #TAB# #TAB# #TAB# indent = indent + 4 #LINE# #TAB# #TAB# elif include_start: #LINE# #TAB# #TAB# #TAB# indent = indent + 1 #LINE# #TAB# #TAB# elif c == '\t': #LINE# #TAB# #TAB# #TAB# indent = indent - 2 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return indent
"#LINE# #TAB# host_filter = None #LINE# #TAB# analysis_dir = os.path.join(repo, 'fe.buildtimetrend') #LINE# #TAB# if os.path.exists(analysis_dir): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f = os.listdir(analysis_dir) #LINE# #TAB# #TAB# #TAB# if 'train' in f: #LINE# #TAB# #TAB# #TAB# #TAB# host_filter = 'train' #LINE# #TAB# #TAB# #TAB# elif 'api' in f: #LINE# #TAB# #TAB# #TAB# #TAB# host_filter = 'api' #LINE# #TAB# return host_filter"
"#LINE# #TAB# signature = inspect_signature(func) #LINE# #TAB# n_inputs = getattr(signature, 'n_inputs', None) #LINE# #TAB# if n_inputs is None: #LINE# #TAB# #TAB# n_inputs = 1 #LINE# #TAB# if n_inputs is not None: #LINE# #TAB# #TAB# for input_type in input_type: #LINE# #TAB# #TAB# #TAB# for i in range(n_inputs): #LINE# #TAB# #TAB# #TAB# #TAB# if input_type.name == input_type.name: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return signature #LINE# #TAB# return signature"
#LINE# #TAB# feed_list = [] #LINE# #TAB# for tag in html: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# feed = feed_list.append(tag.text) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return feed_list
#LINE# #TAB# _pool = None #LINE# #TAB# with _pool_lock: #LINE# #TAB# #TAB# if 'pool' in kwargs: #LINE# #TAB# #TAB# #TAB# _pool = kwargs['pool'] #LINE# #TAB# #TAB# #TAB# del kwargs['pool'] #LINE# #TAB# return _pool
"#LINE# #TAB# cols = dtable.columns(vid) #LINE# #TAB# cols = [col for col in cols if func(dtable, col) == expr] #LINE# #TAB# cols = [col for col in cols if col not in cols] #LINE# #TAB# a = np.zeros((len(cols), len(dtable[vid])) #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# a[col] = np.nan #LINE# #TAB# return a"
#LINE# #TAB# y = a / (k1 * (x - x1) + k2 * (x - x2)) #LINE# #TAB# return y
"#LINE# #TAB# s = _socket.create_connection(addr) #LINE# #TAB# try: #LINE# #TAB# #TAB# s.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, True) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# s.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, False) #LINE# #TAB# s.close() #LINE# #TAB# return s"
"#LINE# #TAB# new_row = { #LINE# #TAB# #TAB# 'id': row.id, #LINE# #TAB# #TAB# 'collection': collection.name, #LINE# #TAB# #TAB#'version': row.version, #LINE# #TAB# #TAB# 'created_at': row.created_at, #LINE# #TAB# #TAB# 'updated_at': row.updated_at, #LINE# #TAB# } #LINE# #TAB# return new_row"
#LINE# #TAB# try: #LINE# #TAB# #TAB# importlib.import_module(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# path = name.split('/')[-1] #LINE# #TAB# return path
#LINE# #TAB# perm = (j1 - j2) / (j1 + j2) #LINE# #TAB# return perm
"#LINE# #TAB# assert tp.is_protocol #LINE# #TAB# result = [] #LINE# #TAB# anytype = AnyType(tp) #LINE# #TAB# for member in tp.protocol_members: #LINE# #TAB# #TAB# typ = get_proper_type(member) #LINE# #TAB# #TAB# if not isinstance(typ, CallableType): #LINE# #TAB# #TAB# #TAB# if typ.is_callable(): #LINE# #TAB# #TAB# #TAB# #TAB# result.extend(find_executable_protocol_members(member)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# return result"
#LINE# #TAB# url = urlparse(path) #LINE# #TAB# if url.scheme!= 'file': #LINE# #TAB# #TAB# return f'{url.scheme}://{url.netloc}'
#LINE# #TAB# try: #LINE# #TAB# #TAB# return value == 1.0 #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# try: #LINE# #TAB# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# sock.bind(('localhost', port)) #LINE# #TAB# #TAB# return sock #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logging.error( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# h = np.zeros(3) #LINE# #TAB# c = np.zeros(3) #LINE# #TAB# for i, vector in enumerate(vectors): #LINE# #TAB# #TAB# x = vector[0] #LINE# #TAB# #TAB# y = vector[1] #LINE# #TAB# #TAB# z = vector[2] #LINE# #TAB# #TAB# h += np.cos(z) * h #LINE# #TAB# #TAB# c += np.sin(z) * h #LINE# #TAB# return c"
"#LINE# #TAB# cls.parser = argparse.ArgumentParser(add_help=False) #LINE# #TAB# cls.parser.add_argument('--sleep-time', dest='sleep_time', default= #LINE# #TAB# #TAB# DEFAULT_SLEEP_TIME, type=int, help='Sleep time in case of connection lost') #LINE# #TAB# return cls.parser"
"#LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# itksize = vnl_vector.size() #LINE# #TAB# shape = [itksize] #LINE# #TAB# pixelType = 'UL' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlUL._set_unicode_view_from_vnl_vector(vnl_vector) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_vector) #LINE# #TAB# return itk_view"
#LINE# #TAB# fmt = '' #LINE# #TAB# if log_is_console(): #LINE# #TAB# #TAB# if log_level == logging.INFO: #LINE# #TAB# #TAB# #TAB# fmt = 'INFO' #LINE# #TAB# #TAB# elif log_level == logging.WARNING: #LINE# #TAB# #TAB# #TAB# fmt = 'WARNING' #LINE# #TAB# return fmt
"#LINE# #TAB# parsedString = email.utils.parse.unquote(messageString) #LINE# #TAB# if parsedString == '': #LINE# #TAB# #TAB# return '' #LINE# #TAB# value = parsedString.split('<')[1] #LINE# #TAB# value = value.replace('>', '') #LINE# #TAB# value = value.replace('""', '') #LINE# #TAB# while value!= '': #LINE# #TAB# #TAB# value = value.replace('<', '') #LINE# #TAB# while value!= '': #LINE# #TAB# #TAB# if messageString.find(value)!= -1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return value"
"#LINE# #TAB# group_version_name = _group_version_name() #LINE# #TAB# if not group_version_name: #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(group_version_name, 'r') as f: #LINE# #TAB# #TAB# text = f.read() #LINE# #TAB# group_version_name = text.strip() #LINE# #TAB# return group_version_name"
"#LINE# #TAB# target_c = target #LINE# #TAB# rc = lib.SDL_Classify(mapper, connection, target_c) #LINE# #TAB# if rc == 0: #LINE# #TAB# #TAB# return target_c #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# retv = None #LINE# #TAB# for ksym in conn.keysymlist(): #LINE# #TAB# #TAB# if kstr.lower() in ksym.lower(): #LINE# #TAB# #TAB# #TAB# retv = conn.get_keycode(ksym) #LINE# #TAB# #TAB# #TAB# if retv is not None: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# elif kstr.lower() == ksym.lower(): #LINE# #TAB# #TAB# #TAB# retv = conn.get_keycode(ksym) #LINE# #TAB# if retv is not None: #LINE# #TAB# #TAB# return retv #LINE# #TAB# return None
#LINE# #TAB# out = [] #LINE# #TAB# for line in txt.split('\n'): #LINE# #TAB# #TAB# out.append(line) #LINE# #TAB# return out
"#LINE# #TAB# if isinstance(prefix, six.string_types) and isinstance(suffix, six. #LINE# #TAB# #TAB# string_types): #LINE# #TAB# #TAB# result = bytes(prefix, suffix) #LINE# #TAB# elif isinstance(prefix, bytes) and isinstance(suffix, bytes): #LINE# #TAB# #TAB# result = bytes(prefix, 'utf-16') #LINE# #TAB# else: #LINE# #TAB# #TAB# result = prefix #LINE# #TAB# return result"
"#LINE# #TAB# options = [(0, _('-- root --'))] #LINE# #TAB# for node in model.get_root_nodes(): #LINE# #TAB# #TAB# options += cls.preprocess_permissions_tree(node, for_node) #LINE# #TAB# return options"
"#LINE# #TAB# if location is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# local_location = location.get('location', None) #LINE# #TAB# if local_location is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# azimuth, altitude = local_location.get('azimuth', None), local_location.get( #LINE# #TAB# #TAB# 'altitude', None) #LINE# #TAB# if azimuth is not None: #LINE# #TAB# #TAB# azimuth = float(azimuth) #LINE# #TAB# elif altitude is not None: #LINE# #TAB# #TAB# altitude = float(altitude) #LINE# #TAB# body['wrapper_location'] = azimuth, altitude"
"#LINE# #TAB# script_location = os.path.abspath(os.path.dirname(sys.argv[0])) #LINE# #TAB# s3_path = os.path.join(script_location, name) #LINE# #TAB# if not os.path.isabs(s3_path): #LINE# #TAB# #TAB# s3_path = os.path.join(os.path.dirname(os.path.realpath(script_location)), s3_path) #LINE# #TAB# return s3_path"
"#LINE# #TAB# return cls.build_send_payload('setup_credentials_disabled', {'cacheDisabled': #LINE# #TAB# #TAB# cacheDisabled}), None"
"#LINE# #TAB# for root, subdirs, files in os.walk(dir): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# yield os.path.join(root, file) #LINE# #TAB# #TAB# for new_dir in subdirs: #LINE# #TAB# #TAB# #TAB# yield new_dir"
#LINE# #TAB# parts = name.split('_') #LINE# #TAB# model = models[parts[0]] #LINE# #TAB# for part in parts[1:]: #LINE# #TAB# #TAB# model = model._replace(part=part) #LINE# #TAB# return model
"#LINE# #TAB# canonical, https, httpswww = (domain.canonical, domain.https, domain. #LINE# #TAB# #TAB# httpswww) #LINE# #TAB# if canonical.host == '127.0.0.1': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# result = [] #LINE# #TAB# for _ in source_iter: #LINE# #TAB# #TAB# result.append(_) #LINE# #TAB# return result
"#LINE# #TAB# frame = inspect.currentframe().f_back.f_back #LINE# #TAB# while frame.f_globals.get(name, None) is not None: #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# if frame is not None: #LINE# #TAB# #TAB# if name == '__init__': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# s = __salt__['cmd.run_all'](pat) #LINE# #TAB# if isinstance(pat, six.string_types): #LINE# #TAB# #TAB# pat = re.compile(pat) #LINE# #TAB# for k in s: #LINE# #TAB# #TAB# if pat.search(k): #LINE# #TAB# #TAB# #TAB# yield k"
#LINE# #TAB# if apps_list is None: #LINE# #TAB# #TAB# return {} #LINE# #TAB# result = {} #LINE# #TAB# for app in apps_list: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result[app] = registry.get_app_config(app) #LINE# #TAB# #TAB# except LookupError: #LINE# #TAB# #TAB# #TAB# result[app] = {} #LINE# #TAB# return result
"#LINE# #TAB# print('keep_factor not called!') #LINE# #TAB# cur_file = open(os.path.join(os.getcwd(), 'cur_file.txt'), 'r') #LINE# #TAB# lines = cur_file.readlines() #LINE# #TAB# cur_file.close() #LINE# #TAB# os.chdir(os.path.dirname(os.path.abspath(cur_file))) #LINE# #TAB# os.chdir(os.path.dirname(os.path.abspath(__file__))) #LINE# #TAB# try: #LINE# #TAB# #TAB# c = open(os.path.join(os.getcwd(), 'c')) #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# c.write('\n'.join(line.split())) #LINE# #TAB# #TAB# c.close() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass"
#LINE# #TAB# new_user = UserClass() #LINE# #TAB# new_user.id = str(user.id) #LINE# #TAB# new_user.email = user.email #LINE# #TAB# new_user.first_name = user.first_name #LINE# #TAB# new_user.last_name = user.last_name #LINE# #TAB# return new_user
"#LINE# #TAB# thread = threading.Thread(parent=parent) #LINE# #TAB# menu = Menu(thread, worker) #LINE# #TAB# menu.show() #LINE# #TAB# thread.daemon = True #LINE# #TAB# thread.start() #LINE# #TAB# if deleteWorkerLater: #LINE# #TAB# #TAB# for obj in menu: #LINE# #TAB# #TAB# #TAB# obj.remove(worker) #LINE# #TAB# #TAB# del menu[obj] #LINE# #TAB# menu.show() #LINE# #TAB# return menu"
"#LINE# #TAB# template_dir = get_template_dir() #LINE# #TAB# index_file = os.path.join(template_dir, ""index"") #LINE# #TAB# quote_table = get_unquote_table(index_file, platform, command) #LINE# #TAB# data = [] #LINE# #TAB# for line in raw_output.splitlines(): #LINE# #TAB# #TAB# items = parse_command_line(line) #LINE# #TAB# #TAB# if items: #LINE# #TAB# #TAB# #TAB# index = items.index(index) #LINE# #TAB# #TAB# #TAB# quote_dict = parse_unquote_items(items) #LINE# #TAB# #TAB# #TAB# data.append(quote_dict) #LINE# #TAB# return data"
"#LINE# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# if key in tgt and isinstance(tgt[key], dict) and isinstance( #LINE# #TAB# #TAB# #TAB# value, list): #LINE# #TAB# #TAB# #TAB# string_sanitize_recursive(tgt[key], value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tgt[key] = value #LINE# #TAB# return tgt"
#LINE# #TAB# d = mass1 / mass2 / xi2 #LINE# #TAB# return d
#LINE# #TAB# alpha2 = alpha * (1 - eps) #LINE# #TAB# sign = np.sign(alpha) #LINE# #TAB# n = 0 #LINE# #TAB# for i in range(alpha2.size): #LINE# #TAB# #TAB# n += alpha2[i] * sign #LINE# #TAB# #TAB# if abs(sign) < eps / 2: #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# return n
"#LINE# #TAB# ConversionFinder) ->Dict[str, MultifileObjectParser]: #LINE# #TAB# parser_finder.set_finder(parser_finder) #LINE# #TAB# conversion_finder.set_finder(parser_finder) #LINE# #TAB# return parser_finder, conversion_finder"
"#LINE# #TAB# attrMap = e.getAttributeMap() #LINE# #TAB# if len(attrMap) > 0: #LINE# #TAB# #TAB# return {'status': 'error', 'errorMessage': attrMap[0]} #LINE# #TAB# if len(attrMap) == 1: #LINE# #TAB# #TAB# return {'status':'success', 'errorMessage': attrMap[1]} #LINE# #TAB# return {'status': 'error','message': e.getErrorMessage(), #LINE# #TAB# #TAB# 'errorMessage': e.getErrorMessage()}"
"#LINE# #TAB# with _tkinter_lock: #LINE# #TAB# #TAB# oldtitle = tkinter.Tk() #LINE# #TAB# #TAB# oldmessage = tkinter.Tk() #LINE# #TAB# #TAB# oldtitle.update(title) #LINE# #TAB# #TAB# oldmessage.update(message) #LINE# #TAB# #TAB# tkinter.destroy() #LINE# #TAB# #TAB# return oldtitle, oldmessage"
#LINE# #TAB# abs_number = str(number) #LINE# #TAB# if abs_number == 0: #LINE# #TAB# #TAB# return '++' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '-' + abs_number
#LINE# #TAB# summary = [0] * size_universe #LINE# #TAB# for set in sets: #LINE# #TAB# #TAB# for x in set: #LINE# #TAB# #TAB# #TAB# for y in x: #LINE# #TAB# #TAB# #TAB# #TAB# summary[y] += 1 #LINE# #TAB# return summary
"#LINE# #TAB# result = os_distribution_name() #LINE# #TAB# result = result.replace('/', '-') #LINE# #TAB# if result!= '': #LINE# #TAB# #TAB# result = result.replace('/', '_') #LINE# #TAB# return result"
"#LINE# #TAB# category = cls.search([('name', '=', uri), ('website', '=', request. #LINE# #TAB# #TAB# nereid_website.id)], limit=1) #LINE# #TAB# if not category and not silent: #LINE# #TAB# #TAB# raise RuntimeError('Category %s not found' % uri) #LINE# #TAB# return category[0] if category else None"
"#LINE# #TAB# for channel_win in sta_win.is_channel(): #LINE# #TAB# #TAB# yield channel_win, len(channel_win) #LINE# #TAB# yield sta_win"
#LINE# #TAB# if n > 1: #LINE# #TAB# #TAB# return a // n #LINE# #TAB# elif n == 1: #LINE# #TAB# #TAB# return a #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# lang = get_language() #LINE# #TAB# log.debug('language version: %s', lang) #LINE# #TAB# global LANGUAGE_VERSION #LINE# #TAB# LANGUAGE_VERSION = lang"
"#LINE# #TAB# if password is None or not is_password_usable(encoded): #LINE# #TAB# #TAB# return False #LINE# #TAB# preferred = get_hasher(preferred) #LINE# #TAB# hasher = identify_hasher(encoded) #LINE# #TAB# must_update = hasher.algorithm!= preferred.algorithm #LINE# #TAB# is_correct = hasher.verify(password, encoded) #LINE# #TAB# if setter and is_correct and must_update: #LINE# #TAB# #TAB# setter(password) #LINE# #TAB# return is_correct"
#LINE# #TAB# if request.authorization is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if request.authorization is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# global _initial_session #LINE# #TAB# _initial_session = None
"#LINE# #TAB# base = os.path.splitext(file)[0] #LINE# #TAB# out = {} #LINE# #TAB# with open(str(base), 'r') as f: #LINE# #TAB# #TAB# out = f.read() #LINE# #TAB# for line in out.split('\n'): #LINE# #TAB# #TAB# if line.startswith('-') or line.startswith('.'): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# i = 1 #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# out[i] = line[:i].strip() #LINE# #TAB# return out"
"#LINE# #TAB# result = {} #LINE# #TAB# if sys.platform.startswith('win'): #LINE# #TAB# #TAB# result['mode'] = 'rw' #LINE# #TAB# if sys.platform!= 'darwin': #LINE# #TAB# #TAB# return result #LINE# #TAB# try: #LINE# #TAB# #TAB# unicode(result['name']) #LINE# #TAB# #TAB# unicode(result['ips']) #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# return result #LINE# #TAB# for key, value in six.iteritems(result): #LINE# #TAB# #TAB# if not key.isalnum(): #LINE# #TAB# #TAB# #TAB# raise ValueError('invalid network key %r' % key) #LINE# #TAB# #TAB# if not value.isspace(): #LINE# #TAB# #TAB# #TAB# result[key] = value.decode('utf-8') #LINE# #TAB# return result"
"#LINE# #TAB# doc = requests.get(ing + '/comments', headers={'User-Agent': #LINE# #TAB# #TAB# 'Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10 (.NET CLR 4.0.20506)'}) #LINE# #TAB# images = [] #LINE# #TAB# for comment in doc.json()['Comments']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# image_id = comment['ImageId'] #LINE# #TAB# #TAB# #TAB# images.append(image_id) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# images.sort() #LINE# #TAB# return images"
#LINE# #TAB# with open(filepath) as json_file: #LINE# #TAB# #TAB# job = json.load(json_file) #LINE# #TAB# return job[property]
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('tmpdir', dtype='string', direction=function.OUT, #LINE# #TAB# #TAB# description='name of the temporary directory') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# Current value was set #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# Directory does not exist #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
"#LINE# #TAB# db = _connect_db(db_url=db_url, db_name=db_name) #LINE# #TAB# del db['_url'] #LINE# #TAB# del db['_name']"
#LINE# #TAB# flow = demux.Flow() #LINE# #TAB# sample = demux.Sample(sample_id) #LINE# #TAB# flow.Add(sample) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# flow.Remove(sample) #LINE# #TAB# #TAB# #TAB# flow.Add(sample) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# flow.Add(sample) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return flow
"#LINE# #TAB# return get_block_overview(block_representation=block_representation, #LINE# #TAB# #TAB# coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['hash']"
#LINE# #TAB# dF = Ft / Fo #LINE# #TAB# return dF
#LINE# #TAB# if request.user.has_password(password): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(field, list) and isinstance(field[0], CompoundList): #LINE# #TAB# #TAB# key_type = 'compound' #LINE# #TAB# #TAB# key_name = field[0] #LINE# #TAB# #TAB# index = None #LINE# #TAB# elif isinstance(field, tuple): #LINE# #TAB# #TAB# key_type, key_name = field #LINE# #TAB# #TAB# index = 1 #LINE# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# key_type = 'list' #LINE# #TAB# #TAB# key_name = field[0] #LINE# #TAB# return key_type, key_name, index"
#LINE# #TAB# results = [] #LINE# #TAB# for series in seriesList: #LINE# #TAB# #TAB# val = safeMin(series) #LINE# #TAB# #TAB# if val is not None and val >= n: #LINE# #TAB# #TAB# #TAB# results.append(series) #LINE# #TAB# return results
#LINE# #TAB# frames = [] #LINE# #TAB# stack = [job] #LINE# #TAB# while stack: #LINE# #TAB# #TAB# new_job = next(stack) #LINE# #TAB# #TAB# depth -= 1 #LINE# #TAB# #TAB# while new_job.depth < depth: #LINE# #TAB# #TAB# #TAB# new_job = next(stack) #LINE# #TAB# #TAB# #TAB# stack.append(new_job) #LINE# #TAB# #TAB# #TAB# job = new_job #LINE# #TAB# #TAB# if job not in cmdsets: #LINE# #TAB# #TAB# #TAB# frames.append(new_job) #LINE# #TAB# return frames
"#LINE# #TAB# left_padding, top_padding = metric_decode_extent(img1, padding) #LINE# #TAB# right_padding, bottom_padding = metric_decode_extent(img2, padding) #LINE# #TAB# img1 = img1[left_padding:right_padding, :] #LINE# #TAB# img2 = img2[left_padding:right_padding, :] #LINE# #TAB# return img1, img2"
#LINE# #TAB# if node1.resname == node2.resname: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif node1.positives == node2.positives: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# file_type = values.pop('file', None) #LINE# #TAB# if file_type is None: #LINE# #TAB# #TAB# return #LINE# #TAB# elif isinstance(file_type, list): #LINE# #TAB# #TAB# return [track_entity(v) for v in file_type] #LINE# #TAB# elif isinstance(file_type, dict): #LINE# #TAB# #TAB# return {k: track_entity(v) for k, v in file_type.items()} #LINE# #TAB# elif isinstance(file_type, tuple): #LINE# #TAB# #TAB# return {k: track_entity(v) for k, v in file_type}"
"#LINE# #TAB# session = generic_session(session) #LINE# #TAB# config = base_config(session) #LINE# #TAB# env_dict = {'BBNDK_CLIENT_ID': config['BBNDK_CLIENT_ID'], #LINE# #TAB# #TAB# 'BBNDK_CLIENT_SECRET': config['BBNDK_CLIENT_SECRET'], 'BBNDK_KAFKA': #LINE# #TAB# #TAB# config['BBNDK_KAFKA'], 'BBNDK_EVALUATION': config[ #LINE# #TAB# #TAB# 'BBNDK_EVALUATION']} #LINE# #TAB# return env_dict"
#LINE# #TAB# if a.base is not None: #LINE# #TAB# #TAB# return a.tobytes() #LINE# #TAB# return a
"#LINE# #TAB# for key in schema: #LINE# #TAB# #TAB# if isinstance(schema[key], list): #LINE# #TAB# #TAB# #TAB# schema[key] = pairs_fix_array(schema[key], path + [key]) #LINE# #TAB# #TAB# elif isinstance(schema[key], dict): #LINE# #TAB# #TAB# #TAB# for sub_path, sub_schema in pairs_fix_array(schema[key], path + [key]): #LINE# #TAB# #TAB# #TAB# #TAB# schema[sub_path] = pairs_fix_array(sub_schema, path + [key]) #LINE# #TAB# return schema"
"#LINE# in_float = float(in_int) #LINE# if in_float.bit_length() > 0: #LINE# #TAB# in_tensor = tf.expand_dims(tf.cast(in_float, dtype=tf.int32), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# new_dim_length, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# axis=0) #LINE# else: #LINE# #TAB# in_tensor = tf.expand_dims(tf.cast(in_float, dtype=tf.int32), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# new_dim_length, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# axis=0) #LINE# return in_tensor"
#LINE# #TAB# pattern = re.compile('(\\$\\{.*\\})$') #LINE# #TAB# new_class_name = class_name #LINE# #TAB# while pattern.search(new_class_name): #LINE# #TAB# #TAB# new_class_name = pattern.search(new_class_name).group() #LINE# #TAB# return new_class_name
"#LINE# #TAB# if np.isscalar(scale): #LINE# #TAB# #TAB# if scale == 0.0: #LINE# #TAB# #TAB# #TAB# scale = 1.0 #LINE# #TAB# #TAB# return scale #LINE# #TAB# elif isinstance(scale, np.ndarray): #LINE# #TAB# #TAB# if copy: #LINE# #TAB# #TAB# #TAB# scale = scale.copy() #LINE# #TAB# #TAB# scale[scale == 0.0] = 1.0 #LINE# #TAB# return scale"
"#LINE# #TAB# content = csv.reader(csvFile, dialect='excel-tab', quotechar='""') #LINE# #TAB# xml = '' #LINE# #TAB# for item in content: #LINE# #TAB# #TAB# xml += etree.tostring(item, pretty_print=True, encoding= #LINE# #TAB# #TAB# #TAB# 'UTF-8') #LINE# #TAB# return xml"
"#LINE# #TAB# if description: #LINE# #TAB# #TAB# return _vnl_repr_with_description(description) #LINE# #TAB# if resource: #LINE# #TAB# #TAB# return _vnl_repr_resource(resource, options) #LINE# #TAB# return ''"
#LINE# #TAB# if method in Lan.serviceTypeLookup.keys(): #LINE# #TAB# #TAB# return Lan.serviceTypeLookup[method] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# restrict_id = cmd.pop('restrict_id') #LINE# #TAB# if restrict_id is None: #LINE# #TAB# #TAB# return pad_id #LINE# #TAB# return restrict_id
"#LINE# #TAB# constants = np.zeros((len(chimerics), len(msa))) #LINE# #TAB# for i, c in enumerate(chimerics): #LINE# #TAB# #TAB# row = [(i + 1) for i in range(len(c))] #LINE# #TAB# #TAB# for j, k in enumerate(msa): #LINE# #TAB# #TAB# #TAB# if not k: #LINE# #TAB# #TAB# #TAB# #TAB# col[j] = np.nan #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# col[k] = np.nan #LINE# #TAB# #TAB# constants[row, col] = np.nan #LINE# #TAB# return constants"
#LINE# #TAB# while len(string) > 0 and string[-1].isdigit(): #LINE# #TAB# #TAB# string = string[:-1] #LINE# #TAB# return string
#LINE# #TAB# frag = cls() #LINE# #TAB# frag.content = pods['content'] #LINE# #TAB# frag._resources = [FragmentResource(**d) for d in pods['resources']] #LINE# #TAB# frag.js_init_fn = pods['js_init_fn'] #LINE# #TAB# frag.js_init_version = pods['js_init_version'] #LINE# #TAB# frag.json_init_args = pods['json_init_args'] #LINE# #TAB# return frag
#LINE# #TAB# try: #LINE# #TAB# #TAB# output = check_output(pip_cmd +'--help') #LINE# #TAB# #TAB# packages = output.split('\n') #LINE# #TAB# except CalledProcessError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# outdated = [] #LINE# #TAB# for package in packages: #LINE# #TAB# #TAB# if package.installed: #LINE# #TAB# #TAB# #TAB# outdated.append(package.name) #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# gbp.log('Removing outdated package: {0}'.format(package)) #LINE# #TAB# return outdated
"#LINE# #TAB# client_files= [] #LINE# #TAB# for group_name in groups: #LINE# #TAB# #TAB# client_files.extend(glob.glob(folder+""/*.*"")) #LINE# #TAB# return client_files"
"#LINE# #TAB# for i, data in enumerate(data_iterator): #LINE# #TAB# #TAB# if net(data): #LINE# #TAB# #TAB# #TAB# i = data.argmax() #LINE# #TAB# return i"
"#LINE# #TAB# title_patterns = {} #LINE# #TAB# with open(os.path.join(os.path.dirname(__file__),'resources', #LINE# #TAB# #TAB# 'title_patterns.txt'), 'r') as file: #LINE# #TAB# #TAB# for line in file: #LINE# #TAB# #TAB# #TAB# pattern, _ = line.strip().partition('\t') #LINE# #TAB# #TAB# #TAB# title_patterns[pattern] = re.compile(pattern) #LINE# #TAB# return title_patterns"
#LINE# #TAB# for host in tree.xpath('//host'): #LINE# #TAB# #TAB# if host.attrib['cve'] == cve: #LINE# #TAB# #TAB# #TAB# yield host
"#LINE# #TAB# from django.conf import settings #LINE# #TAB# email_list = [] #LINE# #TAB# for email in settings.EMAILS: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# user = email_list.pop(email, None) #LINE# #TAB# #TAB# except (KeyError, IndexError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if not user: #LINE# #TAB# #TAB# #TAB# email_list.append(email) #LINE# #TAB# return email_list"
"#LINE# #TAB# cls_name = name.split('.')[-1] #LINE# #TAB# module_name = '.'.join(cls_name.split('.')[:-1]) #LINE# #TAB# cls = get_module(module_name) #LINE# #TAB# while cls is not None: #LINE# #TAB# #TAB# cls = getattr(cls, cls_name) #LINE# #TAB# return cls"
"#LINE# #TAB# if s.endswith('.gz'): #LINE# #TAB# #TAB# f = gzip.open(s, 'rb') #LINE# #TAB# #TAB# return f.read() #LINE# #TAB# return s"
"#LINE# #TAB# tree = ET.parse(stats_xml) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# total_tokens = int(root.find('size/total/tokens').text) #LINE# #TAB# blinded_tokens = int(root.find('blinded/tokens').get('blinded_tokens') #LINE# #TAB# #TAB# ) if total_tokens > blinded_tokens else 0 #LINE# #TAB# stats_xml = tree.getroot() #LINE# #TAB# return stats_xml, blinded_tokens"
"#LINE# #TAB# N = 2 ** 32 #LINE# #TAB# call = np.zeros(N, dtype=np.float64) #LINE# #TAB# call[:, :, (0)] = np.sin(0.5 * np.pi * call[:, :, (1)]) #LINE# #TAB# call[:, :, (2)] = np.cos(0.5 * np.pi * call[:, :, (2)]) #LINE# #TAB# call[:, :, (3)] = np.sin(0.5 * np.pi * call[:, :, (3)]) #LINE# #TAB# call[:, :, (4)] = np.sin(0.5 * np.pi * call[:, :, (1)]) #LINE# #TAB# call[:, :, (5)] = np.cos(0.5 * np.pi * call[:, :, (4)]) #LINE# #TAB# return call"
#LINE# #TAB# v = actor.velocity() #LINE# #TAB# w = v[0] #LINE# #TAB# if w == 0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# theta = v[1] #LINE# #TAB# if theta < 0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# v /= 1000.0 #LINE# #TAB# return v
#LINE# #TAB# try: #LINE# #TAB# #TAB# return RequestInfo._public_get #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# info = RequestInfo() #LINE# #TAB# #TAB# info.request = request #LINE# #TAB# #TAB# return info
"#LINE# #TAB# global _selected_ids #LINE# #TAB# res = list() #LINE# #TAB# stack = _selected_ids #LINE# #TAB# for obj in stack: #LINE# #TAB# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# #TAB# for key in obj.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# if obj[key] =='real': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# res.append(obj) #LINE# #TAB# #TAB# _selected_ids = res #LINE# #TAB# return res"
#LINE# #TAB# #TAB# security = set() #LINE# #TAB# #TAB# for package in include_packages: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# security.add(package) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return security
#LINE# #TAB# summary_string = '' #LINE# #TAB# for command_summary in command_summaries: #LINE# #TAB# #TAB# summary_string += command_summary #LINE# #TAB# return summary_string
"#LINE# #TAB# if response and response.headers: #LINE# #TAB# #TAB# for k, v in response.headers: #LINE# #TAB# #TAB# #TAB# if k.lower() == 'content-type': #LINE# #TAB# #TAB# #TAB# #TAB# return 'text/yaml' #LINE# #TAB# #TAB# #TAB# elif k.lower() == 'content-transfer-encoding': #LINE# #TAB# #TAB# #TAB# #TAB# return 'text/plain' #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return 'text/yaml' #LINE# #TAB# return 'text/yaml'"
"#LINE# #TAB# return cls.build_send_payload('delete_tris_breakpoint', {'eventName': #LINE# #TAB# #TAB# eventName}), None"
"#LINE# #TAB# for role in model.permissions.all(): #LINE# #TAB# #TAB# if not issubclass(role.model, perm_model.model): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if role.model == perm_model.model: #LINE# #TAB# #TAB# #TAB# yield role"
#LINE# #TAB# if 'AT' not in seq: #LINE# #TAB# #TAB# return None #LINE# #TAB# at_content = seq.find('AT') #LINE# #TAB# if at_content == -1: #LINE# #TAB# #TAB# return None #LINE# #TAB# at_content = at_content.group('AT') #LINE# #TAB# return at_content
"#LINE# #TAB# e = [] #LINE# #TAB# e += sanitize_blocks_arith_sub(arg1, arg2) #LINE# #TAB# e += [arg1, arg2] #LINE# #TAB# return e"
#LINE# #TAB# logger = logging.getLogger('genmodel.netutils.cidr_authorized') #LINE# #TAB# try: #LINE# #TAB# #TAB# info = get_client().inspect_container(node) #LINE# #TAB# except NotFound: #LINE# #TAB# #TAB# logger.warning('No container found') #LINE# #TAB# #TAB# return False #LINE# #TAB# return info.get('State') == 'running'
#LINE# #TAB# node = select(key) #LINE# #TAB# node.parent = None #LINE# #TAB# return node
"#LINE# #TAB# if scheme =='stretch': #LINE# #TAB# #TAB# zeroStr = 'tab_long(%d)' % len(stretchDim) #LINE# #TAB# #TAB# image = imageClass(zeroStr) #LINE# #TAB# else: #LINE# #TAB# #TAB# size = imageClass.getDimensions() #LINE# #TAB# #TAB# image = imageClass.crop((stretchDim[0], stretchDim[1], stretchDim[2])) #LINE# #TAB# return image"
#LINE# #TAB# if success is True: #LINE# #TAB# #TAB# return 'SUCCESS' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'FAILED'
"#LINE# #TAB# file_ext = '.'.join(file_ext.split('.')[:-1]) #LINE# #TAB# file_name = file_name.split('.')[-1] #LINE# #TAB# config = {'file_name': file_name, 'ext': file_ext, 'data': data} #LINE# #TAB# if len(config['file_name']) > 0: #LINE# #TAB# #TAB# config['file_name'] = file_name #LINE# #TAB# return config"
"#LINE# #TAB# if 't108_rand_name.py' in filename: #LINE# #TAB# #TAB# res = rand_name_at_end_of_rand_name_re.search(logical_line) #LINE# #TAB# #TAB# if res: #LINE# #TAB# #TAB# #TAB# yield 0, 'T108: Remove hyphen after rand_name()'"
#LINE# #TAB# fname = hparams.train_script_args #LINE# #TAB# if hparams.save_pretrained: #LINE# #TAB# #TAB# fname += '_save' #LINE# #TAB# if hparams.delete_inference: #LINE# #TAB# #TAB# fname += '_delete' #LINE# #TAB# return fname
#LINE# #TAB# protection = parse_course_url(course_url) #LINE# #TAB# if protection: #LINE# #TAB# #TAB# return protection.url #LINE# #TAB# return None
"#LINE# #TAB# task_def = apidef.TaskDef(task_id=cls.task_id, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# name=cls.name, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# api_version=cls.api_version, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# collection_name=cls.collection_name, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# params=cls.params, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# allow_empty=True) #LINE# #TAB# return task_def"
#LINE# #TAB# if len(pn1)!= len(pn2): #LINE# #TAB# #TAB# return False #LINE# #TAB# for x in pn1: #LINE# #TAB# #TAB# if x!= pn2[x]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# global root_id #LINE# #TAB# root_id = str(seed) #LINE# #TAB# return root_id
"#LINE# #TAB# if isinstance(transaction, cls): #LINE# #TAB# #TAB# return transaction.jsonjson_helper_to_be_applied #LINE# #TAB# elif isinstance(transaction, dict): #LINE# #TAB# #TAB# return any(cls.jsonjson_helper_to_be_applied(item) for item in transaction.values()) #LINE# #TAB# elif isinstance(transaction, tuple): #LINE# #TAB# #TAB# return any(cls.jsonjson_helper_to_be_applied(item) for item in transaction #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return False"
"#LINE# #TAB# seq_names = [] #LINE# #TAB# for _ in range(num_seqs): #LINE# #TAB# #TAB# start = start_at #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# name = base_name + str(start) #LINE# #TAB# #TAB# #TAB# if name not in ignored: #LINE# #TAB# #TAB# #TAB# #TAB# seq_names.append(name) #LINE# #TAB# #TAB# #TAB# #TAB# name = '%s_%d' % (base_name, start) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# seq_names.append(name) #LINE# #TAB# return seq_names"
#LINE# #TAB# if a.code < b.code: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif a.code > b.code: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# psi_data = {} #LINE# #TAB# for key in element_map: #LINE# #TAB# #TAB# out_data = element_map[key].copy() #LINE# #TAB# #TAB# out_data['str'] = str(uuid.uuid4()) #LINE# #TAB# #TAB# psi_data[key] = out_data #LINE# #TAB# return psi_data
#LINE# #TAB# #TAB# if update: #LINE# #TAB# #TAB# #TAB# return json.loads(cls.game_release_version_list) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return cls.game_release_version_list
"#LINE# #TAB# item_units = [] #LINE# #TAB# for polymorphic_input in morfs: #LINE# #TAB# #TAB# for word in polymorphic_input: #LINE# #TAB# #TAB# #TAB# item_units.append(CodeUnit(word, file_locator=file_locator)) #LINE# #TAB# return item_units"
#LINE# #TAB# old = payload #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# if old: #LINE# #TAB# #TAB# #TAB# payload = old
"#LINE# #TAB# fingerprint = '' #LINE# #TAB# with open(filename, 'r') as fp: #LINE# #TAB# #TAB# header = fp.readline().strip() #LINE# #TAB# #TAB# for line in fp.splitlines(): #LINE# #TAB# #TAB# #TAB# if line.startswith('#') or line == '': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# fields = line.split() #LINE# #TAB# #TAB# #TAB# fingerprint = ', '.join(fields[1:]) #LINE# #TAB# return fingerprint"
#LINE# #TAB# global _header_copy_buffer_fix #LINE# #TAB# old_maxblock = _header_copy_buffer_fix #LINE# #TAB# _header_copy_buffer_fix = maxblock #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# _header_copy_buffer_fix = old_maxblock
"#LINE# #TAB# if context is None: #LINE# #TAB# #TAB# context = get_global_context() #LINE# #TAB# llvmir = _encode_string(llvmir) #LINE# #TAB# with ffi.OutputString() as errmsg: #LINE# #TAB# #TAB# mod = ModuleRef(ffi.lib.LLVMPY_GetModuleFileName(context, llvmir)) #LINE# #TAB# #TAB# if errmsg: #LINE# #TAB# #TAB# #TAB# mod.close() #LINE# #TAB# #TAB# #TAB# raise RuntimeError('LLVM IR validation error\n{0}'.format(errmsg)) #LINE# #TAB# return mod"
"#LINE# #TAB# y = np.atleast_1d(y) #LINE# #TAB# yr = np.atleast_1d(yr) #LINE# #TAB# psd = np.atleast_1d(psd) #LINE# #TAB# out = np.zeros((len(y),len(yr))) #LINE# #TAB# for i in range(len(yr)): #LINE# #TAB# #TAB# out[i] = y[i] * psd[i] / (yr[i] - yr[i]) #LINE# #TAB# return out"
#LINE# #TAB# total = 0 #LINE# #TAB# edges = nxg.edges(data=True) #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if 'obliquity' in edge[2]: #LINE# #TAB# #TAB# #TAB# total += edge[2]['cost'] #LINE# #TAB# return total
"#LINE# #TAB# src = getattr(settings, 'FUNC_PATH', None) #LINE# #TAB# if src is not None: #LINE# #TAB# #TAB# return src.split(os.path.sep) #LINE# #TAB# for path in sys.path: #LINE# #TAB# #TAB# if path.startswith(settings.STATIC_ROOT): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for f in os.listdir(path): #LINE# #TAB# #TAB# #TAB# fname = os.path.join(path, f) #LINE# #TAB# #TAB# #TAB# if fname.startswith('__'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if fname.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield fname"
"#LINE# #TAB# if include_expired: #LINE# #TAB# #TAB# query = security_select_block_names_query( cur, current_block ) #LINE# #TAB# else: #LINE# #TAB# #TAB# query = security_select_block_names( cur, current_block ) #LINE# #TAB# n_names = 0 #LINE# #TAB# for query in query: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n_names += len(query[0].unique()) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return n_names"
"#LINE# #TAB# res_label = list(residue[0])[0][2] #LINE# #TAB# atom_labels = {x[2] for x in itertools.chain( #LINE# #TAB# #TAB# *residue[1].values())} #LINE# #TAB# if (all(x in atom_labels for x in ['N', 'CA', 'C', 'O'])) and (all(x in #LINE# #TAB# #TAB# atom_labels for x in ['N', 'C', 'O'])) and (all(x!= 'N' for x in #LINE# #TAB# #TAB# residue[1].values())[0]): #LINE# #TAB# #TAB# if res_label in ['N', 'C', 'O']: #LINE# #TAB# #TAB# #TAB# res_label = 'N' + res_label #LINE# #TAB# return res_label"
#LINE# #TAB# global _base_from #LINE# #TAB# _base_from = obj
#LINE# #TAB# url_patterns = [] #LINE# #TAB# for route in current_app.url_map.itervalues(): #LINE# #TAB# #TAB# url_patterns += route.url_patterns #LINE# #TAB# return url_patterns
#LINE# #TAB# obj_description = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open('README.rst') as f: #LINE# #TAB# #TAB# #TAB# obj_description = f.read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return obj_description
"#LINE# #TAB# try: #LINE# #TAB# #TAB# query_string = event.context['request']['url'] #LINE# #TAB# #TAB# event.context['request']['url']['queryString'] = urllib.parse.unquote( #LINE# #TAB# #TAB# #TAB# query_string, errors='replace') #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return event"
#LINE# #TAB# #TAB# data_size = len(data) #LINE# #TAB# #TAB# return cls.PACK_INT(data_size) + cls.PACK_FLOAT(data[0]) + data[1 #LINE# #TAB# #TAB# #TAB# ]
#LINE# #TAB# if SharedInstance.instance is None: #LINE# #TAB# #TAB# clear_cache() #LINE# #TAB# #TAB# SharedInstance.instance = DialogNoseInstance() #LINE# #TAB# return SharedInstance.instance
#LINE# #TAB# events = {} #LINE# #TAB# dom = minidom.parseString(xmlcontent) #LINE# #TAB# for event in dom.getElementsByTagName('event'): #LINE# #TAB# #TAB# if event.nodeName == 'event': #LINE# #TAB# #TAB# #TAB# event = event.childNodes[0] #LINE# #TAB# #TAB# elif event.nodeName == 'content': #LINE# #TAB# #TAB# #TAB# event = event.childNodes[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# event = event.childNodes[0] #LINE# #TAB# #TAB# events[event.nodeName] = event #LINE# #TAB# return events
#LINE# #TAB# decodeDict = {} #LINE# #TAB# for key in h5: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# decodeDict[key] = opts_decodeDictDictDict[key].value #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# if readH5pyDataset: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return decodeDict
#LINE# #TAB# dem_df = pd.read_csv(str(dem_pth)) #LINE# #TAB# start_score = 0 #LINE# #TAB# for i in range(row.shape[1]): #LINE# #TAB# #TAB# val = row[i][0] #LINE# #TAB# #TAB# if val > 0: #LINE# #TAB# #TAB# #TAB# start_score += abs(val - dem_adjustment) * (1 - val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# start_score += abs(val - dem_adjustment) * (1 - val) #LINE# #TAB# return start_score
#LINE# #TAB# signal[np.isnan(signal)] = np.nan #LINE# #TAB# return signal
"#LINE# #TAB# schema_files = glob.glob(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# DIRPATH_SCHEMAS + '*')) #LINE# #TAB# schema_files.sort() #LINE# #TAB# return schema_files"
"#LINE# #TAB# energy_matrix = np.zeros((size, size)) #LINE# #TAB# try: #LINE# #TAB# #TAB# smooth_factor = float(smooth_factor) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValueError('smooth_factor must be a float') #LINE# #TAB# for index in range(0, size): #LINE# #TAB# #TAB# energy_matrix[index, index] = moving_dist_cost(size, #LINE# #TAB# #TAB# #TAB# smooth_factor) #LINE# #TAB# return energy_matrix"
#LINE# #TAB# pattern = '' #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# pattern += line #LINE# #TAB# return pattern
"#LINE# #TAB# if not isinstance(colnames, list): #LINE# #TAB# #TAB# colnames = [colnames] #LINE# #TAB# new_rows = [] #LINE# #TAB# for row in rows: #LINE# #TAB# #TAB# new_row = PseudoNamedTupleRow(colnames, row) #LINE# #TAB# #TAB# new_rows.append(new_row) #LINE# #TAB# return new_rows"
#LINE# #TAB# keys = list(attributes_set) #LINE# #TAB# for trace in log: #LINE# #TAB# #TAB# if 'type' in trace and 'event' in trace[0]: #LINE# #TAB# #TAB# #TAB# for attr in attributes_set: #LINE# #TAB# #TAB# #TAB# #TAB# if attr not in keys: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# attributes_set.add(attr) #LINE# #TAB# return attributes_set
"#LINE# #TAB# if residue is None: #LINE# #TAB# #TAB# res_name = name + '_r' #LINE# #TAB# else: #LINE# #TAB# #TAB# res_name = residue + '_' + name #LINE# #TAB# rnum = generator_number(name, res_name) #LINE# #TAB# if not rnum: #LINE# #TAB# #TAB# return '0' #LINE# #TAB# return rnum"
"#LINE# #TAB# account = get_object_or_404(Site, pk=accountid) #LINE# #TAB# if not request.user.can_access(account): #LINE# #TAB# #TAB# raise PermDeniedException() #LINE# #TAB# fname = get_creds_filename(account) #LINE# #TAB# if not os.path.exists(fname): #LINE# #TAB# #TAB# raise ModoboaException(_('No document available for this user')) #LINE# #TAB# content = decrypt_file(fname) #LINE# #TAB# if param_tools.get_global_parameter('delete_first_dl'): #LINE# #TAB# #TAB# os.remove(fname) #LINE# #TAB# resp = HttpResponse(content) #LINE# #TAB# resp['Content-Type'] = 'application/pdf' #LINE# #TAB# resp['Content-Length'] = len(content) #LINE# #TAB# resp['Content-Disposition'] = build_header(os.path.basename(fname)) #LINE# #TAB# return resp"
"#LINE# #TAB# if ctx is None: #LINE# #TAB# #TAB# ctx = NameCollector() #LINE# #TAB# collector = NameCollector(tree, ctx) #LINE# #TAB# return collector.collect"
"#LINE# #TAB# command_args_package_name = [cls.executable, '-f', file_path, 'Package'] #LINE# #TAB# command_args_package_version = [cls.executable, '-f', file_path, #LINE# #TAB# #TAB# '-v', file_path, 'Version'] #LINE# #TAB# package_name = CM.run_command_check_output(command_args_package_name #LINE# #TAB# #TAB# ).decode('utf-8') #LINE# #TAB# package_version = CM.run_command_check_output( #LINE# #TAB# #TAB# command_args_package_version #LINE# #TAB# ).decode('utf-8') #LINE# #TAB# socket_info = PackageInfo() #LINE# #TAB# socket_info.package = package_name #LINE# #TAB# socket_info.version = package_version #LINE# #TAB# return socket_info"
"#LINE# #TAB# l = np.asarray(l) #LINE# #TAB# n = len(l) #LINE# #TAB# if t == 0: #LINE# #TAB# #TAB# x = np.arange(1, n) #LINE# #TAB# else: #LINE# #TAB# #TAB# x = np.exp(l / t) #LINE# #TAB# y = np.zeros(n) #LINE# #TAB# shift = n / 2 #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(j): #LINE# #TAB# #TAB# #TAB# if l[i] > x[j]: #LINE# #TAB# #TAB# #TAB# #TAB# y[j] = x[j] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# y[j] = y[j] - shift #LINE# #TAB# return y"
"#LINE# #TAB# roles = [] #LINE# #TAB# status = client.status() #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# roles = status.get('roles') #LINE# #TAB# else: #LINE# #TAB# #TAB# roles = status.get('account', {}).get('roles') #LINE# #TAB# #TAB# if roles is None: #LINE# #TAB# #TAB# #TAB# roles = status.get('roles') #LINE# #TAB# #TAB# if not roles: #LINE# #TAB# #TAB# #TAB# roles = status.get('roles') #LINE# #TAB# #TAB# return roles #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# password = getpass.getpass('Enter the password: ') #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# print('Passwords do not exist.') #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return password
#LINE# #TAB# for i in range(32): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(x) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break
"#LINE# #TAB# df = df.copy() #LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# s = [s] #LINE# #TAB# filtered_df = df[df.columns.str.contains(s)] #LINE# #TAB# return filtered_df"
"#LINE# #TAB# output = [] #LINE# #TAB# for key, val in param.items(): #LINE# #TAB# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# #TAB# output.append(key +'='+ val) #LINE# #TAB# #TAB# elif isinstance(val, list): #LINE# #TAB# #TAB# #TAB# output.extend(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output.append(str(val)) #LINE# #TAB# return output"
#LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += calculate_size_data(new_value) #LINE# #TAB# return data_size
"#LINE# #TAB# filename, file_extension = os.path.splitext(fname) #LINE# #TAB# protocol = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(fname, 'rb') as f: #LINE# #TAB# #TAB# #TAB# data = f.read(1024) #LINE# #TAB# #TAB# #TAB# if data[0:4] == b'--': #LINE# #TAB# #TAB# #TAB# #TAB# protocol = b'--' + data[4:] #LINE# #TAB# #TAB# #TAB# elif b'\x00' in data: #LINE# #TAB# #TAB# #TAB# #TAB# protocol = b'--' + b'\x00' #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return protocol"
"#LINE# #TAB# engineio_server.routes[engineio_endpoint] = app #LINE# #TAB# if engineio_server.method == 'GET': #LINE# #TAB# #TAB# engineio_server.url = engineio_endpoint #LINE# #TAB# elif engineio_server.method == 'POST': #LINE# #TAB# #TAB# engineio_endpoint = engineio_server #LINE# #TAB# app.router.add_route('GET', view_func=engineio_request_handler, #LINE# #TAB# #TAB# name=engineio_endpoint) #LINE# #TAB# app.router.add_route('POST', view_func=engineio_request_handler, #LINE# #TAB# #TAB# name=engineio_endpoint) #LINE# #TAB# return app"
#LINE# #TAB# strings = [] #LINE# #TAB# for string in string_set: #LINE# #TAB# #TAB# strings.append(string) #LINE# #TAB# return strings
#LINE# #TAB# for i in range(len(my_list)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# my_list[i] = my_element.value() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return my_list
"#LINE# #TAB# if isinstance(dico, dict): #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# for key in dico: #LINE# #TAB# #TAB# #TAB# out[dico[key]] = key #LINE# #TAB# else: #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# freq = sorted(dico.values(), key=operator.itemgetter(1)) #LINE# #TAB# #TAB# for item in freq: #LINE# #TAB# #TAB# #TAB# out[item] = key #LINE# #TAB# #TAB# return out"
#LINE# #TAB# if str1.count('.') > 1: #LINE# #TAB# #TAB# return str1.split('.')[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return str1
#LINE# #TAB# if options.input: #LINE# #TAB# #TAB# sys.stdin.write('>') #LINE# #TAB# #TAB# sys.stdout.flush() #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string #LINE# #TAB# if options.input: #LINE# #TAB# #TAB# for string in options.input: #LINE# #TAB# #TAB# #TAB# yield string #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string
#LINE# #TAB# domain_pages = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(links): #LINE# #TAB# #TAB# link = [] #LINE# #TAB# #TAB# while len(link) < 10: #LINE# #TAB# #TAB# #TAB# link.append(links[i]) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# domain_pages.append(link) #LINE# #TAB# return domain_pages
"#LINE# #TAB# ""for json output add a full stop if ends in et al"" #LINE# #TAB# json_value = None #LINE# #TAB# for child in soup: #LINE# #TAB# #TAB# if child.parent.name == ""validation"": #LINE# #TAB# #TAB# #TAB# json_value = child #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if json_value: #LINE# #TAB# #TAB# return json_value #LINE# #TAB# return None"
"#LINE# #TAB# joystick_c = unbox(joystick, 'SDL_Joystick *') #LINE# #TAB# return len(joystick_c) + 1"
"#LINE# #TAB# response = Resource(response) #LINE# #TAB# if isinstance(response, ResourceList): #LINE# #TAB# #TAB# return ResourceList(response) #LINE# #TAB# if isinstance(response, dict): #LINE# #TAB# #TAB# return Resource(**response) #LINE# #TAB# return response"
"#LINE# #TAB# values = [] #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return values #LINE# #TAB# for prop, value in value.items(): #LINE# #TAB# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# if value.lower() == 'true': #LINE# #TAB# #TAB# #TAB# #TAB# values.append(True) #LINE# #TAB# #TAB# #TAB# elif value.lower() == 'false': #LINE# #TAB# #TAB# #TAB# #TAB# values.append(False) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# values.append((prop, value)) #LINE# #TAB# return values"
#LINE# #TAB# if require: #LINE# #TAB# #TAB# globals()['validate_generic'] = XsdValidator #LINE# #TAB# else: #LINE# #TAB# #TAB# globals()['validate_generic'] = XsdValidator #LINE# #TAB# return globals()['validate_generic']
#LINE# #TAB# shape_strings = None #LINE# #TAB# if id_list == [chid]: #LINE# #TAB# #TAB# shape_strings = set_shape_strings(chid) #LINE# #TAB# elif id_list == [chid]: #LINE# #TAB# #TAB# for path in set_shape_strings(chid): #LINE# #TAB# #TAB# #TAB# if path.startswith('ENUM'): #LINE# #TAB# #TAB# #TAB# #TAB# shape_strings = list(path[len('ENUM'):]) #LINE# #TAB# return shape_strings
"#LINE# #TAB# return {'type': id_type, 'value': id_value, 'quality': quality}"
"#LINE# #TAB# if attrs is not None: #LINE# #TAB# #TAB# attributes = dict(attrs) #LINE# #TAB# #TAB# for key, value in iteritems(attrs): #LINE# #TAB# #TAB# #TAB# if not key.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError('invalid attribute name: %s' % key) #LINE# #TAB# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# #TAB# attributes[key] = value.strip() #LINE# #TAB# return attributes"
"#LINE# #TAB# params_ = copy.deepcopy(params) #LINE# #TAB# for k, v in params.items(): #LINE# #TAB# #TAB# if k.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# param_name = k.replace('_', '-') #LINE# #TAB# #TAB# params[param_name] = v #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# params[param_name] = configure_schema(v) #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# for item in v: #LINE# #TAB# #TAB# #TAB# #TAB# params[param_name] = configure_schema(item) #LINE# #TAB# return params_"
"#LINE# #TAB# if not 'type' in data_dict: #LINE# #TAB# #TAB# raise ValueError(f""'{type_name}' not found in dict."") #LINE# #TAB# if type_name not in data_dict.keys(): #LINE# #TAB# #TAB# raise ValueError(f""'{type_name}' not found in dict."") #LINE# #TAB# return data_dict[type_name]"
"#LINE# #TAB# threshold = abs(threshold) #LINE# #TAB# position = 0 #LINE# #TAB# for position, basescore in enumerate(scores): #LINE# #TAB# #TAB# if basescore >= threshold: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return position, 0"
"#LINE# #TAB# model = KNeighborsClassifier(n_components=1) #LINE# #TAB# model.read(fname) #LINE# #TAB# lookup_table = dict() #LINE# #TAB# for i, entry in enumerate(model.predict_proba): #LINE# #TAB# #TAB# if entry['n_components'] == 0: #LINE# #TAB# #TAB# #TAB# key = list(entry['lookup_table']) #LINE# #TAB# #TAB# #TAB# lookup_table[key] = np.array(entry['lookup_table']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# key = list(entry['n_components'][i])[0] #LINE# #TAB# #TAB# #TAB# lookup_table[key] = np.array(entry['lookup_table']) #LINE# #TAB# return model, lookup_table"
"#LINE# #TAB# outfile = os.path.join(outdir, 'plot') #LINE# #TAB# os.makedirs(outfile, exist_ok=True) #LINE# #TAB# return outfile"
"#LINE# #TAB# if isinstance(value, string_types): #LINE# #TAB# #TAB# return value #LINE# #TAB# value = urllib.quote(value) #LINE# #TAB# if not value.startswith('/'): #LINE# #TAB# #TAB# url = urlparse.urljoin('s3://', value) #LINE# #TAB# #TAB# return url.scheme + '://' + url.netloc + value.path #LINE# #TAB# return value"
"#LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# ret = f.read() #LINE# #TAB# #TAB# return ret #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return toidpic(txt) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return txt
"#LINE# #TAB# result = [] #LINE# #TAB# while len(content): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj, content = berdecoder(content) #LINE# #TAB# #TAB# #TAB# result.append(obj) #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return result"
"#LINE# #TAB# count = stdio.readInt() #LINE# #TAB# a = create_yaml_d(count, None) #LINE# #TAB# for row in a: #LINE# #TAB# #TAB# for col in range(count): #LINE# #TAB# #TAB# #TAB# a[row][col] = stdio.readInt() #LINE# #TAB# return a"
"#LINE# #TAB# out_file = os.path.join(tmp_dir, 'case_corpora.pkl') #LINE# #TAB# if not os.path.exists(out_file): #LINE# #TAB# #TAB# os.mkdir(out_file) #LINE# #TAB# with tarfile.open(out_file, 'r') as in_handle: #LINE# #TAB# #TAB# for f in in_handle: #LINE# #TAB# #TAB# #TAB# if f.suffix == '.pkl': #LINE# #TAB# #TAB# #TAB# #TAB# with open(f, 'wb') as out_handle: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# out_handle.write(f.read()) #LINE# #TAB# return out_file"
"#LINE# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# val = val.split() #LINE# #TAB# return val"
"#LINE# #TAB# d1 = (left_bytes[0:2] - left_bytes[2:4]) ** 2 + (right_bytes[0:2] - right_bytes[2:4]) ** 2 #LINE# #TAB# d2 = 0 #LINE# #TAB# for i in range(0, len(left_bytes)): #LINE# #TAB# #TAB# d1 += d1 * (right_bytes[i] - left_bytes[i]) #LINE# #TAB# #TAB# d2 += d1 * (left_bytes[i] - right_bytes[i]) ** 2 #LINE# #TAB# return d1, d2"
"#LINE# #TAB# values = card.values.copy() #LINE# #TAB# values.pop('currency', None) #LINE# #TAB# values.pop('value', None) #LINE# #TAB# return {'type': 'credit_card', 'title': card.card_title, 'value': #LINE# #TAB# #TAB# card.value, 'amount': card.amount, 'currency': card. #LINE# #TAB# #TAB# currency, 'amount_remaining': card.amount_remaining,'#LINE# #TAB# #TAB#'remaining_time': card.remaining_time if card.remaining_time else #LINE# #TAB# #TAB# 0, 'details': {'card_type': 'credit', 'currency': card. #LINE# #TAB# #TAB# currency, 'amount_remaining': card.amount_remaining or 0}, 'details': #LINE# #TAB# #TAB# values}"
"#LINE# #TAB# with open(os.devnull, 'wb') as devnull: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if sys.version_info < (3, 5): #LINE# #TAB# #TAB# #TAB# #TAB# proc = subprocess.Popen(['lambda_check', '-L'], stdout= #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# devnull, stderr=subprocess.STDOUT) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# proc = subprocess.Popen(['lambda_check', '-L'], stdout= #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# devnull, stderr=subprocess.STDOUT) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# log.warning('Short Lambda check not available on this OS.') #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
#LINE# #TAB# global kernel_fast_otass #LINE# #TAB# if ota is None: #LINE# #TAB# #TAB# ota = get_otas()[0] #LINE# #TAB# kernel_fast_otass = {} #LINE# #TAB# if ota is not None: #LINE# #TAB# #TAB# kernel_fast_otass['ota'] = ota #LINE# #TAB# return kernel_fast_otass
"#LINE# #TAB# entities = [] #LINE# #TAB# text = text.replace('&', '&amp;') #LINE# #TAB# for part in text.split(' '): #LINE# #TAB# #TAB# if part[0]!= '&': #LINE# #TAB# #TAB# #TAB# entities.append(part[1:]) #LINE# #TAB# #TAB# elif len(part) > 1: #LINE# #TAB# #TAB# #TAB# entities.append(part[1:]) #LINE# #TAB# for entity in entities: #LINE# #TAB# #TAB# if entity[0]!= '&': #LINE# #TAB# #TAB# #TAB# entities.append(entity) #LINE# #TAB# return entities"
#LINE# #TAB# #TAB# cls._status = None #LINE# #TAB# #TAB# pass
"#LINE# #TAB# mime_lock = get_lock_mime() #LINE# #TAB# if lock is not None: #LINE# #TAB# #TAB# with lock.acquire(): #LINE# #TAB# #TAB# #TAB# content = lock.read() #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# content.decode('utf-8') #LINE# #TAB# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# #TAB# log.warning('Unable to upload lock: %s', str(lock)) #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# ref_target = nodes.reference(target_id, env, lineno) #LINE# #TAB# ref_target.attr = 'config' #LINE# #TAB# return ref_target"
"#LINE# #TAB# if not hasattr(obj, 'pk'): #LINE# #TAB# #TAB# raise Exception('{} has no pk'.format(repr(obj))) #LINE# #TAB# d = {} #LINE# #TAB# for k, v in obj.__dict__.items(): #LINE# #TAB# #TAB# if isinstance(v, (list, tuple)): #LINE# #TAB# #TAB# #TAB# v = ','.join([table_encode_dict(x) for x in v]) #LINE# #TAB# #TAB# elif not isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# d[k] = table_encode_dict(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d[k] = v #LINE# #TAB# return d"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return fn._dt_generator #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(vals, (list, tuple, set)): #LINE# #TAB# #TAB# return all(isinstance(v, str) for v in vals) #LINE# #TAB# elif isinstance(vals, (tuple, set)): #LINE# #TAB# #TAB# return all(isinstance(v, str) for v in vals) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if link: #LINE# #TAB# #TAB# y_pred = y * coef #LINE# #TAB# #TAB# deviance = _forward_surface_deviance(family, X, y, weights) #LINE# #TAB# #TAB# deriv = -y_pred #LINE# #TAB# else: #LINE# #TAB# #TAB# deviance = _forward_surface_deviance(family, X, y, weights) #LINE# #TAB# #TAB# deriv = -y_pred #LINE# #TAB# return deviance, deriv"
"#LINE# #TAB# if asse_null_type_re.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'SL317: assertEqual(type(A), B) sentences not allowed'"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ip = ipaddress.ip_address(u'' + ip_address) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if ip.is_loopback or ip.is_link_local or ip.is_loopback: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# result = {} #LINE# #TAB# for logger in logging.Logger.manager.loggerDict.values(): #LINE# #TAB# #TAB# if isinstance(logger, logging.Logger): #LINE# #TAB# #TAB# #TAB# region = logger.getLogger('region') #LINE# #TAB# #TAB# #TAB# if region: #LINE# #TAB# #TAB# #TAB# #TAB# result[region] = [] #LINE# #TAB# #TAB# #TAB# result[region].append(logger.getLogger('name').rsplit(':')[0]) #LINE# #TAB# return result"
#LINE# #TAB# for name in path.split('/'): #LINE# #TAB# #TAB# if name.endswith('.ipynb'): #LINE# #TAB# #TAB# #TAB# return name[:-4] #LINE# #TAB# notebook_name = path.split('/')[-1] #LINE# #TAB# if len(notebook_name) > 2: #LINE# #TAB# #TAB# if notebook_name[0].endswith('.py'): #LINE# #TAB# #TAB# #TAB# notebook_name = notebook_name[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return notebook_name
"#LINE# #TAB# output = value #LINE# #TAB# if isinstance(value, (list, tuple)): #LINE# #TAB# #TAB# for i, _ in enumerate(value): #LINE# #TAB# #TAB# #TAB# output = fmt_str(i, max_width) #LINE# #TAB# elif isinstance(value, datetime): #LINE# #TAB# #TAB# output = fmt_datetime(value, max_width) #LINE# #TAB# elif isinstance(value, date): #LINE# #TAB# #TAB# output = fmt_date(value, max_width) #LINE# #TAB# elif max_width and len(output) > max_width: #LINE# #TAB# #TAB# output = fmt_str(output, max_width) #LINE# #TAB# return output"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(yaml_generate(section, option, raise_exception, default)) #LINE# #TAB# except (yaml.YAMLError, yaml.YAMLError) as exc: #LINE# #TAB# #TAB# if raise_exception and default is None: #LINE# #TAB# #TAB# #TAB# raise exc #LINE# #TAB# #TAB# return default"
#LINE# #TAB# bit_arr = [] #LINE# #TAB# for i in range(len(arr)): #LINE# #TAB# #TAB# for j in range(8): #LINE# #TAB# #TAB# #TAB# bit_arr.append(arr[i][j]) #LINE# #TAB# return bit_arr
"#LINE# #TAB# if not retries: #LINE# #TAB# #TAB# retries = 1 #LINE# #TAB# #TAB# retries = 3 #LINE# #TAB# conn = mysql.connect(host=host, db=db, user=user, passwd=password, #LINE# #TAB# #TAB# timeout=retries) #LINE# #TAB# cursor = conn.cursor() #LINE# #TAB# cursor.execute('PRAGMA foreign_keys = ON') #LINE# #TAB# cur = conn.cursor() #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = cur.fetchone() #LINE# #TAB# #TAB# #TAB# cur.close() #LINE# #TAB# #TAB# except (mysql.OperationalError, mysql.DatabaseError): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield data #LINE# #TAB# #TAB# conn.close() #LINE# #TAB# #TAB# retries = 0"
"#LINE# #TAB# inputs = [] #LINE# #TAB# for key, value in example_command.items(): #LINE# #TAB# #TAB# for input_name, input_value in six.iteritems(value): #LINE# #TAB# #TAB# #TAB# parts = input_name.split(':') #LINE# #TAB# #TAB# #TAB# if len(parts) == 2 and parts[0] == 'input': #LINE# #TAB# #TAB# #TAB# #TAB# inputs.append({'name': parts[0], 'value': input_value}) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# inputs.append({'name': key, 'value': str(value)}) #LINE# #TAB# return inputs"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ca = apps.get_app_config(app_name) #LINE# #TAB# except LookupError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# try: #LINE# #TAB# #TAB# c = ca.get_model_config(model_name) #LINE# #TAB# except LookupError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if c.get('content_type') not in _VALID_CONTENT_TYPES: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# return _VALID_CONTENT_TYPES[c.get('content_type')]
#LINE# #TAB# try: #LINE# #TAB# #TAB# c = app_name.split('.')[-1] #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# try: #LINE# #TAB# #TAB# c = c.split('/')[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# try: #LINE# #TAB# #TAB# c = c.split('/')[-1] #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if len(c)!= 2: #LINE# #TAB# #TAB# raise Http404
"#LINE# #TAB# try: #LINE# #TAB# #TAB# app_name = str(env.config['Ravello']['application']) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# app_name = 'ravello' #LINE# #TAB# app = ravello.applications.get(app_name, None) #LINE# #TAB# if not app: #LINE# #TAB# #TAB# app = ravello.applications.get(__name__, None) #LINE# #TAB# return app"
"#LINE# #TAB# error_code = getattr(error, 'code', None) #LINE# #TAB# if error_code is not None: #LINE# #TAB# #TAB# if error.code == 500: #LINE# #TAB# #TAB# #TAB# message = 'Internal Server Error' #LINE# #TAB# #TAB# elif error.code == 500: #LINE# #TAB# #TAB# #TAB# message = 'Internal Server Error' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# message = 'Unknown Error' #LINE# #TAB# #TAB# raise Exception(message) from error_code"
"#LINE# #TAB# if direction == 'origin': #LINE# #TAB# #TAB# name = '{}-origin{}'.format(name, uuid.uuid4().hex) #LINE# #TAB# elif direction == 'destination': #LINE# #TAB# #TAB# name = '{}-destination{}'.format(name, uuid.uuid4().hex) #LINE# #TAB# elif direction =='source': #LINE# #TAB# #TAB# name = '{}-source{}'.format(name, uuid.uuid4().hex) #LINE# #TAB# elif direction == 'dest': #LINE# #TAB# #TAB# name = '{}-dest{}'.format(name, uuid.uuid4().hex) #LINE# #TAB# return name"
"#LINE# #TAB# dst_ns = int((extent[2] - extent[0])/res + 0.99) #LINE# #TAB# dst_nl = int((extent[3] - extent[1])/res + 0.99) #LINE# #TAB# out = gdal.GetDataset(dst_ns, dst_nl, dtype) #LINE# #TAB# out[:] = dst_nl #LINE# #TAB# return out"
"#LINE# #TAB# dict1 = {} #LINE# #TAB# dict2 = {} #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for i, line in enumerate(f): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# dict1[line[0]] = line[1] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# dict2[line[0]] = line[0] #LINE# #TAB# indices = {} #LINE# #TAB# colnames = dict1 #LINE# #TAB# for i, name in enumerate(dict1): #LINE# #TAB# #TAB# colnames[name] = name #LINE# #TAB# #TAB# indices[name] = i #LINE# #TAB# return dict1, colnames, indices"
"#LINE# #TAB# eigv = [] #LINE# #TAB# eigs = [] #LINE# #TAB# for ind, entry in enumerate(esys_mapdata): #LINE# #TAB# #TAB# if entry[0] == 0: #LINE# #TAB# #TAB# #TAB# eigv.append(entry[1]) #LINE# #TAB# #TAB# elif entry[0] == 1: #LINE# #TAB# #TAB# #TAB# eigv.append(entry[2]) #LINE# #TAB# #TAB# elif entry[0] == 2: #LINE# #TAB# #TAB# #TAB# eigv.append(entry[1]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# eigv.append(entry[0]) #LINE# #TAB# eigenvalues = np.array(eigv) #LINE# #TAB# eigs = np.array(eigs) #LINE# #TAB# return eigenvalues, eigs"
#LINE# #TAB# global _MANAGER_CACHE #LINE# #TAB# if not _MANAGER_CACHE: #LINE# #TAB# #TAB# ret = {} #LINE# #TAB# #TAB# device = salt.utils.napalm.get_device(__opts__) #LINE# #TAB# #TAB# if not device: #LINE# #TAB# #TAB# #TAB# return ret #LINE# #TAB# #TAB# if proxy and salt.utils.napalm.is_proxy(device): #LINE# #TAB# #TAB# #TAB# device = salt.utils.napalm.get_device(device) #LINE# #TAB# #TAB# ret = device.grains #LINE# #TAB# #TAB# if not salt.utils.napalm.is_minion(device): #LINE# #TAB# #TAB# #TAB# ret = {} #LINE# #TAB# #TAB# _MANAGER_CACHE[device] = ret #LINE# #TAB# return _MANAGER_CACHE
#LINE# #TAB# if regex in enabled_regexs: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# parsed = OrderedDict(type=get_type_desc(arg)) #LINE# #TAB# parsed['name'] = arg.__name__ #LINE# #TAB# parsed['signature'] = str(signature_func(arg)) #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getfullargspec(arg)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getargspec(arg)) #LINE# #TAB# parsed['isbuiltin'] = inspect.isbuiltin(arg) #LINE# #TAB# return parsed
#LINE# #TAB# template_file = find_template(template) #LINE# #TAB# if template_file is None: #LINE# #TAB# #TAB# raise TemplateDoesNotExist('Template not found.') #LINE# #TAB# template_path = template_file.get_full_path().split('/') #LINE# #TAB# template_path = '/'.join(template_path[:-1]) #LINE# #TAB# return template_path
#LINE# #TAB# if app_config is None: #LINE# #TAB# #TAB# app_config = ReviewConfig() #LINE# #TAB# config = ReviewConfig(app_config) #LINE# #TAB# config.read_file(ini_config) #LINE# #TAB# pipeline_config = {} #LINE# #TAB# for section in config: #LINE# #TAB# #TAB# pipeline_config[section] = {} #LINE# #TAB# #TAB# for option in config[section]: #LINE# #TAB# #TAB# #TAB# pipeline_config[section][option] = ini_config[section][option] #LINE# #TAB# return pipeline_config
"#LINE# #TAB# from assset.models import SpecValue #LINE# #TAB# values = [] #LINE# #TAB# for key, value in SpecValue.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# values.append(value) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, SpecValue): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# values += spec_value_list(item) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# values.append(value) #LINE# #TAB# return values"
#LINE# #TAB# compression_method = decompressor.COMPRESSION_METHOD.lower() #LINE# #TAB# if compression_method not in cls._kH2CO3_decompressors: #LINE# #TAB# #TAB# raise KeyError('No decompressor for compression method: {0:s}'.format #LINE# #TAB# #TAB# #TAB# (decompressor)) #LINE# #TAB# del cls._kH2CO3_decompressors[compression_method]
#LINE# #TAB# sum = 0 #LINE# #TAB# for byte in data: #LINE# #TAB# #TAB# sum += byte & 255 #LINE# #TAB# return sum % 256
#LINE# #TAB# z = x.copy() #LINE# #TAB# z.update(y) #LINE# #TAB# return z
#LINE# #TAB# try: #LINE# #TAB# #TAB# return url_dict[start_int:total_int] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# psi = -0.0094 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# code = code_object.copy() #LINE# #TAB# dt_grid = code.dt #LINE# #TAB# new_code = type('dt_grid', (dt_grid,), {}) #LINE# #TAB# new_code.body = code_object.body #LINE# #TAB# return new_code"
"#LINE# #TAB# for pattern in matches: #LINE# #TAB# #TAB# if fnmatch(name, pattern): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# new_sdas = np.zeros(sdas.shape) #LINE# #TAB# for i in range(sdas.shape[0]): #LINE# #TAB# #TAB# mib_max = np.max(mib) #LINE# #TAB# #TAB# new_sdas[i] = mib_max + xi_complement * (mib_max - mib) #LINE# #TAB# np.fill_diagonal(reachability_plot, 1.0 - new_sdas[i] #LINE# #TAB# #TAB# ) #LINE# #TAB# return new_sdas"
"#LINE# #TAB# manager = Manager(env, work_dir) #LINE# #TAB# manager.start_process(process_start) #LINE# #TAB# manager.finish_process(process_finish) #LINE# #TAB# results = manager.run(commands) #LINE# #TAB# return results"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return len(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return TYPES_MAP[value.capitalize()] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return TYPES_MAP[value.capitalize()] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
"#LINE# #TAB# fp = open(_input_file_path(name), 'r') #LINE# #TAB# fp.read() #LINE# #TAB# return fp"
#LINE# #TAB# i = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(gen) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# return i #LINE# #TAB# #TAB# i += 1
"#LINE# #TAB# article = wikif.get_article(section) #LINE# #TAB# article_lines = article.split('\n\n') #LINE# #TAB# paragraphs = [] #LINE# #TAB# current_paragraph = 0 #LINE# #TAB# for i, line in enumerate(article_lines): #LINE# #TAB# #TAB# new_paragraph = extract_paragraph(line, min_paragraph_length) #LINE# #TAB# #TAB# if new_paragraph!= '': #LINE# #TAB# #TAB# #TAB# paragraphs.append(new_paragraph) #LINE# #TAB# #TAB# #TAB# current_paragraph = new_paragraph #LINE# #TAB# if current_paragraph: #LINE# #TAB# #TAB# paragraphs.append(article_lines) #LINE# #TAB# return paragraphs"
"#LINE# #TAB# if width == 1: #LINE# #TAB# #TAB# return bin(data) #LINE# #TAB# b = np.zeros(width, dtype=np.uint8) #LINE# #TAB# b[0] = data & 1 #LINE# #TAB# b[1] = data >> 1 #LINE# #TAB# return b"
"#LINE# #TAB# field.setText(str(field.text()).replace(',','')) #LINE# #TAB# for txt in str(field).split(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if float(txt) >= num: #LINE# #TAB# #TAB# #TAB# #TAB# _show_consistency(field, message, True) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# _show_consistency(field, message, False) #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"#LINE# #TAB# if not base: #LINE# #TAB# #TAB# return [] #LINE# #TAB# monos = _generate_bsgs_groups(base, gens) #LINE# #TAB# out = [] #LINE# #TAB# for m in monos: #LINE# #TAB# #TAB# out.append(mon_transformation(base, m)) #LINE# #TAB# return out"
"#LINE# #TAB# path, filename = os.path.split(fullpath) #LINE# #TAB# filename, ext = os.path.splitext(filename) #LINE# #TAB# sys.path.insert(0, path) #LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# data = f.read() #LINE# #TAB# return data, ext"
"#LINE# #TAB# for f in ['.git', '.gitignore']: #LINE# #TAB# #TAB# if f.endswith('.git'): #LINE# #TAB# #TAB# #TAB# full_path = os.path.join(location, f) #LINE# #TAB# #TAB# #TAB# size = os.path.getsize(full_path) #LINE# #TAB# #TAB# #TAB# unsize = os.path.getsize(full_path) #LINE# #TAB# #TAB# #TAB# name = full_path[:-len('.git')] #LINE# #TAB# #TAB# #TAB# location = os.path.join(location, f) #LINE# #TAB# #TAB# #TAB# size = size + unsize #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return name, location, size, unsize"
"#LINE# #TAB# task_sequence = [] #LINE# #TAB# with open(config) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# task, _, sequence = cls._extract_load_task_and_sequence(line, #LINE# #TAB# #TAB# #TAB# #TAB# dependencies) #LINE# #TAB# #TAB# #TAB# task_sequence.append(task) #LINE# #TAB# return task_sequence"
#LINE# #TAB# name_a = model_a.module_name #LINE# #TAB# name_b = model_b.module_name #LINE# #TAB# if model_a.name!= model_b.name: #LINE# #TAB# #TAB# return False #LINE# #TAB# if model_a.modules!= model_b.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# for a_sub in model_a.modules: #LINE# #TAB# #TAB# if a_sub.name!= model_b.name: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# for a_sub in model_a.modules: #LINE# #TAB# #TAB# if a_sub.name!= model_b.name: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if not zip_filename: #LINE# #TAB# #TAB# return [] #LINE# #TAB# cache = {} #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# path = os.path.join(base_path, zip_filename, f) #LINE# #TAB# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# #TAB# cache[f] = os.path.join(path, f) #LINE# #TAB# return cache"
#LINE# #TAB# try: #LINE# #TAB# #TAB# child = mount_point + '/cd' #LINE# #TAB# #TAB# os.unlink(child) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# if elapsed > 0: #LINE# #TAB# #TAB# lease_time -= elapsed #LINE# #TAB# return lease_time
"#LINE# #TAB# tzi = getattr(dt, 'tzinfo', None) #LINE# #TAB# if tzi is not None: #LINE# #TAB# #TAB# dt = dt.astimezone(UTC) #LINE# #TAB# #TAB# tzi = UTC #LINE# #TAB# base = float(dt.toordinal()) #LINE# #TAB# cdate = getattr(dt, 'date', lambda : None)() #LINE# #TAB# if cdate is not None: #LINE# #TAB# #TAB# midnight_time = datetime.time(0, tzinfo=tzi) #LINE# #TAB# #TAB# rdt = datetime.datetime.combine(cdate, midnight_time) #LINE# #TAB# #TAB# base += (dt - rdt).total_seconds() / SEC_PER_DAY #LINE# #TAB# return base"
"#LINE# #TAB# child = node.getparent() #LINE# #TAB# if child is None: #LINE# #TAB# #TAB# return value #LINE# #TAB# if not isinstance(value, list): #LINE# #TAB# #TAB# value = [value] #LINE# #TAB# for x in value: #LINE# #TAB# #TAB# if x in child.inputs: #LINE# #TAB# #TAB# #TAB# return child.inputs[x] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# child.inputs.append(value) #LINE# #TAB# return value"
#LINE# #TAB# if tag in LDAP_TAG_MAP: #LINE# #TAB# #TAB# dn = LDAP_TAG_MAP[tag] #LINE# #TAB# elif tag in LDAP_TAG_MAJOR: #LINE# #TAB# #TAB# dn = LDAP_TAG_MAJOR[tag] #LINE# #TAB# elif tag in LDAP_TAG_COMPLEX: #LINE# #TAB# #TAB# dn = LDAP_TAG_COMPLEX[tag] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('unknown LDAP tag: %r' % tag) #LINE# #TAB# return dn
#LINE# #TAB# queryset = query.copy() #LINE# #TAB# queryset = queryset.filter(Q(deleted=False) | Q(deleted=True) | Q( #LINE# #TAB# #TAB# deleted__isnull=True) | Q(deleted=True) | Q(deleted=True)) #LINE# #TAB# return queryset
"#LINE# #TAB# vertices_neighbours = _compute_vertices_neighbours(nets) #LINE# #TAB# for subgraph_vertices in _get_connected_subgraphs(vertices_resources, #LINE# #TAB# #TAB# vertices_neighbours): #LINE# #TAB# #TAB# cmkee_order = _compute_cuthill_mckee(subgraph_vertices, vertices_neighbours #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# for vertex in cmkee_order: #LINE# #TAB# #TAB# #TAB# yield vertex"
#LINE# #TAB# values = list(cursor.get_values()) #LINE# #TAB# q = collections.OrderedDict() #LINE# #TAB# for item in values: #LINE# #TAB# #TAB# values[item[0]].update(item[1]) #LINE# #TAB# #TAB# q[item[0]].update(item[1]) #LINE# #TAB# for item in cursor.get_children(): #LINE# #TAB# #TAB# for key in list(item[0]): #LINE# #TAB# #TAB# #TAB# q[key][0] = item[0][0] #LINE# #TAB# #TAB# #TAB# q[key][1] = item[0][1] #LINE# #TAB# #TAB# #TAB# q[key][2] = item[1][2] #LINE# #TAB# #TAB# #TAB# q[key][3] = item[2][3] #LINE# #TAB# return q
#LINE# #TAB# yaml_parser = _create_yaml_parser_roundtrip() #LINE# #TAB# return yaml_parser
#LINE# #TAB# installed_dists_by_name = {} #LINE# #TAB# for installed_dist in installed_dists: #LINE# #TAB# #TAB# installed_dists_by_name[installed_dist.project_name] = installed_dist #LINE# #TAB# dist.requires = [installed_dists_by_name[installed_dist.project_name] for #LINE# #TAB# #TAB# installed_dist in installed_dists] #LINE# #TAB# return dist.requires
#LINE# #TAB# if text.endswith(suffix): #LINE# #TAB# #TAB# return text[:-len(suffix)] #LINE# #TAB# return text
"#LINE# #TAB# red, green, blue = xyz #LINE# #TAB# clipping = False #LINE# #TAB# return red, green, blue"
#LINE# #TAB# if i >= 2 ** 28: #LINE# #TAB# #TAB# raise ValueError('value of {} is too large'.format(i)) #LINE# #TAB# elif i < 0: #LINE# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# return i
#LINE# #TAB# global _sources #LINE# #TAB# if _sources is None: #LINE# #TAB# #TAB# with open(module.params['allocation_file']) as f: #LINE# #TAB# #TAB# #TAB# _sources = json.load(f) #LINE# #TAB# return _sources
"#LINE# #TAB# sock = DdpSocket() #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setblocking(0) #LINE# #TAB# return sock"
"#LINE# #TAB# if os.path.isdir(directory): #LINE# #TAB# #TAB# return get_credentials_modules(directory) #LINE# #TAB# if flush_local_modules: #LINE# #TAB# #TAB# for path in os.listdir(directory): #LINE# #TAB# #TAB# #TAB# if path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# path = os.path.join(directory, path) #LINE# #TAB# #TAB# #TAB# #TAB# return get_credentials_modules(path) #LINE# #TAB# return []"
"#LINE# #TAB# payload = {} #LINE# #TAB# if not isinstance(comm, str): #LINE# #TAB# #TAB# comm = comm.split() #LINE# #TAB# for word in comm: #LINE# #TAB# #TAB# if'' in word: #LINE# #TAB# #TAB# #TAB# word = word.split(' ')[0] #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# payload[word] = json.loads(word) #LINE# #TAB# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return payload"
"#LINE# #TAB# p_policy = create_portgroup_policy(pg_name, pg_default_port_config, #LINE# #TAB# #TAB# 'outShapingPolicy') #LINE# #TAB# return p_policy"
"#LINE# #TAB# if url: #LINE# #TAB# #TAB# url = unquote(url) #LINE# #TAB# #TAB# parsed = urlparse(url) #LINE# #TAB# #TAB# return parsed.username, parsed.password #LINE# #TAB# else: #LINE# #TAB# #TAB# return '', ''"
"#LINE# #TAB# data = yaml.load(open(filename, 'r')) #LINE# #TAB# return data['matches'], data['conversions']"
#LINE# #TAB# hash_string = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if elements == 0: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# elements = int(elements) #LINE# #TAB# #TAB# #TAB# hash_string += '#' #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return hash_string
"#LINE# #TAB# #TAB# first = True #LINE# #TAB# #TAB# for l in mol2_lst: #LINE# #TAB# #TAB# #TAB# if first: #LINE# #TAB# #TAB# #TAB# #TAB# first = False #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# first = True #LINE# #TAB# #TAB# config = OrderedDict() #LINE# #TAB# #TAB# for k, v in mol2_lst: #LINE# #TAB# #TAB# #TAB# config[k] = v #LINE# #TAB# #TAB# return config"
"#LINE# #TAB# headers_text = '' #LINE# #TAB# for key in dictnode_tree: #LINE# #TAB# #TAB# if key =='src': #LINE# #TAB# #TAB# #TAB# for inner_key in src: #LINE# #TAB# #TAB# #TAB# #TAB# headers_text += inner_key #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# headers_text += '\n' #LINE# #TAB# final_headers = '' #LINE# #TAB# for key in headers_text: #LINE# #TAB# #TAB# final_headers += key #LINE# #TAB# return final_headers, headers_text"
#LINE# #TAB# try: #LINE# #TAB# #TAB# returning = options['return'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return base_context.returning if base_context else ReturnType.UNKNOWN #LINE# #TAB# else: #LINE# #TAB# #TAB# if type(returning) is str: #LINE# #TAB# #TAB# #TAB# return ReturnType(returning) #LINE# #TAB# #TAB# return returning
"#LINE# #TAB# if use_ordered_dict: #LINE# #TAB# #TAB# dict_ = OrderedDict #LINE# #TAB# if index_col is None: #LINE# #TAB# #TAB# index_col = df.columns[0] #LINE# #TAB# data = list() #LINE# #TAB# for row in df.iterrows(): #LINE# #TAB# #TAB# tmp_dict = _decode_uuid_row(row, index_col, use_ordered_dict) #LINE# #TAB# #TAB# data.append(tmp_dict) #LINE# #TAB# return data"
"#LINE# #TAB# if not now: #LINE# #TAB# #TAB# now = datetime.datetime.now() #LINE# #TAB# offset = timestamp - now #LINE# #TAB# return offset, now"
"#LINE# #TAB# graph = _create_dot_graph(nicknames, relations, format, program, directed) #LINE# #TAB# graph.name = name #LINE# #TAB# graph.graph_type = 'auto' #LINE# #TAB# return graph"
"#LINE# #TAB# all_columns = list(filter(lambda el: el[-2:] == '_I', df.columns)) #LINE# #TAB# for column in all_columns: #LINE# #TAB# #TAB# del df[column]"
"#LINE# #TAB# if in_dict is not None and hasattr(in_dict, 'items'): #LINE# #TAB# #TAB# return in_dict.items() #LINE# #TAB# fixed_dict = {} #LINE# #TAB# for key, val in in_dict.items(): #LINE# #TAB# #TAB# if val is not None: #LINE# #TAB# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# #TAB# fixed_dict[key] = fix_np(val) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# fixed_dict[key] = val #LINE# #TAB# return fixed_dict"
#LINE# #TAB# handler = logging.StreamHandler() #LINE# #TAB# if debug: #LINE# #TAB# #TAB# handler.setLevel(logging.DEBUG) #LINE# #TAB# return handler
"#LINE# #TAB# new_d = {} #LINE# #TAB# for key, value in d.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# new_d[key] = skip + [value] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_d[key] = value #LINE# #TAB# return new_d"
"#LINE# #TAB# frontend_modules = _load_plugins(config.get(""CUSTOM_PLUGIN_MODULE_PATHS""), config[""BASE""], internal_attributes, callback) #LINE# #TAB# for module_name in frontend_modules: #LINE# #TAB# #TAB# module = import_module(module_name) #LINE# #TAB# #TAB# callback(module, config) #LINE# #TAB# return frontend_modules"
"#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# return {'file': func.func_name, 'line': func.func_lineno, #LINE# #TAB# #TAB# #TAB#'source': func.source} #LINE# #TAB# elif sys.platform == 'cygwin': #LINE# #TAB# #TAB# return {'file': func.func_name, 'line': func.lineno,'source': func. #LINE# #TAB# #TAB# #TAB# source} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
#LINE# #TAB# global _boolstr_logged_pickle_safe_model_class #LINE# #TAB# if _boolstr_logged_pickle_safe_model_class is None: #LINE# #TAB# #TAB# _boolstr_logged_pickle_safe_model_class = True #LINE# #TAB# return _boolstr_logged_pickle_safe_model_class
"#LINE# #TAB# #TAB# return [models.DbReference(reference=x.text) for x in entry.iterfind( #LINE# #TAB# #TAB# #TAB# './dbReference', namespaces=NAMESPACES)]"
#LINE# overlap = {} #LINE# for histories_per_epoch in histories: #LINE# #TAB# overlap[histories_per_epoch.keys()[0]] = histories_per_epoch[histories_per_epoch[ #LINE# #TAB# #TAB# histories_per_epoch[histories_per_epoch.keys()[0]]] #LINE# return overlap
"#LINE# #TAB# result = '' #LINE# #TAB# for code, report in get_error_report_list(): #LINE# #TAB# #TAB# result += code + ':'+ str(report) #LINE# #TAB# result += '\n' #LINE# #TAB# result += '\n' #LINE# #TAB# for code, report in get_error_report_list(): #LINE# #TAB# #TAB# result += code + ':'+ str(report) #LINE# #TAB# #TAB# result += '\n' #LINE# #TAB# return result"
"#LINE# #TAB# for name in os.listdir(dirpath): #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(dirpath, name)) and cond(name): #LINE# #TAB# #TAB# #TAB# shutil.rmtree(os.path.join(dirpath, name)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.unlink(os.path.join(dirpath, name)) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# if not isinstance(value, int): #LINE# #TAB# #TAB# raise TypeError('value must be an int, was %s' % type(value)) #LINE# #TAB# if value < 0: #LINE# #TAB# #TAB# return value + '-' #LINE# #TAB# return value"
"#LINE# #TAB# with open(jsonParams, 'r') as f: #LINE# #TAB# #TAB# jsonFile = f.read() #LINE# #TAB# query = json.loads(jsonFile) #LINE# #TAB# return query"
"#LINE# #TAB# result = {} #LINE# #TAB# for i in dir(obj): #LINE# #TAB# #TAB# if callable(getattr(obj, i)) and not i.startswith('_'): #LINE# #TAB# #TAB# #TAB# result[i] = getattr(obj, i) #LINE# #TAB# return result"
#LINE# #TAB# if format == 'insdc': #LINE# #TAB# #TAB# return set() #LINE# #TAB# return _cost_evaluate_formats[format]
"#LINE# #TAB# keys = p[0] #LINE# #TAB# value = p[1] #LINE# #TAB# for k in keys: #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# p[k] = [compare_generalized_star2(i, j) for j in value] #LINE# #TAB# #TAB# elif isinstance(value, dictorsetmaker_star2): #LINE# #TAB# #TAB# #TAB# p[k] = [compare_generalized_star2(i, j) for i in value] #LINE# #TAB# p.pop(0) #LINE# #TAB# return True"
"#LINE# #TAB# theta = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
#LINE# #TAB# if match.end >= len(chars): #LINE# #TAB# #TAB# return True #LINE# #TAB# cells = [] #LINE# #TAB# for c in chars: #LINE# #TAB# #TAB# if c not in match.start: #LINE# #TAB# #TAB# #TAB# cells.append(match.start) #LINE# #TAB# for c in chars: #LINE# #TAB# #TAB# if c not in match.end: #LINE# #TAB# #TAB# #TAB# cells.append(match.end) #LINE# #TAB# return cells
"#LINE# #TAB# recipients = cls.client.generate_recipients(subnet_id=subnet_id, #LINE# #TAB# #TAB# router_id=router_id, name=name) #LINE# #TAB# return recipients"
"#LINE# #TAB# table = formatting.Table(['Id', 'Position']) #LINE# #TAB# table.align['Id'] = 'l' #LINE# #TAB# table.align['Position'] = 'l' #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# row = [rule['id']] #LINE# #TAB# #TAB# for position in rule['positions']: #LINE# #TAB# #TAB# #TAB# row.append([rule['id'], position['from'], position[ #LINE# #TAB# #TAB# #TAB# #TAB# 'to']]) #LINE# #TAB# #TAB# table.add_row(row) #LINE# #TAB# return table"
"#LINE# #TAB# if isinstance(schema, dict): #LINE# #TAB# #TAB# key = 'id' #LINE# #TAB# #TAB# value = schema.get(key) #LINE# #TAB# #TAB# if callable(value): #LINE# #TAB# #TAB# #TAB# value = value() #LINE# #TAB# #TAB# row = ICachedItemMapper(key, value) #LINE# #TAB# else: #LINE# #TAB# #TAB# row = ICachedItemMapper(schema) #LINE# #TAB# return row"
"#LINE# #TAB# return re.match('^(?:http|ftp)s?://(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|localhost|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/?|[/?]\\S+)$' #LINE# #TAB# #TAB#, uri, re.IGNORECASE) is not None"
"#LINE# #TAB# urls = [] #LINE# #TAB# if resource: #LINE# #TAB# #TAB# parts = resource.split(""/"") #LINE# #TAB# #TAB# for part in parts: #LINE# #TAB# #TAB# #TAB# url = ""/"" + part #LINE# #TAB# #TAB# #TAB# if url.endswith("".json""): #LINE# #TAB# #TAB# #TAB# #TAB# url = url[:-4] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# url = ""%s/%s"" % (url, resource) #LINE# #TAB# #TAB# urls.append(url) #LINE# #TAB# return urls"
"#LINE# #TAB# with open(os.devnull, 'w') as devnull: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# subprocess.check_call(command, stdout=devnull) #LINE# #TAB# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for result in apply_func(fget, fset, fdel): #LINE# #TAB# #TAB# #TAB# if result is not None: #LINE# #TAB# #TAB# #TAB# #TAB# return result #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if value_not_found is not None: #LINE# #TAB# #TAB# raise ValueError(value_not_found) #LINE# #TAB# return value_not_found"
"#LINE# #TAB# q = q.copy() #LINE# #TAB# for i in range(q.shape[1]): #LINE# #TAB# #TAB# if q[i, 0]!= x[i]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if q[0].sum()!= q[1].sum(): #LINE# #TAB# #TAB# return False #LINE# #TAB# add_v = add_v #LINE# #TAB# x = np.asarray(x) #LINE# #TAB# if add_v: #LINE# #TAB# #TAB# x = add_v(x) #LINE# #TAB# q[0] = x.min() #LINE# #TAB# q[1] = x.max() #LINE# #TAB# return True"
"#LINE# #TAB# inv_f = f - df_num * df_den #LINE# #TAB# inv_num = np.clip(f - inv_f, 0, 1) #LINE# #TAB# inv_den = np.clip(f - inv_f, 0, 1) #LINE# #TAB# return inv_num, inv_den"
#LINE# #TAB# request_handler.request_user = validator(request_handler.request) #LINE# #TAB# return request_handler
"#LINE# #TAB# if isinstance(path, Path): #LINE# #TAB# #TAB# path = Path(path) #LINE# #TAB# if not isinstance(path, Path): #LINE# #TAB# #TAB# path = str(path) #LINE# #TAB# paths = deque([path]) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# download_file(paths.popleft()) #LINE# #TAB# #TAB# #TAB# paths.appendleft() #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return paths"
"#LINE# #TAB# var = np.ones(X.shape[0]) #LINE# #TAB# for i, cell in enumerate(cells): #LINE# #TAB# #TAB# var[i] = np.var(cell) #LINE# #TAB# return var"
"#LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:trun:exit { name: %r }"" % tsuite[""name""]) #LINE# #TAB# rcode = 0 #LINE# #TAB# for hook in tsuite[""hooks""][""arith""]:#TAB# #LINE# #TAB# #TAB# rcode = script_run(trun, hook) #LINE# #TAB# #TAB# if rcode: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:trun::exit { rcode: %r }"" % rcode, rcode) #LINE# #TAB# return rcode"
"#LINE# #TAB# for root, _, files in os.walk(path): #LINE# #TAB# #TAB# for name in files: #LINE# #TAB# #TAB# #TAB# if name.endswith('.exe'): #LINE# #TAB# #TAB# #TAB# #TAB# name = name[:-4] #LINE# #TAB# #TAB# #TAB# yield name"
"#LINE# #TAB# if nsname and nspid: #LINE# #TAB# #TAB# nspath = '%s/%s/%s' % (nspath, nsname, nspid) #LINE# #TAB# elif nspath is None: #LINE# #TAB# #TAB# nspath = '/tmp/' #LINE# #TAB# elif nsname: #LINE# #TAB# #TAB# nspath = '%s/%s' % (nspath, nsname) #LINE# #TAB# elif nspid is not None: #LINE# #TAB# #TAB# nspath = '%s/%s' % (nspath, nspid) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('nsname and nspid must be specified') #LINE# #TAB# return nspath"
#LINE# #TAB# if not groupids: #LINE# #TAB# #TAB# return [] #LINE# #TAB# for item in items: #LINE# #TAB# #TAB# if item.groupid not in groupids: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# #TAB# items = list(item.items()) #LINE# #TAB# return [item.location for item in items]
#LINE# #TAB# if 'type' in schema: #LINE# #TAB# #TAB# return schema['type'] #LINE# #TAB# return []
"#LINE# #TAB# while hasattr(node, 'children'): #LINE# #TAB# #TAB# node = node.children[0] #LINE# #TAB# yield node"
#LINE# #TAB# if running_tm_ver == 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if metric!= 'l1': #LINE# #TAB# #TAB# raise ValueError('Metric must be ""l1"" or ""l2"".') #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# samples = np.linalg.pinv(samples) #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# samples = np.linalg.pinv(samples) #LINE# #TAB# return samples"
"#LINE# #TAB# for k in dir(cls): #LINE# #TAB# #TAB# if k.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# attr = getattr(cls, k) #LINE# #TAB# #TAB# if callable(attr): #LINE# #TAB# #TAB# #TAB# yield attr, attr #LINE# #TAB# #TAB# elif isinstance(attr, property): #LINE# #TAB# #TAB# #TAB# yield attr, attr"
"#LINE# #TAB# validate(state) #LINE# #TAB# if isinstance(col_names, list): #LINE# #TAB# #TAB# col_names = [col_name for col_name in col_names] #LINE# #TAB# for col in state.test_results: #LINE# #TAB# #TAB# if col_names[col]!= state.test_results[col].keys(): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if match == 'exact': #LINE# #TAB# #TAB# if sort: #LINE# #TAB# #TAB# #TAB# _sort_rows(state, state.test_results, col_names) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _sort_rows(state, state.test_results, col_names) #LINE# #TAB# return True"
"#LINE# #TAB# key = variants['key'] #LINE# #TAB# position = variants['position'] #LINE# #TAB# spacings = [] #LINE# #TAB# for name, value in variants['items']: #LINE# #TAB# #TAB# if name in key: #LINE# #TAB# #TAB# #TAB# if position == 0: #LINE# #TAB# #TAB# #TAB# #TAB# spacings.append(name) #LINE# #TAB# #TAB# #TAB# elif position == 1: #LINE# #TAB# #TAB# #TAB# #TAB# spacings.append(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# spacings.append('{0}[{1}]'.format(name, value)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# position = 0 #LINE# #TAB# return position, spacings"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return requests.head(url, headers={'User-Agent': USER_AGENT})[0].status_code #LINE# #TAB# except requests.ConnectionError: #LINE# #TAB# #TAB# return 404"
"#LINE# #TAB# observed_risk_inputs = np.array(riskinputs) #LINE# #TAB# observed_model = riskmodel.deepcopy() #LINE# #TAB# X = np.dot(riskinputs, observed_risk_inputs) #LINE# #TAB# norm_x = np.sqrt(np.sum(X ** 2, axis=0)) #LINE# #TAB# norm_y = np.sqrt(np.sum(X ** 2, axis=0)) #LINE# #TAB# cond_x = norm_x[0] + norm_y[0] #LINE# #TAB# cond_y = norm_x[1] + norm_y[1] #LINE# #TAB# monitor.info('Condition shield for positive Risk: {0:.6f}'.format(cond_x)) #LINE# #TAB# return cond_x, cond_y"
#LINE# #TAB# plugin_name = plugin_class.NAME.lower() #LINE# #TAB# if plugin_name in cls._plugin_classes: #LINE# #TAB# raise KeyError('Plugin class already set for name: {0:s}.'.format( #LINE# #TAB# #TAB# plugin_class.NAME)) #LINE# #TAB# cls._plugin_classes[plugin_name] = plugin_class
"#LINE# #TAB# data = generate_json(app=app, env=env, region=region) #LINE# #TAB# store = {} #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# store[key] = data[key] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return store"
"#LINE# #TAB# V_convex = array(V) #LINE# #TAB# for i in range(len(V)): #LINE# #TAB# #TAB# for j in range(i + 1, len(V)): #LINE# #TAB# #TAB# #TAB# V_convex[i][j] = V[i][j] * eps / dx / (2 * i + 1) #LINE# #TAB# return V_convex"
#LINE# #TAB# padding = floating_ip.FloatingIP.build_padding(external_network_id= #LINE# #TAB# #TAB# external_network_id) #LINE# #TAB# return padding
#LINE# #TAB# for row in rows: #LINE# #TAB# #TAB# if left: #LINE# #TAB# #TAB# #TAB# row = list(row) #LINE# #TAB# #TAB# yield row
"#LINE# #TAB# return '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# key = clean_case(key) #LINE# #TAB# #TAB# value = clean_case(value) #LINE# #TAB# #TAB# if key == 'extra': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# outfile = outdir / 'factor_gene_match.tsv' #LINE# #TAB# df.to_csv(outfile, index=False, sep='\t', index_col=0) #LINE# #TAB# return outfile"
"#LINE# #TAB# config_diff = [] #LINE# #TAB# for file_name in files_to_diff: #LINE# #TAB# #TAB# config_str = tree.download_file(file_name) #LINE# #TAB# #TAB# if config_str: #LINE# #TAB# #TAB# #TAB# config_dict = parse_config(config_str) #LINE# #TAB# #TAB# #TAB# if config_dict: #LINE# #TAB# #TAB# #TAB# #TAB# yield file_name, config_dict #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result = tree.diff_file(file_name) #LINE# #TAB# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# #TAB# config_diff.append(file_name, result) #LINE# #TAB# return config_diff"
#LINE# #TAB# obj_language = 'en' #LINE# #TAB# if project_folder[-1]!= '/': #LINE# #TAB# #TAB# obj_language = project_folder + '/' + obj_language #LINE# #TAB# return obj_language
"#LINE# #TAB# span = ts.span #LINE# #TAB# start_limit = min(span[0], end) #LINE# #TAB# end_limit = max(span[1], end) #LINE# #TAB# if start_limit < 0 or end_limit < 0: #LINE# #TAB# #TAB# return ts.copy() #LINE# #TAB# ts.pad((0, pad, start_limit)) #LINE# #TAB# ts.fillna(0, inplace=True) #LINE# #TAB# return ts"
#LINE# #TAB# global _state #LINE# #TAB# if _state is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# with io.StringIO() as stream: #LINE# #TAB# #TAB# token = stream.readline().strip() #LINE# #TAB# #TAB# if not token: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# _state = token.decode('utf-8') #LINE# #TAB# return _state
#LINE# #TAB# if Nu == 1: #LINE# #TAB# #TAB# clust = 0 #LINE# #TAB# elif Nu == 2: #LINE# #TAB# #TAB# clust = 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# clust = 0 #LINE# #TAB# return clust
"#LINE# #TAB# name = os.path.splitext(name)[0] #LINE# #TAB# max_size = powerline.segment_conf('cwd','max_dir_size') #LINE# #TAB# if max_size: #LINE# #TAB# #TAB# return name[:max_size] #LINE# #TAB# return name"
"#LINE# #TAB# dimension_string = DUMMY_DIM_TEMPLATE.format(title=format_title( #LINE# #TAB# #TAB# 'Dimension'), experiment=experiment) #LINE# #TAB# return dimension_string"
#LINE# #TAB# for i in data.keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del data[i] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return data
"#LINE# #TAB# if isinstance(obj, datetime.datetime): #LINE# #TAB# #TAB# obj = obj.isoformat() #LINE# #TAB# return obj"
"#LINE# #TAB# table = [] #LINE# #TAB# if lock_base_model: #LINE# #TAB# #TAB# model = Resnet50(0, 50, [0.0, 0.0, 0.0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# model = Resnet50(0, 50, [0.0, 0.0, 0.0]) #LINE# #TAB# return table"
#LINE# #TAB# from PIL import Image #LINE# #TAB# data = bytes(data) #LINE# #TAB# image = Image.open(BytesIO(data)) #LINE# #TAB# return image
#LINE# #TAB# is_eof = False #LINE# #TAB# try: #LINE# #TAB# #TAB# if fhpatch.read(4) == b'': #LINE# #TAB# #TAB# #TAB# is_eof = True #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return is_eof
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('sequence_length', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN, description='sequence length') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was retrieved #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# could not retrieve parameter #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
"#LINE# #TAB# if hasattr(cls, 'exposed_actions'): #LINE# #TAB# #TAB# return cls.exposed_actions #LINE# #TAB# return []"
"#LINE# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# return {} #LINE# #TAB# results = set() #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if line.startswith('@'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# parts = line.split(' ') #LINE# #TAB# #TAB# #TAB# name = parts[0] #LINE# #TAB# #TAB# #TAB# description = parts[1] #LINE# #TAB# #TAB# #TAB# results.add((name, description)) #LINE# #TAB# return results"
#LINE# #TAB# #TAB# java_object = javabridge.get_object(clazz) #LINE# #TAB# #TAB# num_samples = javabridge.get_object_count(java_object) #LINE# #TAB# #TAB# if num_samples == 1: #LINE# #TAB# #TAB# #TAB# return java_object[0] #LINE# #TAB# #TAB# elif num_samples == 2: #LINE# #TAB# #TAB# #TAB# return java_object[0] #LINE# #TAB# #TAB# elif num_samples == 3: #LINE# #TAB# #TAB# #TAB# return [javabridge.get_object(java_object) for java_object in #LINE# #TAB# #TAB# #TAB# #TAB# javabridge.get_object_list(java_object)] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return [javabridge.get_object(java_object) for java_object in #LINE# #TAB# #TAB# #TAB# #TAB#
"#LINE# #TAB# if isinstance(rate, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# scale, interval = rate.split(':') #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise ValueError('Invalid score: {}'.format(rate)) #LINE# #TAB# #TAB# scale = int(interval) #LINE# #TAB# #TAB# if scale < 1: #LINE# #TAB# #TAB# #TAB# raise ValueError('Rate must be between 1 and 1000') #LINE# #TAB# #TAB# return rate * 1000 #LINE# #TAB# if scale > 1000: #LINE# #TAB# #TAB# raise ValueError('Rate must be between 1 and 1000') #LINE# #TAB# return rate * 1000"
#LINE# #TAB# username = job.get('username') #LINE# #TAB# if not username: #LINE# #TAB# #TAB# return job #LINE# #TAB# try: #LINE# #TAB# #TAB# import json #LINE# #TAB# #TAB# if '.' in username: #LINE# #TAB# #TAB# #TAB# return json.loads(username) #LINE# #TAB# #TAB# elif '.' in username: #LINE# #TAB# #TAB# #TAB# return json.loads(username) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return job #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return job
"#LINE# #TAB# text = re.sub('<.*?>', '\\1', text) #LINE# #TAB# text = re.sub('<.*?>', '\\1', text) #LINE# #TAB# text = text.replace('&lt;', '<') #LINE# #TAB# text = text.replace('&gt;', '>') #LINE# #TAB# text = text.replace('&quot;', '""') #LINE# #TAB# return text"
"#LINE# #TAB# if hasattr(obj, '__dict__'): #LINE# #TAB# #TAB# return {k: pop_attrs(v) for k, v in obj.__dict__.items() if not k. #LINE# #TAB# #TAB# #TAB# startswith('_')} #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# return [pop_attrs(o) for o in obj] #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# return {k: pop_attrs(v) for k, v in obj.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# prop = None #LINE# #TAB# try: #LINE# #TAB# #TAB# prop = model_cls._get_property(prop_name) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return prop
"#LINE# #TAB# if hasattr(request, 'form') and request.form['form']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del request.form['form'] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# del request.form['user'] #LINE# #TAB# #TAB# del request.form['password'] #LINE# #TAB# except: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# settings = context.settings #LINE# #TAB# enabled = getattr(settings, 'ADAPTER_ENABLED', False) #LINE# #TAB# if enabled: #LINE# #TAB# #TAB# return #LINE# #TAB# _ AnalyticsContext = context #LINE# #TAB# if _ AnalyticsContext is None: #LINE# #TAB# #TAB# return #LINE# #TAB# _ AnalyticsContext.enabled = True"
#LINE# #TAB# new_gmt_datadir = os.environ.get('GMT_DATADIR') #LINE# #TAB# if os.path.exists(new_gmt_datadir): #LINE# #TAB# #TAB# os.environ['GMT_DATADIR'] = new_gmt_datadir
"#LINE# #TAB# data_files = [] #LINE# #TAB# if os.path.exists(default_config_path): #LINE# #TAB# #TAB# for root, dirs, files in os.walk(default_config_path): #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# if filename.endswith('.json'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# data_files.append(os.path.join(root, filename)) #LINE# #TAB# return data_files"
#LINE# #TAB# extension = os.path.splitext(file_name)[1] #LINE# #TAB# if extension == 'pdf': #LINE# #TAB# #TAB# body = ImageFile(file_name) #LINE# #TAB# elif extension == 'docx': #LINE# #TAB# #TAB# body = DocxFile(file_name) #LINE# #TAB# elif extension == 'docy': #LINE# #TAB# #TAB# body = DocyFile(file_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Unknown or unsupported file extension: {0}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# extension)) #LINE# #TAB# return body
#LINE# #TAB# cluster = list(context['clusters'].values())[0] #LINE# #TAB# return cluster if cluster in cluster else None
#LINE# #TAB# encoded = {} #LINE# #TAB# for key in to_encode: #LINE# #TAB# #TAB# if to_encode[key] == 0: #LINE# #TAB# #TAB# #TAB# del to_encode[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encoded[key] = to_encode[key] #LINE# #TAB# return encoded
"#LINE# #TAB# x = ZeroPadding2D(((1, 0), (1, 0)))(x) #LINE# #TAB# for i in range(num_blocks): #LINE# #TAB# #TAB# x = darknet_conv2d_bn_leaky(num_filters, (3, 3), strides=(2, 2))(x) #LINE# #TAB# return x"
#LINE# #TAB# c['annulus_planet'] = {} #LINE# #TAB# c['annulus_planet'][0] = 14 #LINE# #TAB# c['annulus_planet'][1] = 14 #LINE# #TAB# c['annulus_planet'][2] = 15 #LINE# #TAB# return c
#LINE# #TAB# new_lattice = [] #LINE# #TAB# for coord in frac_coordinates: #LINE# #TAB# #TAB# new_lattice.append(round(coord)) #LINE# #TAB# return new_lattice
"#LINE# #TAB# if prefix is None: #LINE# #TAB# #TAB# prefix = b'' #LINE# #TAB# expnum = int(expnum) #LINE# #TAB# ccd = int(ccd) #LINE# #TAB# if version == 'p': #LINE# #TAB# #TAB# spec1d = read_pcd(expnum, ccd, prefix=prefix) #LINE# #TAB# elif version =='res': #LINE# #TAB# #TAB# spec1d = read_res(expnum, ccd, prefix=prefix) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError(""unknown version: %s"" % version) #LINE# #TAB# return spec1d"
#LINE# #TAB# out = [] #LINE# #TAB# for i in range(len(stack)): #LINE# #TAB# #TAB# out.append(bool(stack[i][0])) #LINE# #TAB# return out
"#LINE# #TAB# if s.startswith(start): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return s.split(start, 1)[1] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return s"
"#LINE# #TAB# if not isinstance(value, (int, float)) and value is not None: #LINE# #TAB# #TAB# raise TypeError(""Glyph left margin must be an :ref:`type-int-float`, "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""not %s."" % type(value).__name__) #LINE# #TAB# return value"
"#LINE# #TAB# if counts.sum() <= n: #LINE# #TAB# #TAB# return counts #LINE# #TAB# cumsum = np.cumsum(counts, dtype=dtype) #LINE# #TAB# result = cumsum * counts.astype(dtype) #LINE# #TAB# result = result.reshape(counts.shape) #LINE# #TAB# return result"
"#LINE# #TAB# result = {} #LINE# #TAB# for key in d: #LINE# #TAB# #TAB# if isinstance(d[key], str): #LINE# #TAB# #TAB# #TAB# result[key] = d[key].encode('utf-8') #LINE# #TAB# #TAB# elif isinstance(d[key], bytes): #LINE# #TAB# #TAB# #TAB# result[key] = d[key].decode('utf-8') #LINE# #TAB# return result"
#LINE# #TAB# if is_pymacaron_model(o): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return json.loads(o) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# return o #LINE# #TAB# return o
#LINE# #TAB# remote_url = request.get_host().rstrip('/') + request.path #LINE# #TAB# return remote_url
#LINE# #TAB# for s in structures: #LINE# #TAB# #TAB# s.lattice.close() #LINE# #TAB# cmap_s = [] #LINE# #TAB# #TAB# for s in structures: #LINE# #TAB# #TAB# #TAB# if s.lattice.lower()!= cmap_s.lower(): #LINE# #TAB# #TAB# #TAB# #TAB# cmap_s.append(s) #LINE# #TAB# return cmap_s
#LINE# #TAB# rc = lib.s3_sha512(message) #LINE# #TAB# return rc
"#LINE# #TAB# if month > 13: #LINE# #TAB# #TAB# raise ValueError('Incorrect month index') #LINE# #TAB# if month in (IYYAR, TAMMUZ, ELUL, TEVETH, VEADAR): #LINE# #TAB# #TAB# return 1 #LINE# #TAB# if month == ADAR and not leap(year): #LINE# #TAB# #TAB# return 2 #LINE# #TAB# if month == HESHVAN and (year_days(year) % 10)!= 5: #LINE# #TAB# #TAB# return 3 #LINE# #TAB# return 4"
"#LINE# #TAB# for chunk in url_chunks_iter(url): #LINE# #TAB# #TAB# chunk = chunk.replace('\n', '') #LINE# #TAB# #TAB# chunk = chunk.replace('\r', '') #LINE# #TAB# #TAB# conf = generate_conf_from_chunk(chunk) #LINE# #TAB# #TAB# yield conf"
#LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if l[i]!= None and l[i]!= l[i + 1]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# calibration_bounds = control_board.get_calibration_bounds() #LINE# #TAB# fitted_params = np.asarray(fitted_params, dtype=np.float64) #LINE# #TAB# for p in fitted_params: #LINE# #TAB# #TAB# calibration_bounds[p] = fitted_params[p] #LINE# #TAB# np.clip(calibration_bounds[0], 0, 1, out=calibration_bounds[1]) #LINE# #TAB# control_board.set_calibration_bounds(calibration_bounds) #LINE# #TAB# return calibration_bounds"
"#LINE# #TAB# country_deaths = instance.get('country_deaths') #LINE# #TAB# total_deaths_per_country = {} #LINE# #TAB# for country in country_deaths: #LINE# #TAB# #TAB# total_deaths_per_country[country] = {} #LINE# #TAB# #TAB# for death in country_deaths[country]: #LINE# #TAB# #TAB# #TAB# total_deaths_per_country[country][death] = 0 #LINE# #TAB# return {'country_deaths': total_deaths_per_country, #LINE# #TAB# #TAB# 'total_deaths_per_country': total_deaths_per_country}"
#LINE# #TAB# root = ElementTree.fromstring(tree) #LINE# #TAB# for child in root: #LINE# #TAB# #TAB# if child.tag == 'create': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# paths = _metadata_in_files() #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if path.startswith(search_string): #LINE# #TAB# #TAB# #TAB# print(path) #LINE# #TAB# #TAB# #TAB# lines = path.splitlines() #LINE# #TAB# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# #TAB# if line.startswith(replacement_line): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# print(line) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield path
#LINE# #TAB# fts = p.fts #LINE# #TAB# if len(fts) == 1: #LINE# #TAB# #TAB# return fts[0] #LINE# #TAB# elif len(fts) == 2: #LINE# #TAB# #TAB# return fts[1] #LINE# #TAB# elif len(fts) == 3: #LINE# #TAB# #TAB# return list(fts)[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return fts[0]
#LINE# #TAB# axis = ord(base[0]) #LINE# #TAB# if align & 1: #LINE# #TAB# #TAB# axis = ord(base[-1]) #LINE# #TAB# elif align & 2: #LINE# #TAB# #TAB# axis = ord(base[-2]) #LINE# #TAB# return axis
#LINE# #TAB# if not analyses_available(analysis_request): #LINE# #TAB# #TAB# return True #LINE# #TAB# unassigned = [] #LINE# #TAB# for analysis in analysis_request.getAnalyses(): #LINE# #TAB# #TAB# analysis_status = api.get_workflow_status_of(analysis) #LINE# #TAB# #TAB# if analysis_status in unassigned: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unassigned.append(analysis_status) #LINE# #TAB# if not unassigned: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if not host: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif host.startswith('[') and host.endswith(']'): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif host == '127.0.0.1': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if arg is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(arg, (list, tuple)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return lookup_dict[arg[0]] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if isinstance(arg, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return lookup_dict[arg] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return lookup_dict[arg] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return arg"
"#LINE# #TAB# sql = ( #LINE# #TAB# #TAB# ""SELECT field_name FROM information_schema.columns WHERE table_name = '{table_name}' AND index_name = '{index}';"" #LINE# #TAB# #TAB# ) #LINE# #TAB# result = connection.execute(sql, db=db, tbl=tbl, index=index) #LINE# #TAB# return result"
#LINE# #TAB# parts = os.path.split(path) #LINE# #TAB# for part in parts[:-1]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(part) #LINE# #TAB# #TAB# except OSError as exc: #LINE# #TAB# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(part): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise
#LINE# #TAB# project_repository_handler = None #LINE# #TAB# repository_tag_name = project_element.attrib['name'] #LINE# #TAB# if repository_tag_name =='repository': #LINE# #TAB# #TAB# project_repository_handler = generate_git_repository(project_element) #LINE# #TAB# elif repository_tag_name == 'tree_item': #LINE# #TAB# #TAB# project_repository_handler = generate_tree_repository(project_element) #LINE# #TAB# return project_repository_handler
#LINE# #TAB# bool=False) ->dict: #LINE# #TAB# if is_msc: #LINE# #TAB# #TAB# table_code = table_code + '_msc' #LINE# #TAB# for key in list(table_code.keys()): #LINE# #TAB# #TAB# if key == table_name: #LINE# #TAB# #TAB# #TAB# return key #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.warn(f'Table code {table_code} not found.') #LINE# #TAB# return None
"#LINE# #TAB# names = [] #LINE# #TAB# run_list = node.get_run_list() #LINE# #TAB# for recipes in run_list: #LINE# #TAB# #TAB# for filename in glob.glob(os.path.join(recipes, filename)): #LINE# #TAB# #TAB# #TAB# if os.path.isfile(os.path.join(recipes, filename)): #LINE# #TAB# #TAB# #TAB# #TAB# names.append(os.path.abspath(os.path.join(recipes, filename))) #LINE# #TAB# return names"
"#LINE# #TAB# base, ext = os.path.splitext(name) #LINE# #TAB# out = {'base': base, 'ext': ext} #LINE# #TAB# return out"
"#LINE# #TAB# mean = [0.485, 0.456, 0.406] #LINE# #TAB# std = [0.229, 0.224, 0.225] #LINE# #TAB# image = image.convert('RGB') #LINE# #TAB# tensor = np.asarray(image) #LINE# #TAB# tensor = crop_and_resize(tensor, 224) #LINE# #TAB# tensor = tensor / 255.0 #LINE# #TAB# tensor[..., 0] -= mean[0] #LINE# #TAB# tensor[..., 1] -= mean[1] #LINE# #TAB# tensor[..., 2] -= mean[2] #LINE# #TAB# tensor[..., 0] /= std[0] #LINE# #TAB# tensor[..., 1] /= std[1] #LINE# #TAB# tensor[..., 2] /= std[2] #LINE# #TAB# assert tensor.shape == (224, 224, 3) #LINE# #TAB# return tensor"
#LINE# #TAB# stage_template = load_stage_template(template_file) #LINE# #TAB# xlsx_file = StringIO() #LINE# #TAB# xlsx_file.write(stage_template) #LINE# #TAB# xlsx_file.seek(0) #LINE# #TAB# response = download_stage_from_file(xlsx_file) #LINE# #TAB# return response
#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# root = os.path.expanduser('~/Library/Application Support') #LINE# #TAB# #TAB# return root #LINE# #TAB# if sys.platform == 'darwin': #LINE# #TAB# #TAB# root = os.path.expandvars('~/Library/Application Support') #LINE# #TAB# #TAB# return root #LINE# #TAB# root = os.path.expanduser('~/Library/Application Support') #LINE# #TAB# if root == '': #LINE# #TAB# #TAB# root = os.path.expanduser('~') #LINE# #TAB# return root
#LINE# #TAB# tokens = line.strip().split('\t') #LINE# #TAB# if len(tokens) == 0: #LINE# #TAB# #TAB# return #LINE# #TAB# prop_dict[tokens[0]] = tokens[1]
"#LINE# #TAB# dt = datetime.strptime(s, DATE_FORMAT) #LINE# #TAB# try: #LINE# #TAB# #TAB# time = datetime.datetime.strptime(dt, DATE_FORMAT) #LINE# #TAB# #TAB# return time.time() #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return 0"
"#LINE# #TAB# split = {'LOGNAME': 'LOGNAME', 'USER', 'LNAME': 'LOGNAME', 'USERNAME': #LINE# #TAB# #TAB# 'USERNAME', 'PASSWORD': 'PASSWORD'} #LINE# #TAB# for old, new in split.items(): #LINE# #TAB# #TAB# if old in os.environ: #LINE# #TAB# #TAB# #TAB# del os.environ[old] #LINE# #TAB# #TAB# #TAB# os.environ[new] = os.environ.pop(old) #LINE# #TAB# return split"
"#LINE# #TAB# names = [] #LINE# #TAB# for name, value in vars(cls).items(): #LINE# #TAB# #TAB# if isinstance(value, property): #LINE# #TAB# #TAB# #TAB# if name!= 'plot': #LINE# #TAB# #TAB# #TAB# #TAB# names.append(name) #LINE# #TAB# return names"
"#LINE# #TAB# conn = sqlite3.connect(database_name) #LINE# #TAB# c = conn.cursor() #LINE# #TAB# cur = conn.cursor() #LINE# #TAB# set_unicode_devices_by_hostname(cur.cursor(), search_string) #LINE# #TAB# conn.commit() #LINE# #TAB# if cur.rowcount == 0: #LINE# #TAB# #TAB# return #LINE# #TAB# rows = cur.fetchall() #LINE# #TAB# conn.close() #LINE# #TAB# return rows"
"#LINE# #TAB# if obj.dtend is not None and isinstance(obj.dtend, datetime): #LINE# #TAB# #TAB# return obj.dtend #LINE# #TAB# elif obj.duration is not None and isinstance(obj.duration, int): #LINE# #TAB# #TAB# delta = timedelta(days=obj.duration) #LINE# #TAB# #TAB# return obj.dtend + delta #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# logger = {} #LINE# #TAB# if typealiases: #LINE# #TAB# #TAB# lines = typealiases.split('\n') #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# if '=' in line: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = line.split('=', 1) #LINE# #TAB# #TAB# #TAB# #TAB# key = key.strip()[7:] #LINE# #TAB# #TAB# #TAB# #TAB# value = value.strip() #LINE# #TAB# #TAB# #TAB# #TAB# logger[key] = value #LINE# #TAB# return logger"
"#LINE# #TAB# result = generate(section, name) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# result = default #LINE# #TAB# return result"
#LINE# #TAB# default_menu_items = extract_default_menu_items(state) #LINE# #TAB# if default_menu_items: #LINE# #TAB# #TAB# return default_menu_items[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# return logger
#LINE# #TAB# try: #LINE# #TAB# #TAB# child.alias = alias_map[child.alias] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# return {param_name: request_parameters[param_name] for param_name in #LINE# #TAB# #TAB# OIDC_DEPENDENCIES}
#LINE# #TAB# config = load_config(filename) #LINE# #TAB# if config.has_section('blimpy'): #LINE# #TAB# #TAB# return config['blimpy']['header_size'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# tris = np.linspace(0, 1, exp_ind.shape[0] + 1) #LINE# #TAB# ax = plt.subplot() #LINE# #TAB# for i in range(exp_ind.shape[1]): #LINE# #TAB# #TAB# tris[i, 0] = swim_speed[i, 0] #LINE# #TAB# #TAB# ax.plot(tris[i - 1, 0], tris[i + 1, 1], color='black', linewidth=3) #LINE# #TAB# ax.set_xlim(0, 2 * np.pi) #LINE# #TAB# ax.set_ylim(0, 2 * np.pi) #LINE# #TAB# return ax"
"#LINE# #TAB# #TAB# cert = cls(token=api_token, id=cert_id) #LINE# #TAB# #TAB# cert.load() #LINE# #TAB# #TAB# return cert"
#LINE# #TAB# path = os.path.basename(path) #LINE# #TAB# path = os.path.splitext(path)[0] #LINE# #TAB# if doc_root!= '' and path.endswith(os.sep): #LINE# #TAB# #TAB# path = path[:-len(os.sep)] + doc_root + path #LINE# #TAB# return path
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(txt) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if value is NoneValue: #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(value, type): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# integer = int(value) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# if integer is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# global folder_name #LINE# #TAB# folder_name = None
"#LINE# #TAB# for idx in indices: #LINE# #TAB# #TAB# if idx == 0: #LINE# #TAB# #TAB# #TAB# return os.path.join(indir,'media') #LINE# #TAB# return None"
"#LINE# #TAB# global _flattened_path #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# return DEFAULT_PATH #LINE# #TAB# if path not in _flattened_path: #LINE# #TAB# #TAB# path = os.path.join(os.getcwd(), path) #LINE# #TAB# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# #TAB# _flattened_path = path #LINE# #TAB# return _flattened_path"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# s.bind(('', start_port)) #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# port = s.getsockname()[1] #LINE# #TAB# #TAB# if port is None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# s.close() #LINE# #TAB# #TAB# return port"
"#LINE# #TAB# old_mtime = None #LINE# #TAB# file_stat = os.stat(file_path) #LINE# #TAB# mtime = file_stat.st_mtime #LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# old_mtime = datetime.fromtimestamp(mtime) #LINE# #TAB# #TAB# except (ValueError, OSError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return mtime, old_mtime"
"#LINE# #TAB# page_path = os.path.join(os.path.dirname(__file__), 'pages', name + '.html') #LINE# #TAB# documentation_page_path = os.path.join(os.path.dirname(__file__), 'docs', page_path) #LINE# #TAB# documentation_page_path = os.path.abspath(documentation_page_path) #LINE# #TAB# if not os.path.exists(page_path): #LINE# #TAB# #TAB# os.makedirs(page_path) #LINE# #TAB# documentation_page_path = os.path.join(os.path.dirname(__file__), 'docs', page_path) #LINE# #TAB# documentation_page_path = os.path.abspath(documentation_page_path) #LINE# #TAB# os.makedirs(documentation_page_path) #LINE# #TAB# return page_path"
"#LINE# #TAB# config = InfoConfig() #LINE# #TAB# config_path = os.path.join(ipython_dir, DEFAULT_CONFIG_FILE) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(config_path, 'r') as config_file: #LINE# #TAB# #TAB# #TAB# config.load(config_file) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# as_dict = {} #LINE# #TAB# for option in sorted(options): #LINE# #TAB# #TAB# key = option[0] #LINE# #TAB# #TAB# value = option[1] #LINE# #TAB# #TAB# if key in as_dict: #LINE# #TAB# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# as_dict[key] = as_settings(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# as_dict[key] = value #LINE# #TAB# return as_dict"
"#LINE# #TAB# distribution_names = {} #LINE# #TAB# distribution_dir = os.path.dirname(__file__) #LINE# #TAB# for distribution in os.listdir(distribution_dir): #LINE# #TAB# #TAB# distribution_name = distribution.replace('.', '_') #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(distribution_dir, distribution_name)): #LINE# #TAB# #TAB# #TAB# distribution_names.update({ #LINE# #TAB# #TAB# #TAB# #TAB# distribution_name: distribution_name #LINE# #TAB# #TAB# #TAB# }) #LINE# #TAB# return distribution_names"
#LINE# #TAB# new_record = SeqRecord(record.seq) #LINE# #TAB# new_record.start = record.start + n_bases #LINE# #TAB# return new_record
"#LINE# #TAB# if priority is None: #LINE# #TAB# #TAB# priority = {'a': 1.0, 'b': -1.0, 'c': -1.0} #LINE# #TAB# mask = priority * np.ones_like(points) #LINE# #TAB# if priority['a']!= priority['b']: #LINE# #TAB# #TAB# mask[priority['a']] = -priority['b'] #LINE# #TAB# if priority['c']!= priority['d']: #LINE# #TAB# #TAB# mask[priority['c']] = -priority['d'] #LINE# #TAB# return mask"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('image_id', dtype='string', direction=function.IN, #LINE# #TAB# #TAB# description='Image_Name') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was set #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# could not set parameter #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# code, _, message = response.partition(' ') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise CommandError('Invalid response: %s' % message) #LINE# #TAB# if code!= 1: #LINE# #TAB# #TAB# raise CommandError('Invalid response: %s' % message) #LINE# #TAB# return message"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_ast = ast.walk(node) #LINE# #TAB# except ast.InferenceError: #LINE# #TAB# #TAB# return None #LINE# #TAB# for module in module_ast[1:]: #LINE# #TAB# #TAB# if isinstance(module, ast.Module): #LINE# #TAB# #TAB# #TAB# return module.function #LINE# #TAB# return None"
"#LINE# #TAB# return { #LINE# #TAB# #TAB# route.name: dict( #LINE# #TAB# #TAB# #TAB# [( #LINE# #TAB# #TAB# #TAB# #TAB# blueprint_bundle_path=path, #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# for path in app.config #LINE# #TAB# #TAB# #TAB# if path.startswith(base_path) #LINE# #TAB# #TAB# ] #LINE# #TAB# #TAB# for route in app.url_map.routes #LINE# #TAB# }"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
#LINE# #TAB# service_names = list() #LINE# #TAB# resource_managers = ResourceManager.__subclasses__() #LINE# #TAB# if admin_required is None or admin_required in resource_managers: #LINE# #TAB# #TAB# return service_names + resource_managers #LINE# #TAB# else: #LINE# #TAB# #TAB# for resource_manager in resource_managers: #LINE# #TAB# #TAB# #TAB# service_names += resource_manager.xml_device_name() #LINE# #TAB# #TAB# #TAB# if resource_manager.admin_required is not None and admin_required!= #LINE# #TAB# #TAB# #TAB# #TAB# resource_manager.admin_required: #LINE# #TAB# #TAB# #TAB# #TAB# service_names += resource_manager.xml_device_name() #LINE# #TAB# #TAB# return service_names
"#LINE# #TAB# for name, item in find_widgets().items(): #LINE# #TAB# #TAB# if test: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# module = import_module(name) #LINE# #TAB# #TAB# if module is None or not issubclass(module, DateTimeBaseInput): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return getattr(module, name) #LINE# #TAB# return None"
"#LINE# #TAB# final_metadata = {} #LINE# #TAB# for f in in_files: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# globbed = glob.glob(f) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not globbed: #LINE# #TAB# #TAB# #TAB# final_metadata[f] = {} #LINE# #TAB# #TAB# #TAB# final_metadata[f] = globbed #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# final_metadata[f] = metadata.get(""tokenize_matches"", True) #LINE# #TAB# return final_metadata"
"#LINE# #TAB# conn = sqlite3.connect(':memory:') #LINE# #TAB# c = conn.cursor() #LINE# #TAB# c.execute('select * from data_mark_tester --database_url sqlite:///gtfs.db') #LINE# #TAB# data_mark_tester_result = [] #LINE# #TAB# for i in range(5): #LINE# #TAB# #TAB# data_mark_tester_result.append((i, c.fetchone()[0])) #LINE# #TAB# conn.close() #LINE# #TAB# return data_mark_tester_result"
"#LINE# #TAB# if not raw_annotations: #LINE# #TAB# #TAB# return {} #LINE# #TAB# mode = raw_annotations.get('mode') #LINE# #TAB# if not mode: #LINE# #TAB# #TAB# return raw_annotations #LINE# #TAB# if module_name and not mode.startswith('pytypes'): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB#'mode': mode, #LINE# #TAB# #TAB# #TAB# 'annotations': raw_annotations #LINE# #TAB# #TAB# } #LINE# #TAB# else: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB#'mode': mode, #LINE# #TAB# #TAB# #TAB# 'annotations': raw_annotations #LINE# #TAB# #TAB# }"
"#LINE# #TAB# if rule == None: #LINE# #TAB# #TAB# raise SDLError('Invalid network rule') #LINE# #TAB# rc = lib.SDL_NetworkGetRule(name, rule) #LINE# #TAB# if rc == 0: #LINE# #TAB# #TAB# raise SDLError('Invalid network rule') #LINE# #TAB# return rc"
#LINE# #TAB# path = os.path.abspath(os.curdir) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# if err.errno!= errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return path
"#LINE# #TAB# for testcase_name in testcases_root: #LINE# #TAB# #TAB# if os.path.isdir(testcase_name) and os.path.basename(testcase_name #LINE# #TAB# #TAB# #TAB# )!= '__init__.py': #LINE# #TAB# #TAB# #TAB# with open(testcase_name, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# testcase_cfg = get_testcase_cfg(testcase_name, svn_dir) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# testcase_cfg.setdefault('auto_in_source', False) #LINE# #TAB# return config"
"#LINE# #TAB# file_name = 'game_logs/forward.csv' #LINE# #TAB# z = get_zip_file(game_url(season), file_name) #LINE# #TAB# df = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='""') #LINE# #TAB# df.columns = ['timestamp', 'game_logs/forward.csv'] #LINE# #TAB# return df"
"#LINE# #TAB# split_title = True #LINE# #TAB# for input_var, input_value in six.iteritems(input_names): #LINE# #TAB# #TAB# if input_value and isinstance(input_value, six.string_types): #LINE# #TAB# #TAB# #TAB# title = input_value #LINE# #TAB# #TAB# #TAB# split_title = False #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if not split_title: #LINE# #TAB# #TAB# return value, input_names #LINE# #TAB# return value, input_names"
#LINE# #TAB# context = {} #LINE# #TAB# rules = {} #LINE# #TAB# log_events = mod.getLogEvents() #LINE# #TAB# for event in log_events: #LINE# #TAB# #TAB# event_name = mod.getEventName() #LINE# #TAB# #TAB# if event.type == 'EVENT': #LINE# #TAB# #TAB# #TAB# rule = extract_rule_for_event(event) #LINE# #TAB# #TAB# #TAB# if rule: #LINE# #TAB# #TAB# #TAB# #TAB# context[event_name] = rule #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# context[event_name] = {} #LINE# #TAB# #TAB# rules[event_name] = rule #LINE# #TAB# return rules
#LINE# #TAB# result = dict(boxed_type) #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# result[key] = data[key] #LINE# #TAB# return result
#LINE# #TAB# unique_repo_list = list() #LINE# #TAB# for repo in repo_list: #LINE# #TAB# #TAB# if repo not in unique_repo_list: #LINE# #TAB# #TAB# #TAB# unique_repo_list.append(repo) #LINE# #TAB# return unique_repo_list
"#LINE# #TAB#ask = re.compile('(?=ask)\\b', re.MULTILINE) #LINE# #TAB# res = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# res.append(ask.search(text)) #LINE# #TAB# #TAB# if not res: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# ask = re.compile('(?=ask)\\b', re.MULTILINE) #LINE# #TAB# res.append(ask.search(text)) #LINE# #TAB# if len(res) > 0: #LINE# #TAB# #TAB# return''.join(res) #LINE# #TAB# return ''"
"#LINE# #TAB# subject_param = vicon.attrs.get('subject_' + name, param) #LINE# #TAB# if not subject_param: #LINE# #TAB# #TAB# return None #LINE# #TAB# res = subject_param.value #LINE# #TAB# if res is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return res"
#LINE# #TAB# if a.dtype!= dtype: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# res = [] #LINE# #TAB# if isinstance(mem_size_str, str): #LINE# #TAB# #TAB# tmp_size = int(round(mem_size_str, 2)) #LINE# #TAB# elif isinstance(mem_size_str, int) and reserve_time is not None: #LINE# #TAB# #TAB# tmp_size = int(round(reserve_time, 2)) #LINE# #TAB# else: #LINE# #TAB# #TAB# tmp_size = int(round(mem_size_str, 1)) #LINE# #TAB# for size in reversed(tmp_size): #LINE# #TAB# #TAB# res.append(size * 1024) #LINE# #TAB# return res"
#LINE# #TAB# with dbf.open(fn) as dbf: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# tokens = dbf.kwargs() #LINE# #TAB# #TAB# #TAB# for col in tokens: #LINE# #TAB# #TAB# #TAB# #TAB# if col not in usecols: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return tokens
"#LINE# #TAB# if len(cloud) < 4: #LINE# #TAB# #TAB# return cloud #LINE# #TAB# if not cloud[3].isdigit() and cloud[3] not in ('/', '-'): #LINE# #TAB# #TAB# return cloud #LINE# #TAB# if not cloud[3].isdigit(): #LINE# #TAB# #TAB# return cloud[:3] + '0' + cloud[3:] #LINE# #TAB# return cloud"
"#LINE# #TAB# output = {} #LINE# #TAB# for k, v in mapping.items(): #LINE# #TAB# #TAB# if isinstance(v, set): #LINE# #TAB# #TAB# #TAB# v = [v] #LINE# #TAB# #TAB# elif isinstance(v, str): #LINE# #TAB# #TAB# #TAB# v = v.decode('utf-8') #LINE# #TAB# #TAB# v = [v] #LINE# #TAB# #TAB# output[k] = v #LINE# #TAB# return output"
"#LINE# #TAB# start = s.find(char, 0) #LINE# #TAB# if start == -1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return start"
"#LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# px4_state = DCM_State(ATT.Roll, ATT.Pitch, ATT.Yaw) #LINE# #TAB# px4_state.update(IMU) #LINE# #TAB# return px4_state"
#LINE# #TAB# from pwd import getpwuid #LINE# #TAB# shell = getpwuid(os.getuid()) #LINE# #TAB# if not shell: #LINE# #TAB# #TAB# shell = '/bin/sh' #LINE# #TAB# return shell
"#LINE# #TAB# return { #LINE# #TAB# #TAB# 'vpc_id': vpc.id, #LINE# #TAB# #TAB# 'gateway_id': vpc.gateway_id, #LINE# #TAB# #TAB# 'username': vpc.username, #LINE# #TAB# #TAB# 'password': vpc.password, #LINE# #TAB# #TAB# 'protocol': vpc.protocol, #LINE# #TAB# #TAB#'region': vpc.region, #LINE# #TAB# #TAB# 'auth_provider': vpc.auth_provider, #LINE# #TAB# }"
#LINE# #TAB# if k not in o: #LINE# #TAB# #TAB# o[k] = {} #LINE# #TAB# if 'type' in o[k]: #LINE# #TAB# #TAB# o[k]['type'] = o[k]['type'] #LINE# #TAB# if 'group' in o[k]: #LINE# #TAB# #TAB# o[k]['group'] = o[k]['group'] + v #LINE# #TAB# elif 'key' in o[k]: #LINE# #TAB# #TAB# o[k]['key'] = o[k]['key'] + v #LINE# #TAB# return
"#LINE# #TAB# label = QLabel() #LINE# #TAB# label.setPixmap(QPixmap(get_image_path(name, default))) #LINE# #TAB# return label"
"#LINE# #TAB# if p.h.startswith('!= '): #LINE# #TAB# #TAB# f = item.font(0) #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# #TAB# item.setFont(0, f) #LINE# #TAB# elif p.h.startswith('!= '): #LINE# #TAB# #TAB# f = item.font(1) #LINE# #TAB# #TAB# f.setBold(False) #LINE# #TAB# else: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# name = path[0] + ext #LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for k, v in obj.iteritems(): #LINE# #TAB# #TAB# #TAB# yield construct_keep(v, name + ext) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for v in obj: #LINE# #TAB# #TAB# #TAB# yield construct_keep(v, name + ext) #LINE# #TAB# elif isinstance(obj, tuple): #LINE# #TAB# #TAB# for v in obj: #LINE# #TAB# #TAB# #TAB# yield construct_keep(v, path + ext) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield name"
"#LINE# #TAB# encoded = [] #LINE# #TAB# for y in x: #LINE# #TAB# #TAB# if isinstance(y, list): #LINE# #TAB# #TAB# #TAB# encoded.extend(encode_val(y)) #LINE# #TAB# #TAB# elif isinstance(y, tuple): #LINE# #TAB# #TAB# #TAB# for z in encode_val(y): #LINE# #TAB# #TAB# #TAB# #TAB# encoded.append(z) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encoded.append(encode(y)) #LINE# #TAB# return encoded"
"#LINE# #TAB# if textfile!= 'trip.txt': #LINE# #TAB# #TAB# raise ValueError('{} is not a valid file name'.format(textfile)) #LINE# #TAB# df = pd.read_csv(os.path.join(textfile_path, textfile), dtype={ #LINE# #TAB# #TAB# 'trip_id': object}, low_memory=False) #LINE# #TAB# if len(df) == 0: #LINE# #TAB# #TAB# raise ValueError('{} has no records'.format(os.path.join( #LINE# #TAB# #TAB# #TAB# textfile_path, textfile))) #LINE# #TAB# df['trip_time'] = pd.to_numeric(df['trip_time']) #LINE# #TAB# df.dropna(inplace=True) #LINE# #TAB# return df"
"#LINE# #TAB# css_path = None #LINE# #TAB# git_dir = join(expanduser('~'), '.git') #LINE# #TAB# if isdir(git_dir): #LINE# #TAB# #TAB# with open(git_dir, 'r') as css_file: #LINE# #TAB# #TAB# #TAB# css_path = join(git_dir, 'css') #LINE# #TAB# #TAB# return css_path #LINE# #TAB# for item in listdir(git_dir): #LINE# #TAB# #TAB# if isdir(item): #LINE# #TAB# #TAB# #TAB# css_path = join(git_dir, item) #LINE# #TAB# #TAB# #TAB# if isfile(css_path): #LINE# #TAB# #TAB# #TAB# #TAB# return css_path #LINE# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return cls
"#LINE# #TAB# if node.left is not None: #LINE# #TAB# #TAB# return reduce(operator.add, [node.left], compute_priority(node.right)) #LINE# #TAB# elif node.right is not None: #LINE# #TAB# #TAB# return reduce(operator.add, node.right) #LINE# #TAB# return 0"
"#LINE# #TAB# data = {} #LINE# #TAB# if dashboard_entry.plugin_entry_copy: #LINE# #TAB# #TAB# data.update(dashboard_entry.plugin_entry_copy.data) #LINE# #TAB# #TAB# data.update(walk_parent_data(dashboard_entry.parent, request)) #LINE# #TAB# return data"
#LINE# #TAB# attrs = fits.getdata(extname) #LINE# #TAB# param_coords = [] #LINE# #TAB# for p in attrs: #LINE# #TAB# #TAB# param_coords.append(np.array(p.coords[0])) #LINE# #TAB# return param_coords
"#LINE# #TAB# theta = 0.12 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"#LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_server_url(), school_id) #LINE# #TAB# resp = do_get_request(url) #LINE# #TAB# code = resp.code #LINE# #TAB# data = resp.data.get('data', {}) if not code else resp.msg #LINE# #TAB# if code: #LINE# #TAB# #TAB# logger.error('Error: Request: {}, Detail: {}'.format(url, data)) #LINE# #TAB# return code, data"
"#LINE# #TAB# blueprint = Blueprint( #LINE# #TAB# #TAB#'should_replace_item', #LINE# #TAB# #TAB# __name__, #LINE# #TAB# #TAB# template_folder='common', #LINE# #TAB# #TAB# static_folder='static', #LINE# #TAB# ) #LINE# #TAB# return blueprint"
"#LINE# #TAB# if 'geometry' in ns: #LINE# #TAB# #TAB# for key, value in ns.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, numbers.Number): #LINE# #TAB# #TAB# #TAB# #TAB# namespace[key] = verify_geometry(value) #LINE# #TAB# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# namespace[key] = verify_geometry(value) #LINE# #TAB# return ns"
#LINE# #TAB# plane = prefix + bytes(obj) + suffix #LINE# #TAB# if not plane: #LINE# #TAB# #TAB# return '' #LINE# #TAB# return plane
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return _mutmut_weight_code(config, exception=exception) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return 0"
"#LINE# #TAB# return isinstance(input_dict, dict) or len(input_dict.keys()) > 1 or input_dict.get( #LINE# #TAB# #TAB#'stem'): #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if alphabet and base_num.startswith('1'): #LINE# #TAB# #TAB# base_num = int(base_num[1:]) #LINE# #TAB# num = base_num #LINE# #TAB# while len(num) < base: #LINE# #TAB# #TAB# num *= alphabet[0] #LINE# #TAB# base_num, num = divmod(num, base) #LINE# #TAB# return num"
#LINE# #TAB# for p in entity_predictions: #LINE# #TAB# #TAB# if 'location' not in p: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# return {'__doc__': ''} #LINE# #TAB# for key in dir(settings): #LINE# #TAB# #TAB# if key.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# val = getattr(settings, key) #LINE# #TAB# #TAB# if callable(val): #LINE# #TAB# #TAB# #TAB# val = val() #LINE# #TAB# return {'__doc__': cls.__doc__}"
#LINE# #TAB# buff = nd4j_array.data() #LINE# #TAB# decoded_array = base64.b64decode(buff) #LINE# #TAB# return decoded_array
#LINE# #TAB# for cls in get_subclasses(Partition): #LINE# #TAB# #TAB# if cls.partition_name == name: #LINE# #TAB# #TAB# #TAB# return cls
#LINE# #TAB# value = data[attribute_name] #LINE# #TAB# return value
"#LINE# #TAB# headers = {'Authorization': 'Bearer'+ url} #LINE# #TAB# r = requests.get(url, headers=headers) #LINE# #TAB# if r.status_code == 401 or r.status_code == 401: #LINE# #TAB# #TAB# print('Authentication failed') #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# print('Authentication successful') #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# params = {'csv_path': base_file, 'csv_name': csv_name,'sep': sep, #LINE# #TAB# #TAB# 'convert_float': convert_float} #LINE# #TAB# return params"
"#LINE# #TAB# tab_evnt = cls.get_sql_table() #LINE# #TAB# qu1 = tab_evnt.select(tab_evnt.id_line, where=clause[1]) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# result = numpy.empty(vector_length) #LINE# #TAB# for i in range(0, vector_length): #LINE# #TAB# #TAB# result[i + shift] = vector[i] #LINE# #TAB# return result"
#LINE# #TAB# out = '' #LINE# #TAB# for c in name: #LINE# #TAB# #TAB# if c.isupper(): #LINE# #TAB# #TAB# #TAB# out += '_' #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# out += c.lower() #LINE# #TAB# return out
#LINE# #TAB# align_to_apic = {} #LINE# #TAB# return align_to_apic
"#LINE# #TAB# if value in [True, 't', 'true']: #LINE# #TAB# #TAB# return value #LINE# #TAB# elif value in [False, 'f', 'false']: #LINE# #TAB# #TAB# return value #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ArgumentTypeError"
#LINE# #TAB# split_type_string = type_string.split('.') #LINE# #TAB# if len(split_type_string) == 2: #LINE# #TAB# #TAB# query_type_string = split_type_string[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# query_type_string = '.'.join(split_type_string) #LINE# #TAB# return query_type_string
"#LINE# #TAB# ncontacts = len(contactMap) #LINE# #TAB# evals = np.zeros((ncontacts, ncontacts)) #LINE# #TAB# for c in contactMap: #LINE# #TAB# #TAB# contact = np.array(contactMap[c]) #LINE# #TAB# #TAB# e, v = np.linalg.eig(contact) #LINE# #TAB# #TAB# evals[c] = e #LINE# #TAB# return evals, evals"
"#LINE# #TAB# poly = byte_vnl_poly(nodes) #LINE# #TAB# if points is not None: #LINE# #TAB# #TAB# if isinstance(points, list): #LINE# #TAB# #TAB# #TAB# vtk_points = [] #LINE# #TAB# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# #TAB# vtk_points.append(byte_vnl_point(point, deep)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# #TAB# vtk_points.append(byte_vnl_point(node, point, dtype)) #LINE# #TAB# #TAB# return poly #LINE# #TAB# else: #LINE# #TAB# #TAB# return poly"
#LINE# #TAB# ip_address = str(ip_address) #LINE# #TAB# while True: #LINE# #TAB# #TAB# found = nmap.search(ip_address) #LINE# #TAB# #TAB# if found == None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# ip_address = found.ip_address #LINE# #TAB# return ip_address
"#LINE# #TAB# for parser, keywords in cls.suggestParsers.items(): #LINE# #TAB# #TAB# if ext in keywords: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return keywords[parser](text) #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# raise ValueError('{} is not a directory'.format(path)) #LINE# #TAB# source_list = cls() #LINE# #TAB# for source_file in [f for f in os.listdir(path) if f.endswith('.sdetect'): #LINE# #TAB# #TAB# source_file = os.path.join(path, source_file) #LINE# #TAB# #TAB# source_list.add_data(source_file) #LINE# #TAB# return source_list"
"#LINE# #TAB# images = [] #LINE# #TAB# preload_images = soup.find_all('preload_images') #LINE# #TAB# for img in preload_images: #LINE# #TAB# #TAB# if img.get('id') == 'CDS': #LINE# #TAB# #TAB# #TAB# for img_url in img.find_all('img'): #LINE# #TAB# #TAB# #TAB# #TAB# images.append(img_url.attrib['src']) #LINE# #TAB# filenames = [f.attrib['filename'] for f in images] #LINE# #TAB# return images, filenames"
"#LINE# #TAB# return pointlist and isinstance(pointlist, list #LINE# #TAB# #TAB# ) or isinstance(pointlist, np.ndarray) and len(pointlist) == 1 and pointlist[0 #LINE# #TAB# #TAB# ].dtype == 'bool'"
#LINE# #TAB# t = time.gps() #LINE# #TAB# s = t / 3600.0 #LINE# #TAB# return s
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno!= errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return True
#LINE# #TAB# name = os.path.expandvars(os.path.expanduser(name)) #LINE# #TAB# if name.startswith('__') and not name.endswith('__'): #LINE# #TAB# #TAB# name = name[1:] #LINE# #TAB# if '/' in name: #LINE# #TAB# #TAB# name = '/' + name #LINE# #TAB# return name
#LINE# #TAB# try: #LINE# #TAB# #TAB# return slice_obj.length() #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# import datetime #LINE# #TAB# update_time = '' #LINE# #TAB# for role in Role.objects.all(): #LINE# #TAB# #TAB# time_str = str(role.update_time) #LINE# #TAB# #TAB# if time_str == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# update_time = datetime.datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S') #LINE# #TAB# return update_time"
"#LINE# #TAB# with pkg_resources.resource_stream(package_name,'resources' #LINE# #TAB# #TAB# ) as stream: #LINE# #TAB# #TAB# if not ignore_cache: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return stream.read().decode('utf-8') #LINE# #TAB# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return stream.read().decode('utf-8') #LINE# #TAB# return None"
"#LINE# #TAB# aic = np.zeros(yk.size) #LINE# #TAB# aic[-1] = 0.0 #LINE# #TAB# for K in range(yk.size - 2, -1, -1): #LINE# #TAB# #TAB# aic[K] = aic[K + 1] + 2 * aic[K + 1] / (NF - K - 2) #LINE# #TAB# return aic"
"#LINE# #TAB# if np.max(y) == np.max(normalisation_parameters['y']): #LINE# #TAB# #TAB# new_y = y #LINE# #TAB# else: #LINE# #TAB# #TAB# new_y = np.normalise(y, normalisation_parameters['y']) #LINE# #TAB# return new_y"
"#LINE# #TAB# assert isinstance(var_instance, SymbolVAR) #LINE# #TAB# from symbols import UTC #LINE# #TAB# if bounds is not None: #LINE# #TAB# #TAB# assert len(bounds) == 1 #LINE# #TAB# #TAB# as_utc = UTC( #LINE# #TAB# #TAB# #TAB# bounds[0], #LINE# #TAB# #TAB# #TAB# var_instance.dtype, #LINE# #TAB# #TAB# #TAB# bounds[1], #LINE# #TAB# #TAB# #TAB# var_instance.offset, #LINE# #TAB# #TAB# #TAB# bounds[2], #LINE# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# as_utc = var_instance #LINE# #TAB# return as_utc"
"#LINE# #TAB# cmake_to_plot = False #LINE# #TAB# if cmake_with_sdist: #LINE# #TAB# #TAB# cmake_to_plot = True #LINE# #TAB# if isinstance(cmake_with_sdist, dict): #LINE# #TAB# #TAB# pass #LINE# #TAB# for command in commands: #LINE# #TAB# #TAB# if command in cmake_with_sdist.keys(): #LINE# #TAB# #TAB# #TAB# cmake_to_plot = True #LINE# #TAB# elif cmake_to_plot: #LINE# #TAB# #TAB# for command in commands: #LINE# #TAB# #TAB# #TAB# if command in cmake_to_plot: #LINE# #TAB# #TAB# #TAB# #TAB# cmake_to_plot = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return cmake_to_plot"
#LINE# #TAB# n = float(n) #LINE# #TAB# Ht_est = sum(Ht_est) / float(n) #LINE# #TAB# Hs_est = sum(Hs_est) / float(n) #LINE# #TAB# if (n - 1.0) * Ht_est == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# Hs_est_ = sum(Hs_est) / float(n) #LINE# #TAB# d_est = (Ht_est - Hs_est_) / float(n) #LINE# #TAB# return d_est
"#LINE# #TAB# b0 = 0.903 * 2 / 3 #LINE# #TAB# b1 = 8.181 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.0909 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['SM'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
#LINE# #TAB# encoding = [] #LINE# #TAB# seen = set() #LINE# #TAB# for label in labels: #LINE# #TAB# #TAB# if label in seen: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if label.lower() in seen: #LINE# #TAB# #TAB# #TAB# encoding.append(label) #LINE# #TAB# #TAB# #TAB# seen.add(label) #LINE# #TAB# return encoding
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return glob(ses_path + '/*.ibl', recursive=True) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# if os.path.isdir(ses_path): #LINE# #TAB# #TAB# #TAB# return [ses_path] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return []"
"#LINE# #TAB# psi = -0.0102 #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"#LINE# #TAB# config = get_default(based_on=based_on, filename=filename) #LINE# #TAB# return config"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# next_link = grab.xpath_one('//a[contains(@class, ""b-pager__next"")]') #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# logging.debug('No results found') #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# v = [] #LINE# #TAB# for x in qs: #LINE# #TAB# #TAB# mean = np.mean(x) #LINE# #TAB# #TAB# if not mean: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# v.append(mean) #LINE# #TAB# return v
"#LINE# #TAB# import os #LINE# #TAB# with open(os.path.join(os.path.dirname(__file__), 'data/pixel-map.csv'), 'r') as f: #LINE# #TAB# #TAB# reader = csv.reader(f) #LINE# #TAB# #TAB# next(reader, None) #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# if row[0] == 'N': #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# x_coords = int(row[0]) * 90 #LINE# #TAB# #TAB# #TAB# #TAB# y_coords = int(row[1]) * 90 #LINE# #TAB# return x_coords, y_coords"
"#LINE# #TAB# if not isinstance(data_dict, dict): #LINE# #TAB# #TAB# return data_dict #LINE# #TAB# result = action.resource_lookup(**data_dict) #LINE# #TAB# if'metadata_modified' not in result: #LINE# #TAB# #TAB# result['metadata_modified'] = timezone.now() #LINE# #TAB# #TAB# result['limit'] = 10 #LINE# #TAB# return result"
"#LINE# #TAB# kernel = np.ones(shape, dtype=np.float32) * np.ones(shape, dtype=np.float32) #LINE# #TAB# return kernel"
#LINE# #TAB# neighbors = y.neighbors #LINE# #TAB# d = dict() #LINE# #TAB# edges = dict() #LINE# #TAB# for neighbor in neighbors: #LINE# #TAB# #TAB# if neighbor[0] == neighbor[1]: #LINE# #TAB# #TAB# #TAB# edges[neighbor[0]] = neighbor[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# edges[neighbor[0]] = neighbor[1] #LINE# #TAB# d = dict() #LINE# #TAB# for neighbor in edges: #LINE# #TAB# #TAB# d[neighbor[0]] = y[neighbor[0]] #LINE# #TAB# #TAB# d[neighbor[1]].reverse() #LINE# #TAB# return d
#LINE# #TAB# if root is not None: #LINE# #TAB# #TAB# entities = graph.depth_first_search(root) #LINE# #TAB# else: #LINE# #TAB# #TAB# entities = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# output = graph.tuplet #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return '' #LINE# #TAB# else: #LINE# #TAB# #TAB# lines = [] #LINE# #TAB# #TAB# for entity in entities: #LINE# #TAB# #TAB# #TAB# lines.append('#TAB#'+ entity + '\n') #LINE# #TAB# #TAB# #TAB# for line in line: #LINE# #TAB# #TAB# #TAB# #TAB# lines.append('#TAB#'+ line) #LINE# #TAB# #TAB# output += '\n' #LINE# #TAB# return lines
#LINE# #TAB# if uri in filter_list: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# loader_lock_var = framework.Var(attr_name) #LINE# #TAB# if not loader_lock_var.exists(): #LINE# #TAB# #TAB# new_attr = framework.Var(attr_name) #LINE# #TAB# #TAB# loader_lock_var.exists() #LINE# #TAB# #TAB# return new_attr #LINE# #TAB# return loader_lock_var
"#LINE# #TAB# payload = {'cmd': command, 'cwd': os.getcwd()} #LINE# #TAB# if path: #LINE# #TAB# #TAB# payload['path'] = path #LINE# #TAB# return payload"
"#LINE# #TAB# json_data = json.dumps(data, sort_keys=True, indent=4) #LINE# #TAB# if isinstance(json_data, str): #LINE# #TAB# #TAB# payload_str = data #LINE# #TAB# elif isinstance(json_data, dict): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# payload_str = json.dumps(json_data, sort_keys=True, indent=4) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# #TAB# f'Payload should be of type dict or str. Got {type(json_data)}') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# f'Payload should be of type dict or str. Got {type(json_data)}') #LINE# #TAB# return payload_str"
#LINE# #TAB# content = {} #LINE# #TAB# if path.is_file(): #LINE# #TAB# #TAB# content['text'] = path.read_text() #LINE# #TAB# #TAB# if path.suffix == '.yml': #LINE# #TAB# #TAB# #TAB# content['text'] = path.read_text() #LINE# #TAB# #TAB# if path.suffix == '.yaml': #LINE# #TAB# #TAB# #TAB# content['text'] = path.read_text() #LINE# #TAB# #TAB# if path.suffix == '.json': #LINE# #TAB# #TAB# #TAB# content['json'] = path.read_json() #LINE# #TAB# #TAB# if path.is_dir(): #LINE# #TAB# #TAB# #TAB# content['dirs'] = _build_dirs(path.resolve()) #LINE# #TAB# return content
#LINE# #TAB# for f in written_files: #LINE# #TAB# #TAB# if 'vocab' not in f: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# scope = [] #LINE# #TAB# if memory: #LINE# #TAB# #TAB# scope.append('--memory={}'.format(memory)) #LINE# #TAB# return scope
"#LINE# #TAB# dim = arr.shape[index] #LINE# #TAB# grad = np.zeros(dim, dtype=g.dtype) #LINE# #TAB# is_sparse = arr.flags.c_contiguous #LINE# #TAB# if is_sparse: #LINE# #TAB# #TAB# grad = _sparse_to_dense(g) #LINE# #TAB# if grad.shape!= dim: #LINE# #TAB# #TAB# grad = _sparse_to_dense(g) #LINE# #TAB# return dim, grad"
#LINE# #TAB# nonzeros = A.nonzero() #LINE# #TAB# if nonzeros.size: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# for i, item in enumerate(mcs.items): #LINE# #TAB# #TAB# del items[i] #LINE# #TAB# return items"
"#LINE# #TAB# return {'id': obj.id,'sectie': obj.sectie, 'capakey': {'id': obj.capakey.id, #LINE# #TAB# #TAB# 'name': obj.name}}"
#LINE# #TAB# user_policy = get_account_policy(user=user) #LINE# #TAB# if not user_policy: #LINE# #TAB# #TAB# return ISecurityPolicy() #LINE# #TAB# return user_policy
#LINE# #TAB# res = [] #LINE# #TAB# for byte in byteData: #LINE# #TAB# #TAB# res.append(utcTextToU16le(byte)) #LINE# #TAB# return res
"#LINE# #TAB# n_sig = params[0] #LINE# #TAB# n_bkg = params[1] #LINE# #TAB# lr_source = np.zeros((n_sig, n_bkg)) #LINE# #TAB# lr_background = np.zeros((n_bkg, n_bkg)) #LINE# #TAB# for i in range(n_sig): #LINE# #TAB# #TAB# bkg = background_2d[i] #LINE# #TAB# #TAB# s = signal_2d[i] #LINE# #TAB# #TAB# lr_source[i] = -np.log(np.sum(s * bkg)) #LINE# #TAB# #TAB# lr_background[i] = -np.sum(np.log(bkg)) #LINE# #TAB# loss = lr_source - lr_background #LINE# #TAB# return loss"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return utils.find_file(flow_name) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if model.description!= '': #LINE# #TAB# #TAB# model.description += '\n' #LINE# #TAB# sbo = '' #LINE# #TAB# if model.objective_type == 'C': #LINE# #TAB# #TAB# sbo +='sbo-c' #LINE# #TAB# if model.objective_type == 'D': #LINE# #TAB# #TAB# sbo +='sbo-d' #LINE# #TAB# if model.objective_type == 'E': #LINE# #TAB# #TAB# sbo +='sbo-e' #LINE# #TAB# return sbo
"#LINE# #TAB# rectangle = rectangle_from_string(extent) #LINE# #TAB# x = rectangle[0] * 1000 #LINE# #TAB# y = rectangle[1] * 1000 #LINE# #TAB# width = rectangle[2] * 1000 #LINE# #TAB# height = rectangle[3] * 1000 #LINE# #TAB# return x, y, width, height"
"#LINE# #TAB# module_scope_name = module_scope[:-1] #LINE# #TAB# model_scope_name = model_name #LINE# #TAB# while model_scope_name in dir(module_scope): #LINE# #TAB# #TAB# model_scope_name = '%s.%s' % (module_scope_name, model_name) #LINE# #TAB# if not module_scope_name.endswith('.'): #LINE# #TAB# #TAB# module_scope_name = module_scope[:-1] #LINE# #TAB# if module_scope_name == '': #LINE# #TAB# #TAB# return None, None #LINE# #TAB# scope = getattr(module_scope, model_scope_name) #LINE# #TAB# if not scope: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# return scope, model_scope_name"
"#LINE# #TAB# if nparray % 2 == 1: #LINE# #TAB# #TAB# rgb = np.linspace(0, 1, nparray) #LINE# #TAB# #TAB# bgr = np.linspace(0, 1, nparray) #LINE# #TAB# else: #LINE# #TAB# #TAB# rgb = np.linspace(0, 1, nparray) #LINE# #TAB# #TAB# bgr = np.linspace(0, 1, nparray) #LINE# #TAB# return rgb, bgr"
#LINE# #TAB# unit = int(epoch_time / 1000000000.0) #LINE# #TAB# human_time = str(epoch_time) #LINE# #TAB# if unit =='s': #LINE# #TAB# #TAB# human_time ='s' #LINE# #TAB# elif unit =='m': #LINE# #TAB# #TAB# human_time ='m' #LINE# #TAB# elif unit == 'd': #LINE# #TAB# #TAB# human_time = 'd' #LINE# #TAB# return human_time
"#LINE# #TAB# if custom_directory is not None: #LINE# #TAB# #TAB# folder_app = os.path.join(custom_directory, 'dataset') #LINE# #TAB# if not os.path.isdir(folder_app): #LINE# #TAB# #TAB# os.makedirs(folder_app) #LINE# #TAB# return folder_app"
#LINE# #TAB# executable_user = None #LINE# #TAB# target_architecture = target.GetArchitecture() #LINE# #TAB# if target_architecture: #LINE# #TAB# #TAB# executable_user = target_architecture[0] #LINE# #TAB# return executable_user
"#LINE# #TAB# if ordchr in ['A', 'C', 'G', 'T']: #LINE# #TAB# #TAB# return '.' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ordchr"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# snapshot_tree = get_snapshot_tree(name, root) #LINE# #TAB# #TAB# return snapshot_tree #LINE# #TAB# except: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# tris = [] #LINE# #TAB# for i in range(len(vec1)): #LINE# #TAB# #TAB# norm = vec1[i] ** 2 + vec1[i + 1] ** 2 + vec1[i] ** 2 #LINE# #TAB# #TAB# tris.append((norm.real, norm.imag)) #LINE# #TAB# return tris"
#LINE# #TAB# output_name = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# output_name = msa.midi.generate_name(number=number) #LINE# #TAB# #TAB# if output_name == '': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# number += 1 #LINE# #TAB# return output_name
#LINE# #TAB# version = inflection.singularize(name) #LINE# #TAB# version = inflection.camelize(version) #LINE# #TAB# version = inflection.camelize(version) #LINE# #TAB# return version
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t') #LINE# #TAB# if isdatetime: #LINE# #TAB# #TAB# df = df.dt.to_datetime() #LINE# #TAB# return df"
#LINE# #TAB# _handler = find_ctrl_handler(control) #LINE# #TAB# return _handler is not None
"#LINE# #TAB# result = function_name #LINE# #TAB# for i, arg in enumerate(argument_list): #LINE# #TAB# #TAB# result += '(' + str(arg) + ')' #LINE# #TAB# return result"
"#LINE# #TAB# process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, #LINE# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# process.communicate() #LINE# #TAB# terminal_output = process.returncode #LINE# #TAB# return terminal_output"
#LINE# #TAB# vm = [] #LINE# #TAB# from_table_keys = set(from_table.keys()) #LINE# #TAB# to_table_keys = set(to_table.keys()) #LINE# #TAB# for key in from_table_keys: #LINE# #TAB# #TAB# while key in to_table_keys: #LINE# #TAB# #TAB# #TAB# vm.append(key.key) #LINE# #TAB# for key in from_table_keys: #LINE# #TAB# #TAB# vm.append(key.key) #LINE# #TAB# return vm
#LINE# #TAB# if a['mtime'] > b['mtime']: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# elif a['mtime'] == b['mtime']: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0
"#LINE# #TAB# if job_class is None: #LINE# #TAB# #TAB# job_class = settings.JOB_CLASS #LINE# #TAB# prop_name = 'RQ_JOB_CLASS' #LINE# #TAB# module = import_module(prop_name) #LINE# #TAB# if hasattr(module, prop_name): #LINE# #TAB# #TAB# prop_class = getattr(module, prop_name) #LINE# #TAB# return prop_class"
#LINE# #TAB# country = request.matchdict['country'] #LINE# #TAB# if country: #LINE# #TAB# #TAB# return {'country': country} #LINE# #TAB# return {}
"#LINE# #TAB# home_folder = os.path.join(os.environ['HOME'], '.gridcal') #LINE# #TAB# if not os.path.exists(home_folder): #LINE# #TAB# #TAB# os.makedirs(home_folder) #LINE# #TAB# return home_folder"
"#LINE# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# raise ValueError('Cannot parse file: {}'.format(path)) #LINE# #TAB# if path.endswith('yaml'): #LINE# #TAB# #TAB# out = yaml.safe_load(path) #LINE# #TAB# elif path.endswith('json'): #LINE# #TAB# #TAB# out = json.safe_load(path) #LINE# #TAB# else: #LINE# #TAB# #TAB# out = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# data = yaml.safe_load(path) #LINE# #TAB# except yaml.YAMLError: #LINE# #TAB# #TAB# raise YAMLError('Cannot parse file: {}'.format(path)) #LINE# #TAB# if not isinstance(data, dict): #LINE# #TAB# #TAB# raise JSONError('Cannot parse file: {}'.format(path)) #LINE# #TAB# return data"
"#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# response = request.json_body #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid request body') #LINE# #TAB# #TAB# if 400 <= response.status_code < 500: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid response code: %d - %d' % (response.status_code, #LINE# #TAB# #TAB# #TAB# #TAB# response.reason)) #LINE# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# sign_file = open(filename, 'rb') #LINE# #TAB# sign_file.seek(0) #LINE# #TAB# arrays = [] #LINE# #TAB# for line in sign_file: #LINE# #TAB# #TAB# array = line.read() #LINE# #TAB# #TAB# array = np.array(array) #LINE# #TAB# #TAB# arrays.append(array) #LINE# #TAB# sign_file.close() #LINE# #TAB# return arrays"
"#LINE# #TAB# if row.start_mark == 'S': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# street_name, number = row.text.split('N') #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# street_name = row.text #LINE# #TAB# #TAB# #TAB# number = '' #LINE# #TAB# else: #LINE# #TAB# #TAB# street_name = row.text.split('N')[0] #LINE# #TAB# #TAB# if street_name.isdigit(): #LINE# #TAB# #TAB# #TAB# return {'street_name': street_name, 'number': number} #LINE# #TAB# return {'street_name': street_name, 'number': row.text}"
"#LINE# #TAB# current_dir = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# specfile = os.path.join(current_dir,'spec.py') #LINE# #TAB# f = open(specfile, 'r') #LINE# #TAB# for line in f: #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# f.close() #LINE# #TAB# spec_path = os.path.join(current_dir, 'log') #LINE# #TAB# with open(spec_path, 'w') as f: #LINE# #TAB# #TAB# f.write(line) #LINE# #TAB# return spec_path"
#LINE# #TAB# with open(path) as fh: #LINE# #TAB# #TAB# content = fh.read() #LINE# #TAB# info_dict = json.loads(content) #LINE# #TAB# return info_dict
#LINE# #TAB# dolphin = load_dolphin() #LINE# #TAB# scenarios = dolphin.scenarios_mapping_hits(label) #LINE# #TAB# return scenarios
#LINE# #TAB# try: #LINE# #TAB# #TAB# del servers[library_name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return find_in_list(value, type_) #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# key = list(value.keys())[0] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return type_(value[key]) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if hasattr(type_, '__module__'): #LINE# #TAB# #TAB# module = type_.__module__ #LINE# #TAB# #TAB# if hasattr(module, '__name__'): #LINE# #TAB# #TAB# #TAB# return find_in_list(value, type_.__name__) #LINE# #TAB# return value"
"#LINE# #TAB# text = text.rstrip('\n') #LINE# #TAB# if len(text) > 0: #LINE# #TAB# #TAB# text = text.replace('\n', end) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(IMAGE_YAML, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# return f.write(text) #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# already_loaded = False #LINE# #TAB# for auc in augs: #LINE# #TAB# #TAB# if 'kolibri' in auc: #LINE# #TAB# #TAB# #TAB# if not already_loaded: #LINE# #TAB# #TAB# #TAB# #TAB# with open(os.path.join(os.path.dirname(__file__), 'kolibri.kol') #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ) as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# kolibri = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# if kolibri!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# already_loaded = True #LINE# #TAB# return already_loaded"
"#LINE# #TAB# exp = [x[0] for x in database.execute( #LINE# #TAB# #TAB# ""select word from pg_catalog.pg_class where relname = '{0}'"".format( #LINE# #TAB# #TAB# term))] #LINE# #TAB# if not exp: #LINE# #TAB# #TAB# return term #LINE# #TAB# else: #LINE# #TAB# #TAB# word = exp[0].strip() #LINE# #TAB# #TAB# database.execute( #LINE# #TAB# #TAB# #TAB# ""select word from pg_catalog.pg_class where relname = '{0}' and relname = '{1}'"".format( #LINE# #TAB# #TAB# #TAB# term, exp[0].strip())) #LINE# #TAB# #TAB# if word: #LINE# #TAB# #TAB# #TAB# return word #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return ''"
"#LINE# #TAB# position_x = 0 #LINE# #TAB# position_y = 0 #LINE# #TAB# while position_x < len(source): #LINE# #TAB# #TAB# msg = mlog.recv_match(source[position_x:position_y]) #LINE# #TAB# #TAB# if msg: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# position_x += 1 #LINE# #TAB# if position_y < len(source) - 1: #LINE# #TAB# #TAB# if mlog.recv_match(source[position_x]): #LINE# #TAB# #TAB# #TAB# position_y += 1 #LINE# #TAB# return position_x, position_y"
"#LINE# #TAB# redis = redis.StrictRedis(host=os.environ['REDIS_HOST'], port=os.environ[ #LINE# #TAB# #TAB# 'REDIS_PORT']) #LINE# #TAB# return redis"
#LINE# #TAB# if cls.__table__ and cls.__table__.primary_key: #LINE# #TAB# #TAB# return cls.__table__.primary_key[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# mn = torch.zeros(len(minval), dtype=float) #LINE# #TAB# mx = torch.zeros(len(maxval), dtype=float) #LINE# #TAB# for i in range(1, len(minval)): #LINE# #TAB# #TAB# mn[i] = torch.sum(minval[i] - maxval[i]) #LINE# #TAB# #TAB# mx[i + 1] = torch.sum(maxval[i] - minval) #LINE# #TAB# mean = torch.mean(mn) #LINE# #TAB# var = torch.var(mn) #LINE# #TAB# return mn, var"
"#LINE# #TAB# if window == 'hanning': #LINE# #TAB# #TAB# return impulse + np.roll(impulse, int(ntaps / 2 - window)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return impulse"
#LINE# #TAB# if options.count == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# for oneliner_id in term.list_oneliners: #LINE# #TAB# #TAB# oneliner_title = oneliner_id.text.strip() #LINE# #TAB# #TAB# if oneliner_title: #LINE# #TAB# #TAB# #TAB# yield term.margin(top_margin + oneliner_title, offset) #LINE# #TAB# #TAB# #TAB# oneliner_id = oneliner_id.split('-')[0] #LINE# #TAB# #TAB# #TAB# yield term.margin(top_margin + oneliner_id, offset) #LINE# #TAB# #TAB# #TAB# oneliner_title ='<{}>'.format(oneliner_title) #LINE# #TAB# #TAB# #TAB# yield term.margin(top_margin + oneliner_id, offset) #LINE# #TAB# #TAB# #TAB# offset -= 1"
"#LINE# #TAB# if isinstance(obs_dict, dict): #LINE# #TAB# #TAB# return obs_dict.values() #LINE# #TAB# else: #LINE# #TAB# #TAB# return obs_dict"
"#LINE# #TAB# visitor = {} #LINE# #TAB# for k, v in inspect.getmembers(func): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if isinstance(v, type): #LINE# #TAB# #TAB# #TAB# #TAB# visitor[k] = prepare_visitor(v) #LINE# #TAB# #TAB# #TAB# elif isinstance(v, str): #LINE# #TAB# #TAB# #TAB# #TAB# visitor[k] = prepare_visitor(v) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return visitor"
"#LINE# #TAB# if target is not None: #LINE# #TAB# #TAB# conf = copy.deepcopy(conf) #LINE# #TAB# #TAB# if 'gerrit' in target: #LINE# #TAB# #TAB# #TAB# target['gerrit'] = target['gerrit'][0] #LINE# #TAB# #TAB# if 'giturl' in target: #LINE# #TAB# #TAB# #TAB# target['giturl'] = target['giturl'][0] #LINE# #TAB# for plugin in target.get('plugins', []): #LINE# #TAB# #TAB# if plugin.get('project', project)!= project: #LINE# #TAB# #TAB# #TAB# del target['plugins'][plugin.get('project', project) #LINE# #TAB# #TAB# #TAB# #TAB# ]"
"#LINE# #TAB# d = [] #LINE# #TAB# for i, rel in enumerate(correlations): #LINE# #TAB# #TAB# d.append(np.argmin(rel)) #LINE# #TAB# vnl_indices = [] #LINE# #TAB# for i, rel in enumerate(correlations): #LINE# #TAB# #TAB# for j, op in enumerate(rel): #LINE# #TAB# #TAB# #TAB# if op == 0: #LINE# #TAB# #TAB# #TAB# #TAB# vnl_indices.append(n_states - i - 1) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# vnl_indices.append(n_states - i - 1) #LINE# #TAB# return vnl_indices"
#LINE# #TAB# links = [] #LINE# #TAB# for msg in msgs: #LINE# #TAB# #TAB# if len(msg.split('\n')) > max_len: #LINE# #TAB# #TAB# #TAB# link = msg.split('\n')[-1] #LINE# #TAB# #TAB# #TAB# if len(link) > max_len: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# links.append(link) #LINE# #TAB# return links
"#LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# raise IOError('File {} not found!'.format(filepath)) #LINE# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# dat = f.read() #LINE# #TAB# #TAB# for line in dat: #LINE# #TAB# #TAB# #TAB# if not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# idx = line.index(':') #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# filepath.remove(line) #LINE# #TAB# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return parse_float(string, fmt=fmt, force=True, output=output) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# bytes = bytearray() #LINE# #TAB# for key, value in ctx.items(): #LINE# #TAB# #TAB# if key == 'checksum': #LINE# #TAB# #TAB# #TAB# return bytes #LINE# #TAB# #TAB# elif isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# bytes += value #LINE# #TAB# return bytes"
"#LINE# #TAB# global _language_handler, _language_handler_lock #LINE# #TAB# if not _language_handler: #LINE# #TAB# #TAB# _language_handler = LanguageHandler(rootdir) #LINE# #TAB# #TAB# _language_handler_lock.acquire() #LINE# #TAB# if not _language_handler_lock.acquire(): #LINE# #TAB# #TAB# _language_handler_lock.release() #LINE# #TAB# _language_handler = None"
"#LINE# #TAB# if type_ in (inet_pton, inet_pton): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif type_ in (ipv4_pton, ipv6_pton): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif type_ in (ipv6_pton, ipv6_pton): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# ret = {'engine': grp} #LINE# #TAB# for k in grp.keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ret[k] = grp[k] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# ret[k] = grp[k] #LINE# #TAB# return ret
"#LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_card_server_url(), sn) #LINE# #TAB# resp = do_get_request(url=url, token=class_card_server_token(), #LINE# #TAB# #TAB# school_id=school_id) #LINE# #TAB# code = resp.code #LINE# #TAB# data = resp.data.get('data', {}) if not code else resp.msg #LINE# #TAB# if code: #LINE# #TAB# #TAB# logger.error('Error: Request: {}, Detail: {}'.format(url, data)) #LINE# #TAB# return code, data"
"#LINE# #TAB# meta = getattr(instance, '_meta', None) #LINE# #TAB# if not meta: #LINE# #TAB# #TAB# return #LINE# #TAB# lines = [] #LINE# #TAB# for field in meta.get_fields(): #LINE# #TAB# #TAB# if not field.editable or field.name == 'id': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# lines.append(field.name) #LINE# #TAB# return lines"
"#LINE# #TAB# local_info = {'__version__': __version__} #LINE# #TAB# with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), #LINE# #TAB# #TAB#'version.py'), 'rt') as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# m = re.match(""__version__ = '([^']+)'"", line) #LINE# #TAB# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# #TAB# local_info['version'] = m.group(1) #LINE# #TAB# return local_info"
"#LINE# #TAB# for column, value in results.items(): #LINE# #TAB# #TAB# if column == 'Mean': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# for row in value: #LINE# #TAB# #TAB# #TAB# #TAB# yield row #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for row in value.values(): #LINE# #TAB# #TAB# #TAB# #TAB# yield row #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield column, value"
"#LINE# #TAB# index = concept_cd.find(join_char) #LINE# #TAB# if index == -1: #LINE# #TAB# #TAB# category_cd = concept_cd #LINE# #TAB# #TAB# data_label = None #LINE# #TAB# else: #LINE# #TAB# #TAB# category_cd = concept_cd[:index] #LINE# #TAB# #TAB# data_label = concept_cd[index + 1:].strip() #LINE# #TAB# return category_cd, data_label"
"#LINE# #TAB# if not is_valid(signature): #LINE# #TAB# #TAB# raise ValueError('Invalid signature') #LINE# #TAB# hasher = RSA.new(key) #LINE# #TAB# try: #LINE# #TAB# #TAB# hasher.verify(xml, signature) #LINE# #TAB# except InvalidSignature: #LINE# #TAB# #TAB# if c14n_exc: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return True"
#LINE# #TAB# if shape == 'gaus': #LINE# #TAB# #TAB# sigma = 2 * sigma #LINE# #TAB# if shape == 'uniform': #LINE# #TAB# #TAB# sigma = sigma / 2 #LINE# #TAB# if shape == 'linear': #LINE# #TAB# #TAB# sigma = sigma * 0.5 #LINE# #TAB# return sigma
#LINE# #TAB# name = name.strip() #LINE# #TAB# if len(name) > 20: #LINE# #TAB# #TAB# name = name[:20] + '...' #LINE# #TAB# return name
"#LINE# #TAB# if hr_data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if filename is None: #LINE# #TAB# #TAB# return filename #LINE# #TAB# with gzip.GzipFile(filename, 'wb') as f: #LINE# #TAB# #TAB# for item in hr_data: #LINE# #TAB# #TAB# #TAB# f.write(item[0] + item[1] + '\n') #LINE# #TAB# return filename"
"#LINE# #TAB# mime, _ = mimetypes.guess_type(path) #LINE# #TAB# if mime is None or mime == 'application/octet-stream': #LINE# #TAB# #TAB# return path #LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# mime_hash = hashlib.sha512(f.read()).hexdigest() #LINE# #TAB# return mime_hash[:size], mime_hash[size:]"
"#LINE# #TAB# if not config_dir: #LINE# #TAB# #TAB# config_dir = DEFAULT_CONFIG_DIR #LINE# #TAB# config_path = os.path.join(config_dir, DEFAULT_CONFIG_FILE) #LINE# #TAB# data = load_config(config_path) #LINE# #TAB# return data"
"#LINE# #TAB# keys = list(params) #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if type(params[key]) == dict: #LINE# #TAB# #TAB# #TAB# for inner_key, inner_value in params[key].items(): #LINE# #TAB# #TAB# #TAB# #TAB# params[key][inner_key] = normalize_key(inner_value) #LINE# #TAB# #TAB# #TAB# #TAB# params[key] = inner_key #LINE# #TAB# #TAB# elif type(params[key]) == list: #LINE# #TAB# #TAB# #TAB# for item in params[key]: #LINE# #TAB# #TAB# #TAB# #TAB# item = normalize_keys(item) #LINE# #TAB# #TAB# #TAB# #TAB# params[key][item] = tokens(item) #LINE# #TAB# return params"
"#LINE# #TAB# content = [] #LINE# #TAB# for child in node: #LINE# #TAB# #TAB# for k, v in child.iteritems(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, etree._Element): #LINE# #TAB# #TAB# #TAB# #TAB# if k.tag == 'XML': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# content.append('XML') #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# content.append(k) #LINE# #TAB# return content"
"#LINE# #TAB# for dir_ in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# if '__init__.py' in dir_: #LINE# #TAB# #TAB# #TAB# yield os.path.join(dir_, 'filters') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield dir_"
#LINE# #TAB# url = order_url + '/types/' + account + '/order/' #LINE# #TAB# data = requests.get(url) #LINE# #TAB# if data.status_code == 200: #LINE# #TAB# #TAB# return data.text #LINE# #TAB# else: #LINE# #TAB# #TAB# if data.status_code == 404: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return data.text
#LINE# #TAB# notifications = registered_notifications() #LINE# #TAB# for notification in notifications: #LINE# #TAB# #TAB# notification.stop() #LINE# #TAB# return True
#LINE# #TAB# ret = libcudart.cuda_user(id) #LINE# #TAB# if ret == -1: #LINE# #TAB# #TAB# raise cudaError('cuda_user returned -1') #LINE# #TAB# return ret
#LINE# #TAB# try: #LINE# #TAB# #TAB# __IPYTHON__ #LINE# #TAB# #TAB# return True #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# custom_name = '' #LINE# #TAB# for char in resource_uri: #LINE# #TAB# #TAB# if char in RAML_CUSTOM_URI_CHARS: #LINE# #TAB# #TAB# #TAB# custom_name = RAML_CUSTOM_URI_CHARS[char] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# pass #LINE# #TAB# return custom_name
"#LINE# #TAB# package_id = None #LINE# #TAB# packages = subprocess.check_output(['pip3', 'freeze']) #LINE# #TAB# for package in packages.split('\n'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# package_id = package.split(' ')[0] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return package_id"
"#LINE# #TAB# covariance_matrix = np.eye(4) #LINE# #TAB# for point in points: #LINE# #TAB# #TAB# covariance_matrix[point[0], point[1]] += 1 #LINE# #TAB# return covariance_matrix"
"#LINE# #TAB# command = ['git','rev-parse', '--git-dir', dirpath] #LINE# #TAB# try: #LINE# #TAB# #TAB# return subprocess.call(command, stdout=subprocess.DEVNULL, stderr= #LINE# #TAB# #TAB# #TAB# subprocess.DEVNULL) #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# if e.returncode!= 0: #LINE# #TAB# #TAB# #TAB# raise"
"#LINE# #TAB# A = np.array(A) #LINE# #TAB# try: #LINE# #TAB# #TAB# np.random.shuffle(A) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# if not np.all(np.array([np.random.choice(A.shape) for i in range(A. #LINE# #TAB# #TAB# shape[1])] + [np.random.choice(A.shape[0]) for i in range(A. #LINE# #TAB# #TAB# shape[1])] + [np.random.choice(A.shape[2]) for i in range(A. #LINE# #TAB# #TAB# shape[2])]): #LINE# #TAB# #TAB# bandwidth_before = A[permutation[:, 0]].mean() #LINE# #TAB# #TAB# bandwidth_after = A[permutation[:, 1]].std() #LINE# #TAB# return A, bandwidth_before, bandwidth_after"
"#LINE# #TAB# return {'year': date.year,'month': date.month, 'day': date.day, #LINE# #TAB# #TAB# 'hour': date.hour,'minute': date.minute,'second': date.second}"
"#LINE# #TAB# worker = threading.Thread(target=_app_t_single, args=(fastq, pkl)) #LINE# #TAB# worker.start() #LINE# #TAB# return worker"
"#LINE# #TAB# if hasattr(filename, 'decode'): #LINE# #TAB# #TAB# filename = filename.decode() #LINE# #TAB# m = member_cache.get(filename, default) #LINE# #TAB# if not m: #LINE# #TAB# #TAB# m = default #LINE# #TAB# return m"
"#LINE# #TAB# #TAB# params = parse_qsl(val) #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# for k, v in params.items(): #LINE# #TAB# #TAB# #TAB# if k in cls.params: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# result[k] = ','.join(v) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# result[k] = v #LINE# #TAB# #TAB# return result"
"#LINE# #TAB# ret = {} #LINE# #TAB# template_dir = os.path.join(os.path.dirname(__file__), 'djinfo') #LINE# #TAB# with open(template_dir, 'w') as f: #LINE# #TAB# #TAB# ret = json.load(f) #LINE# #TAB# return ret"
#LINE# #TAB# obj_files = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# if is_python_file(f): #LINE# #TAB# #TAB# #TAB# obj_files.append(f) #LINE# #TAB# #TAB# elif os.path.isdir(f): #LINE# #TAB# #TAB# #TAB# obj_files.extend(search_obj_files(f)) #LINE# #TAB# return obj_files
#LINE# #TAB# if not description: #LINE# #TAB# #TAB# return description #LINE# #TAB# if not description.startswith('A'): #LINE# #TAB# #TAB# return description #LINE# #TAB# if not description.endswith('.'): #LINE# #TAB# #TAB# return description #LINE# #TAB# return description + '.'
#LINE# #TAB# unique_doc = {} #LINE# #TAB# for ws in nb['worksheets']: #LINE# #TAB# #TAB# for cell in ws['cells']: #LINE# #TAB# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# cell['source'] = cell['source'][:-1] #LINE# #TAB# #TAB# #TAB# #TAB# unique_doc[cell['source']] = cell #LINE# #TAB# return unique_doc
"#LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# return s.decode('utf8') #LINE# #TAB# return s"
#LINE# #TAB# orig = js_table(key) #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# js_table(original) #LINE# #TAB# #TAB# del orig
"#LINE# #TAB# #TAB# if not issubclass(child_class, this_abc): #LINE# #TAB# #TAB# #TAB# raise ValueError('%r is not a subclass of %s' % (child_class, this_abc)) #LINE# #TAB# #TAB# return child_class"
"#LINE# #TAB# pairs = [] #LINE# #TAB# for d in dir_list: #LINE# #TAB# #TAB# prev = None #LINE# #TAB# #TAB# for tmp_dir in d: #LINE# #TAB# #TAB# #TAB# if tmp_dir in DIR_STRS: #LINE# #TAB# #TAB# #TAB# #TAB# if preprocess: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pairs.append((prev, tmp_dir)) #LINE# #TAB# #TAB# #TAB# #TAB# prev = tmp_dir #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# prev = tmp_dir #LINE# #TAB# #TAB# pairs.append((prev, tmp_dir)) #LINE# #TAB# return pairs"
"#LINE# #TAB# global _log_properties #LINE# #TAB# _log_properties = { #LINE# #TAB# #TAB#'version': _DEFAULT_LOG_VERSION, #LINE# #TAB# #TAB#'message': _DEFAULT_LOG_MESSAGE, #LINE# #TAB# #TAB# 'level': _DEFAULT_LOG_LEVEL, #LINE# #TAB# #TAB# 'process': _DEFAULT_LOG_PROCESSING, #LINE# #TAB# }"
#LINE# #TAB# encrypted_models = {} #LINE# #TAB# for model_object in references_json: #LINE# #TAB# #TAB# encrypted_model_object = encrypt_model(model_object) #LINE# #TAB# #TAB# encrypted_models[encrypted_model_object['id']] = encrypted_model_object #LINE# #TAB# return encrypted_models
#LINE# #TAB# app = _get_app() #LINE# #TAB# if app is None: #LINE# #TAB# #TAB# raise ContextIsNotInitializedError #LINE# #TAB# return app
#LINE# #TAB# if num < 1: #LINE# #TAB# #TAB# raise ValueError('Number must be >= 1') #LINE# #TAB# if scheme == 'two': #LINE# #TAB# #TAB# return range(num) #LINE# #TAB# ret = [] #LINE# #TAB# for i in range(num): #LINE# #TAB# #TAB# ret.append(scheme[i]) #LINE# #TAB# return ret
"#LINE# #TAB# global _pystol_timeouts #LINE# #TAB# if _pystol_timeouts is None: #LINE# #TAB# #TAB# _pystol_timeouts = [] #LINE# #TAB# #TAB# return True #LINE# #TAB# for _ in range(0, _pystol_timeouts): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# logging.info('Caught exception while processing %s', e) #LINE# #TAB# #TAB# #TAB# time.sleep(1) #LINE# #TAB# #TAB# #TAB# _pystol_timeouts.append(True) #LINE# #TAB# if stop: #LINE# #TAB# #TAB# _pystol_timeouts = []"
"#LINE# #TAB# opts = 't' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# return [cmd, opts, archive]"
"#LINE# #TAB# a = np.sort(connections) #LINE# #TAB# b = np.sort(a) #LINE# #TAB# return a, b"
"#LINE# #TAB# links = outlinks(shape, node_status=node_status) #LINE# #TAB# links.shape = shape #LINE# #TAB# if return_count: #LINE# #TAB# #TAB# return links, node_status #LINE# #TAB# else: #LINE# #TAB# #TAB# return links"
"#LINE# #TAB# root = tdict #LINE# #TAB# for fname in os.listdir(path): #LINE# #TAB# #TAB# full_fname = os.path.join(path, fname) #LINE# #TAB# #TAB# if not os.path.exists(full_fname): #LINE# #TAB# #TAB# #TAB# tdict.add(full_fname) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tdict[full_fname] = {} #LINE# #TAB# return tdict"
"#LINE# #TAB# if not str(engine.url).startswith('mysql'): #LINE# #TAB# #TAB# return False #LINE# #TAB# variables = dict(engine.execute( #LINE# #TAB# #TAB#'show variables where variable_name like ""innodb_large_prefix"" or variable_name like ""innodb_file_format"";' #LINE# #TAB# #TAB# ).fetchall()) #LINE# #TAB# if variables.get('innodb_file_format', 'Barracuda' #LINE# #TAB# #TAB# ) == 'Barracuda' and variables.get('innodb_large_prefix', 'ON' #LINE# #TAB# #TAB# ) == 'ON': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if not id in cache: #LINE# #TAB# #TAB# return None #LINE# #TAB# return cache[id]
#LINE# #TAB# x = np.asarray(x) #LINE# #TAB# max_value = np.max(x) #LINE# #TAB# probability = np.exp(-max_value / x.sum()) #LINE# #TAB# return probability
#LINE# #TAB# r = dt.isoformat() #LINE# #TAB# if dt.microsecond: #LINE# #TAB# #TAB# r = r[:23] + r[26:] #LINE# #TAB# if r.endswith('+00:00'): #LINE# #TAB# #TAB# r = r[:-6] + 'Z' #LINE# #TAB# return r
"#LINE# #TAB# corner = (x - w) / 2 - (y - h) / 2 #LINE# #TAB# rows = int(corner * pagesize) #LINE# #TAB# colors = [] #LINE# #TAB# for i in range(rows): #LINE# #TAB# #TAB# j = 0 #LINE# #TAB# #TAB# while j < rows - 1: #LINE# #TAB# #TAB# #TAB# c = (i + 1) % pagesize #LINE# #TAB# #TAB# #TAB# if c < 0: #LINE# #TAB# #TAB# #TAB# #TAB# c = 0 #LINE# #TAB# #TAB# #TAB# colors.append((c, j)) #LINE# #TAB# #TAB# #TAB# j += 1 #LINE# #TAB# return colors"
"#LINE# #TAB# for line in data.splitlines(): #LINE# #TAB# #TAB# m = re_control_file.match(line) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# step = parse_step(m.group(1), args) #LINE# #TAB# #TAB# #TAB# if step: #LINE# #TAB# #TAB# #TAB# #TAB# yield step"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(['git', 'ls-files', '--error-unmatch', #LINE# #TAB# #TAB# #TAB# filename], stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# p.wait() #LINE# #TAB# #TAB# return p.returncode == 0"
"#LINE# #TAB# label = code.replace('-', '') #LINE# #TAB# if len(label)!= 2: #LINE# #TAB# #TAB# raise voluptuous.Invalid('Invalid country code: {}'.format(code)) #LINE# #TAB# return label"
"#LINE# #TAB# out_file = out_txt + '.summary.txt' #LINE# #TAB# cmd ='samtools view -h -f 2 > /dev/null {}'.format(bam) #LINE# #TAB# run(cmd) #LINE# #TAB# with open(out_file, 'w') as out_handle: #LINE# #TAB# #TAB# for line in open(bam): #LINE# #TAB# #TAB# #TAB# out_handle.write('\t'.join(line.split())) #LINE# #TAB# return out_file"
#LINE# #TAB# term_name = provider + '_' + region + builder #LINE# #TAB# if os.path.exists(term_name): #LINE# #TAB# #TAB# return term_name #LINE# #TAB# else: #LINE# #TAB# #TAB# return term_name
"#LINE# #TAB# if isinstance(p[1], ast.Union): #LINE# #TAB# #TAB# seen_union = p[1] #LINE# #TAB# #TAB# p[0] = [seen_union] #LINE# #TAB# else: #LINE# #TAB# #TAB# seen_union = p[1] #LINE# #TAB# p[0] = [seen_union]"
"#LINE# #TAB# cols = ((cols - 6) *.85) + 1 #LINE# #TAB# rows = [(cols - 6) *.85] #LINE# #TAB# for row in range(cols): #LINE# #TAB# #TAB# next(rows) #LINE# #TAB# #TAB# url = url[:row] + ""..."" + url[-row:] #LINE# #TAB# #TAB# if shorten and len(url) > cols - 3: #LINE# #TAB# #TAB# #TAB# url = url[:cols - 3] + ""..."" + url[-(cols - 3):] #LINE# #TAB# #TAB# yield url"
"#LINE# #TAB# report = cls.get_brew_name(request_handler, name) #LINE# #TAB# if report: #LINE# #TAB# #TAB# return report #LINE# #TAB# return None"
"#LINE# #TAB# old_schema = cls.get_old_schema(tag) #LINE# #TAB# if old_schema is not None: #LINE# #TAB# #TAB# del schema['old'] #LINE# #TAB# input = {} #LINE# #TAB# for k, v in schema.items(): #LINE# #TAB# #TAB# if k not in old_schema: #LINE# #TAB# #TAB# #TAB# input[k] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# input[k] = v #LINE# #TAB# return input"
#LINE# #TAB# import circularlayout #LINE# #TAB# lines = [] #LINE# #TAB# for node in graph: #LINE# #TAB# #TAB# lines.append(circlayout.plot(node)) #LINE# #TAB# plot_lines(lines) #LINE# #TAB# return lines
"#LINE# #TAB# if check_remote_desktop_mode(): #LINE# #TAB# #TAB# desktop_lock = os.environ.get('VIRTUALenv_LOCKED', '') #LINE# #TAB# #TAB# if desktop_lock: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# return {'items': {'description': 'List of catalogs', 'type': #LINE# #TAB# #TAB# 'List of catalogs', 'limits': {'all': True,'min_periods': 1, #LINE# #TAB# #TAB# 'initial_weight': 0.0, 'additional_weight': 0.0}, 'definitions': {'type': #LINE# #TAB# #TAB# 'List of catalogs', 'additional_weight': 1.0, 'extra_weight': 0.0}, #LINE# #TAB# #TAB#'steps': {'type': 'List of steps', 'initial_weight': 0.0, #LINE# #TAB# #TAB#'step_size': 1.0, 'initial_weight': 0.0}}}"
#LINE# #TAB# app = Netify(config_file) #LINE# #TAB# app.start() #LINE# #TAB# return app
"#LINE# #TAB# slurm_status_mapping = {'RUNNING': RUNNING, 'CANCELLED': CANCELLED, #LINE# #TAB# #TAB# 'COMPLETED': COMPLETED, 'CONFIGURING': PENDING, 'COMPLETING': #LINE# #TAB# #TAB# RUNNING, 'FAILED': FAILED, 'NODE_FAIL': FAILED, 'PENDING': PENDING, #LINE# #TAB# #TAB# 'PREEMPTED': FAILED, 'SUSPENDED': PENDING, 'TIMEOUT': FAILED} #LINE# #TAB# for line in response.split('\n'): #LINE# #TAB# #TAB# if line.strip() in slurm_status_mapping: #LINE# #TAB# #TAB# #TAB# return slurm_status_mapping[line.strip()] #LINE# #TAB# return None"
"#LINE# #TAB# edges = [] #LINE# #TAB# for start, end in ranges: #LINE# #TAB# #TAB# if end > start: #LINE# #TAB# #TAB# #TAB# edges.append((end - start, start)) #LINE# #TAB# #TAB# if start < end: #LINE# #TAB# #TAB# #TAB# edges[-1] += 1 #LINE# #TAB# #TAB# start = end #LINE# #TAB# return edges"
#LINE# #TAB# pos = lut.rfind(label) #LINE# #TAB# if pos == -1: #LINE# #TAB# #TAB# pos = len(label) #LINE# #TAB# else: #LINE# #TAB# #TAB# pos = label.find('|') #LINE# #TAB# #TAB# if pos == -1: #LINE# #TAB# #TAB# #TAB# return -1 #LINE# #TAB# #TAB# pos = label.rfind('-') #LINE# #TAB# if pos == -1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# pos = label.rfind('_') #LINE# #TAB# if pos == -1: #LINE# #TAB# #TAB# return pos + 1 #LINE# #TAB# pos = label.rfind('-') #LINE# #TAB# if pos == -1: #LINE# #TAB# #TAB# return pos #LINE# #TAB# return pos + 1
"#LINE# #TAB# string = str(string) #LINE# #TAB# try: #LINE# #TAB# #TAB# data = array.array('I', string) #LINE# #TAB# except: #LINE# #TAB# #TAB# data = array.array('I', string) #LINE# #TAB# try: #LINE# #TAB# #TAB# data = array.array('I', data) #LINE# #TAB# except: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = array.array('I', data) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# data = array.array('I', data) #LINE# #TAB# return data"
"#LINE# #TAB# if abs(control[0] - test[0]) > 1e-08: #LINE# #TAB# #TAB# yield 0.5 #LINE# #TAB# elif abs(control[1] - test[1]) > 1e-08: #LINE# #TAB# #TAB# yield 0.5 #LINE# #TAB# else: #LINE# #TAB# #TAB# for i in range(1, len(control)): #LINE# #TAB# #TAB# #TAB# if control[i] > test[i]: #LINE# #TAB# #TAB# #TAB# #TAB# yield 0.5 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# yield control[i]"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# p = {} #LINE# #TAB# #TAB# for k, v in PERCENT_DICT.items(): #LINE# #TAB# #TAB# #TAB# p[k] = v #LINE# #TAB# #TAB# return {'percent': p} #LINE# #TAB# except: #LINE# #TAB# #TAB# return {'percent': None}"
#LINE# #TAB# dvs_conn = session.query(models.DVS).filter_by(name=volt_name).first() #LINE# #TAB# if dvs_conn is None: #LINE# #TAB# #TAB# raise exception.DVSNotFound(volt_name=volt_name) #LINE# #TAB# port_group = dvs_conn.port_groups.get_by_name(pg_name).first() #LINE# #TAB# if port_group is None: #LINE# #TAB# #TAB# raise exception.DVSPortGroupNotFound(volt_name=volt_name) #LINE# #TAB# return port_group['vlans'][0]['id']
#LINE# #TAB# if option is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [repr(x) for x in option]
"#LINE# #TAB# if not module_exists(module_name): #LINE# #TAB# #TAB# return False #LINE# #TAB# cmd = ['git','module', module_name] #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_output(cmd) #LINE# #TAB# #TAB# return True #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# import ckan.model as model #LINE# #TAB# if not database_name.startswith('project'): #LINE# #TAB# #TAB# raise ValueError('Database {} does not start with project {}'.format( #LINE# #TAB# #TAB# #TAB# database_name, models.Project.get_name())) #LINE# #TAB# tags = model.get_database_tags(database_name) #LINE# #TAB# for tag in tags: #LINE# #TAB# #TAB# if not tag in session.query(model).filter(model.tags.get(tag)).exists(): #LINE# #TAB# #TAB# #TAB# raise ValueError('Tag {} does not exist in project {}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# tag, models.Project.get_name())) #LINE# #TAB# return database_name"
"#LINE# #TAB# print('Global search result: song_name=%s song_title=%s' % (song_name, #LINE# #TAB# #TAB# song_title)) #LINE# #TAB# print('Search result: song_name=%s' % song_name) #LINE# #TAB# print('Search result: song_title=%s' % song_title) #LINE# #TAB# score = compute_search_score(song_name, song_title, artist) #LINE# #TAB# if score >= 10: #LINE# #TAB# #TAB# return True, score #LINE# #TAB# else: #LINE# #TAB# #TAB# return False, 'No search result'"
#LINE# #TAB# dialects = list(dialects) #LINE# #TAB# random.shuffle(dialects) #LINE# #TAB# return dialects[0]
#LINE# #TAB# if not initialized: #LINE# #TAB# #TAB# hard = False #LINE# #TAB# else: #LINE# #TAB# #TAB# hard = True #LINE# #TAB# return hard
#LINE# #TAB# if not run: #LINE# #TAB# #TAB# validators = {} #LINE# #TAB# else: #LINE# #TAB# #TAB# for func in _get_registered_validators(): #LINE# #TAB# #TAB# #TAB# data = func(run) #LINE# #TAB# #TAB# #TAB# if 'validators' in data: #LINE# #TAB# #TAB# #TAB# #TAB# validators[func['name']] = data['validators'] #LINE# #TAB# return validators
"#LINE# #TAB# if not os.path.exists(filedir_pdf): #LINE# #TAB# #TAB# os.makedirs(filedir_pdf) #LINE# #TAB# out_file = filedir_pdf + '/' + pdf_name + '.pdf' #LINE# #TAB# if not os.path.exists(out_file): #LINE# #TAB# #TAB# os.mkdir(out_file) #LINE# #TAB# with open(out_file, 'wb') as fh: #LINE# #TAB# #TAB# fh.write(pdf_bytes) #LINE# #TAB# return"
#LINE# #TAB# dim = graph.number_of_nodes() #LINE# #TAB# test_case_graph = TestCaseGraph() #LINE# #TAB# for node in graph.nodes(): #LINE# #TAB# #TAB# for child in node: #LINE# #TAB# #TAB# #TAB# if dim not in test_case_graph.node[child]: #LINE# #TAB# #TAB# #TAB# #TAB# test_case_graph.node[child][dim] = [] #LINE# #TAB# #TAB# #TAB# test_case_graph.node[child][dim].append(child) #LINE# #TAB# return test_case_graph
"#LINE# #TAB# infile = os.listdir(dirPath) #LINE# #TAB# data = [] #LINE# #TAB# if modify: #LINE# #TAB# #TAB# for idx, f in enumerate(infile): #LINE# #TAB# #TAB# #TAB# if f.endswith('.csv'): #LINE# #TAB# #TAB# #TAB# #TAB# data.append(pd.read_csv(os.path.join(dirPath, f))) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# data.append([]) #LINE# #TAB# pkg = Package(data=data[0], numLabels=numLabels) #LINE# #TAB# return pkg"
"#LINE# #TAB# return {'sig': base64.b64encode(hmac.new(settings. #LINE# #TAB# #TAB# SIGNATURE_SECRET_KEY, string_to_sign, digestmod=hashlib.sha256).digest()). #LINE# #TAB# #TAB# decode('utf-8')}"
"#LINE# #TAB# child_label = api.generate_child_label(domain) #LINE# #TAB# selinux_flag ='selinux' in logger.upper() #LINE# #TAB# if selinux_flag: #LINE# #TAB# #TAB# child_label = '-'.join([child_label, domain]) #LINE# #TAB# else: #LINE# #TAB# #TAB# child_label = child_label #LINE# #TAB# logger.debug('selinux_flag is set to'+ selinux_flag) #LINE# #TAB# return child_label"
#LINE# #TAB# file_stats = None #LINE# #TAB# try: #LINE# #TAB# #TAB# stats = fsync(file_path) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# if err.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# file_stats = fsync(file_path) #LINE# #TAB# return stats
"#LINE# #TAB# if isinstance(obj, argparse.Namespace): #LINE# #TAB# #TAB# nsdict = dict() #LINE# #TAB# #TAB# for k, v in iteritems(obj): #LINE# #TAB# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# nsdict[k] = region_convert_dict(v) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# nsdict[k] = v #LINE# #TAB# elif isinstance(obj, optparse.OptionParser): #LINE# #TAB# #TAB# return {k: region_convert_dict(v) for k, v in iteritems(obj)} #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
"#LINE# #TAB# result = False #LINE# #TAB# module = get_module() #LINE# #TAB# if module is not None: #LINE# #TAB# #TAB# is_console = getattr(module, 'is_console', False) #LINE# #TAB# #TAB# if is_console: #LINE# #TAB# #TAB# #TAB# result = True #LINE# #TAB# else: #LINE# #TAB# #TAB# result = True #LINE# #TAB# return result"
"#LINE# #TAB# existing_loggers = [] #LINE# #TAB# for name, logger in GATEWAY_LOGGERS: #LINE# #TAB# #TAB# existing_loggers.append((name, logger)) #LINE# #TAB# return existing_loggers"
"#LINE# #TAB# editor = None #LINE# #TAB# for egg in os.listdir(directory): #LINE# #TAB# #TAB# if os.path.isfile(os.path.join(directory, egg)): #LINE# #TAB# #TAB# #TAB# editor = os.path.join(directory, egg) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if editor!= None: #LINE# #TAB# #TAB# return editor #LINE# #TAB# return None"
"#LINE# #TAB# assert rois.shape[1] == df.shape[2] #LINE# #TAB# grouped_rois = np.zeros((df.shape[0], rois.shape[1])) #LINE# #TAB# for i in range(df.shape[0]): #LINE# #TAB# #TAB# grouped_rois[(i), :] = df.loc[rois[:, (i)].values, :] #LINE# #TAB# df_mean_m = df.loc[rois[:, (i)].values, :] #LINE# #TAB# df_mean_m = df_mean_m.T.dot(df_mean_m) #LINE# #TAB# return df_mean_m"
"#LINE# #TAB# fjac = infodic['fjac'] #LINE# #TAB# ipvt = infodic['ipvt'] #LINE# #TAB# n = len(p) #LINE# #TAB# nc = len(p) #LINE# #TAB# x = np.zeros((n, nc)) #LINE# #TAB# for i in range(nc): #LINE# #TAB# #TAB# x[i] = fjac[i](p[i]) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cov_x = np.linalg.inv(np.dot(np.transpose(x), x)).T #LINE# #TAB# #TAB# except LinAlgError: #LINE# #TAB# #TAB# #TAB# raise LinAlgError #LINE# #TAB# return cov_x"
#LINE# #TAB# iterable = iter(iterable) #LINE# #TAB# if n <= 0: #LINE# #TAB# #TAB# return float('nan') #LINE# #TAB# result = 0 #LINE# #TAB# n -= 1 #LINE# #TAB# for x in iterable: #LINE# #TAB# #TAB# result += x #LINE# #TAB# #TAB# if n == 1: #LINE# #TAB# #TAB# #TAB# return result / n #LINE# #TAB# return result / n
"#LINE# #TAB# paths = [] #LINE# #TAB# for directory in os.environ['PATH'].split(':'): #LINE# #TAB# #TAB# if check_platform and os.path.exists(os.path.join(directory, name)): #LINE# #TAB# #TAB# #TAB# return os.path.abspath(os.path.join(directory, name)) #LINE# #TAB# for path in os.environ['PATH'].split(':'): #LINE# #TAB# #TAB# if os.path.exists(os.path.join(directory, name)): #LINE# #TAB# #TAB# #TAB# return path #LINE# #TAB# return None"
"#LINE# #TAB# global frob_coeffs #LINE# #TAB# i %= 2 #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return t_x #LINE# #TAB# return t_x[0], t_x[1] * frob_coeffs[2, i, 1] % 2"
"#LINE# #TAB# rows, cols = x.shape #LINE# #TAB# if fills == 1: #LINE# #TAB# #TAB# for i in range(rows): #LINE# #TAB# #TAB# #TAB# execshell_disable(x[i], y, rows, cols) #LINE# #TAB# else: #LINE# #TAB# #TAB# for i in range(cols): #LINE# #TAB# #TAB# #TAB# execshell_disable(x[i], y, rows, cols) #LINE# #TAB# return"
"#LINE# #TAB# q1 = b * np.pi / 180 #LINE# #TAB# q2 = a * np.cos(q1) + b * np.sin(q1) #LINE# #TAB# r1 = b * np.cos(r1) + b * np.sin(r1) #LINE# #TAB# r2 = a * np.sin(r2) + b * np.cos(r2) #LINE# #TAB# return q1, r2"
#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = sqlite3.connect(db_file) #LINE# #TAB# #TAB# conn.execute('PRAGMA foreign_keys = ON') #LINE# #TAB# #TAB# conn.execute('PRAGMA synchronous = OFF') #LINE# #TAB# #TAB# return conn #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if not os.path.exists(cachefname): #LINE# #TAB# #TAB# return False, None #LINE# #TAB# ext = os.path.splitext(cachefname)[1] #LINE# #TAB# if ext in ALLOWED_EXTENSIONS: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with io.open(cachefname, 'rb') as f: #LINE# #TAB# #TAB# #TAB# #TAB# cached = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# return True, cached #LINE# #TAB# #TAB# except (IOError, OSError): #LINE# #TAB# #TAB# #TAB# return False, None #LINE# #TAB# else: #LINE# #TAB# #TAB# return False, None"
#LINE# #TAB# num_intervals = cur_dev_size - thres_dev_size #LINE# #TAB# if num_intervals == 0: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# delta_intervals = int(num_intervals * base_interval) - int(num_intervals * #LINE# #TAB# #TAB# base_interval) #LINE# #TAB# evaluation_interval = delta_intervals / num_intervals #LINE# #TAB# return evaluation_interval
"#LINE# #TAB# d = {} #LINE# #TAB# if year < 500: #LINE# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# else: #LINE# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# for month in range(1, 13): #LINE# #TAB# #TAB# if date_format not in d: #LINE# #TAB# #TAB# #TAB# d[month] = date_format #LINE# #TAB# #TAB# if date_format == '%m/%d/%Y': #LINE# #TAB# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# #TAB# d[month] = date_format #LINE# #TAB# return d"
"#LINE# #TAB# if variance is None: #LINE# #TAB# #TAB# variance = 0.0 #LINE# #TAB# n = S.shape[0] #LINE# #TAB# P = np.zeros((n,)) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(i): #LINE# #TAB# #TAB# #TAB# P[i, j] = spec_dot_frob(S[i], D[j], variance) #LINE# #TAB# return P"
"#LINE# #TAB# if node is not None: #LINE# #TAB# #TAB# node = node.getElementsByTagName('event')[0] #LINE# #TAB# #TAB# if node.getAttribute('type') == 'pubdate': #LINE# #TAB# #TAB# #TAB# pubdate = {'date': node.getAttribute('date'), 'time': #LINE# #TAB# #TAB# #TAB# #TAB# parse_iso_date(node.getAttribute('time'))} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pubdate = {'date': node.getAttribute('date'), 'time': node. #LINE# #TAB# #TAB# #TAB# #TAB# getAttribute('time')} #LINE# #TAB# else: #LINE# #TAB# #TAB# pubdate = {} #LINE# #TAB# return pubdate"
"#LINE# #TAB# aln_chk = set() #LINE# #TAB# for name in trimmed_aln: #LINE# #TAB# #TAB# for pos in trimmed_aln[1:]: #LINE# #TAB# #TAB# #TAB# if not re.search('X\\d+', name): #LINE# #TAB# #TAB# #TAB# #TAB# aln_chk.add(name) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# aln_chk.difference_update(set(aln_chk)) #LINE# #TAB# return aln_chk"
#LINE# #TAB# client = ofxclient.Client() #LINE# #TAB# client.accounts = accounts #LINE# #TAB# if not days: #LINE# #TAB# #TAB# days = 60 #LINE# #TAB# client.prob_prepare() #LINE# #TAB# while len(client.accounts) < len(accounts): #LINE# #TAB# #TAB# client = client.accounts.next() #LINE# #TAB# out_file = client.open_file() #LINE# #TAB# out_file.seek(0) #LINE# #TAB# data = out_file.read() #LINE# #TAB# out_file.close() #LINE# #TAB# return data
"#LINE# #TAB# if not isinstance(mats[0], sp.spmatrix): #LINE# #TAB# #TAB# raise ValueError('All input matrices must be a list of sp.spmatrix objects.') #LINE# #TAB# if not mats[0].shape == mats[0].shape[1]: #LINE# #TAB# #TAB# raise ValueError('All input matrices must have the same shape.') #LINE# #TAB# nats = len(mats[0].shape) #LINE# #TAB# diff = np.zeros(nats, dtype=np.float32) #LINE# #TAB# for m in mats: #LINE# #TAB# #TAB# diff[m.nonzero()[0]] = m.pixel_median() #LINE# #TAB# #TAB# diff[m.nonzero()[1]] = m.pixel_median() #LINE# #TAB# return diff"
#LINE# #TAB# if profile is None and geni_input is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif profile is not None and type_geni == 'g': #LINE# #TAB# #TAB# return geni_input #LINE# #TAB# elif type_geni == 'b': #LINE# #TAB# #TAB# return profile #LINE# #TAB# else: #LINE# #TAB# #TAB# return profile + '/' + geni_input
"#LINE# #TAB# if mx is None and dtype is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if mn is None: #LINE# #TAB# #TAB# mn = np.array([], dtype=float) #LINE# #TAB# if mx is None: #LINE# #TAB# #TAB# mx = np.array([], dtype=float) #LINE# #TAB# if dtype is not None: #LINE# #TAB# #TAB# dt = np.dtype(dtype) #LINE# #TAB# #TAB# if mn.astype(dt)!= mx.astype(dt): #LINE# #TAB# #TAB# #TAB# mn = np.array([mn, mx]) #LINE# #TAB# if mx.astype(dt)!= mx.astype(dt): #LINE# #TAB# #TAB# mx = np.array([mx, mx]) #LINE# #TAB# return mn == mx"
#LINE# #TAB# if when is None: #LINE# #TAB# #TAB# when = time.time() #LINE# #TAB# else: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# rand_s = str(time.time()) #LINE# #TAB# #TAB# #TAB# rand_c = xxhash(rand_s) #LINE# #TAB# #TAB# #TAB# if not rand_c.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield rand_c
"#LINE# #TAB# if slot.cardinality: #LINE# #TAB# #TAB# return '{}{}'.format(slot.cardinality, char_do(slot.slot)) #LINE# #TAB# return ''"
"#LINE# #TAB# for test in [lambda x: ipaddress.IPv6Address(x)._prefixlen!= 128, lambda #LINE# #TAB# #TAB# x: ipaddress.IPv6Address(x)._prefixlen!= 128]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return bool(test(value)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
#LINE# #TAB# meta = os.stat(filename) #LINE# #TAB# return meta
"#LINE# #TAB# lease = cls.get_cidr_by_id(lease_id) #LINE# #TAB# event_prefixes = cls._get_cidr_event_prefixes(lease_id, status) #LINE# #TAB# if event_prefixes is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# for event_label in event_prefixes: #LINE# #TAB# #TAB# if event_label in lease['status'] or event_label in lease[ #LINE# #TAB# #TAB# #TAB#'reservations'] or event_label in lease['status']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# full_log_file = os.path.join(os.path.expanduser('~'), '.pyweed') #LINE# #TAB# if not os.path.exists(full_log_file): #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# os.unlink(full_log_file) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# click.echo('The file {} does not exist'.format(filename)) #LINE# #TAB# #TAB# return False #LINE# #TAB# if not os.path.isdir(filename): #LINE# #TAB# #TAB# click.echo('The file {} does not exist'.format(filename)) #LINE# #TAB# #TAB# return False #LINE# #TAB# with open(filename, 'r') as fp: #LINE# #TAB# #TAB# yaml.dump(fp, default_flow_style=False) #LINE# #TAB# return True"
"#LINE# #TAB# logger = logging.getLogger(""HTMap"") #LINE# #TAB# if not logger.handlers: #LINE# #TAB# #TAB# hmaplogger = logging.getLogger(""HTMap"") #LINE# #TAB# #TAB# hmaplogger.addHandler(logging.NullHandler()) #LINE# #TAB# return logger"
#LINE# #TAB# trimmed_size = float(untrimmed_alignment_size) #LINE# #TAB# if trimmed_size >= no_sites_trimmed: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1
"#LINE# #TAB# hex = '#%02x%02x%02x' % (color[0], color[1], color[2], color[3]) #LINE# #TAB# r = int(hex[0:2], 16) #LINE# #TAB# g = int(hex[2:4], 16) #LINE# #TAB# b = int(hex[4:6], 16) #LINE# #TAB# return r, g, b"
"#LINE# #TAB# c = numpy.sum(v * v, axis=-1) #LINE# #TAB# as_val = k * v #LINE# #TAB# n = len(as_val) #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return [k, c, as_val] #LINE# #TAB# elif n % 2: #LINE# #TAB# #TAB# return [k, c, as_val] #LINE# #TAB# else: #LINE# #TAB# #TAB# n = n // 2 #LINE# #TAB# #TAB# contsign = numpy.sum(as_val, axis=-1) #LINE# #TAB# #TAB# return [k, c, contsign, n]"
"#LINE# #TAB# if not csv_content: #LINE# #TAB# #TAB# return None #LINE# #TAB# db = [] #LINE# #TAB# for row in csv_content: #LINE# #TAB# #TAB# obj_cell = parse_csv_to_cell(row) #LINE# #TAB# #TAB# obj_table = new_table(obj_cell, type=MyTableCell) #LINE# #TAB# #TAB# db.append(obj_table) #LINE# #TAB# return db"
"#LINE# #TAB# url = '{}/api/v1/schools/{}/units/{}/'.format(SCHOOL_API_BASE_URL, code) #LINE# #TAB# resp = requests.get(url) #LINE# #TAB# code_data = resp.json() #LINE# #TAB# if code_data and code_data.get('code')!= code: #LINE# #TAB# #TAB# raise ValueError('Error fetching data for code: {}'.format( #LINE# #TAB# #TAB# #TAB# code_data['code'])) #LINE# #TAB# for key, value in code_data.items(): #LINE# #TAB# #TAB# if not isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# value = json.dumps(value) #LINE# #TAB# #TAB# resp[key] = value #LINE# #TAB# return resp"
#LINE# #TAB# _node_list = [] #LINE# #TAB# new_tab = [] #LINE# #TAB# for element in tab: #LINE# #TAB# #TAB# if element not in _node_list: #LINE# #TAB# #TAB# #TAB# new_tab.append(element) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _node_list.append(element) #LINE# #TAB# unique_tab = [] #LINE# #TAB# for value in _node_list: #LINE# #TAB# #TAB# if value not in new_tab: #LINE# #TAB# #TAB# #TAB# unique_tab.append(value) #LINE# #TAB# return unique_tab
"#LINE# #TAB# if step > 0: #LINE# #TAB# #TAB# entropy = -np.log(1 + np.exp(-X / step)) #LINE# #TAB# else: #LINE# #TAB# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# #TAB# warnings.simplefilter(""ignore"") #LINE# #TAB# #TAB# #TAB# entropy = np.log(1 + np.exp(-X / step)) #LINE# #TAB# return entropy"
"#LINE# #TAB# cls.command_sub = 'operatingsystem-id' #LINE# #TAB# result = super(OperatingsystemHandler, cls).unpack_handlers(options) #LINE# #TAB# if 'id' in result: #LINE# #TAB# #TAB# raise Exception('operatingsystem-id already exists') #LINE# #TAB# return result"
#LINE# #TAB# diff_mark = m - t #LINE# #TAB# return diff_mark
#LINE# #TAB# mask = (im >= min_value) & (im <= max_value) #LINE# #TAB# return im[mask]
"#LINE# #TAB# tris_PM73 = 25.0 * NA #LINE# #TAB# tris_PM73_conv_factor = tris_PM73_from_ratio_of_harmonics(z, z2) / tris_PM73 #LINE# #TAB# return tris_PM73, tris_PM73_conv_factor"
#LINE# #TAB# vocab = set() #LINE# #TAB# with open(vocab_path) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if line.startswith('region '): #LINE# #TAB# #TAB# #TAB# #TAB# vocab.add(line) #LINE# #TAB# return vocab
"#LINE# #TAB# if allowedreacs is None: #LINE# #TAB# #TAB# allowedreacs = [model] #LINE# #TAB# pre = [model] #LINE# #TAB# post = [model] #LINE# #TAB# while True: #LINE# #TAB# #TAB# for rxn in model.reactions: #LINE# #TAB# #TAB# #TAB# if rxn.material in reacsbounds: #LINE# #TAB# #TAB# #TAB# #TAB# blocked = rxn.material in reacsbounds #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# blocked = [rxn.material] #LINE# #TAB# #TAB# pre.append(blocked) #LINE# #TAB# #TAB# post.append(blocked) #LINE# #TAB# #TAB# if len(post) > 0: #LINE# #TAB# #TAB# #TAB# post.extend(post) #LINE# #TAB# return pre, post"
"#LINE# #TAB# safe_paths = set() #LINE# #TAB# for path in list_of_paths: #LINE# #TAB# #TAB# if not os.access(path, os.W_OK): #LINE# #TAB# #TAB# #TAB# raise IOError('Path %s is not writeable' % path) #LINE# #TAB# #TAB# safe_paths.add(path) #LINE# #TAB# return safe_paths"
"#LINE# #TAB# seconds, microseconds = _ephem_convert_to_seconds_and_microseconds(date) #LINE# #TAB# date = dt.datetime.fromtimestamp(seconds, tzinfo) #LINE# #TAB# date = date.replace(microsecond=microseconds) #LINE# #TAB# return date"
"#LINE# #TAB# new_cred_params = [] #LINE# #TAB# for participant in participants: #LINE# #TAB# #TAB# new_cred = cred_params_by_n_subjects_helper(participant, #LINE# #TAB# #TAB# #TAB# n_adult, n_child) #LINE# #TAB# #TAB# if len(new_cred) > 0: #LINE# #TAB# #TAB# #TAB# new_cred_params.append(new_cred) #LINE# #TAB# return new_cred_params"
#LINE# #TAB# all_keys_files = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# for ini_file in [f for f in os.listdir(path) if f.endswith( #LINE# #TAB# #TAB# #TAB# '.py')]: #LINE# #TAB# #TAB# #TAB# if os.path.isfile(ini_file): #LINE# #TAB# #TAB# #TAB# #TAB# all_keys_files.append(ini_file) #LINE# #TAB# return all_keys_files
"#LINE# #TAB# x = tf.reshape(x, [1, tf.shape(x)[-1]]) #LINE# #TAB# return x"
"#LINE# #TAB# if s: #LINE# #TAB# #TAB# s = '""' + s + '""' #LINE# #TAB# return s"
#LINE# #TAB# if date: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%dT%H:%M:%S') #LINE# #TAB# return ''
#LINE# #TAB# margin = 0 #LINE# #TAB# jump = offset % 4 #LINE# #TAB# if jump: #LINE# #TAB# #TAB# margin += (4 - jump) #LINE# #TAB# while jump: #LINE# #TAB# #TAB# margin += 1 #LINE# #TAB# #TAB# jump = (offset % 4) + 1 #LINE# #TAB# if jump: #LINE# #TAB# #TAB# margin += (4 - jump) #LINE# #TAB# return margin
"#LINE# #TAB# msg = ( #LINE# #TAB# #TAB# 'N318: assertEqual(A, None) or assertEqual(None, A) sentences not allowed' #LINE# #TAB# #TAB# ) #LINE# #TAB# res = assert_equal_start_with_none_re.match(logical_line #LINE# #TAB# #TAB# ) or assert_equal_end_with_none_re.match(logical_line) #LINE# #TAB# if res: #LINE# #TAB# #TAB# yield 0, msg"
#LINE# #TAB# if text is True: #LINE# #TAB# #TAB# return 'True' #LINE# #TAB# if text is False: #LINE# #TAB# #TAB# return 'False' #LINE# #TAB# return text
#LINE# #TAB# states = [] #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# name = 'logger_b(%d)' % i #LINE# #TAB# #TAB# if big_endian: #LINE# #TAB# #TAB# #TAB# name = 'little' + name #LINE# #TAB# #TAB# states.append(name) #LINE# #TAB# return states
#LINE# #TAB# ctx = click.get_current_context(silent=True) #LINE# #TAB# if not ctx.model_id: #LINE# #TAB# #TAB# ctx.model_id = _get_model_name() #LINE# #TAB# return ctx.model_id
#LINE# #TAB# Optional[Exception]=None) ->Any: #LINE# #TAB# value = request.GET.get(name) #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# value = next(iter(request.GET.values())) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# raise error_if_missing #LINE# #TAB# if not value: #LINE# #TAB# #TAB# raise error_if_missing #LINE# #TAB# return value
#LINE# #TAB# name = None #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.type == tokenize.NAME: #LINE# #TAB# #TAB# #TAB# name = token.value.lower() #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return name
#LINE# #TAB# stripped_arg = arg.strip() #LINE# #TAB# if stripped_arg == '': #LINE# #TAB# #TAB# pass #LINE# #TAB# elif stripped_arg.startswith('.'): #LINE# #TAB# #TAB# stripped_arg = stripped_arg[1:] #LINE# #TAB# elif stripped_arg.startswith('='): #LINE# #TAB# #TAB# stripped_arg = [s.strip() for s in stripped_arg.split('=')] #LINE# #TAB# else: #LINE# #TAB# #TAB# stripped_arg = [s.strip() for s in stripped_arg.split('=')] #LINE# #TAB# if stripped_arg[0] not in arg_dict: #LINE# #TAB# #TAB# raise ValueError(f'invalid service argument: {arg}') #LINE# #TAB# return arg_dict
#LINE# #TAB# global DEFERRED_EMAILS #LINE# #TAB# DEFERRED_EMAILS = True
"#LINE# #TAB# model_path = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(model_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# return f.read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# z = open(model_path, 'w') #LINE# #TAB# #TAB# return z.read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# with open(model_path, 'w') as f: #LINE# #TAB# #TAB# return f.read() #LINE# #TAB# finally: #LINE# #TAB# #TAB# os.remove(model_path) #LINE# #TAB# return None"
"#LINE# #TAB# M = nx.Graph() #LINE# #TAB# if nodelist is None: #LINE# #TAB# #TAB# nodelist = list(G.nodes()) #LINE# #TAB# for node in nodelist: #LINE# #TAB# #TAB# M.add_node(node) #LINE# #TAB# for node1, node2 in G.edges(): #LINE# #TAB# #TAB# for node2 in G.edges(): #LINE# #TAB# #TAB# #TAB# M.add_edge(node1, node2) #LINE# #TAB# return M"
#LINE# #TAB# intensity_peaks = [image.where(mask == 1)] #LINE# #TAB# max_intensity_peaks = [max(intensity_peaks) for _ in range(num_peaks)] #LINE# #TAB# return max_intensity_peaks
"#LINE# #TAB# templates = [] #LINE# #TAB# for plugin in cls.list(): #LINE# #TAB# #TAB# template_path = os.path.join(os.path.dirname(plugin.__file__), #LINE# #TAB# #TAB# #TAB# 'templates') #LINE# #TAB# #TAB# if os.path.isfile(template_path): #LINE# #TAB# #TAB# #TAB# templates.append(plugin) #LINE# #TAB# #TAB# elif len(templates) > 1: #LINE# #TAB# #TAB# #TAB# dialog = cls.dialog_factory(template_path, plugin, None) #LINE# #TAB# #TAB# #TAB# dialog.setWindowTitle(plugin.__name__ + 'Plugin') #LINE# #TAB# #TAB# #TAB# dialog.show() #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# dialog = None #LINE# #TAB# return template, dialog"
#LINE# #TAB# if target > 400: #LINE# #TAB# #TAB# if w > h: #LINE# #TAB# #TAB# #TAB# return 0.5 * (h / w) ** target #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 0.5 * (w / h) ** target #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0.5
"#LINE# #TAB# value = str(value).encode('utf-8') #LINE# #TAB# return struct.pack('>I', len(value)) + value"
#LINE# #TAB# for user in doc: #LINE# #TAB# #TAB# if user.text.strip() == '' or len(user.text) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield user
"#LINE# #TAB# url = u'' #LINE# #TAB# blacklist_hostnames = blacklist_hostnames or lib.SDL_getBlacklistHostnames() #LINE# #TAB# lib.SDL_correct_hostname(url, blacklist_hostnames) #LINE# #TAB# return url"
#LINE# #TAB# rc = lib.sdl_sempost(sem) #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return rc
"#LINE# #TAB# b = JobBinary(context, values) #LINE# #TAB# b.load_data() #LINE# #TAB# return b"
#LINE# #TAB# for item in q.get_all(): #LINE# #TAB# #TAB# yield item
"#LINE# #TAB# normalized_editions = {} #LINE# #TAB# num_keys = len(editions) #LINE# #TAB# for k, values in editions.items(): #LINE# #TAB# #TAB# normalized_editions[k] = [] #LINE# #TAB# #TAB# for i, v in enumerate(values): #LINE# #TAB# #TAB# #TAB# normalized_editions[k].append(normalize_encoded_dict(v)) #LINE# #TAB# #TAB# num_keys = len(k) #LINE# #TAB# #TAB# for k in num_keys: #LINE# #TAB# #TAB# #TAB# normalized_editions[k] = [] #LINE# #TAB# #TAB# #TAB# for i in range(num_keys): #LINE# #TAB# #TAB# #TAB# #TAB# normalized_editions[k][num_keys[i]] = v #LINE# #TAB# return normalized_editions"
"#LINE# #TAB# status_code = 0 #LINE# #TAB# data = None #LINE# #TAB# try: #LINE# #TAB# #TAB# packed_message = GOSSIPNotifyMessage.pack(data_type) #LINE# #TAB# #TAB# status_code = packed_message.code #LINE# #TAB# #TAB# data = packed_message.payload #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return status_code, data"
"#LINE# #TAB# closer = get_close_text(l, r, sep, allow_missing_close) #LINE# #TAB# if closer == '<': #LINE# #TAB# #TAB# return '<{}>'.format(expr) #LINE# #TAB# return '<{}>'.format(expr) if expr else '<{}>'"
"#LINE# #TAB# if not os.path.exists(local_data_dir): #LINE# #TAB# #TAB# os.mkdir(local_data_dir) #LINE# #TAB# filepath = os.path.join(local_data_dir, filename) #LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# response = requests.get(url) #LINE# #TAB# #TAB# response.raise_for_status() #LINE# #TAB# return filepath"
"#LINE# #TAB# catch_errors.check_for_period_error(data, period) #LINE# #TAB# wma = left_ratio_moving_average_helper(data, period) #LINE# #TAB# return wma"
#LINE# #TAB# params = [] #LINE# #TAB# for i in range(len(paramMat)): #LINE# #TAB# #TAB# params.append(paramMat[i]) #LINE# #TAB# return params
"#LINE# #TAB# nonzero_entries = np.all(entry.data > 0) #LINE# #TAB# if len(nonzero_entries) > 0: #LINE# #TAB# #TAB# lower, upper = generate_ranges(nonzero_entries) #LINE# #TAB# #TAB# return lower, upper #LINE# #TAB# else: #LINE# #TAB# #TAB# lower, upper = generate_ranges(nonzero_entries) #LINE# #TAB# #TAB# return lower, upper"
"#LINE# #TAB# ustr = tools.bytes2str(ustr) #LINE# #TAB# if not re.match('^[\\w-]+$', ustr): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# as_unit(ustr) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# depth_occurrences = [] #LINE# #TAB# for a in phi.atoms: #LINE# #TAB# #TAB# if a.is_depth: #LINE# #TAB# #TAB# #TAB# depth_occurrences.append(True) #LINE# #TAB# #TAB# elif a.is_operand: #LINE# #TAB# #TAB# #TAB# depth_occurrences.append(False) #LINE# #TAB# return depth_occurrences
"#LINE# #TAB# for offset in range(0, limit): #LINE# #TAB# #TAB# r = q.offset(offset).limit(limit).all() #LINE# #TAB# #TAB# for row in r: #LINE# #TAB# #TAB# #TAB# yield row #LINE# #TAB# #TAB# if len(r) < limit: #LINE# #TAB# #TAB# #TAB# break"
"#LINE# #TAB# ret = {} #LINE# #TAB# c = c_char_p(dict_str) #LINE# #TAB# for k, v in c.items(): #LINE# #TAB# #TAB# if str_ok: #LINE# #TAB# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# #TAB# ret[k] = v #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ret[k] = v #LINE# #TAB# return ret"
#LINE# #TAB# rc = lib.SDL_arr(td) #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# return [0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return rc
"#LINE# #TAB# choices = [(f'<img src=""{img}"" width=""{img.width}"" height=""{img.height}"" title=""{img.title}"">' #LINE# #TAB# #TAB#, img.title) for img in images] #LINE# #TAB# return choices"
"#LINE# #TAB# lst = [l for l in lst if l] #LINE# #TAB# if len(lst) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif max(lst) == list(set(lst)): #LINE# #TAB# #TAB# return max(lst) #LINE# #TAB# else: #LINE# #TAB# #TAB# left_item = lst[0] #LINE# #TAB# #TAB# right_item = lst[1] #LINE# #TAB# #TAB# return left_item, right_item"
"#LINE# #TAB# preorder_db = get_preorder_db(preorder_hash) #LINE# #TAB# select_sql = ""SELECT * FROM variables WHERE preorder_hash =?;"" #LINE# #TAB# args = (preorder_db, preorder_hash) #LINE# #TAB# rows = cur.execute(select_sql, args ) #LINE# #TAB# result = rows.fetchone() #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# cur.execute(select_sql, args ) #LINE# #TAB# #TAB# return 1"
#LINE# #TAB# focus_widget = get_focus_widget() #LINE# #TAB# proxy_index = focus_widget.focused_index() #LINE# #TAB# while proxy_index >= 0: #LINE# #TAB# #TAB# declaration = focus_widget.centralWidget() #LINE# #TAB# #TAB# if declaration is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# proxy_index -= 1 #LINE# #TAB# return declaration
"#LINE# #TAB# assert edges >= 0 #LINE# #TAB# if edges == 0: #LINE# #TAB# #TAB# return [], [] #LINE# #TAB# forward = [] #LINE# #TAB# reverse = [] #LINE# #TAB# for i, j in edges: #LINE# #TAB# #TAB# if j!= i: #LINE# #TAB# #TAB# #TAB# forward.append(j) #LINE# #TAB# #TAB# #TAB# reverse.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# forward.append(i) #LINE# #TAB# #TAB# #TAB# reverse.append(j) #LINE# #TAB# if reverse: #LINE# #TAB# #TAB# return forward, reverse #LINE# #TAB# else: #LINE# #TAB# #TAB# return forward, reverse"
"#LINE# #TAB# out = np.zeros(3) #LINE# #TAB# if transform is None: #LINE# #TAB# #TAB# transform = transform_calc(mass, radius, height) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# out[i] = transform[0] * height * np.cos(i * radius / 180) - transform[1] * radius #LINE# #TAB# #TAB# for j in range(3): #LINE# #TAB# #TAB# #TAB# out[i] = transform[1] * radius - transform[j] * np.sin(i * radius / 180) + transform[2] * #LINE# #TAB# #TAB# #TAB# #TAB# np.cos(i * radius) + transform[3] * height #LINE# #TAB# return out"
#LINE# #TAB# filtered_repos = [] #LINE# #TAB# for repo in repos: #LINE# #TAB# #TAB# if repo in ignore_repos: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# filtered_repos.append(repo) #LINE# #TAB# return filtered_repos
#LINE# #TAB# if value.tzinfo is None: #LINE# #TAB# #TAB# utc_value = value #LINE# #TAB# else: #LINE# #TAB# #TAB# utc_value = value.astimezone(pytz.utc) #LINE# #TAB# utc_value = utc_value.replace(tzinfo=pytz.utc) #LINE# #TAB# return utc_value
#LINE# #TAB# delta = normalize_timedelta(delta) #LINE# #TAB# return f'{delta.days} per {delta.seconds}'
"#LINE# #TAB# new_formula = None #LINE# #TAB# with open(filename, 'rb') as infile: #LINE# #TAB# #TAB# new_formula = infile.read() #LINE# #TAB# return new_formula"
"#LINE# #TAB# val = args[argname] #LINE# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# return [val] #LINE# #TAB# return val"
#LINE# #TAB# global MANAGER_ID #LINE# #TAB# MANAGER_ID = auth
"#LINE# #TAB# if mime_type is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# mime_type = mime_type.lower() #LINE# #TAB# for ext in (b'jpg', b'jpeg', b'png', b'gif'): #LINE# #TAB# #TAB# if mime_type.endswith(ext): #LINE# #TAB# #TAB# #TAB# return ext #LINE# #TAB# return mime_type"
"#LINE# #TAB# name = extract_name_from_id(chebi_id, offline=offline) #LINE# #TAB# if not name: #LINE# #TAB# #TAB# return None #LINE# #TAB# return name[8]"
#LINE# #TAB# global current_urn #LINE# #TAB# if not actor_urn: #LINE# #TAB# #TAB# return None #LINE# #TAB# actor_ref = cls.get_current_actor_ref(actor_urn) #LINE# #TAB# while actor_ref is None: #LINE# #TAB# #TAB# actor_ref = cls.get_current_actor_ref(actor_urn) #LINE# #TAB# return actor_ref
"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(bookmark, 'r') as fp: #LINE# #TAB# #TAB# #TAB# return json.load(fp) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return {}"
"#LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if not isinstance(path, list): #LINE# #TAB# #TAB# #TAB# path = [path] #LINE# #TAB# #TAB# for el in path: #LINE# #TAB# #TAB# #TAB# if isinstance(el, list): #LINE# #TAB# #TAB# #TAB# #TAB# el = [el] #LINE# #TAB# #TAB# #TAB# elif isinstance(el, dict): #LINE# #TAB# #TAB# #TAB# #TAB# el = idx #LINE# #TAB# #TAB# #TAB# path.append(el) #LINE# #TAB# return path"
"#LINE# #TAB# data = copy.deepcopy(obj) #LINE# #TAB# b = data.pop('channel', None) #LINE# #TAB# d = data.pop('downloaded', False) #LINE# #TAB# if b is not None: #LINE# #TAB# #TAB# d['channel'] = b #LINE# #TAB# return d"
#LINE# #TAB# flatten = [] #LINE# #TAB# if 'features' in material: #LINE# #TAB# #TAB# for feature in material['features']: #LINE# #TAB# #TAB# #TAB# flatten.extend(flatten_material(feature)) #LINE# #TAB# else: #LINE# #TAB# #TAB# flatten.append(material) #LINE# #TAB# return flatten
#LINE# #TAB# if not settings_path: #LINE# #TAB# #TAB# return {} #LINE# #TAB# config = ConfigParser() #LINE# #TAB# if not os.path.exists(settings_path): #LINE# #TAB# #TAB# return {} #LINE# #TAB# for path in fs.ls(settings_path): #LINE# #TAB# #TAB# if path.startswith('http'): #LINE# #TAB# #TAB# #TAB# path = path[3:] #LINE# #TAB# #TAB# #TAB# config.read(path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return config
#LINE# #TAB# #TAB# result = [] #LINE# #TAB# #TAB# for child in struct.get('children'): #LINE# #TAB# #TAB# #TAB# if child.get('charge') == 0: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(child) #LINE# #TAB# #TAB# return result
"#LINE# #TAB# assert issparse(A) #LINE# #TAB# if A.dim() not in [2, 3]: #LINE# #TAB# #TAB# normalize_a = A.tocsc() #LINE# #TAB# else: #LINE# #TAB# #TAB# normalize_a = tf.divide(A, k=3) #LINE# #TAB# for _ in range(A.shape[0]): #LINE# #TAB# #TAB# for _ in range(A.shape[1]): #LINE# #TAB# #TAB# #TAB# normalize_a.data[(_), :] = tf.sqrt(norm(A.data[(_), :, :])) #LINE# #TAB# return normalize_a"
#LINE# #TAB# import ctypes as ct #LINE# #TAB# from.util import safe_call as safe_call #LINE# #TAB# from.library import backend #LINE# #TAB# if backend.name()!= 'cuda': #LINE# #TAB# #TAB# raise RuntimeError('Invalid backend loaded') #LINE# #TAB# safe_call(backend.get().afc_init_try_id(idx)) #LINE# #TAB# return
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('start_outputs', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN, description='maximum number of outputs') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was set #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# could not set parameter #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
#LINE# #TAB# output_string = '' #LINE# #TAB# for line in input_array: #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# output_string += line #LINE# #TAB# return output_string
"#LINE# #TAB# version_tuples = [] #LINE# #TAB# for version in versions: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# version_tuples.append((version.major, version.minor, version. #LINE# #TAB# #TAB# #TAB# #TAB# patch, version.micro)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if reverse: #LINE# #TAB# #TAB# version_tuples.reverse() #LINE# #TAB# return version_tuples"
"#LINE# #TAB# if not isinstance(x, tuple) or not isinstance(y, tuple): #LINE# #TAB# #TAB# return False #LINE# #TAB# for i in range(len(x)): #LINE# #TAB# #TAB# if x[i]!= y[i]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if not signature.startswith('^'): #LINE# #TAB# #TAB# return False #LINE# #TAB# if withSignature.startswith('$'): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# try: #LINE# #TAB# #TAB# return environ['CONTENT_LENGTH'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# register = {} #LINE# #TAB# for name, table in tables.items(): #LINE# #TAB# #TAB# if isinstance(table, BaseX): #LINE# #TAB# #TAB# #TAB# register.update(infer_register(table, table_ctor)) #LINE# #TAB# elif isinstance(table, Iterable): #LINE# #TAB# #TAB# for item in table: #LINE# #TAB# #TAB# #TAB# register.update(infer_register(item, table_ctor)) #LINE# #TAB# else: #LINE# #TAB# #TAB# register.update(table_ctor(ex)) #LINE# #TAB# return register"
"#LINE# #TAB# cookie = v1_cookie_codec(input_data, output_data, ratio, converter_type, channels) #LINE# #TAB# return cookie"
"#LINE# #TAB# service = scheme + '://' + host.lower() #LINE# #TAB# if scheme in ['http', 'https']: #LINE# #TAB# #TAB# service +='s' #LINE# #TAB# return service"
"#LINE# #TAB# can_create = user.can_create_topic(category=category, forum_id=forum #LINE# #TAB# #TAB#.pk, user=user) #LINE# #TAB# if can_create: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# opts = dict(gmt=opts) #LINE# #TAB# bids = opts.get('bids', []) #LINE# #TAB# if not bids: #LINE# #TAB# #TAB# return #LINE# #TAB# for id in bids: #LINE# #TAB# #TAB# if opts.get(id): #LINE# #TAB# #TAB# #TAB# raise ValueError('ID not recognized: %s' % id) #LINE# #TAB# #TAB# bids[id] = opts.get(id) #LINE# #TAB# return bids"
"#LINE# #TAB# from.versioneer import VersioneerConfig #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = '' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source ='src/pymor/versioneer.yaml' #LINE# #TAB# cfg.read(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB#'src/pymor/versioneer.yaml')) #LINE# #TAB# return cfg"
"#LINE# #TAB# if len(attempt) == 0 or not longopt_list: #LINE# #TAB# #TAB# return [] #LINE# #TAB# option_names = [v.split('=')[0] for v in longopt_list] #LINE# #TAB# distances = [(_DamerauLevenshtein(attempt, option[0:len(attempt)]), option) #LINE# #TAB# #TAB# #TAB# for option in option_names] #LINE# #TAB# distances.sort() #LINE# #TAB# option_names = [v.split('=')[0] for v in distances] #LINE# #TAB# if len(option_names) > 1: #LINE# #TAB# #TAB# return option_names[0] #LINE# #TAB# suggestions = [] #LINE# #TAB# for name in distances: #LINE# #TAB# #TAB# if option not in option_names: #LINE# #TAB# #TAB# #TAB# suggestions.append(name) #LINE# #TAB# return suggestions"
#LINE# #TAB# try: #LINE# #TAB# #TAB# number = generate_delete_number(claimset_data) #LINE# #TAB# #TAB# if number is not None: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if cigar_tuple[0] == 0 or cigar_tuple[1] <= 10: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0
"#LINE# #TAB# global ENABLED #LINE# #TAB# ENABLED = m.get(' ENABLED') #LINE# #TAB# if not ENABLED: #LINE# #TAB# #TAB# return #LINE# #TAB# execution_interval = getattr(m, 'execution_interval', None) #LINE# #TAB# if execution_interval is None: #LINE# #TAB# #TAB# return #LINE# #TAB# if 'labels' not in execution_interval: #LINE# #TAB# #TAB# execution_interval = getattr(m, 'execution_count', None) #LINE# #TAB# if not execution_interval: #LINE# #TAB# #TAB# return #LINE# #TAB# execution_interval = getattr(m, 'execution_count', None) #LINE# #TAB# if execution_interval is None: #LINE# #TAB# #TAB# return #LINE# #TAB# execution_interval = int(execution_interval * 1000) #LINE# #TAB# setattr(m, 'execution_count', execution_interval) #LINE# #TAB# return True"
#LINE# #TAB# if prefix is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if prefix.startswith('v'): #LINE# #TAB# #TAB# return prefix[len('v') + 1:] #LINE# #TAB# if prefix.startswith('c'): #LINE# #TAB# #TAB# return 'c' + prefix[len('c'):] #LINE# #TAB# return prefix
"#LINE# #TAB# possible_paths = [os.path.dirname(__file__)] #LINE# #TAB# for path in possible_paths: #LINE# #TAB# #TAB# filters_path = os.path.join(path, 'filters') #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mod = importlib.import_module(filters_path) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# filters = getattr(mod, 'filters', None) #LINE# #TAB# #TAB# if filters: #LINE# #TAB# #TAB# #TAB# return filters #LINE# #TAB# return []"
"#LINE# #TAB# client = PBW2ECIClient(host=app.config['PQW2ECI_HOST'], port= #LINE# #TAB# #TAB# app.config['PQW2ECI_PORT'], sandbox=sandbox) #LINE# #TAB# prometheus_client = KafkaClient(host=app.config['PQW2ECI_HOST'], port= #LINE# #TAB# #TAB# app.config['PQW2ECI_PORT']) #LINE# #TAB# try: #LINE# #TAB# #TAB# flask_client.bind(client) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return app"
"#LINE# #TAB# polar_x = np.arctan2(y, center[0]) #LINE# #TAB# polar_y = np.arctan2(y, center[1]) #LINE# #TAB# return polar_x, polar_y"
#LINE# #TAB# if len(spec) == 1: #LINE# #TAB# #TAB# return spec.copy() #LINE# #TAB# df1 = pd.DataFrame(index=spec.index) #LINE# #TAB# df1[spec == 1] = 0 #LINE# #TAB# df1[spec == -1] = 1 #LINE# #TAB# return df1
"#LINE# #TAB# if json: #LINE# #TAB# #TAB# encoded_data = {} #LINE# #TAB# #TAB# for field_name in json_fields: #LINE# #TAB# #TAB# #TAB# encoded_data[field_name] = getattr(node, field_name) #LINE# #TAB# #TAB# return encoded_data #LINE# #TAB# else: #LINE# #TAB# #TAB# return node"
#LINE# #TAB# output = b'' #LINE# #TAB# length = len(toDecode) #LINE# #TAB# while length > 0: #LINE# #TAB# #TAB# char = toDecode[length] #LINE# #TAB# #TAB# if char == '\x00': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# output += char #LINE# #TAB# #TAB# length = len(toDecode) #LINE# #TAB# return output
#LINE# #TAB# params = [] #LINE# #TAB# for param in operator.parameters: #LINE# #TAB# #TAB# params.append(Parameter(name=param.name)) #LINE# #TAB# return params
"#LINE# #TAB# mochad_controller = hass.data[DOMAIN] #LINE# #TAB# devs = config.get(CONF_DEVICES) #LINE# #TAB# add_entities([MochadLight(hass, mochad_controller.ctrl, dev) for dev in #LINE# #TAB# #TAB# devs]) #LINE# #TAB# return True"
"#LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r', newline='', encoding=encoding) #LINE# #TAB# elif encode: #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r', encoding=encoding) #LINE# #TAB# else: #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r') #LINE# #TAB# cached_confidence = file_obj.getvalue() #LINE# #TAB# return cached_confidence"
"#LINE# #TAB# assert len(triangle_list) == 3 #LINE# #TAB# for i, t in enumerate(triangle_list): #LINE# #TAB# #TAB# a = lut.get(t[0], t[1]) #LINE# #TAB# #TAB# b = lut.get(t[2], t[1]) #LINE# #TAB# #TAB# c = lut.get(t[0], t[2]) #LINE# #TAB# #TAB# if c is not None: #LINE# #TAB# #TAB# #TAB# yield a, b, c #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield a, b, c"
"#LINE# #TAB# if hasattr(cls, '__init__'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# spo = unpacked_spo.sign() #LINE# #TAB# algo = spo.algo.upper() #LINE# #TAB# if algo == '0': #LINE# #TAB# #TAB# spo_address = None #LINE# #TAB# else: #LINE# #TAB# #TAB# spo_address = unpacked_spo.address.lstrip('0') #LINE# #TAB# #TAB# spo = utils.Ghid(spo_address) #LINE# #TAB# return spo
#LINE# #TAB# obj = vtk.vtkActor() #LINE# #TAB# obj.SetData(transformed) #LINE# #TAB# return obj
"#LINE# #TAB# clusters = df[cluster_id].unique() #LINE# #TAB# grouped_clusters = [clusters] #LINE# #TAB# for cluster_item in clusters: #LINE# #TAB# #TAB# if len(cluster_item) > 1: #LINE# #TAB# #TAB# #TAB# cluster = clusters[cluster_item] #LINE# #TAB# #TAB# #TAB# grouped_clusters.append(cluster) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# if date_col is not None: #LINE# #TAB# #TAB# grouped_clusters[date_col] = df[cluster_item].str.split(',')[1] #LINE# #TAB# return grouped_clusters"
"#LINE# #TAB# import inspect #LINE# #TAB# import os #LINE# #TAB# global frame_index #LINE# #TAB# frame_index += 1 #LINE# #TAB# try: #LINE# #TAB# #TAB# while 1: #LINE# #TAB# #TAB# #TAB# details = inspect.getouterframes(frame_index) #LINE# #TAB# #TAB# #TAB# print(details[0], end='') #LINE# #TAB# #TAB# #TAB# frame_index -= 1 #LINE# #TAB# finally: #LINE# #TAB# #TAB# del frame_index"
"#LINE# #TAB# logger = logging.getLogger('provider_s3') #LINE# #TAB# logger.debug('[%s] Call %s', event.func_name, arg) #LINE# #TAB# with frame.f_back.f_back: #LINE# #TAB# #TAB# for idx, line in enumerate(frame.f_back.f_locals): #LINE# #TAB# #TAB# #TAB# if's3' in line: #LINE# #TAB# #TAB# #TAB# #TAB# logger.debug('%s: %s', line[0], line[1]) #LINE# #TAB# #TAB# #TAB# #TAB# del frame.f_back #LINE# #TAB# #TAB# #TAB# elif's3' in line: #LINE# #TAB# #TAB# #TAB# #TAB# logger.debug('%s: %s', line[0], line[1]) #LINE# #TAB# #TAB# #TAB# #TAB# del frame.f_back"
"#LINE# #TAB# result = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# result = str(random.randint(0, 999999999)) #LINE# #TAB# return result"
"#LINE# #TAB# data = [] #LINE# #TAB# for key, value in node.items(): #LINE# #TAB# #TAB# if key in headers: #LINE# #TAB# #TAB# #TAB# data.append(key) #LINE# #TAB# #TAB# elif isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# data.append(value) #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# data.append(split_element(value, headers)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data.append(value) #LINE# #TAB# return data"
#LINE# #TAB# if char.isdigit(): #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1
#LINE# #TAB# if gtype == cls.AFFIRMATIVE: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif gtype == cls.IPv4: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif gtype == cls.IPv6: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# outer_indices = sensor_legs_cell(g, all_tensors, s, inputs, #LINE# #TAB# #TAB# i1_cut_i2_wo_output, i1_union_i2) #LINE# #TAB# return outer_indices"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, #LINE# #TAB# #TAB# #TAB# socket.SOCK_STREAM) #LINE# #TAB# #TAB# return value #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# log.debug('Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port)) #LINE# #TAB# #TAB# return u'Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# log.debug('Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port)) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# for name, func in vars(cls).items(): #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if callable(func): #LINE# #TAB# #TAB# #TAB# setattr(cls, name, func) #LINE# #TAB# return cls"
#LINE# #TAB# devices = {} #LINE# #TAB# for var in net.op.variables: #LINE# #TAB# #TAB# devices.update(pop_var_devices(var)) #LINE# #TAB# return devices
"#LINE# #TAB# spp = None #LINE# #TAB# try: #LINE# #TAB# #TAB# email = email.lower().rstrip('@') #LINE# #TAB# #TAB# spp = email.rsplit('@', 1)[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return spp"
"#LINE# #TAB# pairs_data = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(nc_filename, 'r') as nc_file: #LINE# #TAB# #TAB# #TAB# for line in nc_file: #LINE# #TAB# #TAB# #TAB# #TAB# pairs_data.append({'time': line.stat().st_mtime, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'pair_start': line.stat().st_start, 'line_end': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# line.stat().st_end}) #LINE# #TAB# except IOError as ex: #LINE# #TAB# #TAB# if ex.errno!= errno.ENOENT: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return {'pairs_data': pairs_data}"
"#LINE# #TAB# if is_accessible == 0: #LINE# #TAB# #TAB# raise ValueError('face_running_windows() requires is_accessible to be True.') #LINE# #TAB# k = 0 #LINE# #TAB# if stop is None: #LINE# #TAB# #TAB# stop = start #LINE# #TAB# if step is None: #LINE# #TAB# #TAB# step = 1 #LINE# #TAB# w = [] #LINE# #TAB# start = int(start) #LINE# #TAB# stop = int(stop) #LINE# #TAB# if step > 0: #LINE# #TAB# #TAB# w.append(windows(is_accessible, start, stop, step)) #LINE# #TAB# #TAB# start += step #LINE# #TAB# w.reverse() #LINE# #TAB# for i in range(k): #LINE# #TAB# #TAB# w.append(windows[i]) #LINE# #TAB# return w"
#LINE# #TAB# result = {} #LINE# #TAB# params = urlparse(url).query #LINE# #TAB# for key in params: #LINE# #TAB# #TAB# result[key] = parse_qs(params[key]) #LINE# #TAB# return result
#LINE# #TAB# url = d.get('url') #LINE# #TAB# if not url.endswith('/'): #LINE# #TAB# #TAB# url += '/' #LINE# #TAB# return {'url': url + '/' + d['id']}
#LINE# #TAB# global _CLIENTS #LINE# #TAB# return _CLIENTS[name]
#LINE# #TAB# for path in os.environ['PATH'].split(os.pathsep): #LINE# #TAB# #TAB# if not path.endswith('record-without-pyc'): #LINE# #TAB# #TAB# #TAB# yield path
"#LINE# #TAB# return {'exchange': exchange, 'queue': queue_name, 'routing': routing if #LINE# #TAB# #TAB# routing else queue_name}"
"#LINE# #TAB# parts = value.split('=') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# key, path = parts #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if not re.match('^[a-zA-Z0-9_-]+$', path): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# _, path = path.rsplit('=', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not re.match('^[a-zA-Z0-9_-]+$', path): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# result = 1 #LINE# #TAB# while True: #LINE# #TAB# #TAB# r = int(num_n / result) #LINE# #TAB# #TAB# if r > 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# result = r #LINE# #TAB# return result
"#LINE# #TAB# player_config = load_player_config(player_config_path, team) #LINE# #TAB# return player_config"
"#LINE# #TAB# bot_configs = set() #LINE# #TAB# for root, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# if filename.endswith('.yaml'): #LINE# #TAB# #TAB# #TAB# #TAB# file_path = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# bot_configs.add(BotConfigBundle(file_path)) #LINE# #TAB# return bot_configs"
#LINE# #TAB# if credentials.type_indicator not in cls._loader_credentials: #LINE# #TAB# #TAB# cls._loader_credentials[credentials.type_indicator] = credentials
#LINE# #TAB# #TAB# points = [] #LINE# #TAB# #TAB# for item in l: #LINE# #TAB# #TAB# #TAB# points.append(cls.as_xml(item)) #LINE# #TAB# #TAB# return points
"#LINE# #TAB# if alg.has_stg: #LINE# #TAB# #TAB# buf = c.data #LINE# #TAB# else: #LINE# #TAB# #TAB# buf = c.data #LINE# #TAB# res = _up_retrieve_stg(alg, buf) #LINE# #TAB# if res is None: #LINE# #TAB# #TAB# raise Exception('decryption failed') #LINE# #TAB# n = len(res) #LINE# #TAB# buf = bytearray(n) #LINE# #TAB# while len(buf) > 0: #LINE# #TAB# #TAB# if _up_retrieve_block(alg, buf, n): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# s = _up_retrieve_block(alg, buf, n) #LINE# #TAB# #TAB# buf += s #LINE# #TAB# res = b''.join(buf) #LINE# #TAB# return res"
"#LINE# #TAB# global _stroke_width, _fcolor, fill_opacity #LINE# #TAB# if _stroke_width is not None: #LINE# #TAB# #TAB# _stroke_width = _stroke_width #LINE# #TAB# if _stroke_width < 0: #LINE# #TAB# #TAB# raise ValueError('stroke_width must be a positive number.') #LINE# #TAB# if _stroke_width > stroke_width: #LINE# #TAB# #TAB# raise ValueError('stroke_width must be > stroke_width.') #LINE# #TAB# _fill_opacity = fill_opacity #LINE# #TAB# _contourf_idx = _get_contourf_idx(contourf_idx, unit) #LINE# #TAB# _stroke_width = stroke_width #LINE# #TAB# _fcolor = fcolor #LINE# #TAB# _fill_opacity = fill_opacity"
#LINE# #TAB# dir_weight = np.zeros(3) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# dir_weight[i] = detect_weight_vector(points[i]) #LINE# #TAB# return dir_weight
#LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj.from_bytes(data) #LINE# #TAB# #TAB# except GIError as err: #LINE# #TAB# #TAB# #TAB# raise GIError(err) #LINE# #TAB# #TAB# return obj
"#LINE# #TAB# memory_app = _MemoryApp(controller or versions.AVAILABLE_VERSIONS[ #LINE# #TAB# #TAB# versions.DEFAULT_VERSION](), transactional=transactioned) #LINE# #TAB# memory_app.hooks.update({'before_request': before_request, #LINE# #TAB# #TAB# 'after_request': after_request, 'cache': {}}) #LINE# #TAB# return memory_app"
"#LINE# #TAB# geno = concatenate(geno) #LINE# #TAB# fname = geno.filename #LINE# #TAB# with open(fname, 'rb') as f: #LINE# #TAB# #TAB# f.seek(0) #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# N = mat.shape[0] #LINE# #TAB# A = np.zeros((N, N)) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# for j in range(N): #LINE# #TAB# #TAB# #TAB# A[i, j] = cholesky(mat[i, j]) #LINE# #TAB# result = np.zeros(N) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# tmp = np.zeros(N) #LINE# #TAB# #TAB# for j in range(N): #LINE# #TAB# #TAB# #TAB# if np.abs(A[i, j] - A[j, i]) > eps: #LINE# #TAB# #TAB# #TAB# #TAB# tmp[i, j] = A[i, j] - A[j, i] #LINE# #TAB# #TAB# result[i, j] = tmp #LINE# #TAB# return result"
#LINE# #TAB# for row_key in cuts: #LINE# #TAB# #TAB# row = cat_table[row_key] #LINE# #TAB# #TAB# if len(row) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield row
"#LINE# #TAB# response = HttpResponse(content_type='text/csv') #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=weights.csv' #LINE# #TAB# with open(os.path.join(settings.MEDIA_ROOT, 'data/weights.csv'), 'w') as csvfile: #LINE# #TAB# #TAB# reader = csv.DictReader(csvfile) #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# response.append(row) #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=weights.csv' #LINE# #TAB# return response"
#LINE# #TAB# for pair in pairwise(player.history): #LINE# #TAB# #TAB# if pair.preferred == player.preferred: #LINE# #TAB# #TAB# #TAB# return pair #LINE# #TAB# for pair in pairwise(player.history): #LINE# #TAB# #TAB# if pair.preferred > player.preferred: #LINE# #TAB# #TAB# #TAB# return pair #LINE# #TAB# return None
"#LINE# #TAB# from google.catalog import Catalog #LINE# #TAB# language = cls.get_language() #LINE# #TAB# topic = cls.get_topic() #LINE# #TAB# if language and topic: #LINE# #TAB# #TAB# catalog = Catalog(language=language, topic=topic) #LINE# #TAB# else: #LINE# #TAB# #TAB# catalog = Catalog() #LINE# #TAB# for topic in cls.get_topics(): #LINE# #TAB# #TAB# catalog.add_topic(topic=topic) #LINE# #TAB# #TAB# if topic: #LINE# #TAB# #TAB# #TAB# catalog.add_topic(topic=topic) #LINE# #TAB# #TAB# yield catalog"
#LINE# #TAB# if not has_start_stop: #LINE# #TAB# #TAB# return tf.get_variable('info_components_stop_activation_time') #LINE# #TAB# start_stop_threshold = tf.get_variable('info_components_start_stop_activation_time' #LINE# #TAB# #TAB# ) #LINE# #TAB# return start_stop_threshold or tf.get_global_variable('info_components_stop_activation_time_threshold' #LINE# #TAB# #TAB# ) or tf.get_global_variable('info_components_stop_activation_time_threshold' #LINE# #TAB# #TAB# ) or tf.get_variable('info_components_start_stop_activation_time' #LINE# #TAB# #TAB# ) or tf.get_global_variable('info_components_stop_activation_time_threshold' #LINE# #TAB# #TAB# ) or tf.get_variable('info_components_stop_activation_time_threshold' #LINE# #TAB# #TAB# ) or 1.0
"#LINE# #TAB# user = getattr(context['request'], 'user', None) #LINE# #TAB# if not article: #LINE# #TAB# #TAB# article = get_object_or_404(User, pk='article') #LINE# #TAB# if limit: #LINE# #TAB# #TAB# limit = int(limit) #LINE# #TAB# mappings = [] #LINE# #TAB# if user: #LINE# #TAB# #TAB# if not article['id']: #LINE# #TAB# #TAB# #TAB# mappings.append(article['id']) #LINE# #TAB# #TAB# context['categories'] = mappings #LINE# #TAB# if limit: #LINE# #TAB# #TAB# context['limit'] = limit #LINE# #TAB# return mappings"
#LINE# #TAB# a = pos[0] #LINE# #TAB# b = pos[1] #LINE# #TAB# c = pos[2] #LINE# #TAB# d = ((a - b) ** 2 + (c - d) ** 2) ** 0.5 #LINE# #TAB# return 2 * d
#LINE# #TAB# global _nurbsRenderers #LINE# #TAB# ref = baseFunction #LINE# #TAB# if ref == ffi.NULL: #LINE# #TAB# #TAB# return None #LINE# #TAB# nurbsRenderer = _nurbsRendererCache.get(ref) #LINE# #TAB# if nurbsRenderer is None: #LINE# #TAB# #TAB# nurbsRenderer = baseFunction() #LINE# #TAB# _nurbsRenderers[ref] = nurbsRenderer #LINE# #TAB# return nurbsRenderer
"#LINE# #TAB# obj = Rule(name, table.name, inobj.pop('description', None), inobj.pop( #LINE# #TAB# #TAB# 'owner', None), inobj.pop('privileges', []), inobj.pop('handler', #LINE# #TAB# #TAB# None), inobj.pop('privileges', []), inobj.pop('enabled', False)) #LINE# #TAB# obj.fix_privileges() #LINE# #TAB# obj.set_oldname(inobj) #LINE# #TAB# return obj"
#LINE# #TAB# for node in dx_nodes: #LINE# #TAB# #TAB# if not node.is_zone_node: #LINE# #TAB# #TAB# #TAB# yield node
"#LINE# #TAB# a = np.ma.masked_array(a) #LINE# #TAB# other = np.ma.masked_array(other) #LINE# #TAB# overlap = wc_overlap(particle, other, a, side, normalize=normalize) #LINE# #TAB# return overlap"
"#LINE# #TAB# data = refs.get_policy(profile, name, sha) #LINE# #TAB# new_sha = data.get(""head"") #LINE# #TAB# data[""update""] = {""sha"": sha, ""new_sha"": new_sha} #LINE# #TAB# data[""branch""] = name #LINE# #TAB# return new_sha"
"#LINE# #TAB# result = '' #LINE# #TAB# for pos, char in enumerate(name): #LINE# #TAB# #TAB# if char.isupper(): #LINE# #TAB# #TAB# #TAB# if pos == 0: #LINE# #TAB# #TAB# #TAB# #TAB# result += '_' #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if char.islower(): #LINE# #TAB# #TAB# #TAB# result += char #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if char == '_' and pos == 1: #LINE# #TAB# #TAB# #TAB# result += '-' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result += char #LINE# #TAB# return result"
"#LINE# #TAB# if token is not None: #LINE# #TAB# #TAB# credentials = jwt.decode(token) #LINE# #TAB# else: #LINE# #TAB# #TAB# credentials = None #LINE# #TAB# return {'Authorization': 'Bearer {}'.format(credentials.get( #LINE# #TAB# #TAB# 'access_token', ''))}"
"#LINE# #TAB# absolute_name = model_name #LINE# #TAB# if name.startswith('.'): #LINE# #TAB# #TAB# absolute_name = name #LINE# #TAB# elif name.startswith('model'): #LINE# #TAB# #TAB# absolute_name = model_name #LINE# #TAB# elif model_name.endswith('__'): #LINE# #TAB# #TAB# absolute_name = model_name[:-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# absolute_name = name #LINE# #TAB# return absolute_name, absolute_name"
#LINE# #TAB# res = [] #LINE# #TAB# for t in x: #LINE# #TAB# #TAB# if t == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if t[0].isupper() and len(t) > 1 and t[1:].islower(): #LINE# #TAB# #TAB# #TAB# res.append(TK_MAJ) #LINE# #TAB# #TAB# res.append(t.lower()) #LINE# #TAB# return res
#LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# return _generate_random_dataset(num_images) #LINE# #TAB# x = np.arange(num_images) #LINE# #TAB# metadata = _generate_random_dataset(num_images) #LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# datasets = [] #LINE# #TAB# #TAB# for d in x: #LINE# #TAB# #TAB# #TAB# datasets.append(_generate_random_dataset(num_images)) #LINE# #TAB# #TAB# metadata = _generate_random_dataset(num_images) #LINE# #TAB# return metadata
"#LINE# #TAB# logger.info('Loading parameters from %s', file_path) #LINE# #TAB# with open(file_path, 'rb') as fh: #LINE# #TAB# #TAB# parameters = pickle.load(fh) #LINE# #TAB# logger.info('Loaded parameters: %s', parameters) #LINE# #TAB# return parameters"
"#LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# return {cls._clean_string(key): cls._clean_string(val) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for key, val in value.items()} #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# return {cls._clean_string(key): cls._clean_string(val) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for key, val in value.items()} #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# return [IdentityRefWithVote(team_instance, identity_ref.email, #LINE# #TAB# #TAB# identity_ref.id) for identity_ref in reviewers for identity_ref in #LINE# #TAB# #TAB# search_identity_for_team(team_instance)]"
"#LINE# #TAB# if not resource_id: #LINE# #TAB# #TAB# raise ValueError('No resource id provided to generate_mapping') #LINE# #TAB# if not resource_id in _MAPPINGS: #LINE# #TAB# #TAB# with open(_MAPPINGS[resource_id], 'r') as file: #LINE# #TAB# #TAB# #TAB# return file.read() #LINE# #TAB# return _MAPPINGS[resource_id]"
"#LINE# #TAB# if isinstance(t, str): #LINE# #TAB# #TAB# return getattr(t, t[1:]) #LINE# #TAB# elif isinstance(t, list): #LINE# #TAB# #TAB# return [getattr_recursive(item) for item in t] #LINE# #TAB# else: #LINE# #TAB# #TAB# return t"
#LINE# #TAB# by_millen = cor_norm_rot.copy() #LINE# #TAB# by_millen[0] += 1 #LINE# #TAB# if by_millen[1] >= 0: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# d_reduction_factor = 0.0 #LINE# #TAB# for i in range(2): #LINE# #TAB# #TAB# d_reduction_factor += 1 #LINE# #TAB# #TAB# if d_reduction_factor > 0.0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return d_reduction_factor
#LINE# #TAB# result = get_key(secret_key) #LINE# #TAB# return result['data']
"#LINE# #TAB# target_dir = args['target_dir'] #LINE# #TAB# with tempfile.TemporaryDirectory() as tmpdir: #LINE# #TAB# #TAB# repo = Repo(target_dir) #LINE# #TAB# #TAB# pkg_list = repo.get_packages() #LINE# #TAB# #TAB# missing_packages = [] #LINE# #TAB# #TAB# for pkg in pkg_list: #LINE# #TAB# #TAB# #TAB# if pkg.name not in missing_packages: #LINE# #TAB# #TAB# #TAB# #TAB# missing_packages.append(pkg.name) #LINE# #TAB# #TAB# if len(missing_packages) > 0: #LINE# #TAB# #TAB# #TAB# print('Fixing old package ""{}"" to {}'.format(pkg.name, #LINE# #TAB# #TAB# #TAB# #TAB# missing_packages)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return ""OK"""
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(dir) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(dir): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return dir
"#LINE# #TAB# customer = Customer(name=name, email=email, phone=phone) #LINE# #TAB# customer.save() #LINE# #TAB# return customer"
#LINE# #TAB# padding = 0 #LINE# #TAB# n = len(node) #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# if n % 2 == 0: #LINE# #TAB# #TAB# #TAB# padding += 1 #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# return padding
#LINE# #TAB# del _patch_entry[0] #LINE# #TAB# return True
#LINE# #TAB# global app #LINE# #TAB# app = logger
#LINE# #TAB# if data == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# cur_int = data #LINE# #TAB# #TAB# sigma_f = 1.0 / data.std(0) #LINE# #TAB# #TAB# while cur_int > sigma_f: #LINE# #TAB# #TAB# #TAB# cur_int = random.choice(cur_int) #LINE# #TAB# #TAB# #TAB# sigma_f -= sigma_f #LINE# #TAB# #TAB# if cur_int < 1.0: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# data = cur_int #LINE# #TAB# #TAB# if data == '': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return data
#LINE# #TAB# if up: #LINE# #TAB# #TAB# return [arch for arch in arch.get_architectures() if arch.is_explicit()] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [arch for arch in arch.get_architectures() if arch.is_explicit()]
"#LINE# #TAB# test_names = [] #LINE# #TAB# for argname, arg_spec in argument_spec.items(): #LINE# #TAB# #TAB# if isinstance(arg_spec, tuple): #LINE# #TAB# #TAB# #TAB# arg_spec = tuple(arg_spec) #LINE# #TAB# #TAB# test_names.extend(arg_spec) #LINE# #TAB# return test_names"
"#LINE# #TAB# messages = [] #LINE# #TAB# for data in samples: #LINE# #TAB# #TAB# chrom = dd.get_sv_chrom(data) #LINE# #TAB# #TAB# if not chrom: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if chrom not in messages: #LINE# #TAB# #TAB# #TAB# messages.append(""{0}:{1}"".format(chrom, dd.get_sample_name(data))) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# messages.append(""{0}:{1}"".format(chrom, dd.get_sample_name(data))) #LINE# #TAB# return messages"
"#LINE# #TAB# if e5App().usesDarkPalette(): #LINE# #TAB# #TAB# icon = os.path.join('UiExtensionPlugins', 'icons', 'icons-subversion.svg') #LINE# #TAB# else: #LINE# #TAB# #TAB# icon = os.path.join('UiExtensionPlugins', 'icons','subversion.svg') #LINE# #TAB# return {'zzz_subversionPage': [QCoreApplication.translate( #LINE# #TAB# #TAB# 'VcsPySvnPlugin', 'Subversion'), icon, createConfigurationPage, None]}"
"#LINE# #TAB# seq = seq.replace('{', '{{').replace('}', '}}') #LINE# #TAB# return seq"
"#LINE# #TAB# first, last = next(iterator), next(iterator) #LINE# #TAB# for item in iterator: #LINE# #TAB# #TAB# if item >= step: #LINE# #TAB# #TAB# #TAB# yield first, last #LINE# #TAB# #TAB# #TAB# first = last #LINE# #TAB# yield first, last"
"#LINE# #TAB# headers = json.loads(data) #LINE# #TAB# frames = [] #LINE# #TAB# for header in headers: #LINE# #TAB# #TAB# frame = pd.DataFrame(headers[header]) #LINE# #TAB# #TAB# frames.append(frame) #LINE# #TAB# df = pd.concat(frames, axis=1) #LINE# #TAB# return df"
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t', header=None, usecols=[0, 1, 2], #LINE# #TAB# #TAB# names=['chrom', 'pos', 'genotype']) #LINE# #TAB# if 'chrom' not in df.columns: #LINE# #TAB# #TAB# table = 'chrom' #LINE# #TAB# #TAB# df['chrom'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['pos'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['genotype'] = df['chrom'].astype(str) #LINE# #TAB# return df"
"#LINE# #TAB# print(ctx.obj['path']) #LINE# #TAB# print('starting models backend...') #LINE# #TAB# backend_module = importlib.import_module('robotide.backend') #LINE# #TAB# print('using backend: %s' % backend_module) #LINE# #TAB# models = getattr(backend_module,'models') #LINE# #TAB# if models is not None: #LINE# #TAB# #TAB# print('using %s' % models) #LINE# #TAB# return models"
#LINE# #TAB# pmf = VarInt() #LINE# #TAB# try: #LINE# #TAB# #TAB# pmf.ParseFromString(raw_hex) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# raise #LINE# #TAB# return pmf
"#LINE# #TAB# if obj.id: #LINE# #TAB# #TAB# return '<a href=""%s"">%s</a>' % (obj.id, obj.filename) #LINE# #TAB# result = '' #LINE# #TAB# if obj.title: #LINE# #TAB# #TAB# result += '<a href=""%s"">%s</a>' % (obj.title, obj.url()) #LINE# #TAB# if obj.alias: #LINE# #TAB# #TAB# result += '<a href=""%s"">%s</a>' % (obj.alias, obj.url()) #LINE# #TAB# return result"
"#LINE# #TAB# split_name = name.split('/') #LINE# #TAB# if len(split_name) > 1: #LINE# #TAB# #TAB# complex_name = split_name[1] #LINE# #TAB# elif len(split_name) > 1: #LINE# #TAB# #TAB# complex_name = split_name[0] #LINE# #TAB# return complex_name, split_name[2]"
"#LINE# #TAB# if isinstance(condition, dict): #LINE# #TAB# #TAB# for keypath, value in condition.items(): #LINE# #TAB# #TAB# #TAB# if keypath not in user: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# keypath_real = keypath.real #LINE# #TAB# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# #TAB# for value in value: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if value in user[keypath_real]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# return keypath_real #LINE# #TAB# return True"
"#LINE# #TAB# encoded = {} #LINE# #TAB# for key in dict_a: #LINE# #TAB# #TAB# if isinstance(dict_a[key], dict) and isinstance(dict_b[key], dict): #LINE# #TAB# #TAB# #TAB# encoded[key] = encode_unicode(dict_a[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encoded[key] = dict_b[key] #LINE# #TAB# for key in dict_b: #LINE# #TAB# #TAB# if isinstance(dict_b[key], dict): #LINE# #TAB# #TAB# #TAB# encoded[key] = encode_unicode(dict_b[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encoded[key] = dict_a[key] #LINE# #TAB# return encoded"
#LINE# #TAB# if len(str_input) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# uuid_or_title = str_input #LINE# #TAB# if uuid_or_title.startswith('uuid:'): #LINE# #TAB# #TAB# uuid_or_title = uuid_or_title[4:] #LINE# #TAB# elif uuid_or_title.startswith('celery.'): #LINE# #TAB# #TAB# uuid_or_title = uuid_or_title[5:] #LINE# #TAB# elif uuid_or_title.startswith('bicycle.'): #LINE# #TAB# #TAB# uuid_or_title = uuid_or_title[6:] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Invalid UUID: {}'.format(str_input)) #LINE# #TAB# return uuid_or_title
"#LINE# #TAB# r = requests.get(url) #LINE# #TAB# soup = BeautifulSoup(r.text, 'html.parser') #LINE# #TAB# workid = soup.find('a', href=url).get('href') #LINE# #TAB# return workid"
"#LINE# #TAB# x_vector = Parallel(n_jobs)(delayed(_extract_x_vector, MI_FS, k, F, s, #LINE# #TAB# #TAB# are_data_binned)) #LINE# #TAB# return x_vector"
"#LINE# #TAB# if RE_DEFAULT_ARGS.search(logical_line): #LINE# #TAB# #TAB# msg = ( #LINE# #TAB# #TAB# #TAB# 'S360: Use of mutable type in function definition is not allowed in'#LINE# #TAB# #TAB# #TAB# 'function definition.') #LINE# #TAB# #TAB# yield 0, msg"
"#LINE# #TAB# if not degradations_args: #LINE# #TAB# #TAB# return [] #LINE# #TAB# pd_args = [Degradation(**d) for d in degradations_args] #LINE# #TAB# for i, d in enumerate(pd_args): #LINE# #TAB# #TAB# if not isinstance(d, Degradation): #LINE# #TAB# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Degradation %s should be of type list of Degradation' % #LINE# #TAB# #TAB# #TAB# #TAB# d) #LINE# #TAB# return pd_args"
"#LINE# #TAB# references = [] #LINE# #TAB# ref_finder = HTMLReferenceFinder(xml) #LINE# #TAB# for ref in ref_finder: #LINE# #TAB# #TAB# type_ = ref.attrib['type'] #LINE# #TAB# #TAB# ids = ref.attrib['ids'] #LINE# #TAB# #TAB# ref = { #LINE# #TAB# #TAB# #TAB# 'type': type_, #LINE# #TAB# #TAB# #TAB# 'ids': ids #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# asset = Reference( #LINE# #TAB# #TAB# #TAB# xml, #LINE# #TAB# #TAB# #TAB# type_, #LINE# #TAB# #TAB# #TAB# ids #LINE# #TAB# #TAB# ) #LINE# #TAB# #TAB# references.append(asset) #LINE# #TAB# return references"
"#LINE# #TAB# goldenratio = 0.2 #LINE# #TAB# x = 0.8 #LINE# #TAB# y = 0.8 #LINE# #TAB# if figwidth < goldenratio: #LINE# #TAB# #TAB# return figwidth, goldenratio #LINE# #TAB# figwidth = float(figwidth) #LINE# #TAB# return figwidth, y"
#LINE# #TAB# result = 0 #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if filter_function(item): #LINE# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# return result
"#LINE# #TAB# new_dictionary = {} #LINE# #TAB# for key in dictionary: #LINE# #TAB# #TAB# if type(dictionary[key]) == str: #LINE# #TAB# #TAB# #TAB# new_dictionary[key] = re.sub('(?<=\\[)(.*?)(?=\\])', '\\1', dictionary[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_dictionary[key] = dictionary[key] #LINE# #TAB# return new_dictionary"
"#LINE# #TAB# summary_images = pbclient.list_images() #LINE# #TAB# matching = [i for i in summary_images if i['properties']['name'] == #LINE# #TAB# #TAB# image_name and i['properties']['imageType'] == 'SUMMARY'] #LINE# #TAB# if not matching: #LINE# #TAB# #TAB# return [] #LINE# #TAB# summary = sorted(matching, key=lambda x: x['properties']['name']) #LINE# #TAB# for summary_image in summary: #LINE# #TAB# #TAB# if summary_image['properties']['imageType'] == 'HUMAN': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if summary_image['properties']['location'] == location: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# summary.append(summary_image) #LINE# #TAB# return summary"
"#LINE# #TAB# rot = np.array([[0, 0, 0], [0, np.sin(2 * np.pi / 180.0), -np.cos(2 * #LINE# #TAB# #TAB# np.pi / 180.0), 0], [0, np.sin(2 * np.pi / 180.0), np.cos(2 * np.pi / #LINE# #TAB# #TAB# 180.0)]]) #LINE# #TAB# return rot"
"#LINE# #TAB# if arrays is None: #LINE# #TAB# #TAB# arrays = ['a', 'b', 'c', 'd'] #LINE# #TAB# for label in ss: #LINE# #TAB# #TAB# if label =='result': #LINE# #TAB# #TAB# #TAB# yield '[{0}]'.format(label) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield '[{0}]'.format(label) #LINE# #TAB# else: #LINE# #TAB# #TAB# for label in arrays: #LINE# #TAB# #TAB# #TAB# yield '[{0}]'.format(label) #LINE# #TAB# for label in ss: #LINE# #TAB# #TAB# yield '[{0}]'.format(label) #LINE# #TAB# return"
#LINE# #TAB# if'request' in context: #LINE# #TAB# #TAB# user = context['request'].user #LINE# #TAB# #TAB# if user.is_authenticated(): #LINE# #TAB# #TAB# #TAB# return user.get_credentials().get('jobkey') #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# auth = keystoneclient() #LINE# #TAB# name = cloud_name.replace('-', '_') #LINE# #TAB# service = auth.generate_service(name) #LINE# #TAB# return service"
"#LINE# #TAB# if ctx.params['use_files']: #LINE# #TAB# #TAB# files = Path(s).rglob('*') #LINE# #TAB# #TAB# out = [] #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.is_file(): #LINE# #TAB# #TAB# #TAB# #TAB# out.append(f.name) #LINE# #TAB# #TAB# #TAB# elif isinstance(f, str): #LINE# #TAB# #TAB# #TAB# #TAB# out.append(s) #LINE# #TAB# #TAB# elif isinstance(f, list): #LINE# #TAB# #TAB# #TAB# out = [t.strip() for t in f] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out = [s] #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return s"
"#LINE# #TAB# sorted_encoders = sorted(encoders, key=lambda enc: enc.score(trainX, percentile= #LINE# #TAB# #TAB# percentile)) #LINE# #TAB# return sorted_encoders"
"#LINE# #TAB# if len(tok) == 4: #LINE# #TAB# #TAB# tok[0] = LogicalBinOpRule(tok[2], tok[1], tok[3]) #LINE# #TAB# elif len(tok) == 3: #LINE# #TAB# #TAB# tok[0] = LogicalBinOpRule(tok[1], tok[2], tok[3]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return tok"
#LINE# #TAB# conn = sqlite3.connect(filename) #LINE# #TAB# cursor = conn.cursor() #LINE# #TAB# clear_db_results(cursor) #LINE# #TAB# clear_db_errors(cursor) #LINE# #TAB# if reg_score > reg_warning or reg_error > reg_warning: #LINE# #TAB# #TAB# clear_db_errors(cursor) #LINE# #TAB# if reg_warning > reg_error: #LINE# #TAB# #TAB# clear_db_errors(cursor) #LINE# #TAB# return
"#LINE# #TAB# if filename is None: #LINE# #TAB# #TAB# filename = hashlib.md5(url.encode('utf-8')).hexdigest() #LINE# #TAB# cache_file = os.path.join(cache_dir, filename) #LINE# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# with open(cache_file, 'rb') as key_file: #LINE# #TAB# #TAB# #TAB# #TAB# key = base64.b64decode(key_file.read()) #LINE# #TAB# #TAB# #TAB# #TAB# cache.delete(key_file) #LINE# #TAB# #TAB# return key #LINE# #TAB# image = cls.load(url) #LINE# #TAB# image.verify() #LINE# #TAB# os.remove(cache_file) #LINE# #TAB# return image"
"#LINE# #TAB# regex = '.*' #LINE# #TAB# ports = [] #LINE# #TAB# for host, port in hosts_and_ports: #LINE# #TAB# #TAB# regex += r""[\s]{%s}"" % (host, port) #LINE# #TAB# #TAB# if len(port) > 1: #LINE# #TAB# #TAB# #TAB# port = int(port) #LINE# #TAB# #TAB# regex += r""[\s]{%s}"" % (host, port) #LINE# #TAB# regex += r""[\s]{%s}"" % (port, regex) #LINE# #TAB# return regex, ports"
"#LINE# #TAB# assert type(in_file_list) == list #LINE# #TAB# first_row = 0 #LINE# #TAB# for in_file in in_file_list: #LINE# #TAB# #TAB# assert os.path.exists(in_file) #LINE# #TAB# #TAB# cmd = ['rasterstack', 'extract', '-o', out_file, in_file] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# check_call(cmd) #LINE# #TAB# #TAB# except CalledProcessError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# first_row += 1 #LINE# #TAB# out_file.write('\n'.join(cmd)) #LINE# #TAB# return out_file"
"#LINE# #TAB# scencmd =''.join(scencmd.split('\n')[1:]) #LINE# #TAB# with open(scencmd, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# parts = line.strip().split('\t') #LINE# #TAB# #TAB# #TAB# if len(parts) > 2: #LINE# #TAB# #TAB# #TAB# #TAB# scenarios = parts[1:] #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# scenarios = [] #LINE# #TAB# #TAB# yield scenarios"
"#LINE# #TAB# security = [] #LINE# #TAB# packages = security_base() #LINE# #TAB# for package in packages: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# d = os.path.dirname(package) #LINE# #TAB# #TAB# #TAB# if os.path.isdir(d): #LINE# #TAB# #TAB# #TAB# #TAB# security.append(package) #LINE# #TAB# #TAB# #TAB# elif os.path.isfile(os.path.join(d, '__init__.py')): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return security"
"#LINE# #TAB# output = np.zeros(shape=(observable.shape[0] * counts['ndim'] * #LINE# #TAB# #TAB# observable.shape[1])) #LINE# #TAB# _sum = np.sum(counts * observable, axis=1) #LINE# #TAB# _sum = _sum / observable.shape[1] #LINE# #TAB# return output"
"#LINE# #TAB# if re.match( #LINE# #TAB# #TAB# '^(?:http|ftp)s?://(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|localhost|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/?|[/?]\\S+)$' #LINE# #TAB# #TAB#, url): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# if len(sync) < 200: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# ip = str(ip) #LINE# #TAB# try: #LINE# #TAB# #TAB# if netaddr.IPAddress(ip).version == 6: #LINE# #TAB# #TAB# #TAB# return ip #LINE# #TAB# except (TypeError, netaddr.AddrFormatError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return False"
#LINE# #TAB# if argument is not None: #LINE# #TAB# #TAB# return argument #LINE# #TAB# else: #LINE# #TAB# #TAB# return default
#LINE# #TAB# files_kv = {} #LINE# #TAB# for line in serialized.split('\n'): #LINE# #TAB# #TAB# words = line.strip().split() #LINE# #TAB# #TAB# if len(words) == 1: #LINE# #TAB# #TAB# #TAB# files_kv[words[0]] = words[0] #LINE# #TAB# return files_kv
#LINE# #TAB# y = y.copy() #LINE# #TAB# y[y == -1] = 0 #LINE# #TAB# y[y == 1] = 1 #LINE# #TAB# return y
#LINE# #TAB# for volume in volumes: #LINE# #TAB# #TAB# container = container_from_volume(volume) #LINE# #TAB# #TAB# if container.exists(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# click.echo('Creating data-only container {}'.format(name)) #LINE# #TAB# #TAB# container.save() #LINE# #TAB# return container
"#LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# return value"
"#LINE# #TAB# if artist == '' and title == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# file = open(TEMPOR_FILE, 'r') #LINE# #TAB# except: #LINE# #TAB# #TAB# return None #LINE# #TAB# song = file.read() #LINE# #TAB# file.close() #LINE# #TAB# args = song.split('\n') #LINE# #TAB# if len(args) >= 2: #LINE# #TAB# #TAB# if args[1] == artist: #LINE# #TAB# #TAB# #TAB# args[0] = title #LINE# #TAB# #TAB# if args[1] == title: #LINE# #TAB# #TAB# #TAB# return {'artist': song, 'title': song} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return {'artist': song, 'title': title} #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if key in _query_types: #LINE# #TAB# #TAB# return _query_types[key] #LINE# #TAB# for _type in _UPDATE_QUERY_TYPES: #LINE# #TAB# #TAB# if key in _type.keys(): #LINE# #TAB# #TAB# #TAB# return _type.get(key) #LINE# #TAB# return None
"#LINE# #TAB# if not isinstance(templates, (list, tuple)): #LINE# #TAB# #TAB# templates = [templates] #LINE# #TAB# css_host = '' #LINE# #TAB# for template in templates: #LINE# #TAB# #TAB# if hasattr(request, 'META') and request.META.get('HTTP_HOST'): #LINE# #TAB# #TAB# #TAB# css_host += '%s/%s' % (request.META.get('HTTP_HOST'), template) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# css_host += template #LINE# #TAB# return css_host"
#LINE# #TAB# rate_sources = [] #LINE# #TAB# for rxn in model.reactions: #LINE# #TAB# #TAB# if not rxn.reversibility: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# prev_start = rxn.start_time #LINE# #TAB# #TAB# for end in rxn.end_time: #LINE# #TAB# #TAB# #TAB# if prev_start < end: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# rate_sources.append(rxn) #LINE# #TAB# model.reactions = rate_sources
#LINE# #TAB# devices = create_bitbox02_bootloaders() #LINE# #TAB# for device in devices: #LINE# #TAB# #TAB# if device.is_btc: #LINE# #TAB# #TAB# #TAB# devices.append(device) #LINE# #TAB# return devices
#LINE# #TAB# import sep #LINE# #TAB# if type == '': #LINE# #TAB# #TAB# return sep #LINE# #TAB# for key in sep: #LINE# #TAB# #TAB# if type.lower() in key: #LINE# #TAB# #TAB# #TAB# return sep[key] #LINE# #TAB# return None
"#LINE# #TAB# rules_groups = [] #LINE# #TAB# for _ in range(count): #LINE# #TAB# #TAB# rules_groups.append(('', _('Rule group '))) #LINE# #TAB# return rules_groups"
#LINE# #TAB# if layer.keywords.get('exposure_class'): #LINE# #TAB# #TAB# return #LINE# #TAB# else: #LINE# #TAB# #TAB# return
#LINE# #TAB# if a == 1: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# elif a % p == 0: #LINE# #TAB# #TAB# for i in range(p): #LINE# #TAB# #TAB# #TAB# q = a % p #LINE# #TAB# #TAB# #TAB# x = q * i % p #LINE# #TAB# #TAB# #TAB# if x == 0: #LINE# #TAB# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1
"#LINE# #TAB# cm = np.asarray(cm, dtype=np.bool) #LINE# #TAB# if cm.ndim!= 2: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be 2-dimensional.') #LINE# #TAB# if cm.shape[0] < 2: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# elif cm.shape[1] > 2: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# for i in range(cm.shape[0]): #LINE# #TAB# #TAB# for j in range(cm.shape[1]): #LINE# #TAB# #TAB# #TAB# cm[i, j] = cm[i, j] #LINE# #TAB# return cm"
"#LINE# #TAB# status = False #LINE# #TAB# try: #LINE# #TAB# #TAB# data = action.to_jsonjson(rid=rid, unit=unit) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# log.info('JSONRPC queue action %s failed: %s', action.id, e) #LINE# #TAB# #TAB# status = True #LINE# #TAB# return status"
"#LINE# #TAB# result = [] #LINE# #TAB# for flag, value in list(flag_dict.items()): #LINE# #TAB# #TAB# if flag_filter.match(flag): #LINE# #TAB# #TAB# #TAB# result.append((flag, encrypt_unit(value))) #LINE# #TAB# return result"
#LINE# #TAB# if rand: #LINE# #TAB# #TAB# stat_id = str(stat) #LINE# #TAB# else: #LINE# #TAB# #TAB# stat_id = str(stat) #LINE# #TAB# stat['stat_id'] = stat_id #LINE# #TAB# stat['values'] = val #LINE# #TAB# return stat
#LINE# #TAB# try: #LINE# #TAB# #TAB# structure = pnr.to_structure() #LINE# #TAB# except ValueError as e: #LINE# #TAB# #TAB# raise ValueError('The structure of the personnummer is incorrect:'+ #LINE# #TAB# #TAB# #TAB# str(pnr)) #LINE# #TAB# return structure
#LINE# #TAB# has_git_modules = [] #LINE# #TAB# for tool in find_tools(plugin_dir): #LINE# #TAB# #TAB# if tool.has_git_module(): #LINE# #TAB# #TAB# #TAB# has_git_modules.append(tool.module_name) #LINE# #TAB# return has_git_modules
"#LINE# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# axes = [(v, i) for i, v in enumerate(values)] #LINE# #TAB# else: #LINE# #TAB# #TAB# axes = [(v, i) for i, v in enumerate(values)] #LINE# #TAB# histo_edges = np.histogram(edges, bins=axes) #LINE# #TAB# histo_values = histo_edges[0] #LINE# #TAB# return histo_values"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return val.replace('_', '_') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return val"
"#LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB#'sim_single_spingroup_old is deprecated. Please use a spingroup instead', #LINE# #TAB# #TAB# DeprecationWarning, stacklevel=2) #LINE# #TAB# sim_group = sim.SignalGroup(loc_ind, freq_offset, phantom) #LINE# #TAB# sim_group.apply() #LINE# #TAB# signal = sim_group.get_signal() #LINE# #TAB# return signal"
"#LINE# #TAB# objects = {} #LINE# #TAB# for w in words: #LINE# #TAB# #TAB# words_to_comments = [w.lower() for w in w] #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# #TAB# while i < len(words_to_comments): #LINE# #TAB# #TAB# #TAB# words_to_comments[i] = w #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# while words_to_comments[i] == '' and i < len(words_to_comments): #LINE# #TAB# #TAB# #TAB# words_to_comments.pop(i) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# for i in range(len(words_to_comments)): #LINE# #TAB# #TAB# obj = hash_object(words_to_comments[i], i) #LINE# #TAB# #TAB# objects[i] = obj #LINE# #TAB# return objects"
"#LINE# #TAB# if not retries: #LINE# #TAB# #TAB# return {} #LINE# #TAB# inject = {} #LINE# #TAB# for key, retries_config in retries.items(): #LINE# #TAB# #TAB# if isinstance(retries_config, RetryConfig): #LINE# #TAB# #TAB# #TAB# retry_config = retries_config #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# retry_config = RetryConfig(name=key, url=retries_config.url, #LINE# #TAB# #TAB# #TAB# #TAB# timeout=retries_config.timeout) #LINE# #TAB# #TAB# inject[retry_config.name] = retry_config #LINE# #TAB# return inject"
"#LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# first_line = f.readline() #LINE# #TAB# #TAB# if first_line.startswith(b'# Therapist'): #LINE# #TAB# #TAB# #TAB# return first_line.split(b'# ')[2]"
"#LINE# #TAB# import _registry #LINE# #TAB# shell_folder_name = _registry.getUtility(winreg.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders' #LINE# #TAB# #TAB# ) #LINE# #TAB# dir_path = os.path.join(shell_folder_name, csidl_name) #LINE# #TAB# if not os.path.isdir(dir_path): #LINE# #TAB# #TAB# os.makedirs(dir_path) #LINE# #TAB# return dir_path"
#LINE# #TAB# global JINJA_LOCAL_PATH #LINE# #TAB# JINJA_LOCAL_PATH = path
"#LINE# #TAB# from bcbio.pipeline.vcf import VCF #LINE# #TAB# pred = None #LINE# #TAB# with open(infile) as in_handle: #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# if line.startswith(""#""): #LINE# #TAB# #TAB# #TAB# #TAB# line = line[1:] #LINE# #TAB# #TAB# #TAB# #TAB# if not line.startswith(""#""): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if pred is None: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# pred = line #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# pred = pred + line #LINE# #TAB# return pred"
#LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print('Changing role for %s' % extension_names[0]) #LINE# #TAB# active_extensions = list(get_active_extensions()) #LINE# #TAB# for extension_name in extension_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# active_extensions[extension_name] = active_extensions.pop( #LINE# #TAB# #TAB# #TAB# #TAB# extension_name) #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# g = Graph() #LINE# #TAB# g.add_edges_from([(item, item) for item in qs]) #LINE# #TAB# g.add_edges_from([(item, item) for item in qs]) #LINE# #TAB# if limit: #LINE# #TAB# #TAB# g.add_limit(limit) #LINE# #TAB# return g"
"#LINE# #TAB# width_orig = widths[0] #LINE# #TAB# temp = np.zeros(num_samples) #LINE# #TAB# for w in range(1, num_samples): #LINE# #TAB# #TAB# temp[w] = rand.randint(0, len(widths)) #LINE# #TAB# return temp"
#LINE# #TAB# node_id = initial_guess #LINE# #TAB# while node_id not in graph.nodes_iter(): #LINE# #TAB# #TAB# node_id = _format.format(node_id) #LINE# #TAB# return node_id
#LINE# #TAB# if data.dim == 1: #LINE# #TAB# #TAB# data = np.conj(data) #LINE# #TAB# elif data.dim == 2: #LINE# #TAB# #TAB# data = np.conj(data) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('axis must be 1 or 2') #LINE# #TAB# return data
"#LINE# #TAB# functions = [] #LINE# #TAB# for key, value in environ.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# functions.append(import_string(value)) #LINE# #TAB# #TAB# elif isinstance(value, collections.Mapping): #LINE# #TAB# #TAB# #TAB# functions.extend(value) #LINE# #TAB# #TAB# elif isinstance(value, collections.Iterable): #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# functions.append(cls(key, item)) #LINE# #TAB# return functions"
"#LINE# #TAB# json_format = json.Formatter({}) #LINE# #TAB# for col_type in col_types: #LINE# #TAB# #TAB# col = pd.DataFrame(col_types[col_type]) #LINE# #TAB# #TAB# col = col.astype(str) #LINE# #TAB# #TAB# obj = json_format.format(col=col, nan_display=nan_display) #LINE# #TAB# #TAB# if overrides: #LINE# #TAB# #TAB# #TAB# obj.update(overrides) #LINE# #TAB# return obj"
"#LINE# #TAB# sorted_outputs = sorted(outputs, key=lambda x: x[0]) #LINE# #TAB# dt_mean = 0 #LINE# #TAB# for i, output in enumerate(sorted_outputs): #LINE# #TAB# #TAB# dt_mean += output[0] * output[1] #LINE# #TAB# return dt_mean"
"#LINE# #TAB# if autospace: #LINE# #TAB# #TAB# xmlattr = d.copy() #LINE# #TAB# #TAB# xmlattr.append('autospace=%s' % autospace) #LINE# #TAB# #TAB# xmlattr.append('value=%s' % d.get('value', '')) #LINE# #TAB# else: #LINE# #TAB# #TAB# xmlattr = d.copy() #LINE# #TAB# #TAB# for k in xmlattr: #LINE# #TAB# #TAB# #TAB# xmlattr[k] = 0 #LINE# #TAB# return xmlattr"
#LINE# #TAB# dialog = ScrolledMessageDialog(parent) #LINE# #TAB# dialog.setText(text) #LINE# #TAB# dialog.exec_() #LINE# #TAB# return dialog
#LINE# #TAB# table_acl = None #LINE# #TAB# acl_nodes = root.findall('./{%s}acl' % NS_TABLE) #LINE# #TAB# if acl_nodes: #LINE# #TAB# #TAB# for acl_node in acl_nodes: #LINE# #TAB# #TAB# #TAB# table_acl = retrieve_table_acl(acl_node) #LINE# #TAB# return table_acl
"#LINE# #TAB# app = load_app_config() #LINE# #TAB# status, result = app.set_mappings_codes_by_city(city_name, token) #LINE# #TAB# if status: #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError(u'Failed to set mappings for city: {city_name}' #LINE# #TAB# #TAB# #TAB# #TAB#.format(city_name=city_name)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# alpha = np.linspace(0, 1, N) #LINE# #TAB# if hsv: #LINE# #TAB# #TAB# color1 = hsv_to_rgb(color1, alpha) #LINE# #TAB# #TAB# color2 = hsv_to_rgb(color2, alpha) #LINE# #TAB# return [color1, color2]"
#LINE# #TAB# print('Repository set to: {}'.format(repository)) #LINE# #TAB# global _list #LINE# #TAB# if should_list: #LINE# #TAB# #TAB# _list = [True] * len(repository.get_absolute_path()) #LINE# #TAB# global _repo #LINE# #TAB# _repo = repository #LINE# #TAB# _list = [False] * len(repository.get_absolute_path()) #LINE# #TAB# _repo.url = repository.get_absolute_path() #LINE# #TAB# if len(_list) > 0: #LINE# #TAB# #TAB# _list = [True] * len(_list) #LINE# #TAB# global _repo #LINE# #TAB# _repo.url = '/'.join(_list) #LINE# #TAB# _repo.should_list = should_list
"#LINE# #TAB# if six.PY3: #LINE# #TAB# #TAB# if isinstance(string, bytes): #LINE# #TAB# #TAB# #TAB# return string.decode('utf-16') #LINE# #TAB# return string"
"#LINE# #TAB# if create_copy: #LINE# #TAB# #TAB# new_dict = {} #LINE# #TAB# else: #LINE# #TAB# #TAB# new_dict = left.copy() #LINE# #TAB# for key, value in right.items(): #LINE# #TAB# #TAB# if key in new_dict and isinstance(new_dict[key], bytes): #LINE# #TAB# #TAB# #TAB# new_dict[key] = style_bytes_merge(new_dict[key], value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_dict[key] = value #LINE# #TAB# return new_dict"
#LINE# #TAB# if s.group_by: #LINE# #TAB# #TAB# return Selectable(s.group_by[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return s
"#LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return value #LINE# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# return {k: array_vnl_json(v) for k, v in value.items()} #LINE# #TAB# return value"
"#LINE# #TAB# x = data[:, (0)] #LINE# #TAB# y = data[:, (1)] #LINE# #TAB# w = np.sqrt(x ** 2 + y ** 2) #LINE# #TAB# cm = np.zeros((len(data), 2), dtype=np.float64) #LINE# #TAB# for i in range(0, len(data)): #LINE# #TAB# #TAB# cm[i] = w * ((x - data[i]) ** 2 + (y - data[i]) ** 2) / (w * (y - data[i]) ** #LINE# #TAB# #TAB# #TAB# 2) #LINE# #TAB# return cm"
#LINE# #TAB# try: #LINE# #TAB# #TAB# del cls._member_refs[component] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# kwargs = {} #LINE# #TAB# for key in header.keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# kwargs[key] = float(header[key]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return kwargs
#LINE# #TAB# fields = set() #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# for doc_type in doc_types: #LINE# #TAB# #TAB# #TAB# #TAB# cursor.execute(SQL['find_generator_fields']).fetchall() #LINE# #TAB# #TAB# #TAB# #TAB# rows = cursor.fetchall() #LINE# #TAB# #TAB# #TAB# #TAB# for row in rows: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# fields.add(row[0]) #LINE# #TAB# return fields
#LINE# #TAB# d = {} #LINE# #TAB# libsvm = load_libsvm(filename) #LINE# #TAB# for dataset in libsvm: #LINE# #TAB# #TAB# idx = dataset['idx'] #LINE# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# if idx == 0: #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# elif idx < len(libsvm): #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# #TAB# d[dataset['name']][opt_type] = idx #LINE# #TAB# return d
"#LINE# #TAB# if version_string is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# parts = version_string.split('-') #LINE# #TAB# major, minor, revision, prerelease = parts #LINE# #TAB# return major, minor, revision, prerelease"
#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# if not logger.handlers: #LINE# #TAB# #TAB# path = _get_log_path(name) #LINE# #TAB# #TAB# logger.addHandler(logging.NullHandler()) #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.addHandler(logging.StreamHandler()) #LINE# #TAB# return logger
#LINE# #TAB# from os import path #LINE# #TAB# full_path = path.expanduser(f) #LINE# #TAB# path.mkdir(full_path) #LINE# #TAB# if not path.isdir(full_path): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.mkdir(full_path) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass
#LINE# #TAB# record = json_format.MessageToJson(message_proto) #LINE# #TAB# record_dict = json.loads(record) #LINE# #TAB# return record_dict
#LINE# #TAB# it = iter(agg) #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# yield next(it) #LINE# #TAB# #TAB# #TAB# it = next(it) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return #LINE# #TAB# if it.is_prime(): #LINE# #TAB# #TAB# return #LINE# #TAB# if it.is_left: #LINE# #TAB# #TAB# yield next(it) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield next(it) #LINE# #TAB# return
"#LINE# #TAB# try: #LINE# #TAB# #TAB# output = subprocess.check_output(cmd, stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# #TAB# stdout=subprocess.PIPE) #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# err = e.output.decode('utf-8') #LINE# #TAB# #TAB# log_error(err) #LINE# #TAB# #TAB# return e.returncode, err.output"
#LINE# #TAB# p = random.random() #LINE# #TAB# if p < 1.0: #LINE# #TAB# #TAB# return state.apply(updates[p]) #LINE# #TAB# else: #LINE# #TAB# #TAB# state.apply(updates[p - 1]) #LINE# #TAB# #TAB# return state
"#LINE# #TAB# md = jinja_context.copy() #LINE# #TAB# md.update(jinja_context['wsgi.url_map']) #LINE# #TAB# md.update(os.path.join(new_filename, 'index.html')) #LINE# #TAB# return md"
#LINE# #TAB# if not lookfor: #LINE# #TAB# #TAB# return logging.CRITICAL #LINE# #TAB# levels = [] #LINE# #TAB# for i in range(4): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if lookfor.upper() in i: #LINE# #TAB# #TAB# #TAB# #TAB# levels.append(logging.INFO) #LINE# #TAB# #TAB# #TAB# elif lookfor.upper() in i: #LINE# #TAB# #TAB# #TAB# #TAB# levels.append(logging.WARNING) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return levels[0]
"#LINE# #TAB# d = datetime.datetime(r, g, b) #LINE# #TAB# date_str = '%02d%02d%02d' % (d.hour, d.minute, d.second) #LINE# #TAB# return date_str"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return np.sum(np.eye(solution.shape[0] + 1)) - np.sum(np.eye(prediction. #LINE# #TAB# #TAB# #TAB# shape[0])) #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('weights_flow_in_gigabytes', dtype='int32', direction= #LINE# #TAB# #TAB# function.OUT, description='The memory limit of the flow in gigabytes.' #LINE# #TAB# #TAB# ) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"#LINE# #TAB# access_token, _ = remote_gen_refresh_token(session, refresh_token) #LINE# #TAB# return access_token"
#LINE# #TAB# if s is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# s.encode('ascii') #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"#LINE# #TAB# if infotype == 'OBJECT': #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# for line in response.splitlines(): #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if ':' in line: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = line.split(':') #LINE# #TAB# #TAB# #TAB# #TAB# key = key.strip()[7:] #LINE# #TAB# #TAB# #TAB# #TAB# value = value.strip()[:-1] #LINE# #TAB# #TAB# #TAB# #TAB# result[key] = value #LINE# #TAB# else: #LINE# #TAB# #TAB# result = response.splitlines() #LINE# #TAB# return result"
#LINE# #TAB# program = Program() #LINE# #TAB# for field in dictionary['tasks']: #LINE# #TAB# #TAB# program.tasks.append(dictionary[field]) #LINE# #TAB# return program
"#LINE# #TAB# prompt = RawTextInput() #LINE# #TAB# prompt.readline(filename) #LINE# #TAB# res = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res = prompt.readline().strip() #LINE# #TAB# #TAB# except (EOFError, IOError): #LINE# #TAB# #TAB# #TAB# print('EOF while parsing prompt') #LINE# #TAB# #TAB# #TAB# prompt = '' #LINE# #TAB# #TAB# if res == '': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return res"
#LINE# #TAB# style = {} #LINE# #TAB# for obj in path.get_objects(): #LINE# #TAB# #TAB# style['alpha'] = obj.get_alpha() #LINE# #TAB# #TAB# if obj.label is not None: #LINE# #TAB# #TAB# #TAB# if fill: #LINE# #TAB# #TAB# #TAB# #TAB# style['color'] = obj.get_color() #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# style['color'] = obj.get_color() #LINE# #TAB# return style
"#LINE# #TAB# if getattr(sys, 'frozen', False): #LINE# #TAB# #TAB# out_dir = os.path.join(os.getcwd(), 'contents') #LINE# #TAB# elif sys.platform == 'win32': #LINE# #TAB# #TAB# out_dir = os.path.join(os.getcwd(), 'contents') #LINE# #TAB# else: #LINE# #TAB# #TAB# out_dir = os.path.join(os.getcwd(), 'contents') #LINE# #TAB# return out_dir"
#LINE# #TAB# obj = lib.sdl_data_get(path) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# raise SDLError('Invalid data path: %s' % path) #LINE# #TAB# return obj
#LINE# #TAB# key = hashlib.sha1(data).digest() #LINE# #TAB# key = base64.b64encode(key.encode('utf-8')) #LINE# #TAB# return key
#LINE# #TAB# cookie_value = get_cookie_value(name) #LINE# #TAB# if cookie_value == value: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError
"#LINE# #TAB# f = open(dest, 'w') #LINE# #TAB# try: #LINE# #TAB# #TAB# json.dump(src, f, indent=2, separators=(',', ': '), sort_keys=True, #LINE# #TAB# #TAB# #TAB# separators=(',', ': ')) #LINE# #TAB# finally: #LINE# #TAB# #TAB# f.close() #LINE# #TAB# return"
"#LINE# #TAB# mX = np.asarray(mX, dtype=float) #LINE# #TAB# mY = np.asarray(mY, dtype=float) #LINE# #TAB# pX = queryGeoTransform(mX, mY, geoTransform) #LINE# #TAB# pY = queryGeoTransform(mY, mX + geoTransform[0], geoTransform[1]) #LINE# #TAB# return pX, pY"
#LINE# #TAB# if atom.symbol == 'X': #LINE# #TAB# #TAB# return atom #LINE# #TAB# if atom.symbol == 'Y': #LINE# #TAB# #TAB# return atom #LINE# #TAB# if atom.symbol == 'Z': #LINE# #TAB# #TAB# return atom #LINE# #TAB# return atom
"#LINE# #TAB# pool_name = request.GET.get('pool_name', None) #LINE# #TAB# if pool_name: #LINE# #TAB# #TAB# request.session['pool_name'] = '' #LINE# #TAB# #TAB# request.session['clipboard'] = None #LINE# #TAB# #TAB# request.response.status = '200 OK' #LINE# #TAB# else: #LINE# #TAB# #TAB# request.session['pool_name'] = 'None'"
#LINE# #TAB# global _keyword #LINE# #TAB# _keyword = passwd
"#LINE# #TAB# try: #LINE# #TAB# #TAB# decoded = base64.b64decode(address) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(decoded, bytes): #LINE# #TAB# #TAB# address = decoded.decode() #LINE# #TAB# parts = decoded.split(':') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if parts[0] == '01': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if x < 0.0: #LINE# #TAB# #TAB# return -np.log(a[x]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return a[x]
#LINE# #TAB# logging.debug('Skipping skip reload...') #LINE# #TAB# return
"#LINE# #TAB# identifier = ensure_bookmark(bookmark) #LINE# #TAB# if len(identifier) == 1: #LINE# #TAB# #TAB# return identifier[0], identifier[1] #LINE# #TAB# return None, None"
#LINE# #TAB# sep = '\t' #LINE# #TAB# if options.test: #LINE# #TAB# #TAB# sep = '\t' #LINE# #TAB# return sep
#LINE# #TAB# if resolver_helper.type_indicator not in cls._resolver_helpers: #LINE# #TAB# #TAB# raise KeyError('Resolver helper object not set for type indicator: {0:s}.' #LINE# #TAB# #TAB# #TAB#.format(resolver_helper.type_indicator)) #LINE# #TAB# resolver_helper.type_indicator = None
"#LINE# #TAB# n = 100 * percent #LINE# #TAB# files = [] #LINE# #TAB# for r1, r2 in zip(R1, R2): #LINE# #TAB# #TAB# n, f = os.path.splitext(os.path.basename(r1)) #LINE# #TAB# #TAB# if os.path.splitext(f)[1] in ['.R1', '.R2']: #LINE# #TAB# #TAB# #TAB# subset = os.path.basename(f) #LINE# #TAB# #TAB# #TAB# if subset in files: #LINE# #TAB# #TAB# #TAB# #TAB# files.append(subset) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# files.append(f) #LINE# #TAB# return files"
"#LINE# #TAB# session = db.get_reader_session() #LINE# #TAB# with session.begin(): #LINE# #TAB# #TAB# res = any(session.query(m).filter(m.tenant_id == tenant_id).count() for #LINE# #TAB# #TAB# #TAB# m in [models_v2.Network, models_v2.Port]) #LINE# #TAB# return res"
#LINE# #TAB# t = threading.Queue() #LINE# #TAB# while True: #LINE# #TAB# #TAB# if time.time() - t.time > num: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# task.run() #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# logging.debug(e) #LINE# #TAB# #TAB# #TAB# yield None #LINE# #TAB# #TAB# #TAB# time.sleep(0.1) #LINE# #TAB# #TAB# #TAB# t.join() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
#LINE# #TAB# if score_c >= 50 and score_c <= 85: #LINE# #TAB# #TAB# return 'Risk' #LINE# #TAB# if score_c >= 85 and score_c <= 85: #LINE# #TAB# #TAB# return 'Fair' #LINE# #TAB# if score_c >= 85 and score_c <= 85: #LINE# #TAB# #TAB# return 'Fair' #LINE# #TAB# if score_c >= 85: #LINE# #TAB# #TAB# return 'Fair' #LINE# #TAB# return 'None'
"#LINE# #TAB# align = {'left': lambda x: 'align({0})'.format(x), 'right': lambda x: 'align({1})' #LINE# #TAB# #TAB#.format(x), 'center': lambda x: 'align({0})'.format(x), 'bottom': lambda x: #LINE# #TAB# #TAB# 'align({0})'.format(x), 'center': lambda x: 'align({1})'.format(x), #LINE# #TAB# #TAB# 'top': lambda x: 'align({0})'.format(x), 'bottom': lambda x: 'align({1})'.format( #LINE# #TAB# #TAB# x), 'top': lambda x: 'align({0})'.format(x), 'bottom': lambda x: 'align({1})'.format( #LINE# #TAB# #TAB# x), 'center': lambda x: 'align({0})'.format(x))} #LINE# #TAB# return align"
#LINE# #TAB# errors = np.sqrt((y1 - y) ** 2 + (z1 - z) ** 2) #LINE# #TAB# for i in range(len(pt_1)): #LINE# #TAB# #TAB# if pt_1[i]!= pt_2[i]: #LINE# #TAB# #TAB# #TAB# errors[i] = errors_avg((y1[i] - pt_1[i]) ** 2 + (y2[i] - #LINE# #TAB# #TAB# #TAB# #TAB# pt_2[i]) ** 2) #LINE# #TAB# for i in range(len(pt_1)): #LINE# #TAB# #TAB# if pt_1[i]!= pt_2[i]: #LINE# #TAB# #TAB# #TAB# errors[i] = errors_avg((y1[i] - pt_1[i]) ** 2 + (y2[i] - #LINE# #TAB# #TAB# #TAB# #TAB# pt_2[i]) ** 2) #LINE# #TAB# return errors
#LINE# #TAB# for w in get_pool(): #LINE# #TAB# #TAB# if title in w.titles: #LINE# #TAB# #TAB# #TAB# if not exact and w.title.startswith(title): #LINE# #TAB# #TAB# #TAB# #TAB# return w #LINE# #TAB# #TAB# return None
"#LINE# #TAB# resource_registry = remote.get_resource_registry() #LINE# #TAB# groups = resource_registry.get_groups() #LINE# #TAB# user_info = [] #LINE# #TAB# for group in groups: #LINE# #TAB# #TAB# user_info.append(group.get_user_info()) #LINE# #TAB# try: #LINE# #TAB# #TAB# user_info.sort() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# user_info = None #LINE# #TAB# return user_info, groups"
"#LINE# #TAB# file_path = config[main_section] #LINE# #TAB# file_path = os.path.expandvars(file_path) #LINE# #TAB# if not os.path.isabs(file_path): #LINE# #TAB# #TAB# file_path = os.path.join(os.getcwd(), file_path) #LINE# #TAB# config[main_section] = file_path"
"#LINE# #TAB# parent = node.parent #LINE# #TAB# while parent is not None: #LINE# #TAB# #TAB# if isinstance(parent, dict): #LINE# #TAB# #TAB# #TAB# parent = parent.get('children', None) #LINE# #TAB# #TAB# if parent is not None: #LINE# #TAB# #TAB# #TAB# return generate_resource(parent) #LINE# #TAB# #TAB# parent = parent.get('parent', None) #LINE# #TAB# return None"
"#LINE# #TAB# out = bk.BKTensor(0) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# out += bk.eq(state0.items_pauli(state1, i), bk.eq(state0.items_pauli(state2, i), bk. #LINE# #TAB# #TAB# #TAB# eq(state1, i))) #LINE# #TAB# return out"
"#LINE# #TAB# x1, y1 = p #LINE# #TAB# x2, y2 = p #LINE# #TAB# if x1!= x2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if y1!= y2: #LINE# #TAB# #TAB# return False #LINE# #TAB# z1 = x1 - a #LINE# #TAB# z2 = y1 - b #LINE# #TAB# return z1 == z2"
"#LINE# #TAB# body = request_body #LINE# #TAB# if body is None: #LINE# #TAB# #TAB# body = '' #LINE# #TAB# elif isinstance(request_body, dict): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# body = request_body['xml'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if not isinstance(body, bytes): #LINE# #TAB# #TAB# return body #LINE# #TAB# if isinstance(body, list): #LINE# #TAB# #TAB# body = [body] #LINE# #TAB# xml = etree.tostring(body, pretty_print=True) #LINE# #TAB# return xml"
"#LINE# #TAB# encoding = sys.getfilesystemencoding() #LINE# #TAB# for char in input: #LINE# #TAB# #TAB# if not char.isalpha(): #LINE# #TAB# #TAB# #TAB# yield encoding.decode(char, errors) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield char"
"#LINE# #TAB# if module.startswith('homeassistant.components.'): #LINE# #TAB# #TAB# return #LINE# #TAB# else: #LINE# #TAB# #TAB# mod = module.replace('homeassistant.components', '') #LINE# #TAB# #TAB# if not os.path.exists(mod): #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# plt.close(mod) #LINE# #TAB# #TAB# #TAB# os.remove(mod) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# hms = {} #LINE# #TAB# if not isinstance(r, list): #LINE# #TAB# #TAB# r = np.array(r, dtype=np.uint8) #LINE# #TAB# if not isinstance(g, list): #LINE# #TAB# #TAB# g = np.array(g, dtype=np.uint8) #LINE# #TAB# if not isinstance(b, list): #LINE# #TAB# #TAB# b = np.array(b, dtype=np.uint8) #LINE# #TAB# for index, item in enumerate(hms): #LINE# #TAB# #TAB# r, g, b = item #LINE# #TAB# #TAB# dist = abs(r - g) + abs(b - b) #LINE# #TAB# #TAB# hms[index] = rgb_to_hms[dist] #LINE# #TAB# return hms"
"#LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# output = subprocess.check_output('ip addr create -a') #LINE# #TAB# email = output.strip() #LINE# #TAB# return email"
#LINE# #TAB# result = mags[int(lag * maglen / magmed):int(lag * maglen / magstd)] #LINE# #TAB# return result
"#LINE# #TAB# if symbol: #LINE# #TAB# #TAB# mask = Xlib.Xop.set_timezone(display, symbol) #LINE# #TAB# else: #LINE# #TAB# #TAB# mask = 0 #LINE# #TAB# return mask"
"#LINE# #TAB# env = np.zeros(iq_array.shape) #LINE# #TAB# for i in range(iq_array.shape[0]): #LINE# #TAB# #TAB# env[i] = hilbert_transform(iq_array[(i), :]) #LINE# #TAB# return env"
#LINE# #TAB# request_func._inject_header_head = True #LINE# #TAB# return request_func
"#LINE# #TAB# if this._depends_on: #LINE# #TAB# #TAB# for dep in this._depends_on: #LINE# #TAB# #TAB# #TAB# without_correction_setter(dep, value) #LINE# #TAB# return value"
#LINE# #TAB# title_fasta_map = {} #LINE# #TAB# if reference_fasta_map_param: #LINE# #TAB# #TAB# for key in reference_fasta_map_param.keys(): #LINE# #TAB# #TAB# #TAB# title_fasta_map[key] = reference_fasta_map_param[key] #LINE# #TAB# return title_fasta_map
#LINE# #TAB# try: #LINE# #TAB# #TAB# new_value = _libc_string_ctypes() #LINE# #TAB# #TAB# ctypes.windll.advapi32.glibc_version_string(ctypes.byref(new_value)) #LINE# #TAB# finally: #LINE# #TAB# #TAB# ctypes.windll.advapi32.glibc_string_ctypes(new_value) #LINE# #TAB# return new_value
"#LINE# #TAB# try: #LINE# #TAB# #TAB# from IPython import get_ipython #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# get_ipython() #LINE# #TAB# module = get_ipython(package) #LINE# #TAB# for key, value in script_information.items(): #LINE# #TAB# #TAB# if 'use' in key: #LINE# #TAB# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# #TAB# print('Adding %s' % key) #LINE# #TAB# #TAB# #TAB# yaml.dump(value, module, default_flow_style=False) #LINE# #TAB# #TAB# #TAB# module = copy.deepcopy(module) #LINE# #TAB# return module"
"#LINE# #TAB# task_perms = getattr(task, 'task_perms', None) #LINE# #TAB# if task_perms: #LINE# #TAB# #TAB# user.task_perms = task_perms"
#LINE# #TAB# allowed_graph = nx.Graph() #LINE# #TAB# for node_id in node_ids: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# allowed_graph.add_node(node_id) #LINE# #TAB# #TAB# except nx.NetworkXNoNode: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return allowed_graph
#LINE# #TAB# seen = set() #LINE# #TAB# for i in seq: #LINE# #TAB# #TAB# if i not in seen: #LINE# #TAB# #TAB# #TAB# seen.add(i) #LINE# #TAB# #TAB# #TAB# yield i #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield i
"#LINE# #TAB# ret = {} #LINE# #TAB# for method in klass.public_methods(): #LINE# #TAB# #TAB# if hasattr(method, name): #LINE# #TAB# #TAB# #TAB# ret = getattr(method, name) #LINE# #TAB# return ret"
"#LINE# #TAB# if len(tokens) > 1: #LINE# #TAB# #TAB# args, assignment_tokens = tokens #LINE# #TAB# else: #LINE# #TAB# #TAB# args = [] #LINE# #TAB# running = True #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.type == 'NAME': #LINE# #TAB# #TAB# #TAB# name, args = process_name(token) #LINE# #TAB# #TAB# #TAB# running = running and process_args(args) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# args.append(token) #LINE# #TAB# return args, assignment_tokens"
"#LINE# #TAB# if method in ('get', 'delete'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if method in ('delete', 'put'): #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(params, str): #LINE# #TAB# #TAB# params = json.loads(params) #LINE# #TAB# method = method.lower() #LINE# #TAB# if method in ('get', 'post'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if encoding is None: #LINE# #TAB# #TAB# encoding = get_encoding() #LINE# #TAB# finish_value = _set_value(data, encoding) #LINE# #TAB# return finish_value"
"#LINE# sdr = [] #LINE# inputBits = int(numActiveInputBits) #LINE# for _ in range(numDims): #LINE# #TAB# sdr.append(random.randint(0, 2 ** 32 - 1)) #LINE# #TAB# inputBits = int(numActiveInputBits) #LINE# #TAB# for _ in range(numDims): #LINE# #TAB# sdr.append(random.randint(0, 2 ** 32 - 1)) #LINE# return sdr"
#LINE# #TAB# db_handler = cls.get_redis_db_handler() #LINE# #TAB# if not db_handler: #LINE# #TAB# #TAB# logging.error('no redis db handler found') #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# return db_handler
#LINE# #TAB# with open(location) as csvfile: #LINE# #TAB# #TAB# reader = csv.DictReader(csvfile) #LINE# #TAB# #TAB# rows = [row for row in reader] #LINE# #TAB# #TAB# return rows
"#LINE# #TAB# val = generate_service_int_property_value(dev_ref, key) #LINE# #TAB# return val"
"#LINE# #TAB# cz_unitaries = [] #LINE# #TAB# for i in range(len(unitaries) - 2): #LINE# #TAB# #TAB# if unitaries[i + 1][0] == 'Z': #LINE# #TAB# #TAB# #TAB# cz_unitaries.append(unitaries[i]) #LINE# #TAB# #TAB# elif unitaries[i + 1][0] == 'O': #LINE# #TAB# #TAB# #TAB# cz_unitaries.append(unitaries[i + 1][1]) #LINE# #TAB# if cz_unitaries: #LINE# #TAB# #TAB# first_k = unitaries[0] #LINE# #TAB# #TAB# for j in range(1, len(cz_unitaries)): #LINE# #TAB# #TAB# #TAB# if cz_unitaries[j] == 'CZ': #LINE# #TAB# #TAB# #TAB# #TAB# return first_k + 1 #LINE# #TAB# return cz_unitaries"
#LINE# #TAB# client = ControllerClient() #LINE# #TAB# result = client.get_chute_info(name) #LINE# #TAB# if len(result) > 0: #LINE# #TAB# #TAB# click.echo('Is the chute {} exists? '.format(chute.id)) #LINE# #TAB# elif len(result) > 1: #LINE# #TAB# #TAB# click.echo('Is the chute with this name? ') #LINE# #TAB# elif len(result) == 0: #LINE# #TAB# #TAB# click.echo('No information.') #LINE# #TAB# else: #LINE# #TAB# #TAB# click.echo('\n{}'.format(result)) #LINE# #TAB# return result
#LINE# #TAB# if task in run_name: #LINE# #TAB# #TAB# return True #LINE# #TAB# for run in run_name: #LINE# #TAB# #TAB# if task in run[run]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# model_one_scaling_factors = model_one.build_scaling_factors(S12) #LINE# #TAB# model_two_scaling_factors = model_two.build_scaling_factors(S12) #LINE# #TAB# model_one_scaling_factors = model_one.build_scaling_factors(S12) #LINE# #TAB# model_two_scaling_factors = model_two.build_scaling_factors(S12) #LINE# #TAB# return model_one_scaling_factors, model_two_scaling_factors"
#LINE# #TAB# if old_value is None: #LINE# #TAB# #TAB# return new_value is None #LINE# #TAB# elif old_value == new_value: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif old_value == '': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if name1 is None or name2 is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# pattern = '^(' + re.escape(name1) + ')$') #LINE# #TAB# match = re.match(pattern, name2) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# validate(config) #LINE# #TAB# _validate_params(config) #LINE# #TAB# return config
"#LINE# #TAB# output_times_dict = {} #LINE# #TAB# for table in database.get_tables(table_name): #LINE# #TAB# #TAB# row = table.get_row(column) #LINE# #TAB# #TAB# if row is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# output_times_dict[row.name] = {key: value for key, value in row.items()} #LINE# #TAB# return output_times_dict"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for node in tree.iter(): #LINE# #TAB# #TAB# #TAB# node.tag = '{}{}'.format(URI, node.tag) #LINE# #TAB# #TAB# yield node #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# if not hasattr(cls, 'DESCRIPTION'): #LINE# #TAB# #TAB# return cls.DEFAULT_DESCRIPTION #LINE# #TAB# alias = cls.DESCRIPTION #LINE# #TAB# if len(alias) > 1: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# alias = alias[0].description #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return alias"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# regimes, _ = parse_regimes_from_file(bytecode_path) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return regimes[0]"
#LINE# #TAB# sdata = {} #LINE# #TAB# sdata['level'] = 'logged' #LINE# #TAB# sdata['HID'] = 'Household' #LINE# #TAB# sdata['AREA'] = 'Area' #LINE# #TAB# sdata['HID'] = 'Household' #LINE# #TAB# sdata['HID'] = 'ID' #LINE# #TAB# sdata['HID'] = 'ID' #LINE# #TAB# sdata['Household'] = 'ID' #LINE# #TAB# return sdata
#LINE# #TAB# if boolstr_re.match(i): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# do = json.loads(config['do']) #LINE# #TAB# if not isinstance(do, dict): #LINE# #TAB# #TAB# raise ValueError('Objective function must be specified as a json file') #LINE# #TAB# for key in do.keys(): #LINE# #TAB# #TAB# if key.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(do[key], str): #LINE# #TAB# #TAB# #TAB# do[key] = json.loads(do[key]) #LINE# #TAB# return do"
"#LINE# #TAB# if isinstance(maybe_dttm, datetime.datetime): #LINE# #TAB# #TAB# maybe_dttm = _format_datetime(maybe_dttm) #LINE# #TAB# return maybe_dttm"
#LINE# #TAB# new = [] #LINE# #TAB# for el in lst: #LINE# #TAB# #TAB# new.append(el[index]) #LINE# #TAB# return new
#LINE# #TAB# user = User.objects.get(user_access_token=user_access_token) #LINE# #TAB# return user.user_email
#LINE# #TAB# if obj.type =='multivariate': #LINE# #TAB# #TAB# return {'action': 'create sections'} #LINE# #TAB# return {}
"#LINE# #TAB# col = dataframe[colname] #LINE# #TAB# col_numerics = col.loc[col.apply(lambda x: isinstance(x, (int, float)))] #LINE# #TAB# dataframe.loc[col.notnull() & col.apply(lambda x: not isinstance(x, bool)), colname #LINE# #TAB# #TAB# ] = col_numerics.mode().flip() #LINE# #TAB# return dataframe"
#LINE# #TAB# b = rnd.random() #LINE# #TAB# while len(b) < length: #LINE# #TAB# #TAB# b += b #LINE# #TAB# return b
#LINE# #TAB# if method == 'between': #LINE# #TAB# #TAB# windows = np.round(time / len(time)).astype(int) #LINE# #TAB# elif method == 'date': #LINE# #TAB# #TAB# windows = np.round(time / len(time)).astype(int) #LINE# #TAB# elif method == 'average': #LINE# #TAB# #TAB# windows = np.round(time / len(time)).astype(int) #LINE# #TAB# elif method == 'count': #LINE# #TAB# #TAB# windows = np.round(time / len(time)).astype(int) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('invalid method argument') #LINE# #TAB# return windows
"#LINE# #TAB# url = urlparse.urlparse(url) #LINE# #TAB# authority = url.netloc #LINE# #TAB# path = url.path #LINE# #TAB# if not path.endswith('/api/v1/login'): #LINE# #TAB# #TAB# path += '/api/v1/login' #LINE# #TAB# token = urllib.parse.urlencode({'u': username, 'u': username}) #LINE# #TAB# try: #LINE# #TAB# #TAB# r = requests.get(path, headers=HEADERS) #LINE# #TAB# #TAB# r.raise_for_status() #LINE# #TAB# #TAB# return r.json()['key'] #LINE# #TAB# except requests.exceptions.HTTPError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None"
"#LINE# #TAB# ip_address = None #LINE# #TAB# try: #LINE# #TAB# #TAB# ip_address = subprocess.check_output(['ip','show']) #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ip_address"
#LINE# #TAB# new_tail = sbody.copy() #LINE# #TAB# new_tail.power_references_tail(num_rows) #LINE# #TAB# return new_tail
"#LINE# #TAB# result = {} #LINE# #TAB# for key, value in vertex_json.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for inner_key, inner_value in escape_small(value): #LINE# #TAB# #TAB# #TAB# #TAB# result[inner_key] = inner_value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[key] = escape_small(value) #LINE# #TAB# return result"
"#LINE# #TAB# for row in range(w): #LINE# #TAB# #TAB# for col in range(h): #LINE# #TAB# #TAB# #TAB# w_col = w[row - room_size * col - 1] #LINE# #TAB# #TAB# #TAB# h_col = h[row - room_size * col - 1] #LINE# #TAB# #TAB# #TAB# for x in range(w_col - room_size * row - 1): #LINE# #TAB# #TAB# #TAB# #TAB# for y in range(h_col - room_size * row + 1): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield x, y #LINE# #TAB# #TAB# #TAB# #TAB# if delete_chance > 0.33 and abs(w_col[x] - w_col[y]) < delete_chance: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield x, y"
"#LINE# #TAB# group_counts = np.asarray(group_sizes) #LINE# #TAB# shrinkage = np.zeros(group_counts[0].shape, dtype=np.int) #LINE# #TAB# for g in group_counts: #LINE# #TAB# #TAB# shrinkage[g.shape[0]:g.shape[1]] = 1 #LINE# #TAB# res = np.zeros(group_counts[0].shape, dtype=np.int) #LINE# #TAB# for g in group_sizes: #LINE# #TAB# #TAB# for i in range(g.shape[1]): #LINE# #TAB# #TAB# #TAB# if np.sum(g[i]) < min_n_obs: #LINE# #TAB# #TAB# #TAB# #TAB# res[i] = g[i] #LINE# #TAB# return res"
"#LINE# #TAB# threads = [] #LINE# #TAB# root_dir = os.path.normpath(root_dir) #LINE# #TAB# while True: #LINE# #TAB# #TAB# new_dir = os.path.join(root_dir, '') #LINE# #TAB# #TAB# if os.path.isdir(new_dir): #LINE# #TAB# #TAB# #TAB# if empty_only: #LINE# #TAB# #TAB# #TAB# #TAB# dirs = [] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dirs.append(new_dir) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if not os.path.isdir(new_dir): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# threads.append(new_dir) #LINE# #TAB# return threads"
#LINE# #TAB# model.stop_env() #LINE# #TAB# return model
#LINE# #TAB# nb_x = ax.get_nrows() #LINE# #TAB# nb_y = ax.get_ncols() #LINE# #TAB# if nb_x!= nb_y: #LINE# #TAB# #TAB# return False #LINE# #TAB# if nb_x!= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# if nb_y!= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# import tempfile #LINE# #TAB# if filename.endswith("".gz""): #LINE# #TAB# #TAB# filename = filename[:-3] #LINE# #TAB# with tempfile.NamedTemporaryFile(delete=False) as t: #LINE# #TAB# #TAB# shutil.move(filename, t.name) #LINE# #TAB# return filename"
"#LINE# #TAB# unique_dict = {} #LINE# #TAB# for key, val in ordered_pairs: #LINE# #TAB# #TAB# if key in unique_dict: #LINE# #TAB# #TAB# #TAB# raise ValueError('Duplicate key: %r' % (key,)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unique_dict[key] = val #LINE# #TAB# return unique_dict"
"#LINE# #TAB# from tkinter import mainwindow #LINE# #TAB# root = mainwindow.MainWindow() #LINE# #TAB# root.attributes('-topmost', True) #LINE# #TAB# root.attributes('-bottommost', True) #LINE# #TAB# root.update() #LINE# #TAB# size = 75 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# root.mainwindow(size) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# #TAB# root.destroy() #LINE# #TAB# #TAB# #TAB# if e.errno!= errno.ENOENT: #LINE# #TAB# #TAB# #TAB# #TAB# break"
#LINE# #TAB# ftrue = np.asarray(ftrue) #LINE# #TAB# alpha = np.asarray(alpha) #LINE# #TAB# beta = np.asarray(beta) #LINE# #TAB# while ftrue.size > 0: #LINE# #TAB# #TAB# rnd = np.random.random() #LINE# #TAB# #TAB# fval = ftrue[rnd] + alpha * beta * ftrue[rnd] #LINE# #TAB# #TAB# yield fval #LINE# #TAB# #TAB# ftrue = ftrue[rnd]
#LINE# #TAB# username = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# username = os.environ['SUDO_USER'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return username
#LINE# #TAB# sacct_info = {} #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = sacct_stream.readline().decode('utf-8') #LINE# #TAB# #TAB# if line == '': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# record = SacctRecord.from_line(line) #LINE# #TAB# #TAB# if record is None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# sacct_info[record.id] = record #LINE# #TAB# return sacct_info
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# f = faker(formatter=formatter) #LINE# #TAB# #TAB# return f(value) #LINE# #TAB# else: #LINE# #TAB# #TAB# return value
"#LINE# #TAB# for pos, refl, iBeg, iFin in profList: #LINE# #TAB# #TAB# yc[iBeg:iFin] += refl[11 + im] * refl[9 + im] * refl[10 + im] #LINE# #TAB# return yc"
#LINE# #TAB# map = {} #LINE# #TAB# if os.path.exists('pypi/map.txt'): #LINE# #TAB# #TAB# with open('pypi/map.txt') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.rstrip('\n') #LINE# #TAB# #TAB# #TAB# #TAB# if line!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# key = line.split('==')[0].strip() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# map[key] = key #LINE# #TAB# return map
#LINE# #TAB# if doc: #LINE# #TAB# #TAB# for tag in doc: #LINE# #TAB# #TAB# #TAB# yield tag
"#LINE# #TAB# user = getattr(request, 'user', None) #LINE# #TAB# if not user: #LINE# #TAB# #TAB# raise Http404('User not found.') #LINE# #TAB# if template_name.endswith('.htm'): #LINE# #TAB# #TAB# template_name = template_name + '.htm' #LINE# #TAB# template = loader.get_template(user, template_name) #LINE# #TAB# if template is None: #LINE# #TAB# #TAB# template = loader.get_template(user, None) #LINE# #TAB# if template is None: #LINE# #TAB# #TAB# request.response.status = 404 #LINE# #TAB# #TAB# template = loader.get_template(user, template_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# request.response.status = template_name #LINE# #TAB# return template"
"#LINE# #TAB# if isinstance(s, bool): #LINE# #TAB# #TAB# return s #LINE# #TAB# if not isinstance(s, six.text_type): #LINE# #TAB# #TAB# s = s.decode('utf-8') #LINE# #TAB# return s"
#LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# set_options(header_element_background_color=color) #LINE# #TAB# return DEFAULT_HEADER_ELEMENT_BACKGROUND_COLOR
"#LINE# #TAB# return [(re.compile('(?P<mod_date>\\d+)(\\.\\d+)', re.IGNORECASE), ( #LINE# #TAB# #TAB# re.compile('(?P<commit_sha>\\d+)(\\.\\d+)', re.IGNORECASE), (re.compile( #LINE# #TAB# #TAB# '(?P<commit_date>\\d+)(\\.\\d+)(\\.\\d+)', re.IGNORECASE), #LINE# #TAB# #TAB# re.IGNORECASE), ('(?P<commit_sha>\\d+)(\\.\\d+)', re.IGNORECASE), ( #LINE# #TAB# #TAB# re.compile('(?P<commit_sha>\\d+)(\\.\\d+)', re.IGNORECASE), (re.compile( #LINE# #TAB# #TAB# '(?P<commit_date>\\d+)(\\.\\d+)', re.IGNORECASE), re.DOTALL))]"
"#LINE# #TAB# if not protcol: #LINE# #TAB# #TAB# protcol = mzidtsvdata.HEADER_MASTER_PROT #LINE# #TAB# top_psms_acc = generate_top_psms(psms, protcol) #LINE# #TAB# for protein in proteins: #LINE# #TAB# #TAB# prot_acc = protein[prottabledata.HEADER_PROTEIN] #LINE# #TAB# #TAB# prec_area = calculate_smart_quant(top_psms_acc, prot_acc) #LINE# #TAB# #TAB# outprotein = {k: v for k, v in protein.items()} #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][prottabledata.HEADER_AREA #LINE# #TAB# #TAB# #TAB# ][None]] = str(prec_area) #LINE# #TAB# #TAB# yield outprotein"
"#LINE# #TAB# pattern = re.compile('\\S') #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = pattern.sub('', text) #LINE# #TAB# #TAB# if line in text: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if dedupe: #LINE# #TAB# #TAB# #TAB# yield line #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield line"
#LINE# #TAB# cur = None #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if cur is not None: #LINE# #TAB# #TAB# #TAB# yield cur #LINE# #TAB# #TAB# #TAB# cur = item #LINE# #TAB# yield cur
"#LINE# #TAB# w = find_window_for_buffer_name(event.cli, event.cli.current_buffer_name) #LINE# #TAB# b = event.cli.current_buffer #LINE# #TAB# if w: #LINE# #TAB# #TAB# if w.render_info: #LINE# #TAB# #TAB# #TAB# info = w.render_info #LINE# #TAB# #TAB# #TAB# if w.vertical_scroll < info.content_height - info.window_height: #LINE# #TAB# #TAB# #TAB# #TAB# if info.cursor_position.y <= info.configured_scroll_offsets.top: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# b.cursor_position += b.document.get_cursor_down_position() #LINE# #TAB# #TAB# #TAB# #TAB# w.vertical_scroll += 1"
#LINE# #TAB# fullname = url.split(':')[-1] #LINE# #TAB# widget_fullname = fullname.split('/')[-2] #LINE# #TAB# global _widget_fullname #LINE# #TAB# _widget_fullname = widget_fullname
"#LINE# #TAB# if a_dtype == b_dtype: #LINE# #TAB# #TAB# return True, np.float32 #LINE# #TAB# else: #LINE# #TAB# #TAB# calc_dtype = _has_scalar_dtype(a_dtype) #LINE# #TAB# #TAB# if calc_dtype: #LINE# #TAB# #TAB# #TAB# return True, calc_dtype #LINE# #TAB# #TAB# res_dtype = _has_scalar_dtype(b_dtype) #LINE# #TAB# #TAB# if res_dtype: #LINE# #TAB# #TAB# #TAB# return True, res_dtype #LINE# #TAB# return False, np.float32"
"#LINE# #TAB# result = content['html'] #LINE# #TAB# result = result.replace('&gt;', '>') #LINE# #TAB# result = result.replace('&lt;', '<') #LINE# #TAB# result = result.replace('&quot;', '""') #LINE# #TAB# return result"
#LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if 'label' in cell: #LINE# #TAB# #TAB# #TAB# new_cell = copy.deepcopy(cell) #LINE# #TAB# #TAB# #TAB# new_cell['source'] = nb.cells[cell['source']]['source'] #LINE# #TAB# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# new_cell['source'] = new_cell['source'] #LINE# #TAB# #TAB# #TAB# yield new_cell
"#LINE# #TAB# col = [c for c in df.columns if c.startswith('CHR')] #LINE# #TAB# df = df[col] #LINE# #TAB# n = len(df) #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return df #LINE# #TAB# df.loc[:, (col)] = df.loc[:, (col)].apply(lambda x: cred_DIPPR(x, n)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return df"
#LINE# #TAB# define_media = request.registry.get(CONFIG_FILE_NAME) #LINE# #TAB# if define_media: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# define_media = request.registry[CONFIG_FILE_NAME] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if define_media: #LINE# #TAB# #TAB# return define_media #LINE# #TAB# try: #LINE# #TAB# #TAB# request.registry[CONFIG_FILE_NAME] = request.registry[CONFIG_FILE_NAME] #LINE# #TAB# #TAB# return request.registry[CONFIG_FILE_NAME] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
"#LINE# #TAB# enable_regions = tz.get_in([""config"", ""algorithm"", ""regions""], batch, lambda x: True) #LINE# #TAB# if enable_regions is not None: #LINE# #TAB# #TAB# return [x[""regions""] for x in enable_regions] #LINE# #TAB# else: #LINE# #TAB# #TAB# return batch"
#LINE# #TAB# gateway = {} #LINE# #TAB# for i in cls.indices(): #LINE# #TAB# #TAB# gateway[i] = [i] #LINE# #TAB# return gateway
#LINE# #TAB# global _POOL #LINE# #TAB# _POOL = seed
"#LINE# #TAB# x, y = P #LINE# #TAB# return x, y"
#LINE# #TAB# new_state = copy.deepcopy(state) #LINE# #TAB# new_state.items_without_pickup = pickup #LINE# #TAB# return new_state
#LINE# #TAB# conn = boto.connect_s3() #LINE# #TAB# c = conn.get_client('s3') #LINE# #TAB# k = c.get_bucket_key(bucket) #LINE# #TAB# try: #LINE# #TAB# #TAB# c.comment(comment) #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# print(e.response['Error']['Message']) #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# marker = VnlMarker(x, y, marker, color=color) #LINE# #TAB# return x, y"
"#LINE# #TAB# model = zeros((len(pred), len(root))) #LINE# #TAB# path = root #LINE# #TAB# while True: #LINE# #TAB# #TAB# i = path.next() #LINE# #TAB# #TAB# if i == target: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif pred[i] == target: #LINE# #TAB# #TAB# #TAB# model.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pred[i] = i #LINE# #TAB# #TAB# #TAB# node = path.next() #LINE# #TAB# #TAB# #TAB# if node == root: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return model"
"#LINE# #TAB# sensitivity = np.asarray(sensitivity, dtype=float) #LINE# #TAB# if sensitivity.max() > 1e-06: #LINE# #TAB# #TAB# return 1.0 #LINE# #TAB# elif sensitivity.min() < 0: #LINE# #TAB# #TAB# return sensitivity / 10 #LINE# #TAB# return 1.0"
#LINE# #TAB# terms_to_delete = [] #LINE# #TAB# for term in facets_terms: #LINE# #TAB# #TAB# if term['facet_type'] == 'FT': #LINE# #TAB# #TAB# #TAB# terms_to_delete.append(term) #LINE# #TAB# return terms_to_delete
"#LINE# #TAB# if configuration.get('uri'): #LINE# #TAB# #TAB# uri = configuration['uri'] #LINE# #TAB# elif 'uri' in configuration: #LINE# #TAB# #TAB# uri = os.path.join(application.metadata['doc_dir'], 'uri') #LINE# #TAB# #TAB# uri = os.path.join(application.metadata['doc_dir'], 'uri') #LINE# #TAB# _append_to_element(configuration, 'uri', uri) #LINE# #TAB# return configuration"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return _find_library.current #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return _find_library.current #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
"#LINE# #TAB# if hasattr(module, 'get_views_iso8601'): #LINE# #TAB# #TAB# return module.get_views_iso8601() #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
#LINE# #TAB# key = hashlib.sha256(secret).digest() #LINE# #TAB# key = base64.urlsafe_b64encode(key) #LINE# #TAB# return key
"#LINE# #TAB# offset = request.GET.get('offset', None) #LINE# #TAB# if offset: #LINE# #TAB# #TAB# quotes = get_quotes(offset) #LINE# #TAB# #TAB# if len(quotes) == 1: #LINE# #TAB# #TAB# #TAB# context = {} #LINE# #TAB# #TAB# #TAB# context['public'] = True #LINE# #TAB# #TAB# #TAB# context['quote'] = quotes[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# context['quote'] = None #LINE# #TAB# else: #LINE# #TAB# #TAB# context['quote'] = None #LINE# #TAB# return context"
"#LINE# #TAB# if not G.is_connected(): #LINE# #TAB# #TAB# return #LINE# #TAB# nodes = list(G.nodes()) #LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if not G.has_node(node): #LINE# #TAB# #TAB# #TAB# G.add_node(node) #LINE# #TAB# #TAB# #TAB# G.remove_node(node) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if any(G.degree(node) > 1 for node in nodes): #LINE# #TAB# #TAB# #TAB# G.add_edge(node, node) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# G.add_edge(node, node) #LINE# #TAB# return"
"#LINE# #TAB# if callable(line_or_func): #LINE# #TAB# #TAB# return find_code_index(line_or_func, lines) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# string_data = {} #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# string_data[item] = str(item) #LINE# #TAB# return string_data
#LINE# #TAB# timeout = os.getenv('TIMEOUT') #LINE# #TAB# if timeout is not None: #LINE# #TAB# #TAB# return hashlib.md5sum(src_file).hexdigest() #LINE# #TAB# return None
#LINE# #TAB# global _data_func #LINE# #TAB# _data_func = key_func
"#LINE# #TAB# with open(file_name, 'w') as f: #LINE# #TAB# #TAB# csv_doc = json.load(f) #LINE# #TAB# #TAB# bdd.extend(csv_doc) #LINE# #TAB# if load_order: #LINE# #TAB# #TAB# return bdd #LINE# #TAB# else: #LINE# #TAB# #TAB# return bdd"
"#LINE# #TAB# filename = path.split('.')[0] #LINE# #TAB# lang1 = filename.split('-')[0] #LINE# #TAB# lang2 = filename.split('-')[1] #LINE# #TAB# return lang1, lang2"
#LINE# #TAB# all_ids = cls.list() #LINE# #TAB# matching = [] #LINE# #TAB# for id in ids: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# matching.append(cls.id_tree.find_one({'_id': id})) #LINE# #TAB# #TAB# except OperationalError as e: #LINE# #TAB# #TAB# #TAB# if e.args[0] == 'InvalidId': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# matching.append(id) #LINE# #TAB# return matching
"#LINE# #TAB# DataFrame) ->pd.DataFrame: #LINE# #TAB# manager = pd.DataFrame(index=interactions.index, columns=interactions.columns) #LINE# #TAB# if not complexes: #LINE# #TAB# #TAB# return manager #LINE# #TAB# manager.index = interactions.index #LINE# #TAB# for i in range(interactions.shape[0]): #LINE# #TAB# #TAB# if interactions[i].iloc[0] not in complexes: #LINE# #TAB# #TAB# #TAB# manager.drop(interactions[i].index, axis=1, inplace=True) #LINE# #TAB# return manager"
"#LINE# #TAB# index_size = 0 #LINE# #TAB# total_shape = 0 #LINE# #TAB# index_shape = [(n // 2 if n % 2 == 0 else n // 4) for n in range(v. #LINE# #TAB# #TAB# get_value_size())] #LINE# #TAB# total_shape = sum(total_shape) #LINE# #TAB# shape = [(v.size(i) if n % 2 else v.size(i)) for i in range(v. #LINE# #TAB# #TAB# get_value_size())] #LINE# #TAB# return shape, total_shape[index_size:], index_shape[index_size:], total_shape[ #LINE# #TAB# #TAB# index_size + 1:]"
"#LINE# #TAB# with mp.Pool() as pool: #LINE# #TAB# #TAB# _inputs = list(inputs) #LINE# #TAB# #TAB# for adj_lst in _inputs: #LINE# #TAB# #TAB# #TAB# if isinstance(adj_lst, dict): #LINE# #TAB# #TAB# #TAB# #TAB# for adj_key, adj_val in adj_lst.items(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if adj_key in adj_lst: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# inputs[adj_key] += adj_val #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# inputs[adj_key] += adj_val #LINE# #TAB# return inputs"
"#LINE# #TAB# return [os.path.basename(f) for f in os.listdir(os.getcwd()) if os.path. #LINE# #TAB# #TAB# isfile(os.path.join(os.getcwd(), f)) and not f.startswith('.')]"
"#LINE# #TAB# dialog = NexusDialog(nexus_client, name) #LINE# #TAB# script_delete = nexus_client.scripts.delete(name) #LINE# #TAB# dialog.exec_() #LINE# #TAB# return dialog"
#LINE# #TAB# #TAB# logging_logger = logging.getLogger(__name__) #LINE# #TAB# #TAB# logging_logger.setLevel(logging.DEBUG) #LINE# #TAB# #TAB# if opts.quiet: #LINE# #TAB# #TAB# #TAB# logging_logger.setLevel(logging.WARNING) #LINE# #TAB# #TAB# logging_logger.propagate = True #LINE# #TAB# #TAB# return logging_logger
"#LINE# #TAB# if options is None: #LINE# #TAB# #TAB# options = {} #LINE# #TAB# compatibility = options.get(""compatibility"", None) #LINE# #TAB# if compatibility is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# compatibility = os.environ[""COMPOSE_COMPATIBILITY""] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if compatibility and working_dir.is_git(): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# ""compatibility"": compatibility, #LINE# #TAB# #TAB# #TAB# ""working_dir"": working_dir, #LINE# #TAB# #TAB# } #LINE# #TAB# else: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# ""compatibility"": compatibility, #LINE# #TAB# #TAB# }"
#LINE# #TAB# shape = list(data.shape) #LINE# #TAB# if size < 2: #LINE# #TAB# #TAB# return shape #LINE# #TAB# unit = shape[-1] #LINE# #TAB# start = 0 #LINE# #TAB# stop = size #LINE# #TAB# while stop < shape[1]: #LINE# #TAB# #TAB# end = start + unit #LINE# #TAB# #TAB# yield data[start:end] #LINE# #TAB# #TAB# start = end #LINE# #TAB# #TAB# stop = start + unit #LINE# #TAB# yield data[start:stop]
"#LINE# #TAB# if isinstance(connection, sqlite3.Connection): #LINE# #TAB# #TAB# return True #LINE# #TAB# keys = [key for key in connection.keys() if isinstance(key, str)] #LINE# #TAB# if not all(keys): #LINE# #TAB# #TAB# return False #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if isinstance(connection[key], str): #LINE# #TAB# #TAB# #TAB# connection.close() #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# site_defaults = {} #LINE# #TAB# for key, value in os.environ.items(): #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# site_defaults[key] = value #LINE# #TAB# return site_defaults"
"#LINE# #TAB# if content_path is None: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# lib_dir = os.path.dirname(content_path) #LINE# #TAB# filename = os.path.join(lib_dir, 'translate') #LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# data = f.read() #LINE# #TAB# dst = ffi.new('unsigned char[]', len(data)) #LINE# #TAB# lib.SDL_Translate(data, dst, content) #LINE# #TAB# return dst"
"#LINE# #TAB# with open(pathname, 'rb') as fd: #LINE# #TAB# #TAB# id_ = ord(fd.read(1)) #LINE# #TAB# #TAB# size = len(fd) #LINE# #TAB# #TAB# fd.seek(id_) #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = fd.read(size) #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# pathname.write(data + b'\x00' * size) #LINE# #TAB# #TAB# #TAB# fd.close() #LINE# #TAB# return pathname"
"#LINE# #TAB# response.headers['Content-Type'] = 'application/json' #LINE# #TAB# response.headers['Cache-Control'] ='max-age=0' #LINE# #TAB# response.headers['Expires'] = '0' #LINE# #TAB# response.headers['Cache-Control'] = 'public, max-age=0' #LINE# #TAB# return response"
"#LINE# #TAB# dt_vnl = dt_vnl(f, key, q) #LINE# #TAB# dt_vnl.name = key #LINE# #TAB# return dt_vnl"
#LINE# #TAB# if 'timestamp' in params: #LINE# #TAB# #TAB# timestamp = params['timestamp'] #LINE# #TAB# #TAB# del params['timestamp'] #LINE# #TAB# params['timestamp'] = timestamp #LINE# #TAB# return url + '?' + params
"#LINE# #TAB# minx = np.min(polygon) #LINE# #TAB# miny = np.max(polygon) #LINE# #TAB# area = 0 #LINE# #TAB# for i in range(miny): #LINE# #TAB# #TAB# x = polygon[i] #LINE# #TAB# #TAB# y = polygon[i + 1] #LINE# #TAB# #TAB# uv = np.cross(x, y) #LINE# #TAB# #TAB# xmax = np.max(x) #LINE# #TAB# #TAB# ymin = np.min(y) #LINE# #TAB# #TAB# ymax = np.max(y) #LINE# #TAB# #TAB# area += uv * ((ymax - xmin) * (uv - xmin)) #LINE# #TAB# return area"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# f = open(file, 'r') #LINE# #TAB# #TAB# data = f.read() #LINE# #TAB# #TAB# f.close() #LINE# #TAB# #TAB# data = yaml.load(data) #LINE# #TAB# #TAB# f.close() #LINE# #TAB# except: #LINE# #TAB# #TAB# data = {} #LINE# #TAB# return data"
#LINE# #TAB# node = section #LINE# #TAB# while node.parent is not None: #LINE# #TAB# #TAB# node = node.parent #LINE# #TAB# return node.alias
"#LINE# #TAB# html = ET.tostring(elem, encoding='utf-8') #LINE# #TAB# if html.tag == 'html': #LINE# #TAB# #TAB# html = html.replace('&quot;', '""').replace('&apos;', ""'"") #LINE# #TAB# #TAB# return html #LINE# #TAB# if pretty_print: #LINE# #TAB# #TAB# html = html.replace('&amp;', '&') #LINE# #TAB# #TAB# return html #LINE# #TAB# return html"
"#LINE# #TAB# data = {'key': objid} #LINE# #TAB# created = datetime.utcnow() #LINE# #TAB# table = sqlalchemy.Table(table_name).select_related('table').where(table.c. #LINE# #TAB# #TAB# created == datetime.utcnow()).get() #LINE# #TAB# if table.c.id == objid: #LINE# #TAB# #TAB# data['key'] = table.c.id #LINE# #TAB# #TAB# created = datetime.utcnow() #LINE# #TAB# return created, data"
"#LINE# #TAB# centroid = np.mean(points, axis=0) #LINE# #TAB# for p in points: #LINE# #TAB# #TAB# x = p[0] #LINE# #TAB# #TAB# y = p[1] #LINE# #TAB# #TAB# mean = np.mean(p, axis=0) #LINE# #TAB# #TAB# std = np.std(p, axis=0) #LINE# #TAB# #TAB# centroid[0] = mean[0] #LINE# #TAB# #TAB# centroid[1] = mean[1] #LINE# #TAB# return centroid"
#LINE# #TAB# if friendly: #LINE# #TAB# #TAB# return 'Alignak live state synthesis' #LINE# #TAB# return 'livesynthesis'
"#LINE# #TAB# lines = string.count('\n', line_offset) #LINE# #TAB# if lines > 0: #LINE# #TAB# #TAB# column = index - string.rfind('\n', line_offset, 0, index) #LINE# #TAB# else: #LINE# #TAB# #TAB# column = 0 #LINE# #TAB# return lines, column"
#LINE# #TAB# deleted_messages = [] #LINE# #TAB# for msg in messages: #LINE# #TAB# #TAB# if len(deleted_messages) > 1: #LINE# #TAB# #TAB# #TAB# yield msg #LINE# #TAB# #TAB# #TAB# deleted_messages.append(msg) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield None #LINE# #TAB# #TAB# #TAB# deleted_messages.append(msg) #LINE# #TAB# for msg in deleted_messages: #LINE# #TAB# #TAB# yield msg
#LINE# #TAB# groups = skil.api.set_groups_by_type(resource_type) #LINE# #TAB# return groups
#LINE# #TAB# impact_function.com_preprocessor = None #LINE# #TAB# return impact_function
"#LINE# #TAB# if type(obj) == list: #LINE# #TAB# #TAB# string = ', '.join(format_to_str(o) for o in obj) #LINE# #TAB# else: #LINE# #TAB# #TAB# string = obj #LINE# #TAB# return string"
#LINE# #TAB# if enabled is True: #LINE# #TAB# #TAB# plugin_enabled = True #LINE# #TAB# else: #LINE# #TAB# #TAB# plugin_enabled = False #LINE# #TAB# return plugin_enabled
"#LINE# #TAB# word = word.lower() #LINE# #TAB# lis = [] #LINE# #TAB# if len(word) > 3 and word[0].isalpha(): #LINE# #TAB# #TAB# lis.append('alpha') #LINE# #TAB# if len(word) > 2 and word[0].isupper(): #LINE# #TAB# #TAB# lis.append('num') #LINE# #TAB# #TAB# if len(word) > 3 and word[0].isupper(): #LINE# #TAB# #TAB# #TAB# lis.append('non-alpha') #LINE# #TAB# #TAB# if len(word) > 2 and word[0].isupper(): #LINE# #TAB# #TAB# #TAB# lis.append('number') #LINE# #TAB# if len(word) > 0 and word[0].isupper(): #LINE# #TAB# #TAB# lis.append('other') #LINE# #TAB# return ','.join(lis) if lis else ''"
"#LINE# #TAB# if not PY3: #LINE# #TAB# #TAB# return obj #LINE# #TAB# if isinstance(obj, tuple): #LINE# #TAB# #TAB# return tuple(native_parens(i) for i in obj) #LINE# #TAB# return obj"
#LINE# #TAB# label = [] #LINE# #TAB# for key in config: #LINE# #TAB# #TAB# if key not in label: #LINE# #TAB# #TAB# #TAB# label.append(key) #LINE# #TAB# label.sort() #LINE# #TAB# return label
#LINE# #TAB# email_family = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# for char in table: #LINE# #TAB# #TAB# #TAB# if char.isupper(): #LINE# #TAB# #TAB# #TAB# #TAB# email_family = char #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return email_family
#LINE# #TAB# cp = '' #LINE# #TAB# while n: #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# #TAB# cp += '+' #LINE# #TAB# return cp
"#LINE# #TAB# body = json.dumps(obj, cls=DjangoJSONEncoder) #LINE# #TAB# return body"
"#LINE# #TAB# for k, v in six.iteritems(nested_dict): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# for i in matching_validator(v): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return k, v #LINE# #TAB# return None"
#LINE# #TAB# lines = description.split('\n') #LINE# #TAB# return [normalize_world(line) for line in lines]
#LINE# #TAB# global _instances #LINE# #TAB# if _instances is None: #LINE# #TAB# #TAB# _instances = Client() #LINE# #TAB# return _instances
"#LINE# #TAB# code = _cal_code[i] #LINE# #TAB# if isinstance(code, int): #LINE# #TAB# #TAB# return code * 256 + i #LINE# #TAB# else: #LINE# #TAB# #TAB# return code"
"#LINE# #TAB# profile_dir = cls.task_dir_path.format(task_id) #LINE# #TAB# if create: #LINE# #TAB# #TAB# os.makedirs(profile_dir, exist_ok=True) #LINE# #TAB# else: #LINE# #TAB# #TAB# os.makedirs(profile_dir) #LINE# #TAB# return profile_dir"
"#LINE# #TAB# attr_result = conn.get_dn_attribute(filtr, dn, attr) #LINE# #TAB# assert attr_result is not None #LINE# #TAB# if attr_result is not None: #LINE# #TAB# #TAB# return attr_result[0] #LINE# #TAB# return None"
#LINE# #TAB# assert tap_stream_id in state.tap_streams #LINE# #TAB# if tap_stream_id in state.tap_streams[state.tap_stream_id][ #LINE# #TAB# #TAB#'versions']: #LINE# #TAB# #TAB# return state.tap_streams[tap_stream_id][ #LINE# #TAB# #TAB# #TAB#'versions'][state.tap_stream_id][state.tap_stream_id][ #LINE# #TAB# #TAB# #TAB#'versions'][state.tap_stream_id][state.tap_stream_id][ #LINE# #TAB# #TAB# #TAB#'versions']: #LINE# #TAB# #TAB# return state.tap_streams[tap_stream_id][state.tap_stream_id][ #LINE# #TAB# #TAB# #TAB#'versions'][state.tap_stream_id][state.tap_stream_id][ #LINE# #TAB# #TAB# #TAB#'versions'][state.tap_stream_id][state.tap_stream_id]
"#LINE# #TAB# root = raw.getroot() #LINE# #TAB# parts = root.tag.split('}') #LINE# #TAB# if len(parts) == 1: #LINE# #TAB# #TAB# return clean_folder(parts[0]), {} #LINE# #TAB# elif len(parts) == 2: #LINE# #TAB# #TAB# name = parts[0] #LINE# #TAB# #TAB# attr = parts[1] #LINE# #TAB# #TAB# if not name.endswith('/'): #LINE# #TAB# #TAB# #TAB# name += '/' #LINE# #TAB# #TAB# return full_name, attr #LINE# #TAB# else: #LINE# #TAB# #TAB# return '/'.join(parts), {}"
#LINE# #TAB# try: #LINE# #TAB# #TAB# json.loads(val) #LINE# #TAB# #TAB# return True #LINE# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if edges is None: #LINE# #TAB# #TAB# edges = graph.edges() #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# graph.edge(edge[0], edge[1]) #LINE# #TAB# #TAB# graph.edge(edge[0], edge[1]) #LINE# #TAB# #TAB# graph.edge(edge[1], edge[2]) #LINE# #TAB# return graph"
#LINE# #TAB# new_file = filename[:-4] + '.db' #LINE# #TAB# if os.path.exists(new_file): #LINE# #TAB# #TAB# os.unlink(new_file) #LINE# #TAB# return new_file
#LINE# #TAB# if dir == 0: #LINE# #TAB# #TAB# return '#FF9900' #LINE# #TAB# elif dir == 1: #LINE# #TAB# #TAB# return '#FF9900' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '#FF9900'
"#LINE# #TAB# server ='ssh://' + server #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# keyfile ='-i'+ keyfile #LINE# #TAB# cmd ='ssh -f '+ server #LINE# #TAB# proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# stdout, stderr = proc.communicate() #LINE# #TAB# if not stdout: #LINE# #TAB# #TAB# raise RuntimeError('ssh failed') #LINE# #TAB# return stdout"
#LINE# #TAB# for i in numbers: #LINE# #TAB# #TAB# if i > 5: #LINE# #TAB# #TAB# #TAB# return 5.0 #LINE# #TAB# return 0.5
"#LINE# #TAB# for feature in FEATURES: #LINE# #TAB# #TAB# yield feature[0], feature[1]"
#LINE# #TAB# if integer_string.isdigit(): #LINE# #TAB# #TAB# value = int(integer_string) #LINE# #TAB# else: #LINE# #TAB# #TAB# return integer_string #LINE# #TAB# if strict: #LINE# #TAB# #TAB# if value < 0: #LINE# #TAB# #TAB# #TAB# raise ValueError() #LINE# #TAB# #TAB# if cutoff is not None and value >= cutoff: #LINE# #TAB# #TAB# #TAB# raise ValueError() #LINE# #TAB# return value
"#LINE# #TAB# nPolys = len(p.monomers) #LINE# #TAB# Turns = np.zeros((nPolys, nPolys)) #LINE# #TAB# for m in p.monomers: #LINE# #TAB# #TAB# Turns[m.id] = 1 #LINE# #TAB# #TAB# for i in range(nPolys): #LINE# #TAB# #TAB# #TAB# Turns[i][m.id] += 1 #LINE# #TAB# del p.monomers[0] #LINE# #TAB# p.monomers[0] = Turns[0]"
"#LINE# #TAB# global _prev_x, _prev_y #LINE# #TAB# global _prev_width, _prev_y #LINE# #TAB# if x!= _prev_x: #LINE# #TAB# #TAB# x = _prev_x #LINE# #TAB# if y!= _prev_y: #LINE# #TAB# #TAB# y = _prev_y #LINE# #TAB# #TAB# prev_x = x #LINE# #TAB# #TAB# if y!= _prev_y: #LINE# #TAB# #TAB# #TAB# y = y #LINE# #TAB# #TAB# _prev_x = x #LINE# #TAB# #TAB# _prev_y = y #LINE# #TAB# elif x!= _prev_x: #LINE# #TAB# #TAB# x = _prev_x #LINE# #TAB# elif y!= _prev_y: #LINE# #TAB# #TAB# y = _prev_y #LINE# #TAB# _prev_x = x #LINE# #TAB# _prev_y = y"
"#LINE# #TAB# strainL = [geneT[g] for g in genesO] #LINE# #TAB# numGenes = len(strainL) #LINE# #TAB# if numGenes == 0: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# else: #LINE# #TAB# #TAB# idxT = [strainL[i] for i in range(numGenes)] #LINE# #TAB# #TAB# for g in genesO: #LINE# #TAB# #TAB# #TAB# genesO.remove(g) #LINE# #TAB# #TAB# #TAB# idxT[0] = geneT[g] #LINE# #TAB# #TAB# #TAB# idxT[1] = strainL[i] #LINE# #TAB# #TAB# return idxT"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return obj._worker_body #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return {}
"#LINE# #TAB# if not doc: #LINE# #TAB# #TAB# return [], [] #LINE# #TAB# else: #LINE# #TAB# #TAB# frames = [] #LINE# #TAB# #TAB# for frame in doc.get('frames', []): #LINE# #TAB# #TAB# #TAB# if 'import_name' in frame[0]: #LINE# #TAB# #TAB# #TAB# #TAB# importer_name = frame[0]['import_name'] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# importer_name = frame[0]['name'] #LINE# #TAB# #TAB# #TAB# frames.append((importer_name, frame)) #LINE# #TAB# #TAB# return frames"
"#LINE# #TAB# if day is None: #LINE# #TAB# #TAB# day = date.today() #LINE# #TAB# start = date(year=today.year, month=today.month, day=day.day, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# hour=0, minute=0, second=0) #LINE# #TAB# if day.weekday() == 6: #LINE# #TAB# #TAB# start -= timedelta(days=1) #LINE# #TAB# return start"
#LINE# #TAB# docstring = config['docstring'] #LINE# #TAB# del config['docstring'] #LINE# #TAB# return docstring
#LINE# #TAB# key = sha256(address.encode()).digest() #LINE# #TAB# looks = lookup_looks(key) #LINE# #TAB# return looks
"#LINE# #TAB# uri = '' #LINE# #TAB# for key in params: #LINE# #TAB# #TAB# if key == '_': #LINE# #TAB# #TAB# #TAB# uri += '?' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# uri += '{}://{}'.format(key, params[key]) #LINE# #TAB# return uri"
"#LINE# #TAB# path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# regex = '(\\d+)' #LINE# #TAB# vector = np.loadtxt(path, delimiter='\\s+') #LINE# #TAB# vector = [float(x) for x in vector] #LINE# #TAB# return vector"
"#LINE# #TAB# if isinstance(fnode, astroid.FunctionDef): #LINE# #TAB# #TAB# names = list() #LINE# #TAB# #TAB# for param in fnode.parameters: #LINE# #TAB# #TAB# #TAB# if isinstance(param.value, astroid.Name): #LINE# #TAB# #TAB# #TAB# #TAB# names.append(param.value.id) #LINE# #TAB# #TAB# elif isinstance(param.value, astroid.Name): #LINE# #TAB# #TAB# #TAB# names.append(param.value.id) #LINE# #TAB# elif isinstance(fnode, astroid.ClassDef): #LINE# #TAB# #TAB# names = list() #LINE# #TAB# #TAB# for name in fnode.names: #LINE# #TAB# #TAB# #TAB# names.append(node.name) #LINE# #TAB# #TAB# return names #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
#LINE# #TAB# letters = [] #LINE# #TAB# for line in fin_txt: #LINE# #TAB# #TAB# if line.startswith('#') or line == '\n': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# l = line.strip().split() #LINE# #TAB# #TAB# if len(l) == 1 and not l[0].isalpha(): #LINE# #TAB# #TAB# #TAB# letters.append(l[0]) #LINE# #TAB# return letters
#LINE# #TAB# realm_manager = RealMeManager() #LINE# #TAB# try: #LINE# #TAB# #TAB# auth_strength = realm_manager.get_authentication_strength(request) #LINE# #TAB# except NoResultFound: #LINE# #TAB# #TAB# return None #LINE# #TAB# return auth_strength
#LINE# #TAB# dev_template = InputTemplate(dev_type=dev_type) #LINE# #TAB# dev_obj = dev_template.create_input(dev_name) #LINE# #TAB# return dev_obj
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for k, v in obj.items(): #LINE# #TAB# #TAB# #TAB# obj[k] = TitleCase(v) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# obj[item] = TitleCase(item) #LINE# #TAB# return obj"
#LINE# #TAB# path_split = path.split('.') #LINE# #TAB# if len(path_split) == 1: #LINE# #TAB# #TAB# return 'file' #LINE# #TAB# if path_split[0] =='sqlite': #LINE# #TAB# #TAB# return'sqlite' #LINE# #TAB# return 'file'
#LINE# #TAB# with open(CONFIG_FILE) as f: #LINE# #TAB# #TAB# config = json.load(f) #LINE# #TAB# #TAB# device.configuration = config
"#LINE# #TAB# if month > WEEKDAYS.index(year): #LINE# #TAB# #TAB# month = 1 #LINE# #TAB# if month > WEEKDAYS.index(week): #LINE# #TAB# #TAB# month = 1 #LINE# #TAB# if month == 0: #LINE# #TAB# #TAB# month = 12 #LINE# #TAB# return year, month, weekday"
#LINE# #TAB# url = url.lower() #LINE# #TAB# for method in BLACKLIST_URLS: #LINE# #TAB# #TAB# if url.lower() in method.lower(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if not val: #LINE# #TAB# #TAB# return False #LINE# #TAB# val = str(val).lower() #LINE# #TAB# try: #LINE# #TAB# #TAB# if val.lower() in ['0', 'true', 'yes']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif val.lower() in ['1', 'true', 'yes']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return results[0] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return []
#LINE# #TAB# if np.isscalar(data) or len(data)!= 1: #LINE# #TAB# #TAB# return data #LINE# #TAB# key = list(data.keys())[0] #LINE# #TAB# if len(data[key]) == 1 and key in dataset.vdims: #LINE# #TAB# #TAB# return data[key][0]
"#LINE# #TAB# assert isinstance(x, np.ndarray) #LINE# #TAB# if ndim is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# x = np.asarray(x) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# raise ValueError('x must be an array') #LINE# #TAB# if x.ndim!= ndim: #LINE# #TAB# #TAB# raise ValueError('x.shape must be ({}, {})'.format(ndim, x.ndim)) #LINE# #TAB# return np.all(np.hstack((x.T, x))) == x"
#LINE# #TAB# dirname = os.path.dirname(path) #LINE# #TAB# if not os.path.exists(dirname): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(dirname) #LINE# #TAB# #TAB# except OSError as exc: #LINE# #TAB# #TAB# #TAB# if exc.errno == errno.EEXIST: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise
"#LINE# #TAB# if remove_brackets_content: #LINE# #TAB# #TAB# trans = pangloss.remove_content_in_brackets(trans, ""[]"") #LINE# #TAB# trans = fr_nlp("" "".join(trans.split()[:])) #LINE# #TAB# trans = "" "".join([token.lower_ for token in trans if not token.is_punct]) #LINE# #TAB# return trans"
"#LINE# #TAB# _, ext = os.path.splitext(name) #LINE# #TAB# if ext in VALID_EXTENSIONS: #LINE# #TAB# #TAB# mode = ext #LINE# #TAB# else: #LINE# #TAB# #TAB# mode = False #LINE# #TAB# return mode"
"#LINE# #TAB# out = '' #LINE# #TAB# for category, content in entry['categories'].items(): #LINE# #TAB# #TAB# if type(content) is list: #LINE# #TAB# #TAB# #TAB# content = '\n'.join([preprocess_pdf(x) for x in content]) #LINE# #TAB# #TAB# out += '\n' #LINE# #TAB# #TAB# elif type(content) is dict: #LINE# #TAB# #TAB# #TAB# content = '\n'.join([preprocess_pdf(x) for x in content]) #LINE# #TAB# #TAB# out += '\n' #LINE# #TAB# return out"
#LINE# #TAB# if is_path(value): #LINE# #TAB# #TAB# return value #LINE# #TAB# elif value.startswith('.'): #LINE# #TAB# #TAB# return value[1:] #LINE# #TAB# else: #LINE# #TAB# #TAB# return '*'
#LINE# #TAB# if type(pvm_state) == dict and 'power_state' in pvm_state: #LINE# #TAB# #TAB# power_state = pvm_state['power_state'] #LINE# #TAB# elif type(pvm_state) == list: #LINE# #TAB# #TAB# for i in pvm_state: #LINE# #TAB# #TAB# #TAB# if type(i) == int: #LINE# #TAB# #TAB# #TAB# #TAB# power_state = pvm_state[i] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise RuntimeError('Unknown os state %s' % pvm_state) #LINE# #TAB# else: #LINE# #TAB# #TAB# power_state = 0 #LINE# #TAB# return power_state
#LINE# #TAB# m = y1 - y2 - y1 / x2 - x1 #LINE# #TAB# return m
"#LINE# #TAB# if hasattr(data, 'timesteps'): #LINE# #TAB# #TAB# if data.timesteps: #LINE# #TAB# #TAB# #TAB# for k, v in data.timesteps.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if k in ['start', 'end']: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# #TAB# data.timesteps[k] = v #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# first_day_of_week = isoweekday(iso_year) - 1 #LINE# #TAB# if first_day_of_week == 7: #LINE# #TAB# #TAB# return 53 #LINE# #TAB# elif first_day_of_week == 8: #LINE# #TAB# #TAB# return 52 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 52
#LINE# #TAB# t.value = t.value[1:].upper() #LINE# #TAB# return t
"#LINE# #TAB# sa = mac2str(source) #LINE# #TAB# sa += b'\x00' * (4 - len(sa)) #LINE# #TAB# mac = mac2str(dest) #LINE# #TAB# mic = MIC(mic_key, sa, sa + b'\x00' * (4 - len(sa)) + b'\x00' * (4 - len(sa)) + data) #LINE# #TAB# return mic.decrypt(data)[:4]"
#LINE# #TAB# temp = tempfile.NamedTemporaryFile(delete=False) #LINE# #TAB# temp.write(a.tolist()) #LINE# #TAB# return temp.name
"#LINE# #TAB# produced = {} #LINE# #TAB# for name, blob_producers in ssa.items(): #LINE# #TAB# #TAB# blob_name, version = blob_producers #LINE# #TAB# #TAB# if version is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# index = operator_to_index(blob_name) #LINE# #TAB# #TAB# produced[index] = name #LINE# #TAB# return produced"
"#LINE# #TAB# p = sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) #LINE# #TAB# if p < n: #LINE# #TAB# #TAB# return x1, y1 #LINE# #TAB# elif p > n * 2: #LINE# #TAB# #TAB# return x2, y2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return x1, y1"
#LINE# #TAB# cidr = None #LINE# #TAB# if pObj.node_type == 'cluster': #LINE# #TAB# #TAB# if pObj.cidr: #LINE# #TAB# #TAB# #TAB# cidr = pObj.cidr #LINE# #TAB# #TAB# elif pObj.node_type == 'hypothesis_Node': #LINE# #TAB# #TAB# #TAB# if pObj.cidr: #LINE# #TAB# #TAB# #TAB# #TAB# cidr = pObj.cidr #LINE# #TAB# return cidr
"#LINE# #TAB# val = None #LINE# #TAB# try: #LINE# #TAB# #TAB# val = os.stat(filename).st_size #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# log.warning('File {0} does not exist. Error: {1}'.format(filename, e)) #LINE# #TAB# return val"
#LINE# #TAB# 'Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.' #LINE# #TAB# import urllib3 #LINE# #TAB# 'Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.' #LINE# #TAB# connectionpool.ssl_wrap_socket = ssl_wrap_socket
#LINE# #TAB# if not value: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not is_string(value): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not value.startswith('x-') or not value.endswith('-'): #LINE# #TAB# #TAB# return False #LINE# #TAB# value = value[1:] #LINE# #TAB# try: #LINE# #TAB# #TAB# int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# logger.exception('Virtual interrupt detected') #LINE# #TAB# #TAB# raise
#LINE# #TAB# try: #LINE# #TAB# #TAB# for part in value.split(): #LINE# #TAB# #TAB# #TAB# yield int(part) #LINE# #TAB# #TAB# #TAB# yield float(part) #LINE# #TAB# except: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield value #LINE# #TAB# #TAB# #TAB# yield value #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# if node.children[0] == node.children[1]: #LINE# #TAB# #TAB# n = len(node.children) #LINE# #TAB# #TAB# if n == 1: #LINE# #TAB# #TAB# #TAB# n = node.children[0] #LINE# #TAB# #TAB# elif n == 2: #LINE# #TAB# #TAB# #TAB# n = np.array([node.children[0], node.children[1]]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# n = np.array([node.children[0], node.children[1]]) #LINE# #TAB# #TAB# bin_scale(n, n) #LINE# #TAB# else: #LINE# #TAB# #TAB# n = np.array([node.children[0], node.children[1]]) #LINE# #TAB# return n"
"#LINE# #TAB# _path = Path(AbstractSample._full_path('constraint_rstrip.csv')) #LINE# #TAB# df = pd.read_csv(_path, encoding='latin1') #LINE# #TAB# return df.iloc[:size]"
#LINE# #TAB# if source not in _library_cache: #LINE# #TAB# #TAB# response = requests.get(source) #LINE# #TAB# #TAB# if response.status_code == 404: #LINE# #TAB# #TAB# #TAB# raise ValueError('library {} not found'.format(source)) #LINE# #TAB# #TAB# _library_cache[source] = response.json() #LINE# #TAB# return _library_cache[source]
"#LINE# #TAB# if net is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(net, PetriNet): #LINE# #TAB# #TAB# return net.bound #LINE# #TAB# if isinstance(net, Place): #LINE# #TAB# #TAB# net = net.copy() #LINE# #TAB# #TAB# del net.bound #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = cdf.name #LINE# #TAB# shape = (cdf.shape[0] - 1) // 2 + 1, cdf.shape[-1] #LINE# #TAB# s = Suite(name=name, dimensions=shape) #LINE# #TAB# for i, elem in enumerate(cdf): #LINE# #TAB# #TAB# s.add_element(elem) #LINE# #TAB# return s"
"#LINE# #TAB# f = open(filename, 'rb') #LINE# #TAB# f.seek(2048) #LINE# #TAB# posbyte = 0 #LINE# #TAB# allsentences = '' #LINE# #TAB# for _ in list(range(32)): #LINE# #TAB# #TAB# tt = f.read(32) #LINE# #TAB# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# #TAB# if s1!= '': #LINE# #TAB# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# #TAB# posbyte += 32 #LINE# #TAB# tt = f.read(1024) #LINE# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# if s1!= '': #LINE# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# f.close() #LINE# #TAB# return allsentences"
"#LINE# #TAB# response = json.loads(open(json_fn, 'r').read()) #LINE# #TAB# return response"
"#LINE# #TAB# if num_bins is None: #LINE# #TAB# #TAB# num_bins=len(G.nodes()) #LINE# #TAB# bin_labels = range(num_bins) #LINE# #TAB# attr_values = pd.Series([data[attr] for u, v, key, data in G.edges(data=True)]) #LINE# #TAB# cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels) #LINE# #TAB# colors = get_colors(num_bins, cmap, start, stop) #LINE# #TAB# token_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats] #LINE# #TAB# return token_colors"
#LINE# #TAB# if schema['type'] == 'array': #LINE# #TAB# #TAB# prefix = 'array_' #LINE# #TAB# elif schema['type'] == 'array': #LINE# #TAB# #TAB# prefix = 'array_' #LINE# #TAB# elif schema['type'] == 'date': #LINE# #TAB# #TAB# prefix = 'date_' #LINE# #TAB# elif schema['type'] == 'date': #LINE# #TAB# #TAB# prefix = 'date_' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('unsupported input type: {0!r}'.format(schema)) #LINE# #TAB# return prefix
#LINE# #TAB# try: #LINE# #TAB# #TAB# return array.get_pgroupvolume(module.params['name']) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
#LINE# #TAB# for part in get_packages_names(sender): #LINE# #TAB# #TAB# if part == sender: #LINE# #TAB# #TAB# #TAB# yield part
"#LINE# #TAB# if cache.writeable(): #LINE# #TAB# #TAB# status = return_status.HANDLED #LINE# #TAB# else: #LINE# #TAB# #TAB# times = e.payload['times'] #LINE# #TAB# #TAB# timeout = random.uniform(0.001, cache.timeout(times)) #LINE# #TAB# #TAB# cache.post_fifo(Event(signal=signals.cache_file_write, payload={ #LINE# #TAB# #TAB# #TAB# 'times': times + 1}), period=timeout, times=1, deferred=True) #LINE# #TAB# #TAB# status = return_status.HANDLED #LINE# #TAB# return status"
"#LINE# #TAB# b0 = 0.0215 #LINE# #TAB# b1 = 0.2122 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.00084 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['In'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# elements = browser.execute_script( #LINE# #TAB# #TAB#'return ($ || jQuery)(arguments[0]).get();', selector) #LINE# #TAB# attributes = elements[0].get('attributes', []) #LINE# #TAB# if not attributes: #LINE# #TAB# #TAB# return #LINE# #TAB# for attr in attributes: #LINE# #TAB# #TAB# browser.execute_script( #LINE# #TAB# #TAB# #TAB#'return ($ || jQuery)(arguments[0]).get();', attr) #LINE# #TAB# return"
"#LINE# #TAB# ar = np.asanyarray(ar) #LINE# #TAB# if sort: #LINE# #TAB# #TAB# ar = ar.sort_values(ascending=False) #LINE# #TAB# groups = np.unique(ar, return_index=True) #LINE# #TAB# return groups"
#LINE# #TAB# try: #LINE# #TAB# #TAB# if noupdate: #LINE# #TAB# #TAB# #TAB# cr.update(xmlid) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cr.delete(xmlid) #LINE# #TAB# except: #LINE# #TAB# #TAB# if warn: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# if isinstance(string, unicode): #LINE# #TAB# #TAB# return [string] #LINE# #TAB# assert 0 <= len(string) <= width #LINE# #TAB# result = [] #LINE# #TAB# for ch in string: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n = len(ch) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# n = 0 #LINE# #TAB# #TAB# if n > width: #LINE# #TAB# #TAB# #TAB# result.append(ch) #LINE# #TAB# return result"
"#LINE# #TAB# first = sample[0] #LINE# #TAB# second = sample[1] - interval #LINE# #TAB# return first, second"
#LINE# #TAB# if impact_function.__class__.__name__ == 'Function': #LINE# #TAB# #TAB# deriv_type = impact_function.__class__.__name__ #LINE# #TAB# #TAB# if deriv_type == 'CrossProduct': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# elif deriv_type == 'Projector': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# elif deriv_type == 'Projector': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return impact_function
#LINE# #TAB# mode = os.stat(filepath).st_mode #LINE# #TAB# if stat.S_ISLNK(mode): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# import matplotlib.pyplot as plt #LINE# #TAB# _, fig, ax = contour.get_figure() #LINE# #TAB# try: #LINE# #TAB# #TAB# ax = plt.get_subplot() #LINE# #TAB# except: #LINE# #TAB# #TAB# return None #LINE# #TAB# if ax == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# dmax = contour.get_aspect_ratio() #LINE# #TAB# if dmax == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# ax = fig.add_subplot(1) #LINE# #TAB# plt.tight_layout() #LINE# #TAB# ax.set_aspect_ratio(dmax) #LINE# #TAB# return dmax"
#LINE# #TAB# new_env_dicts = [] #LINE# #TAB# for data_dict in data_dicts: #LINE# #TAB# #TAB# for record in data_dict.values(): #LINE# #TAB# #TAB# #TAB# record = format_value_for_spreadsheet(record) #LINE# #TAB# #TAB# #TAB# new_env_dicts.append(record) #LINE# #TAB# return new_env_dicts
"#LINE# #TAB# if key not in cache: #LINE# #TAB# #TAB# return #LINE# #TAB# curr_data = cache[key] #LINE# #TAB# if not curr_data: #LINE# #TAB# #TAB# return #LINE# #TAB# rundata = curr_data.get('rundata', {}) #LINE# #TAB# data = rundata.get('results', {}) #LINE# #TAB# if not data: #LINE# #TAB# #TAB# return #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if not isinstance(v, list): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cache[key] = v #LINE# #TAB# return data"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# result = requests.get(url, headers={'User-Agent': #LINE# #TAB# #TAB# #TAB# 'Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10 (.NET CLR 4.0.20506)'}) #LINE# #TAB# #TAB# result.raise_for_status() #LINE# #TAB# #TAB# return result.text #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# raise"
"#LINE# #TAB# n_test = X_test.shape[1] #LINE# #TAB# n_history = X_history.shape[1] #LINE# #TAB# sst = svd(X_history) #LINE# #TAB# plotdata = np.zeros((n_test, n_components)) #LINE# #TAB# for i in range(n_test): #LINE# #TAB# #TAB# for j in range(n_history): #LINE# #TAB# #TAB# #TAB# plotdata[(i), :] = sst[(i), :] #LINE# #TAB# for i in range(n_components): #LINE# #TAB# #TAB# plotdata[:, (i)] = sst[(i), :] #LINE# #TAB# return plotdata"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# x_order = list(sorted(x_samples))[0] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# x_order = [(0) for _ in range(x_samples.shape[0])] #LINE# #TAB# geno_order = [list(sorted(geno_samples))[0] for _ in range(geno_samples.shape[0])] #LINE# #TAB# return x_order, geno_order"
"#LINE# #TAB# dirpath = os.path.abspath(dirpath) #LINE# #TAB# if not os.path.isdir(dirpath): #LINE# #TAB# #TAB# dirs = [os.path.join(dirpath, fname) for fname in os.listdir(dirpath)] #LINE# #TAB# #TAB# for name in dirs: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.rmdir(os.path.join(dirpath, name)) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return dirs #LINE# #TAB# else: #LINE# #TAB# #TAB# return dirpath"
"#LINE# #TAB# socks_proxy = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# socks_proxy.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# socks_proxy.settimeout(timeout) #LINE# #TAB# _socks = socks_proxy #LINE# #TAB# return socks_proxy"
"#LINE# #TAB# if object_dict.get(FIELD_NAME) in parent_object_dict: #LINE# #TAB# #TAB# if isinstance(object_dict, dict): #LINE# #TAB# #TAB# #TAB# return FIELD_NAME in object_dict and object_dict[FIELD_NAME #LINE# #TAB# #TAB# #TAB# #TAB# ] is not None and parent_object_dict[FIELD_NAME] is not None #LINE# #TAB# #TAB# return FIELD_NAME in object_dict and object_dict[FIELD_NAME #LINE# #TAB# #TAB# #TAB# ] is not None and parent_object_dict[FIELD_NAME] is not None #LINE# #TAB# return False"
"#LINE# #TAB# global entry_gui, entry_kernel #LINE# #TAB# entry_gui = gui #LINE# #TAB# if kernel is not None: #LINE# #TAB# #TAB# if entry_gui not in kernel.gui_dict: #LINE# #TAB# #TAB# #TAB# kernel.gui_dict[entry_gui] = entry_kernel #LINE# #TAB# entry_kernel = kernel_dict[entry_gui] #LINE# #TAB# _have_duplicate.add(entry_gui) #LINE# #TAB# return entry_gui"
#LINE# #TAB# mv = np.asarray(lv) #LINE# #TAB# for i in range(lv.shape[0]): #LINE# #TAB# #TAB# if lt1[i]!= lt2[i]: #LINE# #TAB# #TAB# #TAB# lv[i] = lv[i] + (lt1[i] - lt2[i]) * lt2[i] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lv[i] = lv[i] + (lt1[i] - lt2[i]) * lt2[i]
"#LINE# #TAB# obs = xw.sum(axis=1) #LINE# #TAB# Nw = weights.shape[0] #LINE# #TAB# s = np.zeros((Nw, Nw), dtype=float) #LINE# #TAB# for ii in range(Nw): #LINE# #TAB# #TAB# for jj in range(Nw): #LINE# #TAB# #TAB# #TAB# iif = ii[jj] #LINE# #TAB# #TAB# #TAB# s[iif, jj] = 1.0 / weights[iif] * (xw[iii] - xw[jj]) #LINE# #TAB# return s"
"#LINE# #TAB# if len(ops) == 1: #LINE# #TAB# #TAB# return ops[0] #LINE# #TAB# elif len(ops) > 1: #LINE# #TAB# #TAB# return ops[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# new_ops = [] #LINE# #TAB# #TAB# for op in ops: #LINE# #TAB# #TAB# #TAB# new_ops.append(PauliTerm(op, out_metric_term_term(op))) #LINE# #TAB# #TAB# return new_ops"
"#LINE# #TAB# seq = vnl_sequence() #LINE# #TAB# seq.update(d) #LINE# #TAB# return {'id': d['id'], 'type': d['type'],'start': d['start'], 'end': d[ #LINE# #TAB# #TAB# 'end'], 'time': d['time']}"
"#LINE# #TAB# return get_blockchain_overview(coin_symbol=coin_symbol, api_key=api_key)[ #LINE# #TAB# #TAB# 'blocks'][0]['hash']"
"#LINE# #TAB# hasher = hashlib.sha256() #LINE# #TAB# with tf.io.gfile.GFile(image_file, 'rb') as f: #LINE# #TAB# #TAB# hasher.update(f.read()) #LINE# #TAB# comment_hash = hasher.hexdigest() #LINE# #TAB# return comment_hash"
#LINE# #TAB# for line in file_path_or_generator: #LINE# #TAB# #TAB# html = encode_html_line(line) #LINE# #TAB# #TAB# yield html
"#LINE# #TAB# lines = [] #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# first_line = email.first_line #LINE# #TAB# #TAB# last_line = email.last_line #LINE# #TAB# #TAB# if first_line and last_line[0] in ('\n', '\r'): #LINE# #TAB# #TAB# #TAB# lines.append(first_line) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lines.append(last_line) #LINE# #TAB# body = '' #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# body += '{} <{}> {}'.format(first_line[0], last_line[1]) #LINE# #TAB# return body"
"#LINE# #TAB# if data.shape[0] > 2: #LINE# #TAB# #TAB# return data #LINE# #TAB# data_eV = data.mean(0) #LINE# #TAB# data_nu = data.var(0) #LINE# #TAB# while np.abs(data_eV - data_nu.mean()) > 1e-08: #LINE# #TAB# #TAB# data_eV = data_eV / (np.sqrt(data_eV ** 2) + np.sqrt(data_nu)) #LINE# #TAB# #TAB# data_nu = data_nu / (np.sqrt(data_eV ** 2)) #LINE# #TAB# return data_eV, data_nu"
#LINE# #TAB# df = df[config.search_engine]['contaminants'].isin(config. #LINE# #TAB# #TAB# search_engine['leading_protein']) #LINE# #TAB# return df
"#LINE# #TAB# if not overwrite and prefix: #LINE# #TAB# #TAB# filenames = [os.path.relpath(f, prefix) for f in os.listdir(output_dir)] #LINE# #TAB# else: #LINE# #TAB# #TAB# filenames = [os.path.relpath(f, prefix) for f in os.listdir(output_dir)] #LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# if filename.endswith('.gz'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# barcode = barcodes.get(filename) #LINE# #TAB# #TAB# if barcode and not os.path.exists(barcode): #LINE# #TAB# #TAB# #TAB# os.makedirs(barcode) #LINE# #TAB# return filenames"
"#LINE# #TAB# szsec, shsec = create_message() #LINE# #TAB# n = c_uint() #LINE# #TAB# db.put(szsec + n + shsec) #LINE# #TAB# db.commit() #LINE# #TAB# return szsec + n + shsec"
"#LINE# #TAB# out_dict = {} #LINE# #TAB# for factor in input_list: #LINE# #TAB# #TAB# if factor not in out_dict: #LINE# #TAB# #TAB# #TAB# out_dict[factor] = [] #LINE# #TAB# #TAB# if type(input_list[factor]) == list: #LINE# #TAB# #TAB# #TAB# for i in range(1, len(input_list)): #LINE# #TAB# #TAB# #TAB# #TAB# if input_list[i].count(factor)!= 1: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# out_dict[factor].append(input_list[i]) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# out_dict[factor].append(input_list[i]) #LINE# #TAB# return out_dict"
"#LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# ax = plt.gca() #LINE# #TAB# n = net_exposures.shape[0] #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# net_exposures_lines = use_dist_exposures_net(net_exposures[i], ax) #LINE# #TAB# return net_exposures_lines"
#LINE# #TAB# pinned_memory = mmap.pinned_memory(num_bytes) #LINE# #TAB# if pinned_memory: #LINE# #TAB# #TAB# buffer = pinned_memory.get(num_bytes) #LINE# #TAB# #TAB# _free(pinned_memory) #LINE# #TAB# #TAB# return buffer #LINE# #TAB# return None
"#LINE# #TAB# reference_list = list() #LINE# #TAB# with open('rfc-index.txt', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# title = line.strip() #LINE# #TAB# #TAB# #TAB# if title: #LINE# #TAB# #TAB# #TAB# #TAB# reference_list.append(title) #LINE# #TAB# return reference_list"
"#LINE# #TAB# qfont = QFont() #LINE# #TAB# for key, value in font.properties().items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# qfont.setAttribute(key, value) #LINE# #TAB# return qfont"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# chunk = receiver.recv(1024) #LINE# #TAB# #TAB# #TAB# #TAB# if not chunk: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# #TAB# yield chunk #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# if e.errno in (errno.ECONNRESET, errno.EINTR): #LINE# #TAB# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise"
#LINE# #TAB# results = {} #LINE# #TAB# for person in fake.persons(): #LINE# #TAB# #TAB# results[person['id']] = person.id #LINE# #TAB# return results
"#LINE# #TAB# dir_1 = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# dir_2 = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# dir_3 = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# dir_4 = os.path.join(dir_1, dir_2) #LINE# #TAB# return dir_3"
"#LINE# #TAB# columns = [] #LINE# #TAB# for col in n.columns: #LINE# #TAB# #TAB# name = col.name #LINE# #TAB# #TAB# sqltype = str(col.type) #LINE# #TAB# #TAB# if name == 'timestamp': #LINE# #TAB# #TAB# #TAB# col_name = 'timestamp' #LINE# #TAB# #TAB# #TAB# columns.append((name, col_name)) #LINE# #TAB# return columns"
"#LINE# #TAB# half = int(f2 / (f1 + f2)) #LINE# #TAB# f1 = f1 * 0.5 #LINE# #TAB# f2 = f2 * 0.5 #LINE# #TAB# f1 = (f1 + half) * 0.5 #LINE# #TAB# f2 = (f2 + half) * 0.5 #LINE# #TAB# return f1, f2"
#LINE# #TAB# if item is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return item
#LINE# #TAB# routes = [] #LINE# #TAB# app.config['core']['list'] = routes #LINE# #TAB# return routes
"#LINE# #TAB# char_list = list(s) #LINE# #TAB# char_list.sort() #LINE# #TAB# code_ranges = [] #LINE# #TAB# for i in range(0, len(char_list)): #LINE# #TAB# #TAB# code1 = chr(ord(char_list[i]) + i) #LINE# #TAB# #TAB# code2 = chr(ord(char_list[i + 1]) + i) #LINE# #TAB# #TAB# code_ranges.append(code1) #LINE# #TAB# return code_ranges"
"#LINE# #TAB# obj = {'model': cls} #LINE# #TAB# for attr in dir(cls): #LINE# #TAB# #TAB# obj[attr] = getattr(cls, attr) #LINE# #TAB# if isinstance(cls, models.ManyToOneRel): #LINE# #TAB# #TAB# obj['type_name'] = f'{type_prefix}{attr}' #LINE# #TAB# elif isinstance(cls, models.DateField): #LINE# #TAB# #TAB# obj['type_name'] = f'{type_prefix}{attr}' #LINE# #TAB# elif isinstance(cls, models.TimeField): #LINE# #TAB# #TAB# obj['type_name'] = f'{type_prefix}{attr}' #LINE# #TAB# return obj"
#LINE# #TAB# if diff is None: #LINE# #TAB# #TAB# diff = {} #LINE# #TAB# arr = np.asarray(arr) #LINE# #TAB# if arr.ndim == 2: #LINE# #TAB# #TAB# if arr.dtype == object: #LINE# #TAB# #TAB# #TAB# diff['shape'] = arr.shape #LINE# #TAB# #TAB# #TAB# diff['arr'] = arr #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# diff['shape'] = arr.shape #LINE# #TAB# diff = to_blob_proto(diff) #LINE# #TAB# if diff.shape!= arr.shape: #LINE# #TAB# #TAB# raise ValueError('Unexpected diff shape: {}'.format(diff.shape)) #LINE# #TAB# return diff
#LINE# #TAB# i = 0 #LINE# #TAB# while i < len(nodes): #LINE# #TAB# #TAB# if nodes[i].tag == 'nav_extender': #LINE# #TAB# #TAB# #TAB# nodes.pop(i) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# return nodes
"#LINE# #TAB# if incremental_state is not None: #LINE# #TAB# #TAB# full_key = _get_full_transaction_key(module, key) #LINE# #TAB# else: #LINE# #TAB# #TAB# full_key = _get_transaction_key(module, key) #LINE# #TAB# return full_key, incremental_state"
"#LINE# #TAB# data = {} #LINE# #TAB# for m in MODULE_DATA_MODULES: #LINE# #TAB# #TAB# for k, v in m.items(): #LINE# #TAB# #TAB# #TAB# if k in os.environ: #LINE# #TAB# #TAB# #TAB# #TAB# data[k] = os.environ[k] #LINE# #TAB# return data"
"#LINE# #TAB# if isinstance(uri, Namespace): #LINE# #TAB# #TAB# return uri.uri #LINE# #TAB# return uri"
#LINE# #TAB# if user is not None: #LINE# #TAB# #TAB# user = parse.quote(user) #LINE# #TAB# if password is not None: #LINE# #TAB# #TAB# password = parse.quote(password) #LINE# #TAB# return user + ':' + password
#LINE# #TAB# chunks = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = data.pop(0) #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if line.startswith(b'#') or not line.strip(): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# chunk = parse_chunk(line) #LINE# #TAB# #TAB# if not chunk: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# chunks.append(chunk) #LINE# #TAB# return chunks
#LINE# #TAB# if type(t) == list: #LINE# #TAB# #TAB# l = [] #LINE# #TAB# #TAB# for item in t: #LINE# #TAB# #TAB# #TAB# if type(item) == dict: #LINE# #TAB# #TAB# #TAB# #TAB# l.append(item) #LINE# #TAB# #TAB# #TAB# elif type(item) == list: #LINE# #TAB# #TAB# #TAB# #TAB# for item2 in item: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item2 #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield item
#LINE# #TAB# if flags & os.O_RDWR: #LINE# #TAB# #TAB# return 'w' #LINE# #TAB# elif flags & os.O_CREAT: #LINE# #TAB# #TAB# return 'f' #LINE# #TAB# elif flags & os.O_WRONLY: #LINE# #TAB# #TAB# return 'w' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'w'
"#LINE# #TAB# penn_tree = re.sub('\\s+','', penn_tree) #LINE# #TAB# penn_tree = re.sub('\\s+','', penn_tree) #LINE# #TAB# return penn_tree"
#LINE# #TAB# index2 = index.astype('int64') / 10 ** 9 #LINE# #TAB# return index2
"#LINE# #TAB# if ch is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# docstring = config.get('docstring', {}) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not docstring: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if vi_mode: #LINE# #TAB# #TAB# return vi_mode[0] #LINE# #TAB# return None
"#LINE# #TAB# module = _load_default_config() #LINE# #TAB# if file is None: #LINE# #TAB# #TAB# return module #LINE# #TAB# path = os.path.abspath(file) #LINE# #TAB# parent = os.path.dirname(__file__) #LINE# #TAB# for key, val in vars(module).items(): #LINE# #TAB# #TAB# if path.startswith(parent): #LINE# #TAB# #TAB# #TAB# return val #LINE# #TAB# parent = os.path.dirname(parent) #LINE# #TAB# for key, val in vars(module).items(): #LINE# #TAB# #TAB# if path.startswith(key): #LINE# #TAB# #TAB# #TAB# return val #LINE# #TAB# return default_config"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_output(['youtube-dl', '--help']) #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# weights = np.zeros((n_features, n_features)) #LINE# #TAB# n_off_diag = int((n_features ** 2 - n_features) / 2) #LINE# #TAB# weights[np.triu_indices(n_features, k=1)] = 0.1 * lam_scale * prng.randn( #LINE# #TAB# #TAB# n_off_diag) + 0.25 * lam_scale #LINE# #TAB# weights[weights < 0] = 0 #LINE# #TAB# return weights"
"#LINE# #TAB# datamean = data.mean(axis=2).imag #LINE# #TAB# datameanmin, datameanmax = rtlib.sigma_clip(datamean.flatten()) #LINE# #TAB# good = n.where((datamean > datameanmin) & (datamean < datameanmax)) #LINE# #TAB# logger.debug('Clipped to %d%% of data (%.3f to %.3f). Noise = %.3f.' % #LINE# #TAB# #TAB# (100.0 * len(good[0]) / len(datamean.flatten()), datameanmin, datameanmax, #LINE# #TAB# #TAB# datameanmax)) #LINE# #TAB# logger.debug('Clipped to %d%% of data (%.3f to %.3f). Noise = %.3f.' % #LINE# #TAB# #TAB# (100.0 * len(good[0]) / len(datamean.flatten()), datameanmax, #LINE# #TAB# #TAB# datameanmin, dat"
#LINE# #TAB# ret = [] #LINE# #TAB# allReactions = set() #LINE# #TAB# for rea in fvaMinmax: #LINE# #TAB# #TAB# if rea in allReactions: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# rxn = list(rea) #LINE# #TAB# #TAB# flux = fvaMinmax[rea] #LINE# #TAB# #TAB# if flux <= 0.0: #LINE# #TAB# #TAB# #TAB# ret.append(rea) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# allReactions.add(rxn) #LINE# #TAB# return ret
"#LINE# #TAB# if type(value) == str: #LINE# #TAB# #TAB# return value.replace('-', '') #LINE# #TAB# if type(value) == int: #LINE# #TAB# #TAB# return value * 10 #LINE# #TAB# if type(value) == float: #LINE# #TAB# #TAB# return value * 100 #LINE# #TAB# if type(value) == int: #LINE# #TAB# #TAB# return value * 100 #LINE# #TAB# if type(value) == list: #LINE# #TAB# #TAB# return [interpolate_bearing(x) for x in value] #LINE# #TAB# return value"
#LINE# #TAB# for name in tdt_paths: #LINE# #TAB# #TAB# if name in path: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if isinstance(n, float) and n.is_integer(): #LINE# #TAB# #TAB# if n == 1.0: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Cannot convert {} to int since it is not a number.' #LINE# #TAB# #TAB# #TAB# #TAB#.format(n)) #LINE# #TAB# return n"
#LINE# #TAB# context['channel_for_slug'] = slug #LINE# #TAB# if title: #LINE# #TAB# #TAB# context['title'] = title #LINE# #TAB# if text: #LINE# #TAB# #TAB# context['text'] = text #LINE# #TAB# if limit: #LINE# #TAB# #TAB# context['limit'] = limit #LINE# #TAB# return context
"#LINE# #TAB# assert_type_or_raise(array, dict, parameter_name='array') #LINE# #TAB# data = Result.validate_array(array) #LINE# #TAB# data['type'] = u(array.get('type')) #LINE# #TAB# data['data_file_id'] = u(array.get('data_file_id')) #LINE# #TAB# data['data_file_unique_id'] = u(array.get('data_file_unique_id')) #LINE# #TAB# return data"
"#LINE# #TAB# startyear -= 1 #LINE# #TAB# while startyear < 1: #LINE# #TAB# #TAB# yield startyear, 1 #LINE# #TAB# #TAB# startyear += 1 #LINE# #TAB# try: #LINE# #TAB# #TAB# yield startyear, 365 #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# except: #LINE# #TAB# #TAB# pass"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return PIL.Image.open(io.BytesIO(image_bytes)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logger.error(e) #LINE# #TAB# #TAB# return None
#LINE# #TAB# pkeys = g.keys() #LINE# #TAB# for key in pkeys: #LINE# #TAB# #TAB# if g.has_get(key): #LINE# #TAB# #TAB# #TAB# yield key
#LINE# #TAB# r = [] #LINE# #TAB# for i in dir(sys): #LINE# #TAB# #TAB# if i.startswith('_'): #LINE# #TAB# #TAB# #TAB# r.append(i) #LINE# #TAB# return r
#LINE# #TAB# if root.tag == 'comments': #LINE# #TAB# #TAB# for child in root.iter('./comment'): #LINE# #TAB# #TAB# #TAB# r = extract_release_from_comment(child) #LINE# #TAB# #TAB# #TAB# if r: #LINE# #TAB# #TAB# #TAB# #TAB# return r #LINE# #TAB# if root.tag == 'archive': #LINE# #TAB# #TAB# for child in root.iter('archive'): #LINE# #TAB# #TAB# #TAB# r = extract_archive_from_comment(child) #LINE# #TAB# #TAB# #TAB# return r #LINE# #TAB# return None
"#LINE# #TAB# #TAB# parsed = urlparse(host) #LINE# #TAB# #TAB# port = int(parsed.port) #LINE# #TAB# #TAB# lang = '{scheme}://{host}:{port}'.format(scheme=cls.scheme, host=host, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# port=port) #LINE# #TAB# #TAB# return lang"
"#LINE# #TAB# result = {} #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# result = value #LINE# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# result = vnl_list(value) #LINE# #TAB# elif isinstance(value, tuple): #LINE# #TAB# #TAB# result = vnl_dict(value) #LINE# #TAB# return result"
"#LINE# #TAB# cidr = [] #LINE# #TAB# for line in content.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line or line.startswith('cidr'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# cidr.append(line) #LINE# #TAB# if len(cidr) > 500: #LINE# #TAB# #TAB# return '%s %s' % (cidr[0], cidr[1]) #LINE# #TAB# return '%s %s' % (cidr[0], cidr[1])"
"#LINE# #TAB# n_samples = X.shape[1] #LINE# #TAB# C = np.zeros((n_samples, num_folds)) #LINE# #TAB# for i in range(num_folds): #LINE# #TAB# #TAB# C[(i), :] = GraphLassoCV(X[(i), :], mode='full') #LINE# #TAB# return C"
"#LINE# #TAB# img = check_img_ndim(img, ncomp) #LINE# #TAB# out = ndimage.transform.transform_3d_image(img, ncomp, norm_type=norm_type) #LINE# #TAB# if norm_type == 'perc': #LINE# #TAB# #TAB# out = _norm_perc(out) #LINE# #TAB# return out"
#LINE# #TAB# if user.is_active and user.is_superuser: #LINE# #TAB# #TAB# return True #LINE# #TAB# if model.objects.filter(user=user).exists(): #LINE# #TAB# #TAB# snippet_type = model._meta.snippet_type #LINE# #TAB# #TAB# if snippet_type == 'add': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if snippet_type == 'change': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if snippet_type == 'delete': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# src = session_vars.get('toyz.txt_src', None) #LINE# #TAB# if src is not None: #LINE# #TAB# #TAB# if toyz_settings.raw_mode: #LINE# #TAB# #TAB# #TAB# mode = 'r' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mode = 'r' #LINE# #TAB# else: #LINE# #TAB# #TAB# mode = 'w' #LINE# #TAB# txt_content = session_vars.get('toyz.txt_content', None) #LINE# #TAB# if txt_content is not None: #LINE# #TAB# #TAB# txt_content = txt_content.encode('utf-8') #LINE# #TAB# response = {'status':'success', 'content': txt_content} #LINE# #TAB# return response"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# a = np.clip(xm, a, out=P) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# a = np.clip(xm, a, out=P) #LINE# #TAB# return a"
"#LINE# #TAB# found = find_featured_artists(artist, albumartist) #LINE# #TAB# if found: #LINE# #TAB# #TAB# return found[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# bar = cls() #LINE# #TAB# for key, value in dic.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# bar.content += cls.extract_as_dict(value) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# bar.content += cls.extract_as_dict(value) #LINE# #TAB# #TAB# elif isinstance(value, bar.Bar): #LINE# #TAB# #TAB# #TAB# bar.content += dic[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise NotImplementedError(key, value) #LINE# #TAB# return bar"
"#LINE# #TAB# epilog = parser.epilog #LINE# #TAB# if not epilog.startswith(u'\n'): #LINE# #TAB# #TAB# epilog = textwrap.dedent(epilog) #LINE# #TAB# with open(epilog, 'r') as fh: #LINE# #TAB# #TAB# lines = fh.readlines() #LINE# #TAB# #TAB# lines = reversed(lines) #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith(u'\n'): #LINE# #TAB# #TAB# #TAB# #TAB# line = line[2:] #LINE# #TAB# #TAB# #TAB# lines.insert(0, line) #LINE# #TAB# return epilog"
#LINE# #TAB# neuron.h.load_file('morphology.hoc') #LINE# #TAB# neuron.h.load_file('biophysics.hoc') #LINE# #TAB# neuron.h.load_file('template.hoc') #LINE# #TAB# print('Loading cell bAC217_L5_BP_d0cc8d7615') #LINE# #TAB# cell.bAC217_L5_BP_d0cc8d7615(1 if add_synapses else 0) #LINE# #TAB# return neuron.h.bAC217_L5_BP_d0cc8d7615
"#LINE# #TAB# old_times = times_needed.copy() #LINE# #TAB# sorted_times = sorted(times_needed.items(), key=lambda t: t[0]) #LINE# #TAB# for i in range(len(sorted_times)): #LINE# #TAB# #TAB# if sorted_times[i][0] < times_needed[i][0]: #LINE# #TAB# #TAB# #TAB# sorted_times[i][0] = times_needed[i][0] - _db[i][0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sorted_times[i][0] = times_needed[i][0] - _db[i][0] #LINE# #TAB# times_needed.drop(sorted_times, axis=1, inplace=True) #LINE# #TAB# return times_needed"
"#LINE# #TAB# for suffix in ('-', '_'): #LINE# #TAB# #TAB# if name.endswith(suffix): #LINE# #TAB# #TAB# #TAB# return name #LINE# #TAB# return name"
#LINE# #TAB# if dst.exists(): #LINE# #TAB# #TAB# pass
"#LINE# #TAB# if not hasattr(cls, '_upper'): #LINE# #TAB# cls._upper = timestamp #LINE# #TAB# return #LINE# #TAB# if timestamp > cls._upper: #LINE# #TAB# cls._upper = timestamp"
"#LINE# #TAB# with open(file_name, 'rb') as f: #LINE# #TAB# #TAB# image = Image.open(f) #LINE# #TAB# #TAB# image = image.convert('RGB') #LINE# #TAB# return image"
"#LINE# #TAB# typing.Any) ->tf.Tensor: #LINE# #TAB# attention_input_shape = attention_input.get_shape().as_list() #LINE# #TAB# attention_mask_shape = attention_mask.get_shape().as_list() #LINE# #TAB# attention_input_shape[0] = attention_input_shape[0] #LINE# #TAB# attention_output_shape = tf.reshape(attention_input_shape, [1, attention_mask_shape[1]]) #LINE# #TAB# return attention_output_shape"
"#LINE# #TAB# r = session.get(url) #LINE# #TAB# soup = BeautifulSoup(r.text, 'lxml') #LINE# #TAB# types = [] #LINE# #TAB# for item in soup.find_all('a', attrs={'href': url}): #LINE# #TAB# #TAB# if 'class' in item.attrib: #LINE# #TAB# #TAB# #TAB# types.append(item.attrib['class']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# links = list(item.find_all('a', attrs={'href': url})) #LINE# #TAB# #TAB# #TAB# if len(links) > 0: #LINE# #TAB# #TAB# #TAB# #TAB# return links[0]['href'] #LINE# #TAB# return None"
"#LINE# #TAB# #TAB# changes = [] #LINE# #TAB# #TAB# previous_value = line[0] #LINE# #TAB# #TAB# for current_value in line[1:]: #LINE# #TAB# #TAB# #TAB# if current_value!= previous_value: #LINE# #TAB# #TAB# #TAB# #TAB# changes.append((current_value, previous_value)) #LINE# #TAB# #TAB# #TAB# previous_value = current_value #LINE# #TAB# #TAB# return changes"
#LINE# #TAB# if max_prime == 2: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif max_prime == 3: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif max_prime == 4: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# obj = cls._si_unit_objects.get(name, None) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# myokit_unit = cls._si_unit_objects.get(name, None) #LINE# #TAB# #TAB# if myokit_unit is None: #LINE# #TAB# #TAB# #TAB# raise CellMLError('Unknown units name ""' + str(name) + '"".') #LINE# #TAB# #TAB# obj.name = name #LINE# #TAB# #TAB# cls._si_unit_objects[name] = myokit_unit #LINE# #TAB# return obj"
"#LINE# #TAB# matches, unmatched = [], [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if cond(item): #LINE# #TAB# #TAB# #TAB# matches.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unmatched.append(item) #LINE# #TAB# return matches, unmatched"
#LINE# #TAB# if not 'term' in req.keys(): #LINE# #TAB# #TAB# return {} #LINE# #TAB# query = {'term': req['term']} #LINE# #TAB# if 'country' in req.keys(): #LINE# #TAB# #TAB# query['country'] = req['country'] #LINE# #TAB# if 'country_code' in req.keys(): #LINE# #TAB# #TAB# query['country_code'] = req['country_code'] #LINE# #TAB# if 'display_name' in req.keys(): #LINE# #TAB# #TAB# query['display_name'] = req.keys['display_name'] #LINE# #TAB# if 'country_code' in req.keys(): #LINE# #TAB# #TAB# query['country_code'] = req.keys['country_code'] #LINE# #TAB# return query
#LINE# #TAB# s = np.linalg.norm(pt1 - pt2) #LINE# #TAB# s += np.linalg.norm(pt3 - pt1) #LINE# #TAB# return s
"#LINE# #TAB# global current_thread #LINE# #TAB# current_thread = threading.Thread(None, callback) #LINE# #TAB# return current_thread"
#LINE# #TAB# if warningstuple[2] is InterfaceWarning: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"#LINE# #TAB# packet = p.Packet(MsgType.Imu) #LINE# #TAB# packet.add_subpacket(p.SetListOfBytes(ImuMsgCode.SetGrad, #LINE# #TAB# #TAB# AckCode.OK)) #LINE# #TAB# return packet"
"#LINE# #TAB# if kind == ""linear"": #LINE# #TAB# #TAB# k = 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# k = 0 #LINE# #TAB# mask = create_topography(h, lon, lat, dx, k) #LINE# #TAB# if plot: #LINE# #TAB# #TAB# plt.close(mask) #LINE# #TAB# return mask"
#LINE# #TAB# task_instance_id = task_instances[0].id #LINE# #TAB# while task_instance_id in task_instances: #LINE# #TAB# #TAB# if is_dn_node(task_instance_id): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# task_instance_id += 1 #LINE# #TAB# return task_instance_id
"#LINE# #TAB# command = ['flow', '--list', service] #LINE# #TAB# try: #LINE# #TAB# #TAB# flow_output = subprocess.check_output(command).decode('utf-8') #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# return False #LINE# #TAB# for line in flow_output.split('\n'): #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# region = line.split(':')[0].strip() #LINE# #TAB# #TAB# if region: #LINE# #TAB# #TAB# #TAB# yield region #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield region"
#LINE# #TAB# scheduled_at = group.scheduled_at #LINE# #TAB# group.host = host #LINE# #TAB# group.scheduled_at = scheduled_at #LINE# #TAB# return group
"#LINE# #TAB# theta = 0.06 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"#LINE# #TAB# if op in opc_size_cache: #LINE# #TAB# #TAB# return opc_size_cache[op] #LINE# #TAB# size = 0 #LINE# #TAB# if hasattr(op,'size'): #LINE# #TAB# #TAB# size += op.size #LINE# #TAB# #TAB# if size < 8: #LINE# #TAB# #TAB# #TAB# size = 8 #LINE# #TAB# #TAB# op_size_cache[op] = size #LINE# #TAB# return size"
#LINE# #TAB# for i in iterator: #LINE# #TAB# #TAB# if test is None: #LINE# #TAB# #TAB# #TAB# yield i #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield i
#LINE# #TAB# if rotate: #LINE# #TAB# #TAB# images = rotate_test(images) #LINE# #TAB# if flip: #LINE# #TAB# #TAB# images = flip_test(images) #LINE# #TAB# return images
"#LINE# #TAB# songToSplit = songToSplit.replace(songToSplit[0:3], '') #LINE# #TAB# songToSplit = songToSplit.replace(songToSplit[3:], '') #LINE# #TAB# return songToSplit[:start1], songToSplit[start2:]"
"#LINE# #TAB# data, train_data, test_data = load_imdb_data() #LINE# #TAB# if n_gram: #LINE# #TAB# #TAB# n_gram_hash = hashlib.sha256(n_gram.encode('utf-8')).hexdigest() #LINE# #TAB# #TAB# train_data += n_gram_hash #LINE# #TAB# #TAB# test_data += n_gram_hash #LINE# #TAB# return data, train_data, test_data"
#LINE# #TAB# if cls._mean_strel is None: #LINE# #TAB# #TAB# from.dial import Speed dial #LINE# #TAB# #TAB# cls._mean_strel = Speed dial() #LINE# #TAB# return cls._mean_strel
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('peers_relation_id', dtype='int32', direction= #LINE# #TAB# #TAB# function.OUT) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
#LINE# #TAB# #TAB# axis_indices = [] #LINE# #TAB# #TAB# for var in vars_list: #LINE# #TAB# #TAB# #TAB# axis_indices.append(fluent.scope.get_axis(var).idx) #LINE# #TAB# #TAB# return axis_indices
"#LINE# #TAB# config_path = find_config_file(config_dir) #LINE# #TAB# if config_path is None: #LINE# #TAB# #TAB# print('Unable to find configuration. Creating default one in', #LINE# #TAB# #TAB# #TAB# config_dir) #LINE# #TAB# #TAB# config_path = create_default_config_file(config_dir, detect_location) #LINE# #TAB# return config_path"
"#LINE# #TAB# return { #LINE# #TAB# #TAB#'method': spec.method, #LINE# #TAB# #TAB# 'enable_lru': spec.enable_lru, #LINE# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# 'username': spec.username, #LINE# #TAB# #TAB# #TAB# 'password': spec.password, #LINE# #TAB# #TAB# #TAB# 'python_path': spec.python_path, #LINE# #TAB# #TAB# #TAB#'su_path': spec.become_exe(), #LINE# #TAB# #TAB# #TAB# 'connect_timeout': spec.timeout, #LINE# #TAB# #TAB# #TAB#'remote_name': get_remote_name(spec), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"#LINE# #TAB# print('Building FASTA', f) #LINE# #TAB# with open(f, 'rb') as f_obj: #LINE# #TAB# #TAB# buf = f_obj.read(4) #LINE# #TAB# return [bytes_to_be_int(buf[0:3]), bytes_to_be_int(buf[3:6]), bytes_to_be_int( #LINE# #TAB# #TAB# buf[6:8]), bytes_to_be_int(buf[8:9]), bytes_to_be_int(buf[9:10]), bytes_to_be_int( #LINE# #TAB# #TAB# buf[10:12]), bytes_to_be_int(buf[12:14])]"
"#LINE# #TAB# ses, auto_close = ensure_session(engine_or_session) #LINE# #TAB# obj = ses.query(cls).get(_id) #LINE# #TAB# if auto_close: #LINE# #TAB# #TAB# ses.close() #LINE# #TAB# return obj"
#LINE# #TAB# ngram_set = set() #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# ngram_set.add(line.rstrip()) #LINE# #TAB# parts = [] #LINE# #TAB# ngram_set_old = set() #LINE# #TAB# while True: #LINE# #TAB# #TAB# chunk = random.choice(ngram_set) #LINE# #TAB# #TAB# if chunk in ngram_set: #LINE# #TAB# #TAB# #TAB# parts.append(chunk) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# parts.reverse() #LINE# #TAB# return parts
"#LINE# #TAB# if 'program_patterns' in details: #LINE# #TAB# #TAB# for pattern in details['program_patterns']: #LINE# #TAB# #TAB# #TAB# matches = re.findall(pattern, pattern) #LINE# #TAB# #TAB# #TAB# if matches: #LINE# #TAB# #TAB# #TAB# #TAB# return [m.group(1) for m in matches] #LINE# #TAB# return []"
"#LINE# #TAB# glob_path = os.path.join(path, '*.md') #LINE# #TAB# for filename in glob.glob(glob_path): #LINE# #TAB# #TAB# if filename.endswith('.md'): #LINE# #TAB# #TAB# #TAB# m = load_elemental_material(filename) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# m.clear() #LINE# #TAB# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# global _information_collections #LINE# #TAB# if not _information_collections: #LINE# #TAB# #TAB# _information_collections = [InformationCollection()] #LINE# #TAB# return _information_collections
#LINE# #TAB# doc_type = doc_type.lower() #LINE# #TAB# for label in page.DOC_TYPES: #LINE# #TAB# #TAB# if doc_type.startswith(label): #LINE# #TAB# #TAB# #TAB# return 'class: %s' % label #LINE# #TAB# return doc_type
"#LINE# #TAB# verts = np.array(vertices) #LINE# #TAB# faces = np.array(faces) #LINE# #TAB# is_valid = faces.shape == (3, 3) #LINE# #TAB# if not is_valid: #LINE# #TAB# #TAB# raise ValueError('faces must be (3,3)!') #LINE# #TAB# normals = np.vstack([verts[:, (i)] for i in range(3)]) #LINE# #TAB# normals /= normals.sum() #LINE# #TAB# return normals"
#LINE# #TAB# w = np.sqrt(qx ** 2 + qy ** 2 + qz ** 2) #LINE# #TAB# if qx.shape[0] == 0: #LINE# #TAB# #TAB# qx = qx[()] #LINE# #TAB# if qy.shape[0] == 0: #LINE# #TAB# #TAB# qy = qy[()] #LINE# #TAB# if qz.shape[0] == 0: #LINE# #TAB# #TAB# qz = qz[()] #LINE# #TAB# return w
"#LINE# #TAB# val = [] #LINE# #TAB# for i in range(0, size): #LINE# #TAB# #TAB# val.append(b'') #LINE# #TAB# #TAB# yield val"
"#LINE# #TAB# requirements = set() #LINE# #TAB# requirements_file = open('requirements.in', 'r') #LINE# #TAB# for line in requirements_file: #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# requirements.add(line.split('==')[0]) #LINE# #TAB# pipfile_lock_path = os.path.join(os.path.dirname(os.path.realpath(__file__) #LINE# #TAB# #TAB# ), 'Pipfile.lock') #LINE# #TAB# if pipfile_lock_path not in sys.path: #LINE# #TAB# #TAB# sys.path.append(pipfile_lock_path) #LINE# #TAB# pipfile_lock_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# 'Pipfile.lock') #LINE#"
#LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if l[i] is True: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# paths = [path] #LINE# #TAB# found = [path for path in paths if path.name == name] #LINE# #TAB# if found: #LINE# #TAB# #TAB# return found #LINE# #TAB# for root, _, files in os.walk(path): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# if file.name == name: #LINE# #TAB# #TAB# #TAB# #TAB# found.append(path) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# for p in found: #LINE# #TAB# #TAB# if p.is_dir(): #LINE# #TAB# #TAB# #TAB# return Path(p) #LINE# #TAB# return None"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise SpacegroupNotFoundError( #LINE# #TAB# #TAB# #TAB# #TAB# 'invalid spacegroup %s, setting %i not found in data base' % #LINE# #TAB# #TAB# #TAB# #TAB# ( spacegroup, setting ) ) #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# break"
"#LINE# #TAB# if isinstance(obj, (list, tuple, set)): #LINE# #TAB# #TAB# return all(retrieve_json(i) for i in obj) #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# return all(retrieve_json(v) for v in obj.values()) #LINE# #TAB# return obj"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return predicate(value) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# layout_types = cls.search([('domain', '=', domain), ('name', '=', name), ( #LINE# #TAB# #TAB#'version', '=', version)]) #LINE# #TAB# if not layout_types: #LINE# #TAB# #TAB# return None #LINE# #TAB# return layout_types[0]"
"#LINE# #TAB# with open(cmds_file, 'r') as f: #LINE# #TAB# #TAB# cmds_tuple = [] #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# for which in which_list: #LINE# #TAB# #TAB# #TAB# #TAB# if which in line: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# cmds_tuple.append(line) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# cmds_tuple.append(line) #LINE# #TAB# #TAB# return cmds_tuple"
#LINE# #TAB# if mem is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# time = np.asarray(time) #LINE# #TAB# mem = np.asarray(mem) #LINE# #TAB# out = False #LINE# #TAB# if time.dtype!= np.dtype('int'): #LINE# #TAB# #TAB# out = True #LINE# #TAB# if mem.dtype!= np.dtype('float'): #LINE# #TAB# #TAB# out = False #LINE# #TAB# if out: #LINE# #TAB# #TAB# return not np.asarray(out) #LINE# #TAB# return out
"#LINE# #TAB# size = ffi.new('size_t *') #LINE# #TAB# _generator_create_string_length(handle, key, size) #LINE# #TAB# return size[0]"
#LINE# #TAB# if mana_div == 0: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# mana_div = int(mana_div) #LINE# #TAB# result = '' #LINE# #TAB# while mana_div > 0: #LINE# #TAB# #TAB# result += str(mana_div % 100) +'' #LINE# #TAB# #TAB# mana_div //= 100 #LINE# #TAB# return result
"#LINE# #TAB# _, s = s.shape #LINE# #TAB# assert s.ndim == 3 #LINE# #TAB# k = s.shape[1] #LINE# #TAB# if k == 1: #LINE# #TAB# #TAB# return s * np.conj(s) #LINE# #TAB# elif k == 2: #LINE# #TAB# #TAB# return s ** 2 #LINE# #TAB# s /= n ** (k - 1) #LINE# #TAB# return s"
"#LINE# #TAB# return {'script': 'pip install --dynamic-sql <{0}>'.format(config_dir), #LINE# #TAB# #TAB#'script_args': ['-c', '--config.dir', config_dir]}"
"#LINE# #TAB# ps_script = ( #LINE# #TAB# #TAB# 'Get-VM -VMName ""{}"" | Select VMName, Snapshots | ConvertTo-Json' #LINE# #TAB# #TAB#.format(vm_name)) #LINE# #TAB# rs = run_ps(ps_script) #LINE# #TAB# return rs"
"#LINE# #TAB# sessions = [] #LINE# #TAB# for c in broker.connections: #LINE# #TAB# #TAB# if isinstance(c, CleavingContext): #LINE# #TAB# #TAB# #TAB# session = c.get_session() #LINE# #TAB# #TAB# #TAB# if session is not None: #LINE# #TAB# #TAB# #TAB# #TAB# sessions.append((c, session.get_timestamp())) #LINE# #TAB# return sessions"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return not os.path.getsize(file_path) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return True
"#LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# soup = _get_soup_from_url(url) #LINE# #TAB# icon_url = _get_url(url, 'icon') #LINE# #TAB# if icon_url is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# soup.find(icon_url) #LINE# #TAB# return icon_url"
"#LINE# #TAB# if not re.match(""^[0-9]+$"", number): #LINE# #TAB# #TAB# raise ValueError(""Invalid Finnish bank account"") #LINE# #TAB# if number[0]!= ""0"": #LINE# #TAB# #TAB# raise ValueError(""Invalid Finnish profile number"") #LINE# #TAB# parts = number.split(""-"") #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# raise ValueError(""Invalid Finnish profile number"") #LINE# #TAB# return parts[1]"
"#LINE# #TAB# username = click.prompt('Please enter your One Codex (email)') #LINE# #TAB# if api_key: #LINE# #TAB# #TAB# return username, api_key #LINE# #TAB# password = click.prompt( #LINE# #TAB# #TAB# 'Please enter your password (typing will be hidden)', hide_input=True) #LINE# #TAB# api_key = fn_qt_api_key(server, api_key) #LINE# #TAB# return username, password"
#LINE# #TAB# while value: #LINE# #TAB# #TAB# value = node.input[0] #LINE# #TAB# return True
#LINE# #TAB# if type(data) is str: #LINE# #TAB# #TAB# data = bytearray(data) #LINE# #TAB# csum = 0 #LINE# #TAB# for b in data: #LINE# #TAB# #TAB# csum += b #LINE# #TAB# #TAB# csum %= 256 #LINE# #TAB# return csum & 255
"#LINE# #TAB# registry = _get_current_registry() #LINE# #TAB# for name, cls in registry.items(): #LINE# #TAB# #TAB# if name not in _new_services: #LINE# #TAB# #TAB# #TAB# _new_services[name] = fun #LINE# #TAB# return fun"
"#LINE# #TAB# if start_time is None: #LINE# #TAB# #TAB# start_time = time.time() #LINE# #TAB# new_df = source.copy(deep=True) #LINE# #TAB# new_df.index = stretched_julian_enforce(start_time, factor) #LINE# #TAB# return new_df"
#LINE# #TAB# scheme_id = spec.get('scheme') #LINE# #TAB# if not scheme_id: #LINE# #TAB# #TAB# scheme_id = org_client.get_preferred_org_id() #LINE# #TAB# if org_client.get_project_id() == spec.get('project_id'): #LINE# #TAB# #TAB# scheme_id = org_client.get_preferred_org_id() #LINE# #TAB# return scheme_id
"#LINE# #TAB# m = Manifold() #LINE# #TAB# error = '' #LINE# #TAB# with open(fn, 'r') as f: #LINE# #TAB# #TAB# if line[0] == '#': #LINE# #TAB# #TAB# #TAB# line = f.readline().strip() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise ValueError(error) #LINE# #TAB# #TAB# m = line.split() #LINE# #TAB# #TAB# if len(m[0]) == 2: #LINE# #TAB# #TAB# #TAB# error = m[1] #LINE# #TAB# #TAB# m[0].line = int(m[0]) #LINE# #TAB# return m"
"#LINE# #TAB# customer_idurl = packetid.CustomerIDURL(backupID) #LINE# #TAB# if backupID not in cache: #LINE# #TAB# #TAB# return #LINE# #TAB# cache[backupID] = dict(customer_idurl=customer_idurl) #LINE# #TAB# if blockNum not in cache[backupID]: #LINE# #TAB# #TAB# return #LINE# #TAB# response = requests.post(customer_idurl, json=cache[backupID][blockNum]) #LINE# #TAB# try: #LINE# #TAB# #TAB# data = json.loads(response.text) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return #LINE# #TAB# cache[backupID][blockNum] = data #LINE# #TAB# return"
"#LINE# #TAB# settings = current_app.config.get('RESTFUL_IMAGES', {}) #LINE# #TAB# if current_app.debug: #LINE# #TAB# #TAB# settings.setdefault('indent', 4) #LINE# #TAB# #TAB# settings.setdefault('sort_keys', not PY3) #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = make_response(dumped, code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
#LINE# #TAB# matches = [] #LINE# #TAB# pos = 0 #LINE# #TAB# while pos < len(buff): #LINE# #TAB# #TAB# matched = pattern.search(buff[pos:]) #LINE# #TAB# #TAB# if matched: #LINE# #TAB# #TAB# #TAB# yield prefix + matched.group(1) +'' + matched.group(2) #LINE# #TAB# #TAB# pos = matched.end() #LINE# #TAB# #TAB# if matched.group(3): #LINE# #TAB# #TAB# #TAB# yield prefix + matched.group(3) +'' + matched.group(4) #LINE# #TAB# #TAB# matches.append(buff[pos:]) #LINE# #TAB# #TAB# pos = matched.end() #LINE# #TAB# return ''.join(matches) + prefix
"#LINE# #TAB# if val is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# pairs = {} #LINE# #TAB# if isinstance(val, (list, tuple)): #LINE# #TAB# #TAB# for i, v in enumerate(val): #LINE# #TAB# #TAB# #TAB# pairs[i] = analysis_pairs(v) #LINE# #TAB# elif isinstance(val, dict): #LINE# #TAB# #TAB# for k, v in val.items(): #LINE# #TAB# #TAB# #TAB# pairs[k] = analysis_pairs(v) #LINE# #TAB# return pairs"
#LINE# #TAB# try: #LINE# #TAB# #TAB# output = sys.stdout #LINE# #TAB# #TAB# sys.stdout = StringIO() #LINE# #TAB# #TAB# yield output #LINE# #TAB# finally: #LINE# #TAB# #TAB# sys.stdout = None
"#LINE# #TAB# paths = set() #LINE# #TAB# for f in maya.cmds.ls(l=True, type='file'): #LINE# #TAB# #TAB# if maya.cmds.referenceQuery(f, isNodeReferenced=True): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# texture_path = maya.cmds.getAttr(os.path.normpath('.'.join([f, #LINE# #TAB# #TAB# #TAB# 'fontName']))) #LINE# #TAB# #TAB# if texture_path: #LINE# #TAB# #TAB# #TAB# paths.add(texture_path) #LINE# #TAB# return paths"
"#LINE# #TAB# found_format = None #LINE# #TAB# last_index = -1 #LINE# #TAB# while True: #LINE# #TAB# #TAB# if file_name.endswith('.vcf'): #LINE# #TAB# #TAB# #TAB# found_format = True #LINE# #TAB# #TAB# #TAB# file_name = file_name[:last_index] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if found_format is not None: #LINE# #TAB# #TAB# #TAB# file_name = file_name[last_index + 1:] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# last_index = file_name.rfind('.') #LINE# #TAB# if found_format is not None: #LINE# #TAB# #TAB# file_name = file_name[:found_format] #LINE# #TAB# return file_name, found_format"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.must_handle_array = True #LINE# #TAB# function.addParameter('jobs_index', dtype='int32', direction= #LINE# #TAB# #TAB# function.OUT) #LINE# #TAB# function.addParameter('source_label', dtype='string', direction= #LINE# #TAB# #TAB# function.OUT) #LINE# #TAB# function.addParameter('target_label', dtype='string', direction= #LINE# #TAB# #TAB# function.OUT) #LINE# #TAB# function.addParameter('label', dtype='string', direction=function.OUT) #LINE# #TAB# function.addParameter('count', dtype='int32', direction=function.LENGTH) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was retrieved #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #"
"#LINE# #TAB# sorted_keys = sorted(response.keys(), key=lambda k: k[0]) #LINE# #TAB# for key in sorted_keys: #LINE# #TAB# #TAB# yield key, response[key]"
"#LINE# #TAB# env = req.get('AWS4_REQUEST_ENV', '') #LINE# #TAB# if env: #LINE# #TAB# #TAB# parts = env.split(':') #LINE# #TAB# else: #LINE# #TAB# #TAB# parts = [env] #LINE# #TAB# string_params = {} #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# if part in cano_req: #LINE# #TAB# #TAB# #TAB# string_params[part] = cano_req[part] #LINE# #TAB# if '-' in string_params: #LINE# #TAB# #TAB# string_params['-'] = string_params['-'].replace('-', '_') #LINE# #TAB# string = '{}-{}'.format(scope, string_params) #LINE# #TAB# return string"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cio.tell() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return -1
#LINE# #TAB# rows = bb[:2] + bb[2:] #LINE# #TAB# cols = bb[:3] + bb[3:] #LINE# #TAB# rows = [int(row) for row in rows] #LINE# #TAB# cols = [int(col) for col in cols] #LINE# #TAB# factor = factor if factor!= _EPS else 1 #LINE# #TAB# for row in range(rows): #LINE# #TAB# #TAB# for col in range(cols): #LINE# #TAB# #TAB# #TAB# bb[0] += factor * row[col] #LINE# #TAB# #TAB# #TAB# bb[1] += factor * col[col] #LINE# #TAB# for row in range(rows): #LINE# #TAB# #TAB# bb[0] -= factor #LINE# #TAB# #TAB# bb[1] += factor * row[col] #LINE# #TAB# return bb
#LINE# #TAB# chrom = chrom_str.split('-')[0] #LINE# #TAB# _sorted_key[chrom] = int(chrom_str) #LINE# #TAB# return chrom
"#LINE# #TAB# if urlconf is None: #LINE# #TAB# #TAB# urlconf = get_default_urlconf() #LINE# #TAB# if isinstance(path, Resolver): #LINE# #TAB# #TAB# return path.match(urlconf.resolve(path)) #LINE# #TAB# elif path.startswith('/'): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# try: #LINE# #TAB# #TAB# import json #LINE# #TAB# #TAB# return True #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# try: #LINE# #TAB# #TAB# all_scores = [x for x in s] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# all_scores = [x for x in s.strip().split(',') if x] #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return {} #LINE# #TAB# score = {'a': 0, 'b': 0, 'c': 0} #LINE# #TAB# all_scores.remove(score) #LINE# #TAB# all_scores = [x for x in all_scores if x!= 0] #LINE# #TAB# return all_scores"
"#LINE# #TAB# LOG.debug('update_err_binding() called') #LINE# #TAB# session = bc.get_writer_session() #LINE# #TAB# binding = session.query(nexus_models_v2.NexusNVEBinding).filter_by(vni #LINE# #TAB# #TAB# =vni, switch_ip=switch_ip, device_id=device_id).one() #LINE# #TAB# if binding: #LINE# #TAB# #TAB# session.delete(binding) #LINE# #TAB# #TAB# session.flush() #LINE# #TAB# #TAB# return binding #LINE# #TAB# return None"
"#LINE# #TAB# for fmt in ['%m/%d/%Y', '%m/%d/%Y %H:%M:%S']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return datetime.datetime.strptime(value, fmt) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return None"
#LINE# #TAB# if key in lst: #LINE# #TAB# #TAB# del lst[key] #LINE# #TAB# return lst
"#LINE# #TAB# if request.user.is_authenticated: #LINE# #TAB# #TAB# engine = create_engine(request.user.username, echo=False) #LINE# #TAB# elif request.user.is_staff: #LINE# #TAB# #TAB# engine = create_staff(request.user) #LINE# #TAB# else: #LINE# #TAB# #TAB# engine = create_engine('sqlite:///:memory:') #LINE# #TAB# return engine"
"#LINE# #TAB# for root, dirs, files in os.walk(restore_dir): #LINE# #TAB# #TAB# operative_config = files[0] #LINE# #TAB# #TAB# vm_operative_config = os.path.join(root, 'vm_operative_config') #LINE# #TAB# #TAB# if os.path.isfile(vm_operative_config): #LINE# #TAB# #TAB# #TAB# return vm_operative_config #LINE# #TAB# return None"
#LINE# #TAB# if len(trace) < 2: #LINE# #TAB# #TAB# raise ValueError('Invalid trace') #LINE# #TAB# map = {} #LINE# #TAB# for activity in trace: #LINE# #TAB# #TAB# if activity not in map: #LINE# #TAB# #TAB# #TAB# map[activity] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# map[activity] += 1 #LINE# #TAB# return map
"#LINE# #TAB# match = re.match( #LINE# #TAB# #TAB# '^([0-9]+)\\.([0-9]+)(\\.([0-9]+))?(\\.([0-9]+))?(\\.([0-9]+))?(\\.([0-9]+))?(\\.([0-9]+))?(\\.([0-9]+))?$' #LINE# #TAB#, version) #LINE# #TAB# return match.group(1) if match else None"
#LINE# #TAB# cmd = _f1_show(api) #LINE# #TAB# if cmd == 'clear': #LINE# #TAB# #TAB# return 0 #LINE# #TAB# return 1
#LINE# #TAB# return [ #LINE# #TAB# #TAB# _backup_yaml_header_string(pkg_library.header) #LINE# #TAB# #TAB# for pkg_library in pkg_libraries #LINE# #TAB# ]
"#LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# byte_lines = f.readlines() #LINE# #TAB# #TAB# file_data = dict() #LINE# #TAB# #TAB# for line in byte_lines: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = line.split() #LINE# #TAB# #TAB# #TAB# #TAB# file_data[key] = value #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return file_data"
#LINE# #TAB# for rel in model.relationships: #LINE# #TAB# #TAB# props[rel.rel_type].location = rel.location
"#LINE# #TAB# date_acid_map_list = [] #LINE# #TAB# map_file_path = os.path.join(os.path.dirname(__file__), 'data', #LINE# #TAB# #TAB# 'date_acid_maps.csv') #LINE# #TAB# with open(map_file_path, 'r') as f: #LINE# #TAB# #TAB# reader = csv.reader(f) #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# date_acid_map_list.append(row[0]) #LINE# #TAB# return date_acid_map_list"
"#LINE# #TAB# return {'name': wallet_name, 'config': {'engine': 'postgres', 'host': #LINE# #TAB# #TAB# 'localhost', 'port': 443, 'dbname': 'localhost', 'user': {'name': #LINE# #TAB# #TAB# 'localhost', 'port': 443}, 'private_key': {'name': wallet_name, #LINE# #TAB# #TAB# 'password': None, 'password': None,'server': {'name': wallet_name, #LINE# #TAB# #TAB# 'port': 443}}}"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ip = int(s) #LINE# #TAB# #TAB# if ip < 0 or ip > 255: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# flux_path = os.path.abspath(flux_path) #LINE# #TAB# if os.path.isfile(flux_path): #LINE# #TAB# #TAB# flux_file = os.path.join(flux_path, 'flux.bin') #LINE# #TAB# #TAB# cmd ='samtools flux -f %s' % flux_file #LINE# #TAB# #TAB# if not no_errors: #LINE# #TAB# #TAB# #TAB# cmd +='-c'#LINE# #TAB# #TAB# return cmd #LINE# #TAB# elif os.path.isfile(flux_path): #LINE# #TAB# #TAB# flux_cmd = 'flux -f %s' % flux_path #LINE# #TAB# #TAB# if not no_errors: #LINE# #TAB# #TAB# #TAB# cmd +='-v' #LINE# #TAB# return flux_cmd"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return api.nova.server_group_list(request) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# exceptions.handle(request, _('Unable to retrieve Nova server groups.')) #LINE# #TAB# #TAB# return []"
"#LINE# #TAB# curr_dir = os.getcwd() #LINE# #TAB# file_path = os.path.join(curr_dir, file_name) #LINE# #TAB# if force or not os.path.isfile(file_path): #LINE# #TAB# #TAB# return {} #LINE# #TAB# data = None #LINE# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# dir_path = os.path.dirname(file_path) #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# curr_dir = os.path.join(dir_path, key) #LINE# #TAB# #TAB# data[key] = curr_dir #LINE# #TAB# return data"
"#LINE# #TAB# if not path: #LINE# #TAB# #TAB# path = [] #LINE# #TAB# for key in b: #LINE# #TAB# #TAB# if key in a: #LINE# #TAB# #TAB# #TAB# if isinstance(a[key], dict) and key in a: #LINE# #TAB# #TAB# #TAB# #TAB# override_dict(a[key], b[key]) #LINE# #TAB# #TAB# #TAB# elif a[key] == b[key]: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# a[key] = b[key] #LINE# #TAB# return a"
#LINE# #TAB# for item in delta: #LINE# #TAB# #TAB# file_id = item[3] #LINE# #TAB# #TAB# if file_id is not None: #LINE# #TAB# #TAB# #TAB# yield item
"#LINE# #TAB# filter_fn = filter_fn or _default_filter #LINE# #TAB# matches = [] #LINE# #TAB# for cell in obs['cells']: #LINE# #TAB# #TAB# name = filter_fn(cell['name']) #LINE# #TAB# #TAB# if not name: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# match = units.match(name, filter_fn, owner, unit_type, tag) #LINE# #TAB# #TAB# if not match: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# matches.append(match) #LINE# #TAB# return matches"
"#LINE# #TAB# val = isinstance(pe, _bp('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('InternalEntity')) #LINE# #TAB# return val"
"#LINE# #TAB# desc = table.description() #LINE# #TAB# args = {'table': desc} #LINE# #TAB# if isinstance(table, Column): #LINE# #TAB# #TAB# args['columns'] = [column.name for column in table.columns] #LINE# #TAB# return desc"
#LINE# #TAB# if file.exists(): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# text = re.sub('---', u'-', text) #LINE# #TAB# text = re.sub('---', u'-', text) #LINE# #TAB# return text"
#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = json.loads(response.text) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# expiry = data.get('expiration') #LINE# #TAB# #TAB# if expiry == 0: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(s, list): #LINE# #TAB# #TAB# return [merge_data(v) for v in s] #LINE# #TAB# return s"
#LINE# #TAB# if not regex: #LINE# #TAB# #TAB# return urls #LINE# #TAB# for url in urls: #LINE# #TAB# #TAB# if not regex.search(url): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# warnings.warn('This function is deprecated as of OMERO 5.3.0', #LINE# #TAB# #TAB# DeprecationWarning) #LINE# #TAB# if not bs_terms or not bp_terms: #LINE# #TAB# #TAB# return False #LINE# #TAB# for idx, term in enumerate(bs_terms): #LINE# #TAB# #TAB# if term in bp_terms: #LINE# #TAB# #TAB# #TAB# if not mask_zero(term): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# cls.var_map = {} #LINE# #TAB# cls.indptr = [] #LINE# #TAB# cls.outptr = [] #LINE# #TAB# for i in range(0, 30): #LINE# #TAB# #TAB# cls.var_map[i] = {} #LINE# #TAB# #TAB# cls.indptr[i] = i #LINE# #TAB# cls.outptr = np.array([]) #LINE# #TAB# for i in range(0, 30): #LINE# #TAB# #TAB# cls.outptr[i] = np.array([]) #LINE# #TAB# cls.outptr = np.array([]) #LINE# #TAB# return cls.var_map"
"#LINE# #TAB# return ['Burst', 'EmitFromCenter', 'Velocity in fixed angle', #LINE# #TAB# #TAB# 'Random speed']"
"#LINE# #TAB# if ignore is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# last_modified = None #LINE# #TAB# for parent, dirs, files in os.walk(path): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# if file == ignore: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if os.path.isdir(os.path.join(parent, file)): #LINE# #TAB# #TAB# #TAB# #TAB# last_modified = find_role_modified_timestamp(os.path.join(parent, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# file)) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if last_modified is not None: #LINE# #TAB# #TAB# return last_modified #LINE# #TAB# return None"
"#LINE# #TAB# split_toml = input_toml.split('|') #LINE# #TAB# output_list = [] #LINE# #TAB# for element in split_toml: #LINE# #TAB# #TAB# if element == 'callback': #LINE# #TAB# #TAB# #TAB# for i in range(1, len(split_toml)): #LINE# #TAB# #TAB# #TAB# #TAB# output_list.append('') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output_list.append(element) #LINE# #TAB# return output_list"
"#LINE# #TAB# parent = code_vnl_parent(index) #LINE# #TAB# if parent is not None: #LINE# #TAB# #TAB# return u'{}{}'.format(parent, code_vnl_nice_name(index)) #LINE# #TAB# return u'{}'.format(index)"
"#LINE# #TAB# if len(image.shape) == 2: #LINE# #TAB# #TAB# mag_spectrum = image.reshape(image.shape[0], image.shape[1], 1) #LINE# #TAB# elif len(image.shape) == 3: #LINE# #TAB# #TAB# mag_spectrum = image.reshape(image.shape[0], image.shape[1], 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Image should have only 2 or 3 dimensions') #LINE# #TAB# return mag_spectrum"
#LINE# #TAB# is_history = False #LINE# #TAB# if path: #LINE# #TAB# #TAB# name = os.path.basename(path) #LINE# #TAB# #TAB# if name.endswith('.history'): #LINE# #TAB# #TAB# #TAB# is_history = True #LINE# #TAB# return is_history
#LINE# #TAB# message = exp.message() if exp.message else '' #LINE# #TAB# component = Exception(message) #LINE# #TAB# return component
#LINE# #TAB# user = get_user_by_login(login_or_id) #LINE# #TAB# if user is None: #LINE# #TAB# #TAB# return 'User not found' #LINE# #TAB# try: #LINE# #TAB# #TAB# lockfile = UserLock.objects.get(login=login_or_id) #LINE# #TAB# #TAB# lockfile.disable() #LINE# #TAB# except UserLock.DoesNotExist: #LINE# #TAB# #TAB# return 'User not found' #LINE# #TAB# else: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# diff_features = np.diff(feature) #LINE# #TAB# diff_first = diff_features[0] #LINE# #TAB# diff_second = diff_features[1] #LINE# #TAB# return diff_first, diff_second"
"#LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# line = in_handle.readline() #LINE# #TAB# m = re.match(r'^(\d+)\s+(.*)$', line) #LINE# #TAB# if m: #LINE# #TAB# #TAB# x_box = m.group(1) #LINE# #TAB# else: #LINE# #TAB# #TAB# x_box = {} #LINE# #TAB# return x_box"
#LINE# #TAB# if type(_yaml_bool_value) == bool: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# req.env.update(collector_addr=collector_addr, prefix=prefix) #LINE# #TAB# return req"
#LINE# #TAB# try: #LINE# #TAB# #TAB# data = zk.get_children(topic) #LINE# #TAB# exceptzk.NoNodeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# result = {} #LINE# #TAB# if'min-isr' in data: #LINE# #TAB# #TAB# result['isr'] = data['min-isr'] #LINE# #TAB# else: #LINE# #TAB# #TAB# result['isr'] = None #LINE# #TAB# return result
"#LINE# #TAB# min_ = None #LINE# #TAB# max_ = None #LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if val is None: #LINE# #TAB# #TAB# #TAB# min_ = val #LINE# #TAB# #TAB# elif math.isnan(val): #LINE# #TAB# #TAB# #TAB# min_ = val #LINE# #TAB# #TAB# elif math.isinf(val): #LINE# #TAB# #TAB# #TAB# max_ = val #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if min_ is None: #LINE# #TAB# #TAB# #TAB# #TAB# min_ = float('inf') #LINE# #TAB# #TAB# #TAB# if max_ is None: #LINE# #TAB# #TAB# #TAB# #TAB# max_ = float('inf') #LINE# #TAB# return min_, max_"
