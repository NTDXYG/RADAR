#LINE# #TAB# if c == '\x00': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# _g_min = float(ctypes.c_float)() #LINE# #TAB# _g_max = float(ctypes.c_float)() #LINE# #TAB# _g_chain = float(ctypes.c_int(idx_chain)) #LINE# #TAB# _bounds = (_g_min - _g_max) * 3 #LINE# #TAB# if _bounds[0]!= -1: #LINE# #TAB# #TAB# lower_bound = _bounds[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# lower_bound = _bounds[1] #LINE# #TAB# if _bounds[2]!= -1: #LINE# #TAB# #TAB# upper_bound = _bounds[2] #LINE# #TAB# else: #LINE# #TAB# #TAB# upper_bound = None #LINE# #TAB# return lower_bound, upper_bound"
"#LINE# #TAB# if not len(argv) > 1: #LINE# #TAB# #TAB# return 'gt_comand_ad_arv' #LINE# #TAB# first_arg = argv[0] #LINE# #TAB# if not first_arg.startswith('-'): #LINE# #TAB# #TAB# argv.insert(1, '--%s' % first_arg) #LINE# #TAB# cmd_name = argv[1] #LINE# #TAB# if len(argv) > 2: #LINE# #TAB# #TAB# if not argv[2]: #LINE# #TAB# #TAB# #TAB# args = argv[2:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# args = argv[2:] #LINE# #TAB# else: #LINE# #TAB# #TAB# args = argv[1:] #LINE# #TAB# return cmd_name, args"
"#LINE# #TAB# data = raw[1:] + b'\x00' #LINE# #TAB# if len(data) < 2: #LINE# #TAB# #TAB# return None #LINE# #TAB# i = raw.find(b'\x00', 0, data.find(b'\xff', 0, data.find(b'\xff', 0, #LINE# #TAB# #TAB# data.find(b'\xff', 0, data.find(b'\xff', 0, data.find(b'\xff', 0, #LINE# #TAB# #TAB# 0)))) #LINE# #TAB# if i < len(data) - 2: #LINE# #TAB# #TAB# return None #LINE# #TAB# data = data[i + 1:i + 2] #LINE# #TAB# if len(data) - 2 < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# return (data[i + 2:i + 3], data[i + 3:i + 4]) + b'\x00'"
"#LINE# #TAB# if gain > 1: #LINE# #TAB# #TAB# return _skip_play(gain, peak) #LINE# #TAB# if peak > 1: #LINE# #TAB# #TAB# return _skip_play(peak, gain - 1) #LINE# #TAB# return ''"
"#LINE# #TAB# if isinstance(e, urllib.error.HTTPError) and e.code == 404: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# tpe = None #LINE# #TAB# raw_headers = page.find_all('div', {'class': 'text-plain'}) #LINE# #TAB# for header in raw_headers: #LINE# #TAB# #TAB# if header[0] == 'name': #LINE# #TAB# #TAB# #TAB# ftype = header[1].split(';')[0] #LINE# #TAB# #TAB# #TAB# tpe = {'name': ftype} #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if tpe is None: #LINE# #TAB# #TAB# tpe = 'text/plain' #LINE# #TAB# return tpe"
"#LINE# #TAB# if cut: #LINE# #TAB# #TAB# value = cut(value) #LINE# #TAB# if maximum: #LINE# #TAB# #TAB# value = max(value) #LINE# #TAB# if minimum: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return float(value) / float(minimum) #LINE# #TAB# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if pad: #LINE# #TAB# #TAB# return float(value) #LINE# #TAB# return value"
#LINE# #TAB# try: #LINE# #TAB# #TAB# for r in pkg_resources.working_set: #LINE# #TAB# #TAB# #TAB# if r.key in dist.requires: #LINE# #TAB# #TAB# #TAB# #TAB# yield r.key #LINE# #TAB# except pkg_resources.DistributionNotFound: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# assert len(signal) >= 2 #LINE# #TAB# center = np.mean(signal) #LINE# #TAB# w = len(signal) / 2 #LINE# #TAB# half_width = w / 2 #LINE# #TAB# half_center = center - half_width #LINE# #TAB# return center, half_center"
#LINE# #TAB# o = ctx.out_object #LINE# #TAB# return o
"#LINE# #TAB# auth_user = models.User.objects( #LINE# #TAB# #TAB# username=trust_id, #LINE# #TAB# #TAB# password=None, #LINE# #TAB# #TAB# domain=settings.SITE_KEYSTONE_DOMAIN, #LINE# #TAB# ).get() #LINE# #TAB# auth_user = models.User.objects.get( #LINE# #TAB# #TAB# username=auth_user.username, #LINE# #TAB# #TAB# password=auth_user.password, #LINE# #TAB# #TAB# domain=auth_user.domain #LINE# #TAB# ).get() #LINE# #TAB# if auth_user.client is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# client = auth_user.client('keystone') #LINE# #TAB# return client"
#LINE# #TAB# r = [] #LINE# #TAB# start = 0 #LINE# #TAB# for delim in delims: #LINE# #TAB# #TAB# if delim == '\\': #LINE# #TAB# #TAB# #TAB# end = len(txt) #LINE# #TAB# #TAB# #TAB# r.append(txt[start:end]) #LINE# #TAB# #TAB# #TAB# start = end #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# r.append(delim) #LINE# #TAB# #TAB# #TAB# end = len(txt) #LINE# #TAB# return r
"#LINE# #TAB# try: #LINE# #TAB# #TAB# bool_str = str_2_hex(raw_input(prompt)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""'{0}' did not match a boolean expression (true/false, yes/no, t/f, y/n)"" #LINE# #TAB# #TAB# #TAB#.format(bool_str)) #LINE# #TAB# #TAB# return False #LINE# #TAB# hex_result = True #LINE# #TAB# while hex_result: #LINE# #TAB# #TAB# hex_result = False #LINE# #TAB# return hex_result"
"#LINE# #TAB# cmdlist = [cmd, '-d'] #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# cmdlist.append('-v') #LINE# #TAB# outfile = util.get_single_outfile(outdir, archive) #LINE# #TAB# cmdlist.extend([""-o"", outfile, archive]) #LINE# #TAB# return cmdlist"
"#LINE# #TAB# output = subprocess.check_output(['uname', '-s']) #LINE# #TAB# return output.split()[0]"
#LINE# #TAB# forest = get_minimum_spanning_ forest(graph) #LINE# #TAB# trees = [] #LINE# #TAB# for node in forest: #LINE# #TAB# #TAB# if graph.in_degree(node) > 0: #LINE# #TAB# #TAB# #TAB# if graph.out_degree(node) > 0: #LINE# #TAB# #TAB# #TAB# #TAB# trees.append(node) #LINE# #TAB# return trees
"#LINE# #TAB# command = ['DISM', 'text', '--list-instances'] #LINE# #TAB# if name_only: #LINE# #TAB# #TAB# command.append('-n') #LINE# #TAB# else: #LINE# #TAB# #TAB# command.append('-n') #LINE# #TAB# result = subprocess.check_output(command).decode('utf-8') #LINE# #TAB# instances = [] #LINE# #TAB# for line in result.split('\n'): #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# instance_id = line.split()[1] #LINE# #TAB# #TAB# instances.append({'id': instance_id, 'name': line.strip()}) #LINE# #TAB# return instances"
"#LINE# #TAB# if allow_none is True: #LINE# #TAB# #TAB# check_text_ype(objects, allowed_type, name) #LINE# #TAB# else: #LINE# #TAB# #TAB# check_text_ype(objects, allowed_type, name) #LINE# #TAB# return objects"
#LINE# #TAB# d = length / np.sqrt(q ** 2 + width ** 2 + gravity ** 2) #LINE# #TAB# if d < 1e-6: #LINE# #TAB# #TAB# return q #LINE# #TAB# elif d > 1e-6: #LINE# #TAB# #TAB# return q / d #LINE# #TAB# else: #LINE# #TAB# #TAB# return q
"#LINE# #TAB# apps = [] #LINE# #TAB# with open('dever_config.py', 'r') as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line or line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# json.loads(line) #LINE# #TAB# #TAB# #TAB# except json.decoder.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# apps.append(line.split('.')[0]) #LINE# #TAB# return apps"
"#LINE# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# input_content = json.load(f) #LINE# #TAB# #TAB# if not isinstance(input_content, dict): #LINE# #TAB# #TAB# #TAB# raise TypeError('The content of the file {} is not a dictionary'. #LINE# #TAB# #TAB# #TAB# #TAB# format(file_path)) #LINE# #TAB# #TAB# for key, value in input_content.items(): #LINE# #TAB# #TAB# #TAB# yield key, value"
"#LINE# #TAB# field_name = lookup_path.split('__', 1)[0] #LINE# #TAB# field = opts.get_field(field_name) #LINE# #TAB# if hasattr(field,'rel') and isinstance(field.rel, models.ManyToManyRel #LINE# #TAB# #TAB# ) or isinstance(field, models.related.RelatedObject #LINE# #TAB# #TAB# ) and not field.field.unique: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# del app['patc_ap'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# d = d.copy() #LINE# #TAB# if country not in d.columns: #LINE# #TAB# #TAB# return d #LINE# #TAB# df = pd.DataFrame(d[country]) #LINE# #TAB# df.columns = ['day'] #LINE# #TAB# spread = df.groupby('day')[['country']].apply(lambda x: x[1]) #LINE# #TAB# days = spread[0].sum() * timedelta(days=1) #LINE# #TAB# return days
#LINE# #TAB# unknown_fields = set(fields) - allowed_fields.keys() #LINE# #TAB# if unknown_fields: #LINE# #TAB# #TAB# raise ValueError('Unknown status: {0}'.format(unknown_fields)) #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# if field not in fields: #LINE# #TAB# #TAB# #TAB# raise ValueError('Field {0} not found in allowed_fields'.format(field)) #LINE# #TAB# return fields
#LINE# #TAB# if version: #LINE# #TAB# #TAB# parse_version_string(version) #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# header_record = HeaderRecord() #LINE# #TAB# header_record.name = file_name #LINE# #TAB# header_record.rows = row #LINE# #TAB# return header_record
#LINE# #TAB# #TAB# regex = re.compile(tag) #LINE# #TAB# #TAB# out = [] #LINE# #TAB# #TAB# for var in row: #LINE# #TAB# #TAB# #TAB# match = regex.match(var) #LINE# #TAB# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# #TAB# col = match.group(1) #LINE# #TAB# #TAB# #TAB# #TAB# out.append(col) #LINE# #TAB# #TAB# return out
#LINE# #TAB# global mid2fields #LINE# #TAB# if mid2fields is None: #LINE# #TAB# #TAB# mid2fields = {} #LINE# #TAB# return mid2fields
"#LINE# #TAB# n = len(observations) #LINE# #TAB# index = int(np.ceil(n / lag)) #LINE# #TAB# new_observations = np.empty(n, dtype=observations.dtype) #LINE# #TAB# for i in range(n - lag): #LINE# #TAB# #TAB# new_observations[i] = observations[i + lag:i + stride] #LINE# #TAB# return new_observations"
"#LINE# #TAB# if call == 'action': #LINE# #TAB# #TAB# raise SaltCloudSystemExit( #LINE# #TAB# #TAB# #TAB# 'The get_pv_header function must be called with'#LINE# #TAB# #TAB# #TAB# '-f or --function, or with the --list-pv-headers option' #LINE# #TAB# #TAB# ) #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = get_session() #LINE# #TAB# if call!= 'action': #LINE# #TAB# #TAB# raise SaltCloudSystemExit( #LINE# #TAB# #TAB# #TAB# 'The get_pv_header function must be called with -f or --function.' #LINE# #TAB# #TAB# ) #LINE# #TAB# pv_header = _query(session, 'VMware.get_pv_header', name) #LINE# #TAB# return pv_header"
#LINE# #TAB# user_db = Credentials.get_user_db() #LINE# #TAB# user = User.query.filter_by(username=username).first() #LINE# #TAB# if not user: #LINE# #TAB# #TAB# raise InvalidServiceKey('User does not exist in database') #LINE# #TAB# user = user_db.search(username=username).one() #LINE# #TAB# user_key = None #LINE# #TAB# if user_key: #LINE# #TAB# #TAB# user_key = user_db.search(password=password).one() #LINE# #TAB# #TAB# if user_key: #LINE# #TAB# #TAB# #TAB# return user_key['service_key'] #LINE# #TAB# return None
#LINE# #TAB# if len(time)!= len(signal): #LINE# #TAB# #TAB# raise ValueError('time and signal must have the same length.') #LINE# #TAB# if verb!= 'full': #LINE# #TAB# #TAB# if len(time)!= len(signal): #LINE# #TAB# #TAB# #TAB# raise ValueError('signal and time must have the same length.') #LINE# #TAB# return True
"#LINE# #TAB# element = etree.Element( #LINE# #TAB# #TAB# 'etee', #LINE# #TAB# #TAB# xmlns=""urn:oasis:names:tc:SAML:2.0:protocol"" #LINE# #TAB# ) #LINE# #TAB# for child in doc: #LINE# #TAB# #TAB# element.append(create_etee_plugin_element(child, encoding)) #LINE# #TAB# return element"
"#LINE# #TAB# tc = [] #LINE# #TAB# for i in range(0, len(data) - window_len + 1): #LINE# #TAB# #TAB# tc.append(sum(data[i:i + window_len])) #LINE# #TAB# return tc"
"#LINE# #TAB# percent_overlap = (max(max1 - min1), min(max1, min2)) / (max(max1, max2)) #LINE# #TAB# return 100 * percent_overlap"
"#LINE# #TAB# return os.path.exists(os.path.join(directory, cache_file_path) #LINE# #TAB# #TAB# ) and os.path.getmtime(os.path.join(directory, cache_file_path)) <= cache_mtime"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = operator_name() #LINE# #TAB# logger.info('Getting ID for operator {}'.format(name)) #LINE# #TAB# driver = load_driver() #LINE# #TAB# operator_id = driver.get_operator_id() #LINE# #TAB# logger.info('Found operator {} in operator {}'.format(name, #LINE# #TAB# #TAB# operator_id)) #LINE# #TAB# return operator_id"
"#LINE# #TAB# starts = [] #LINE# #TAB# for partition_dict in partition_dicts: #LINE# #TAB# #TAB# start_time = partition_dict['p'] #LINE# #TAB# #TAB# end_time = partition_dict['count'] #LINE# #TAB# #TAB# if start_time and end_time > 0: #LINE# #TAB# #TAB# #TAB# starts.append(Partition(name=partition_dict['name'], p= #LINE# #TAB# #TAB# #TAB# #TAB# partition_dict['p'], count=partition_dict['count'])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# starts.append(Partition(name=partition_dict['name'], p= #LINE# #TAB# #TAB# #TAB# #TAB# partition_dict['p'], count=partition_dict['count'])) #LINE# #TAB# return starts"
#LINE# #TAB# call_nans = [] #LINE# #TAB# for i in range(len(array)): #LINE# #TAB# #TAB# if array[i][idx_col]!= None: #LINE# #TAB# #TAB# #TAB# call_nans.append(array[i][idx_col]) #LINE# #TAB# if len(call_nans) > 0: #LINE# #TAB# #TAB# return call_nans[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'NA'
#LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# #TAB# LOG.error('Folder %s does not exist' % path) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# ret = {'name': name, #LINE# #TAB# #TAB# 'changes': {}, #LINE# #TAB# #TAB#'result': None, #LINE# #TAB# #TAB# 'comment': ''} #LINE# #TAB# if __opts__['test']: #LINE# #TAB# #TAB# ret['comment'] = 'Running traffic_manager process on all nodes in the cluster' #LINE# #TAB# #TAB# return ret #LINE# #TAB# __salt__['traffic_server.restart_scene']() #LINE# #TAB# ret['result'] = True #LINE# #TAB# ret['comment'] = 'Running traffic_server process on all nodes in the cluster' #LINE# #TAB# return ret"
#LINE# #TAB# scheme = handler_name.split('.')[-1] #LINE# #TAB# module = '.'.join(scheme.split('.')[0:-1]) #LINE# #TAB# del scheme #LINE# #TAB# module = importlib.import_module(module) #LINE# #TAB# return module
#LINE# #TAB# global _t #LINE# #TAB# if _t is None: #LINE# #TAB# #TAB# _t = {} #LINE# #TAB# return _t
#LINE# #TAB# _stoe_fro_fild = None #LINE# #TAB# if field.value() == '_': #LINE# #TAB# #TAB# _stoe_fro_fild = True #LINE# #TAB# else: #LINE# #TAB# #TAB# _stoe_fro_fild = False #LINE# #TAB# return _stoe_fro_fild
"#LINE# #TAB# if boatd is None: #LINE# #TAB# #TAB# boatd = Boatd() #LINE# #TAB# content = boatd.get('/waypoints') #LINE# #TAB# return [Point(content[i]['x'], content[i]['y']) for i in range(len( #LINE# #TAB# #TAB# content))]"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.can_handle_array = True #LINE# #TAB# function.addParameter('index_of_the_model', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN) #LINE# #TAB# function.addParameter('index_of_the_model', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was set #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# could not set parameter #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
#LINE# #TAB# fragmnt_dict = {} #LINE# #TAB# with open(fastafile) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# seq = line.strip() #LINE# #TAB# #TAB# #TAB# if not seq: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# fragmnt_dict[seq.id] = len(seq.get_fragment_lengths()) #LINE# #TAB# return fragmnt_dict
"#LINE# #TAB# result = c_int() #LINE# #TAB# lib.TCOD_gt_devie_per(byref(result), byref(result)) #LINE# #TAB# return result.value"
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# result = [] #LINE# #TAB# for key, value in dict.items(): #LINE# #TAB# #TAB# if type(value) == bytes: #LINE# #TAB# #TAB# #TAB# result.append(name_value_decode(value)) #LINE# #TAB# #TAB# elif type(value) == list: #LINE# #TAB# #TAB# #TAB# for i in value: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(name_value_decode(i)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(name_value_decode(value)) #LINE# #TAB# return result"
#LINE# #TAB# if not os.path.isfile(filename): #LINE# #TAB# #TAB# raise ValueError('Not a file: {}'.format(filename)) #LINE# #TAB# split_filename = filename.split('/') #LINE# #TAB# if len(split_filename) < 2: #LINE# #TAB# #TAB# raise ValueError('Not a Python source file: {}'.format( #LINE# #TAB# #TAB# #TAB# split_filename[0])) #LINE# #TAB# if not split_filename[1]: #LINE# #TAB# #TAB# raise ValueError('Not a Python source file: {}'.format(split_filename[1])) #LINE# #TAB# return True
"#LINE# #TAB# ""non-byline authors"" #LINE# #TAB# detail = detail.lower() #LINE# #TAB# bit_transfers = [] #LINE# #TAB# for author in soup.find_all(""author""): #LINE# #TAB# #TAB# bit_transfers.append(form_bit_transfer(author, detail)) #LINE# #TAB# return bit_transfers"
"#LINE# #TAB# from.modules import browserify #LINE# #TAB# if not babelify: #LINE# #TAB# #TAB# return False #LINE# #TAB# if export_as is None: #LINE# #TAB# #TAB# export_as = entry_point + '.js' #LINE# #TAB# with open(output_file, 'w') as out_file: #LINE# #TAB# #TAB# source = browserify(entry_point, babelify=babelify) #LINE# #TAB# #TAB# source = [source] #LINE# #TAB# path = os.path.dirname(output_file) #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# with open(path, 'w') as out_file: #LINE# #TAB# #TAB# yaml.dump(source, out_file, default_flow_style=False) #LINE# #TAB# return True"
"#LINE# #TAB# barcode = re.sub('[\\s]{2,}','', barcode) #LINE# #TAB# barcode = re.sub('[\\s]{2,}','', barcode) #LINE# #TAB# barcode = re.sub('[\\s]{2,}','', barcode) #LINE# #TAB# return [ #LINE# #TAB# #TAB# barcode, #LINE# #TAB# #TAB# re.sub('^[^a-z]*', '', barcode #LINE# #TAB# #TAB# ), #LINE# #TAB# #TAB# re.sub('^[^a-z]*', '', barcode #LINE# #TAB# #TAB# ), #LINE# #TAB# #TAB# re.sub('[\\s]{2,}','', barcode #LINE# #TAB# #TAB# ), #LINE# #TAB# #TAB# re.sub('^[^a-z]*', '', barcode #LINE# #TAB# #TAB# ), #LINE# #TAB# ]"
#LINE# #TAB# family_matri = m.copy() #LINE# #TAB# for row in range(m.shape[1]): #LINE# #TAB# #TAB# val = [float(m[row][0]) for row in range(m.shape[0])] #LINE# #TAB# #TAB# for col in range(m.shape[1]): #LINE# #TAB# #TAB# #TAB# family_matri.data[row][col] = val[0] #LINE# #TAB# family_matri.data[0] = family_matri.data[0] #LINE# #TAB# family_matri.data[1] = family_matri.data[1] #LINE# #TAB# return family_matri
"#LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.gaussian_p, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# if not os.path.exists(file_to_align): #LINE# #TAB# #TAB# return None #LINE# #TAB# clustered_fn = file_to_align + '.fasta' #LINE# #TAB# cmd ='mafft -c {0} -o {1}'.format(clustered_fn, file_to_align) #LINE# #TAB# p = subprocess.Popen(cmd, stdout=subprocess.PIPE) #LINE# #TAB# stdout, stderr = p.communicate() #LINE# #TAB# if stderr: #LINE# #TAB# #TAB# warnings.warn(stderr) #LINE# #TAB# #TAB# return None #LINE# #TAB# return stdout"
"#LINE# #TAB# encode = unicodedata.normalize('NFKD', value) #LINE# #TAB# if isinstance(encode, unicode): #LINE# #TAB# #TAB# encode = encode.encode('$') #LINE# #TAB# return encode"
#LINE# #TAB# hints = get_moderate_hints(forum) #LINE# #TAB# n_topics = len(hints) #LINE# #TAB# if n_topics == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# required_topics = get_required_topics(forum) #LINE# #TAB# for t in required_topics: #LINE# #TAB# #TAB# if t not in hints: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# for t in hints: #LINE# #TAB# #TAB# if t in hints[0]: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# return n_topics
#LINE# #TAB# if not sp.issparse(array): #LINE# #TAB# #TAB# return False #LINE# #TAB# if array.shape[0]!= array.shape[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# return {'__kind__': kind_inst, 'class': fqname_for(v.__class__), 'args': #LINE# #TAB# #TAB# encode([v])}"
"#LINE# #TAB# return {'info': {'active': True},'response': {'type': 'application/json', #LINE# #TAB# #TAB# 'headers': {'Content-Type': 'application/json', 'Accept': #LINE# #TAB# #TAB# 'application/json', 'User-Agent': 'v1/%s (%s %s) %s/%s' % (__program_name__, #LINE# #TAB# #TAB# __version__, platform.system(), platform.release(), platform. #LINE# #TAB# #TAB# python_implementation(), platform.python_version()}, 'components': [{'name': #LINE# #TAB# #TAB# 'ai','version': '0.0.0'}}, {'name': 'tornado','version': #LINE# #TAB# #TAB# 'json-api', 'components': [{'name': 'ai','version': '0.0.0'}}, {'name': #LINE# #TAB# #TAB# 'ai', 'components': [{'name': 'ai','version': '0.0.0'}]}]}"
#LINE# #TAB# curret_resources = {} #LINE# #TAB# for file in os.listdir(repo): #LINE# #TAB# #TAB# if '.curret' in file: #LINE# #TAB# #TAB# #TAB# curret_resources[file] = repo.get_git_ref(file) #LINE# #TAB# return curret_resources
#LINE# #TAB# chunks = [] #LINE# #TAB# for node in graph_object: #LINE# #TAB# #TAB# for edge_data in graph_object[node]: #LINE# #TAB# #TAB# #TAB# chunk = {} #LINE# #TAB# #TAB# #TAB# chunk['label'] = graph_identifier + '_' + str(edge_data[0]) #LINE# #TAB# #TAB# #TAB# if edge_data[1] == node: #LINE# #TAB# #TAB# #TAB# #TAB# chunk['relation'] = edge_data[2] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# chunk['relation'] = edge_data[1] #LINE# #TAB# #TAB# #TAB# chunks.append(chunk) #LINE# #TAB# return chunks
"#LINE# #TAB# client = descriptor['client'] #LINE# #TAB# address = descriptor['address'] #LINE# #TAB# if not client.accept(address): #LINE# #TAB# #TAB# return None, address #LINE# #TAB# errors = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# client.accept(address) #LINE# #TAB# #TAB# except errors as e: #LINE# #TAB# #TAB# #TAB# errors.append(e) #LINE# #TAB# #TAB# address = None #LINE# #TAB# return client, address"
"#LINE# #TAB# from geometry_msgs.msg import Transform #LINE# #TAB# q = np.array([[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0], [-1.0, #LINE# #TAB# #TAB# 0.0, 0.0]]) #LINE# #TAB# r = Transform() #LINE# #TAB# r.set_dual_quat(dual_quat) #LINE# #TAB# return r"
"#LINE# #TAB# if all(isinstance(b, ndarray) for b in blocks): #LINE# #TAB# #TAB# for d in blocks: #LINE# #TAB# #TAB# #TAB# if d.shape[0] == d.shape[1]: #LINE# #TAB# #TAB# #TAB# #TAB# blocks[d.ravel()] = 0 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# blocks[d.ravel()] += 1 #LINE# #TAB# if all(isinstance(b, ndarray) for b in blocks): #LINE# #TAB# #TAB# for d in blocks: #LINE# #TAB# #TAB# #TAB# if d.shape[0]!= d.shape[1]: #LINE# #TAB# #TAB# #TAB# #TAB# blocks[d.ravel()] = 0 #LINE# #TAB# return blocks"
#LINE# #TAB# b = next(a) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield b #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# b = next(a) #LINE# #TAB# return b
#LINE# #TAB# target = klass(attributes) #LINE# #TAB# if previous_object is not None: #LINE# #TAB# #TAB# target = previous_object.import_target(target) #LINE# #TAB# return target
"#LINE# #TAB# settings = current_app.config.get('KNX_PRETT_SETTINGS', {}) #LINE# #TAB# if current_app.debug: #LINE# #TAB# #TAB# settings.setdefault('indent', 4) #LINE# #TAB# #TAB# settings.setdefault('sort_keys', not PY3) #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = make_response(dumped, code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
#LINE# #TAB# idx = subdomain_df.index.get_level_values(0) #LINE# #TAB# subdomains = subdomain_df.iloc[idx] #LINE# #TAB# subdomains.index = idx + 1 #LINE# #TAB# return subdomains
#LINE# #TAB# if len(key)!= 32: #LINE# #TAB# #TAB# raise ValueError('invalid key length %d' % len(key)) #LINE# #TAB# b3 = base64.b64encode(key).decode('utf-8') #LINE# #TAB# padding = b3[len(b3) // 4:] #LINE# #TAB# b3 = b3 + padding + b3[len(b3) // 4:] #LINE# #TAB# return b3
"#LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
#LINE# #TAB# node = conn.create_node(name) #LINE# #TAB# while True: #LINE# #TAB# #TAB# opt = conn.describe_vm(name) #LINE# #TAB# #TAB# if opt['Target']: #LINE# #TAB# #TAB# #TAB# node.append(opt['Target']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return node
"#LINE# #TAB# image = cut_image(imgcol, collection, date, validate=validate) #LINE# #TAB# return image"
"#LINE# #TAB# if re_search_category_assert_vald.match(line): #LINE# #TAB# #TAB# yield 0, line"
#LINE# #TAB# i = 0 #LINE# #TAB# while i < len(lines): #LINE# #TAB# #TAB# line = lines[i] #LINE# #TAB# #TAB# c = line[-1] #LINE# #TAB# #TAB# if c == '\\': #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# #TAB# yield line #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield c #LINE# #TAB# #TAB# #TAB# i += 1
#LINE# #TAB# if tz is None: #LINE# #TAB# #TAB# tz = dateutil.tz.gettz('UTC') #LINE# #TAB# if relPeriod == 'today': #LINE# #TAB# #TAB# return dateutil.tz.localize(datetime.datetime.now(tz=tz)) #LINE# #TAB# elif relPeriod =='monthly': #LINE# #TAB# #TAB# return dateutil.tz.localize(datetime.datetime.now(tz=tz) - timedelta(days=1) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# elif relPeriod == 'yesterday': #LINE# #TAB# #TAB# return dateutil.tz.localize(datetime.datetime.now(tz=tz)) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError
"#LINE# #TAB# mdf = h5py.File(path, 'r') #LINE# #TAB# mdf['model'] = np.array(mdf['model']) #LINE# #TAB# mdf['weights'] = np.array(mdf['weights']) #LINE# #TAB# return mdf"
"#LINE# #TAB# return getattr(sys, 'frozen', False) is not None and os.environ.get( #LINE# #TAB# #TAB# 'COCOTB_ANSI_OUTPUT', 1 #LINE# #TAB# #TAB# ) == 1"
#LINE# #TAB# try: #LINE# #TAB# #TAB# float(s) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if type(c)!= tuple: #LINE# #TAB# #TAB# c = [c] #LINE# #TAB# n = len(c) #LINE# #TAB# if n < 3: #LINE# #TAB# #TAB# raise ValueError('invalid color') #LINE# #TAB# if c[0]!= '#' or c[1]!= '#': #LINE# #TAB# #TAB# raise ValueError('invalid color') #LINE# #TAB# r, g, b, a = c[:2], c[2:4], c[4:] #LINE# #TAB# if a < 1: #LINE# #TAB# #TAB# r += 1 #LINE# #TAB# g += 1 #LINE# #TAB# b += 1 #LINE# #TAB# return r / 255.0, g / 255.0, b / 255.0"
"#LINE# #TAB# n = X.shape[0] #LINE# #TAB# t = np.arange(1, int(n) + 1) * w #LINE# #TAB# for i in range(1, n): #LINE# #TAB# #TAB# if X[i] > theta: #LINE# #TAB# #TAB# #TAB# t[i] = t[i - 1] #LINE# #TAB# return t"
"#LINE# #TAB# if dd.get_align_bam(data) == ""train"": #LINE# #TAB# #TAB# samples = dd.get_align_bam(data) #LINE# #TAB# #TAB# out = [] #LINE# #TAB# #TAB# for sample in samples: #LINE# #TAB# #TAB# #TAB# ref = dd.get_ref(sample) #LINE# #TAB# #TAB# #TAB# out.append(""{}:{}"".format(sample, ref)) #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return data"
"#LINE# #TAB# F = 1 #LINE# #TAB# for field in analysis_fields: #LINE# #TAB# #TAB# if field in data_frame.columns: #LINE# #TAB# #TAB# #TAB# F += 1 #LINE# #TAB# result = {} #LINE# #TAB# for row in data_frame: #LINE# #TAB# #TAB# tmp = data_frame[row].copy() #LINE# #TAB# #TAB# d = result.get(row, None) #LINE# #TAB# #TAB# if d is not None: #LINE# #TAB# #TAB# #TAB# result[row] = d #LINE# #TAB# return result"
"#LINE# #TAB# if key == 'latitude': #LINE# #TAB# #TAB# x = float(url.split('/')[3]) #LINE# #TAB# #TAB# y = float(url.split('/')[4]) #LINE# #TAB# #TAB# radius = float(radius) / 60 #LINE# #TAB# #TAB# return x, y #LINE# #TAB# elif key == 'longitude': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# y = float(url.split('/')[5]) #LINE# #TAB# #TAB# #TAB# radius = float(url.split('/')[4]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return url #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return url + key, radius #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page in site.interwikis_pages(total=total): #LINE# #TAB# #TAB# yield page
"#LINE# #TAB# with open(os.devnull, 'w') as devnull: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = json.load(devnull) #LINE# #TAB# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# return 'Not found' #LINE# #TAB# #TAB# if data.get('password') is None: #LINE# #TAB# #TAB# #TAB# return 'No password provided' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return data['password']"
"#LINE# #TAB# header = numpy.zeros(image.shape, dtype=numpy.uint8) #LINE# #TAB# header[:, :, (0)] = image[:, :, (1)] #LINE# #TAB# header[:, :, (2)] = image[:, :, (3)] #LINE# #TAB# return header"
"#LINE# #TAB# if isinstance(e.op, F): #LINE# #TAB# #TAB# op = e.op #LINE# #TAB# #TAB# if op.type == 'UnaryOp': #LINE# #TAB# #TAB# #TAB# return coose_weekday_nod(e.args[0]) #LINE# #TAB# #TAB# return e.args[0] #LINE# #TAB# return e"
"#LINE# #TAB# if hasattr(artist, 'root'): #LINE# #TAB# #TAB# artist.root = kwargs['root'] #LINE# #TAB# else: #LINE# #TAB# #TAB# artist.root = kwargs['root'] #LINE# #TAB# return artist"
#LINE# #TAB# uniq = set() #LINE# #TAB# for element in iterable: #LINE# #TAB# #TAB# if element in uniq: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# uniq.add(element) #LINE# #TAB# #TAB# yield element
"#LINE# #TAB# new_env = {} #LINE# #TAB# for lib in libs: #LINE# #TAB# #TAB# method = env.get(lib, None) #LINE# #TAB# #TAB# if method is not None: #LINE# #TAB# #TAB# #TAB# new_env[method] = os.path.join(env[lib], method) #LINE# #TAB# for k, v in new_env.items(): #LINE# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# libs[k] = [susbt_lib(v) for v in v] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_env[k] = v #LINE# #TAB# return new_env"
#LINE# #TAB# if value: #LINE# #TAB# #TAB# return '1' #LINE# #TAB# return '0'
"#LINE# #TAB# if as_id.count('-') == 1: #LINE# #TAB# #TAB# as_id = str(as_id) #LINE# #TAB# #TAB# parts = as_id.split('-') #LINE# #TAB# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# #TAB# return parts[0], parts[1] #LINE# #TAB# return None, as_id"
"#LINE# #TAB# if opts is None: #LINE# #TAB# #TAB# opts = {} #LINE# #TAB# root, ext = os.path.splitext(path) #LINE# #TAB# if ext == '.pem': #LINE# #TAB# #TAB# ext = '.pem' #LINE# #TAB# if ext == '.pem': #LINE# #TAB# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# with open(path, 'rb') as fp: #LINE# #TAB# #TAB# #TAB# #TAB# ext = '.pem' #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# gpgutils.verify_key(fp, opts) #LINE# #TAB# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# global _MAX_PENDING_INDICES #LINE# #TAB# _MAX_PENDING_INDICES = limit
#LINE# #TAB# api = get_api() #LINE# #TAB# with catch_raise_api_exception(): #LINE# #TAB# #TAB# if api.get_repo(orgrepo) == 'fork': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False
#LINE# #TAB# col = get_color(colorname) #LINE# #TAB# rgb = np.array([(int(x * alpha) / 255.0) for x in col]) #LINE# #TAB# return rgb
"#LINE# #TAB# user_port = default_port #LINE# #TAB# host = default_user #LINE# #TAB# if '@' in host_string: #LINE# #TAB# #TAB# user, port = host_string.split('@') #LINE# #TAB# else: #LINE# #TAB# #TAB# host = host_string #LINE# #TAB# if ':' in host: #LINE# #TAB# #TAB# host, port = host.split(':') #LINE# #TAB# try: #LINE# #TAB# #TAB# port = int(port) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# port = default_port #LINE# #TAB# return [(host, port, user)]"
"#LINE# #TAB# groups = [] #LINE# #TAB# old_groups = [] #LINE# #TAB# with open(f, 'r') as f_obj: #LINE# #TAB# #TAB# for line in f_obj: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# old_groups.append(line) #LINE# #TAB# return old_groups"
"#LINE# #TAB# if f is None: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# if '.' in f: #LINE# #TAB# #TAB# return float(f), None #LINE# #TAB# else: #LINE# #TAB# #TAB# return f, 0.0"
"#LINE# #TAB# if not isinstance(value, list): #LINE# #TAB# #TAB# raise TypeError('piel_2d_delete requires a list') #LINE# #TAB# if not all(isinstance(x, int) for x in value): #LINE# #TAB# #TAB# raise TypeError('piel_2d_delete requires an integer value') #LINE# #TAB# return value"
#LINE# #TAB# result = {} #LINE# #TAB# if p_date: #LINE# #TAB# #TAB# result['year'] = p_date.year #LINE# #TAB# #TAB# result['month'] = p_date.month #LINE# #TAB# #TAB# result['day'] = p_date.day #LINE# #TAB# #TAB# result['hour'] = p_date.hour #LINE# #TAB# #TAB# result['minute'] = p_date.minute #LINE# #TAB# #TAB# result['second'] = p_date.second #LINE# #TAB# #TAB# result['microsecond'] = p_date.microsecond #LINE# #TAB# return result
"#LINE# #TAB# properties_dict = {} #LINE# #TAB# for i in range(0, len(sql_raw_list), 2): #LINE# #TAB# #TAB# properties_dict[sql_raw_list[i]] = sql_raw_list[i + 1] #LINE# #TAB# return properties_dict"
"#LINE# #TAB# poly = poly.flatten() #LINE# #TAB# for i in range(len(bounds) - 1): #LINE# #TAB# #TAB# b = bounds[i] #LINE# #TAB# #TAB# within_poly = aea_intersection(poly, b, algorithm) #LINE# #TAB# #TAB# if within_poly: #LINE# #TAB# #TAB# #TAB# yield i #LINE# #TAB# if len(poly) > 1: #LINE# #TAB# #TAB# for i in range(len(bounds) - 1): #LINE# #TAB# #TAB# #TAB# yield i"
#LINE# #TAB# global _stat_runer_perm #LINE# #TAB# _stat_runer_perm = False
#LINE# #TAB# for line in text.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line and not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if line in MAPPINGS: #LINE# #TAB# #TAB# #TAB# #TAB# yield MAPPINGS[line] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# yield line
#LINE# #TAB# notebook_extension = get_notebook_extension() #LINE# #TAB# notebook_extension.collect_path = False
"#LINE# #TAB# P = sp.eye(B.shape[0]) - sp.eye(C.shape[0]) #LINE# #TAB# r = sp.empty((B.shape[0], C.shape[0]), dtype=bool) #LINE# #TAB# for i in range(A.shape[0]): #LINE# #TAB# #TAB# r[i] = projct_r(B[i], C[i]) #LINE# #TAB# return r"
#LINE# #TAB# n_atoms = len(atom_map[center_data['atom_name']]) #LINE# #TAB# return n_atoms
"#LINE# #TAB# names = [] #LINE# #TAB# for k in tree: #LINE# #TAB# #TAB# new_k = k #LINE# #TAB# #TAB# if isinstance(tree[k], dict): #LINE# #TAB# #TAB# #TAB# parent_key = list(k.keys())[0] #LINE# #TAB# #TAB# #TAB# new_k = construct_object(tree[parent_key]) #LINE# #TAB# #TAB# #TAB# names.append(new_k) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# names.append(new_k) #LINE# #TAB# return names"
"#LINE# #TAB# fields = line.rstrip().split('\t') #LINE# #TAB# return fields[1], fields[2]"
"#LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# if isinstance(rule, Comment): #LINE# #TAB# #TAB# #TAB# yield rule.xml:id"
#LINE# #TAB# for path in dl_paths: #LINE# #TAB# #TAB# for url in url_dict[path]: #LINE# #TAB# #TAB# #TAB# if url in path: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# data = object_fro_seet_df(df, station_name) #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return data #LINE# #TAB# return [data]"
"#LINE# #TAB# if isinstance(op, ops.Selection): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# table = op.selections #LINE# #TAB# #TAB# return [table, name] #LINE# #TAB# elif isinstance(op, ops.Aggregation): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [table, name] #LINE# #TAB# elif isinstance(op, ops.AggregationSet): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [table, name] #LINE# #TAB# elif isinstance(op, ops.AggregationSet): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [table, name] #LINE# #TAB# else: #LINE# #TAB# #TAB# return op.args"
"#LINE# #TAB# modules = [] #LINE# #TAB# has_error = False #LINE# #TAB# for module_name in module_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# modules.append(import_module(module_name)) #LINE# #TAB# #TAB# #TAB# has_error = True #LINE# #TAB# except ImportError as e: #LINE# #TAB# #TAB# logging.error('Error importing %s', module_name) #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# for module in modules: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# download_module(module) #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# logging.error('Error importing %s', module_name) #LINE# #TAB# return has_error, download_module"
#LINE# #TAB# multi = bytes() #LINE# #TAB# n = len(parameters) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# val = bytes([parameters[i][0]] * n) #LINE# #TAB# #TAB# val += bytes([parameters[i][1]] * n) #LINE# #TAB# #TAB# val += bytes([parameters[i][2]] * n) #LINE# #TAB# return multi
"#LINE# #TAB# for path in VENV_DIRS: #LINE# #TAB# #TAB# for root, dirs, files in os.walk(path): #LINE# #TAB# #TAB# #TAB# dirs[:] = [d for d in dirs if not os.path.isdir(os.path.join(root, d))] #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# config = loads(f.read()) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield {'name': filename, 'channel': config['channel']}"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# bool_val = fd.read(1) #LINE# #TAB# #TAB# #TAB# if not bool_val: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return int(bool_val), bool_val"
#LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(str(value)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# value = None #LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(str(value)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# value = None #LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# if value == None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# value = Decimal(str(value)) #LINE# #TAB# if value == None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return value
#LINE# #TAB# backend = _get_backend(name) #LINE# #TAB# if backend is None: #LINE# #TAB# #TAB# backend = gt_resources()[0] #LINE# #TAB# return backend
#LINE# #TAB# item = list(item) #LINE# #TAB# for i in item[0:-1]: #LINE# #TAB# #TAB# if i == 'dimensions': #LINE# #TAB# #TAB# #TAB# item[0] = process_grup_in_dim(i) #LINE# #TAB# #TAB# elif i =='meta': #LINE# #TAB# #TAB# #TAB# item[0] = process_grup_in_meta(i) #LINE# #TAB# #TAB# elif i == 'value_meta': #LINE# #TAB# #TAB# #TAB# item[i] = process_grup_in_value_meta(i) #LINE# #TAB# return item
"#LINE# #TAB# if not _sched: #LINE# #TAB# #TAB# return None #LINE# #TAB# sched = [] #LINE# #TAB# for key, value in iteritems(_sched): #LINE# #TAB# #TAB# if key not in sched: #LINE# #TAB# #TAB# #TAB# sched.append((key, value)) #LINE# #TAB# #TAB# elif isinstance(_sched[key], tuple): #LINE# #TAB# #TAB# #TAB# for i, value in enumerate(_sched[key]): #LINE# #TAB# #TAB# #TAB# #TAB# sched.append((key, i + 1)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sched.append((key, value)) #LINE# #TAB# return sched"
"#LINE# #TAB# encoded = rlp[start:start + 4] #LINE# #TAB# length = len(encoded) - 4 #LINE# #TAB# return encoded, length"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return socket.gethostbyname(hostname) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# return fallback
#LINE# #TAB# cfg = ConfigParser() #LINE# #TAB# cfg_path = os.path.abspath(file_path) #LINE# #TAB# conf_file = os.path.expanduser(cfg_path) #LINE# #TAB# if os.path.isfile(conf_file) and force: #LINE# #TAB# #TAB# config_parser.read(conf_file) #LINE# #TAB# else: #LINE# #TAB# #TAB# config_parser.read(conf_file) #LINE# #TAB# return cfg
"#LINE# #TAB# flux_vec_sum = vec_uniform_calc_weight_flu(flux_vec, biomass_index) #LINE# #TAB# return flux_vec_sum / flux_vec_sum"
"#LINE# #TAB# image = numpy.array(image, dtype=numpy.uint8) #LINE# #TAB# matrix = color_correlation_svd_sqrt * image.shape #LINE# #TAB# outline = numpy.zeros(image.shape, dtype=numpy.uint8) #LINE# #TAB# outline[:, :, (0)] = image #LINE# #TAB# outline[:, :, (1)] = color_correlation_svd_sqrt #LINE# #TAB# outline[:, :, (2)] = matrix #LINE# #TAB# return outline"
#LINE# #TAB# for x in store: #LINE# #TAB# #TAB# if x[0] == path or x[1] == path: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# dtypes = [x.dtype for x in scalars_or_arrays] #LINE# #TAB# out = pd.DataFrame( #LINE# #TAB# #TAB# scalars=dtypes, #LINE# #TAB# #TAB# columns=scalars_or_arrays.columns, #LINE# #TAB# #TAB# index=scalars_or_arrays.index, #LINE# #TAB# #TAB# dtype=x.dtype #LINE# #TAB# ) #LINE# #TAB# return out"
#LINE# #TAB# try: #LINE# #TAB# #TAB# status = request.db.health_check() #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# abort(json.dumps({'error': str(e)}) #LINE# #TAB# if status: #LINE# #TAB# #TAB# return jsonify({'status': status}) #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'status':'success'}
#LINE# #TAB# norm_string = string #LINE# #TAB# lines = [] #LINE# #TAB# for _ in range(length): #LINE# #TAB# #TAB# line = norm_string.splitlines() #LINE# #TAB# #TAB# if len(line) > 0: #LINE# #TAB# #TAB# #TAB# lines.append(line[0]) #LINE# #TAB# #TAB# #TAB# norm_string = '\n'.join(lines) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lines.append(line) #LINE# #TAB# return lines
"#LINE# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# if cb: #LINE# #TAB# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# setup_win_socket(cb) #LINE# #TAB# return sock"
"#LINE# #TAB# for path, method in environ['PATH_INFO'].iteritems(): #LINE# #TAB# #TAB# if path.endswith('/'): #LINE# #TAB# #TAB# #TAB# path = path[:-1] #LINE# #TAB# #TAB# final_path = environ.get('PATH_INFO', final_path) #LINE# #TAB# #TAB# #TAB# if not final_path.endswith('/'): #LINE# #TAB# #TAB# #TAB# #TAB# final_path = final_path[:-1] #LINE# #TAB# #TAB# #TAB# start_response('404 %s %s' % (method, final_path)) #LINE# #TAB# return ['404']"
"#LINE# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# lines = f[()] #LINE# #TAB# return lines"
"#LINE# #TAB# tot_actions = len(traces) #LINE# #TAB# if weight_vec is not None: #LINE# #TAB# #TAB# weights = weight_vec #LINE# #TAB# else: #LINE# #TAB# #TAB# weights = np.ones(tot_actions, dtype=np.float64) #LINE# #TAB# root = np.zeros((dims, tot_actions)) #LINE# #TAB# for i in range(dims): #LINE# #TAB# #TAB# root[i] = np.random.randn(weights) #LINE# #TAB# return root"
#LINE# #TAB# if uids is None: #LINE# #TAB# #TAB# uids = {} #LINE# #TAB# for pif in pifs: #LINE# #TAB# #TAB# if pif['uid'] in uids: #LINE# #TAB# #TAB# #TAB# pif['uids'] = uids[pif['uid']] #LINE# #TAB# else: #LINE# #TAB# #TAB# for uid in uids: #LINE# #TAB# #TAB# #TAB# pif['uids'] = [uid]
"#LINE# #TAB# absolute_row = [] #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# absolute_row.append(crate_absolut_field(layer, field)) #LINE# #TAB# return absolute_row"
"#LINE# #TAB# ret = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True) #LINE# #TAB# ret = ret.decode('utf-8') #LINE# #TAB# return ret"
"#LINE# #TAB# value = z(data, loperand, roperand, is_not) #LINE# #TAB# if value == '': #LINE# #TAB# #TAB# return False #LINE# #TAB# if extras: #LINE# #TAB# #TAB# value = apply_extras(value, extras) #LINE# #TAB# return value"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return value
"#LINE# #TAB# if sample_width > 255: #LINE# #TAB# #TAB# pad = sample_width - 255 #LINE# #TAB# #TAB# ret = struct.pack('>L', 0, sample_width) + buf #LINE# #TAB# else: #LINE# #TAB# #TAB# ret = struct.pack('>L', 0, sample_width) + buf #LINE# #TAB# return ret"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type = typ.type #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(arg, bytes) and bytes(arg).decode('utf-8') == arg: #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type(typ, arg) #LINE# #TAB# except abi_type.NotEncodable: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# rv = {} #LINE# #TAB# for attr, val in vars(obj).items(): #LINE# #TAB# #TAB# if not attr.startswith('_'): #LINE# #TAB# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# #TAB# val = [as_knx(item) for item in val] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# val.append(as_knx(val)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# rv[attr] = val #LINE# #TAB# return rv"
"#LINE# #TAB# io = '' #LINE# #TAB# layer = band['layer'] #LINE# #TAB# filename = str(band['filename']) #LINE# #TAB# if layer in netCDF4._file_types: #LINE# #TAB# #TAB# io += f'{filename}:{layer}' #LINE# #TAB# elif filename == 'netcdf': #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# for name, layer in zip(filename, layer): #LINE# #TAB# #TAB# #TAB# io += f'${filename}:{layer}' #LINE# #TAB# return io"
"#LINE# #TAB# imgae_float = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# imgae_float = f['data'][0][0] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return imgae_float"
#LINE# #TAB# host_name = exc_type.__module__ + '.' + exc_type.__name__ #LINE# #TAB# HostError._hosts.append(host_name) #LINE# #TAB# return exc_type
"#LINE# #TAB# encode = {} #LINE# #TAB# for key, val in props.items(): #LINE# #TAB# #TAB# if key == 'email': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# encode[key] = leaf_module_encode_ref(val) #LINE# #TAB# #TAB# elif isinstance(val, dict): #LINE# #TAB# #TAB# #TAB# encode[key] = leaf_module_encode_ref(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encode[key] = val #LINE# #TAB# if username is not None: #LINE# #TAB# #TAB# props['username'] = username #LINE# #TAB# return props"
"#LINE# #TAB# index = engine.execute( #LINE# #TAB# #TAB# 'SELECT * FROM {} WHERE table_name = %s AND index_name = %s' #LINE# #TAB# #TAB#, (table_name, index_name)) #LINE# #TAB# if index: #LINE# #TAB# #TAB# index.name = index_name #LINE# #TAB# #TAB# return index #LINE# #TAB# item = engine.execute( #LINE# #TAB# #TAB# 'SELECT * FROM {} WHERE table_name = %s AND index_name = %s' #LINE# #TAB# #TAB#, (table_name, index_name)) #LINE# #TAB# if item: #LINE# #TAB# #TAB# item.execute( #LINE# #TAB# #TAB# #TAB# 'SELECT * FROM {} WHERE index_name = %s' % (table_name, index_name)) #LINE# #TAB# #TAB# return item[0] #LINE# #TAB# return None"
"#LINE# #TAB# if len(chunk) < 12: #LINE# #TAB# #TAB# raise ValueError('bad datagram length') #LINE# #TAB# datagrm_len = chunk[0] & 15 #LINE# #TAB# datagrm = chunk[1] & 127 | chunk[2] << 8 | chunk[3] #LINE# #TAB# if datagrm_len!= 0: #LINE# #TAB# #TAB# if datagrm == 0: #LINE# #TAB# #TAB# #TAB# raise ValueError('bad datagrm') #LINE# #TAB# #TAB# datagrm = -datagrm #LINE# #TAB# rec = logRecord(datagrm_len, datagrm, chunk[4:]) #LINE# #TAB# return rec"
#LINE# #TAB# if not installed_packages: #LINE# #TAB# #TAB# installed_packages = [] #LINE# #TAB# installed_packages.append(package) #LINE# #TAB# for installed_package in installed_packages: #LINE# #TAB# #TAB# if not find_package(installed_package): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return tuple(map(int, s)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return s"
#LINE# #TAB# objec = gl_gen_function(n) #LINE# #TAB# gl_gen_function('advance') #LINE# #TAB# if objec == 1: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return objec - 1
#LINE# #TAB# for char in sentence: #LINE# #TAB# #TAB# if char.isspace(): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# soup = BeautifulSoup(html, 'lxml') #LINE# #TAB# for link in soup.find_all('a', href=True): #LINE# #TAB# #TAB# if link.get('href').startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# resources = link.get('href').split('/') #LINE# #TAB# #TAB# if resources: #LINE# #TAB# #TAB# #TAB# yield resources"
"#LINE# #TAB# dx = np.abs(data[0] - spacing[0]) #LINE# #TAB# dy = np.abs(data[1] - spacing[1]) #LINE# #TAB# if packed: #LINE# #TAB# #TAB# x, y = 0, 0 #LINE# #TAB# elif data[0] > data[1]: #LINE# #TAB# #TAB# x = data[0] - spacing[0] #LINE# #TAB# #TAB# y = data[1] - spacing[1] #LINE# #TAB# return dx, dy, x, y"
"#LINE# #TAB# global root #LINE# #TAB# if root is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# root = torch.device('/dev/ttyACM*') #LINE# #TAB# #TAB# except (NameError, torch.DeviceNotFoundError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return root"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# stageId = _get_stage_id(restApiId, stageName) #LINE# #TAB# #TAB# conn.flush() #LINE# #TAB# #TAB# return {'stageId': stageId} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'error': __utils__['boto3.get_error'](e)}"
#LINE# #TAB# x = time_series.mean(axis=0) #LINE# #TAB# std = (time_series - x.mean()) ** 2 #LINE# #TAB# return 1 - std
"#LINE# #TAB# requirements = [] #LINE# #TAB# if not hasattr(cls, '_children'): #LINE# #TAB# #TAB# return [cls] #LINE# #TAB# for child in cls._children: #LINE# #TAB# #TAB# requirements += serialize_prime_requirements(child) #LINE# #TAB# return requirements"
#LINE# #TAB# ncc = 0 #LINE# #TAB# for model in models: #LINE# #TAB# #TAB# ncc += stochastic_ncc(model) #LINE# #TAB# return ncc / ncc
"#LINE# #TAB# R = numpy.asarray(R) #LINE# #TAB# Omega = numpy.asarray(Omega) #LINE# #TAB# assert len(R) == 3 #LINE# #TAB# S = Omega.shape[0] #LINE# #TAB# P = numpy.zeros((len(S), 3)) #LINE# #TAB# P[0] = R[0] * Omega[0] #LINE# #TAB# P[1] = R[1] * Omega[1] #LINE# #TAB# P[2] = R[2] * Omega[2] #LINE# #TAB# return P"
#LINE# #TAB# names = string.split(sep) #LINE# #TAB# names.sort() #LINE# #TAB# return names
#LINE# #TAB# url = equiv_url(url_parts) #LINE# #TAB# is_insecure = not url.path.endswith('/') #LINE# #TAB# if not is_insecure: #LINE# #TAB# #TAB# url = url[:-1] #LINE# #TAB# return url
"#LINE# #TAB# soup = Trimesh() #LINE# #TAB# for i in range(face_count): #LINE# #TAB# #TAB# soup.faces = [random.choice(faces) for j in range(0, face_count)] #LINE# #TAB# return soup"
"#LINE# #TAB# with open(movie_path, 'rb') as f: #LINE# #TAB# #TAB# movie_json = json.load(f) #LINE# #TAB# image_uuid = movie_json['image_uuid'] #LINE# #TAB# x = int(image_uuid[:8]), int(image_uuid[8:10]), int(image_uuid[10:12] #LINE# #TAB# #TAB# ) #LINE# #TAB# y = int(image_uuid[12:16]) #LINE# #TAB# return x, y"
"#LINE# #TAB# ref_url = urlparse(flask.request.host_url) #LINE# #TAB# test_url = urlparse(urljoin(flask.request.host_url, target)) #LINE# #TAB# return test_url.scheme in ('http', 'https' #LINE# #TAB# #TAB# ) and ref_url.netloc == test_url.netloc"
#LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(['git','status'], stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# print('ERROR: File not found:'+ str(e)) #LINE# #TAB# #TAB# p.wait() #LINE# #TAB# #TAB# if p.returncode == 0: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return p.stdout.decode('utf-8').rstrip('\n') #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print('ERROR:'+ str(e)) #LINE# #TAB# #TAB# return None"
#LINE# #TAB# country_code = None #LINE# #TAB# for key in profile['addresses']: #LINE# #TAB# #TAB# if 'country_code' in key: #LINE# #TAB# #TAB# #TAB# country_code = key['country_code'] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if country_code is not None: #LINE# #TAB# #TAB# return country_code #LINE# #TAB# return None
#LINE# #TAB# if child.node_type!= parent.node_type: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not child.name or not parent.name: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif parent.name == child.node_type and not child.children: #LINE# #TAB# #TAB# return False #LINE# #TAB# for child_node in child.children: #LINE# #TAB# #TAB# if not child_node.is_eventual: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# for name, field in iter_fields(node): #LINE# #TAB# #TAB# if isinstance(field, Fortran): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# ite_func_links(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield name, field"
"#LINE# #TAB# ierr = c_int() #LINE# #TAB# lib.sdl_rewrite_vin(vin, byref(ierr)) #LINE# #TAB# if ierr.value!= 0: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return ierr.value"
"#LINE# #TAB# attributes = parse_preferences(file, preferences) #LINE# #TAB# used = {} #LINE# #TAB# for attr in attributes: #LINE# #TAB# #TAB# if isinstance(attr, str): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# attr = attributes[attr] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# used[attr] = attr #LINE# #TAB# return used"
"#LINE# #TAB# new_text = text #LINE# #TAB# while new_text.find('#')!= -1: #LINE# #TAB# #TAB# new_text = new_text.replace('#', '#') #LINE# #TAB# #TAB# new_text = new_text.replace('&gt;', '>') #LINE# #TAB# return new_text"
"#LINE# #TAB# if not hasattr(request,'reqes'): #LINE# #TAB# #TAB# return None #LINE# #TAB# res = {} #LINE# #TAB# for key in request.reqes: #LINE# #TAB# #TAB# if key.startswith('__'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# val = getattr(request, key) #LINE# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# res[key] = as_reques(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res[key] = val #LINE# #TAB# return res"
"#LINE# #TAB# blob = get_blob(issue_id, talker) #LINE# #TAB# ssue_line = generate_ssue_line(blob) #LINE# #TAB# create_issue_line(filename, issue_id, talker) #LINE# #TAB# return ssue_line"
"#LINE# #TAB# return [ #LINE# #TAB# #TAB# f""{invariants[0].name}: {invariants[1].code}, {invariants[2].name}: {invariants[3]};"" #LINE# #TAB# #TAB# for invariants in invariants #LINE# #TAB# ]"
"#LINE# #TAB# for l in lines: #LINE# #TAB# #TAB# n = 0 #LINE# #TAB# #TAB# while n < len(l): #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# if l[0] == '#': #LINE# #TAB# #TAB# #TAB# l = l[1:] #LINE# #TAB# #TAB# if not l[0].isdigit(): #LINE# #TAB# #TAB# #TAB# l = '-' + l #LINE# #TAB# #TAB# yield n, l"
"#LINE# #TAB# if value == 0 or isinstance(value, basestring) and value.strip() == '0': #LINE# #TAB# #TAB# value = None #LINE# #TAB# return value"
"#LINE# #TAB# if isinstance(d, list): #LINE# #TAB# #TAB# return [choose_mat(i) for i in d] #LINE# #TAB# elif isinstance(d, dict): #LINE# #TAB# #TAB# return {k: choose_mat(v) for k, v in d.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return d"
"#LINE# #TAB# original_audio_bin = np.array(original_audio_bin) #LINE# #TAB# decoded_audio_bin = np.array(decoded_audio_bin) #LINE# #TAB# height = np.max(np.abs(original_audio_bin.shape[0])) #LINE# #TAB# width = np.max(np.abs(decoded_audio_bin.shape[0])) #LINE# #TAB# return original_audio_bin, decoded_audio_bin, height, width"
"#LINE# #TAB# configured = [] #LINE# #TAB# for prefix in prefixes: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# config = getattr(orig, prefix) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(config, dict): #LINE# #TAB# #TAB# #TAB# configured.extend(config) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# configured.append(prefix) #LINE# #TAB# return configured"
#LINE# #TAB# unpacked_records = [] #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# name = record.name #LINE# #TAB# #TAB# if name in ame_unpacked_records: #LINE# #TAB# #TAB# #TAB# unpacked_records.append(ame_unpacked_records[name]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unpacked_records.append(suffix) #LINE# #TAB# return unpacked_records
"#LINE# #TAB# ts_validate(ws, proj) #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass"
#LINE# #TAB# source_lists = [token_list for token_list in token_lists if len( #LINE# #TAB# #TAB# token_list) > num_topics] #LINE# #TAB# source_lists.sort() #LINE# #TAB# return [source_list[0] for source_list in source_lists]
#LINE# #TAB# arg_spec = _get_cached_arg_spec(fn) #LINE# #TAB# if arg_spec.defaults: #LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# f'{fn.__name__}.{arg_spec.args[0]}' #LINE# #TAB# #TAB# #TAB# for arg in arg_spec.defaults #LINE# #TAB# #TAB# ] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# f'{fn.__name__}.{arg_spec.name}' #LINE# #TAB# #TAB# #TAB# for f in arg_spec.args #LINE# #TAB# #TAB# ]
"#LINE# #TAB# if using_cpu_only or job_name not in ('serving', 'learner'): #LINE# #TAB# #TAB# return '' #LINE# #TAB# if job_name =='serving': #LINE# #TAB# #TAB# return gpu_id #LINE# #TAB# proxy_instance_id = '{}_{}_{}'.format(gpu_id, job_name, time.strftime( #LINE# #TAB# #TAB# '%Y%m%d%H%M%S')) #LINE# #TAB# return proxy_instance_id"
"#LINE# #TAB# assert os.path.isfile(executable) #LINE# #TAB# pah_encode_exe = os.path.join(os.path.dirname(__file__), executable) #LINE# #TAB# if not os.path.exists(pah_encode_exe): #LINE# #TAB# #TAB# raise PahEncodeException('{} not found: {}'.format(pah_encode_exe, #LINE# #TAB# #TAB# #TAB# executable)) #LINE# #TAB# return pah_encode_exe"
#LINE# #TAB# print('Please enter the type of recording directory (default: %s)' % #LINE# #TAB# #TAB# file_dir) #LINE# #TAB# have_schedule = input('Please enter the type of recording directory (default: %s)' #LINE# #TAB# #TAB# % file_dir) #LINE# #TAB# if len(have_schedule)!= 1: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Please enter the type of recording directory (default: %s)' % #LINE# #TAB# #TAB# #TAB# have_schedule) #LINE# #TAB# return have_schedule[0]
#LINE# #TAB# for char in value: #LINE# #TAB# #TAB# if char in unquoted: #LINE# #TAB# #TAB# #TAB# return char #LINE# #TAB# return value
#LINE# #TAB# import requests #LINE# #TAB# try: #LINE# #TAB# #TAB# response = requests.get(GA_URL) #LINE# #TAB# #TAB# response.raise_for_status() #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# rs = compile(line, filename, 'exec') #LINE# #TAB# #TAB# #TAB# for d in definitions: #LINE# #TAB# #TAB# #TAB# #TAB# rs = compile(r, d, filename, 'exec') #LINE# #TAB# #TAB# #TAB# rs.write('\n') #LINE# #TAB# return"
"#LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode cautious --opt-mode=optN,' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# models = clyngor.solve_from_grounded(grounding, options=options) #LINE# #TAB# best_model = None #LINE# #TAB# for model in models.discard_quotes.by_arity.with_optimization: #LINE# #TAB# #TAB# best_model = model #LINE# #TAB# return best_model"
#LINE# #TAB# results = [] #LINE# #TAB# if text: #LINE# #TAB# #TAB# form_text = text.strip() #LINE# #TAB# #TAB# lines = form_text.split('\n') #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# if line and not line.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# results.append(base_class(line.strip())) #LINE# #TAB# #TAB# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# results.append(base_class()) #LINE# #TAB# #TAB# return results #LINE# #TAB# return []
#LINE# #TAB# iterable = iter(iterable) #LINE# #TAB# while True: #LINE# #TAB# #TAB# if wrap: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield next(iterable) #LINE# #TAB# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# yield next(iterable) #LINE# #TAB# #TAB# if iterable[-1] == step: #LINE# #TAB# #TAB# #TAB# break
"#LINE# #TAB# global _sart_updat_t #LINE# #TAB# ifupdater is None: #LINE# #TAB# #TAB# updater = UPDATEER #LINE# #TAB# _sart_updat_t = time.time() #LINE# #TAB# thread = threading.Thread(target=_sart_updat_t, args=(updater,)) #LINE# #TAB# thread.daemon = True #LINE# #TAB# _sart_updat_t.start() #LINE# #TAB# return"
#LINE# #TAB# if len(data) > 1: #LINE# #TAB# #TAB# return data[mask] #LINE# #TAB# else: #LINE# #TAB# #TAB# return data[mask]
"#LINE# #TAB# if filepath is None: #LINE# #TAB# #TAB# filepath = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# #TAB# '.fydarc') #LINE# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# config = yaml.safe_load(f) #LINE# #TAB# return config"
"#LINE# #TAB# string = re.sub('(.)([A-Z][a-z]+)', '\\1_\\2', string) #LINE# #TAB# string = re.sub('([a-z0-9])([A-Z])', '\\1_\\2', string) #LINE# #TAB# string = re.sub('([a-z0-9])([A-Z])', '\\1_\\2', string) #LINE# #TAB# string = re.sub('([a-z0-9])([A-Z])', '\\1_\\2', string) #LINE# #TAB# return string"
"#LINE# #TAB# for f in os.listdir(dirname): #LINE# #TAB# #TAB# if os.path.isfile(os.path.join(dirname, f)): #LINE# #TAB# #TAB# #TAB# card = cars_frm_dr(os.path.join(dirname, f)) #LINE# #TAB# #TAB# #TAB# if card: #LINE# #TAB# #TAB# #TAB# #TAB# yield card"
#LINE# #TAB# fig = plt.figure() #LINE# #TAB# ax = fig.add_subplot(111) #LINE# #TAB# ax.set_title('Building blocks...') #LINE# #TAB# b = [] #LINE# #TAB# for i in blocks: #LINE# #TAB# #TAB# if len(b) == 1: #LINE# #TAB# #TAB# #TAB# b.append(plot_build_blck(b[0])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# b.append(plot_build_blck(b[0])) #LINE# #TAB# fig.set_tight_layout(True) #LINE# #TAB# return fig
"#LINE# #TAB# if isinstance(data, dict): #LINE# #TAB# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# #TAB# if k in cls.cntents_fields: #LINE# #TAB# #TAB# #TAB# #TAB# yield cls.cntents_fields[k] #LINE# #TAB# elif isinstance(data, list): #LINE# #TAB# #TAB# for v in data: #LINE# #TAB# #TAB# #TAB# for k, v in cls.contains_cntents(v): #LINE# #TAB# #TAB# #TAB# #TAB# yield k #LINE# #TAB# elif isinstance(data, tuple): #LINE# #TAB# #TAB# for v in data: #LINE# #TAB# #TAB# #TAB# yield v"
#LINE# #TAB# if not content_types: #LINE# #TAB# #TAB# return 'application/json' #LINE# #TAB# content_types = [x.lower() for x in content_types] #LINE# #TAB# for content_type in content_types: #LINE# #TAB# #TAB# if content_type.startswith('image/'): #LINE# #TAB# #TAB# #TAB# return 'image/%s' % content_type #LINE# #TAB# return 'application/json'
#LINE# #TAB# logger_menu = [] #LINE# #TAB# for c in class_.__bases__: #LINE# #TAB# #TAB# logger_menu.extend(set_logger_menu(c)) #LINE# #TAB# return logger_menu
"#LINE# #TAB# times = [] #LINE# #TAB# for i, t in enumerate(score): #LINE# #TAB# #TAB# if t == 5: #LINE# #TAB# #TAB# #TAB# times.append(COLORS_PREA[i]) #LINE# #TAB# #TAB# #TAB# t = 0 #LINE# #TAB# #TAB# elif t == 6: #LINE# #TAB# #TAB# #TAB# times.append(COLORS_POSTA[i]) #LINE# #TAB# #TAB# #TAB# t = 0 #LINE# #TAB# return times"
"#LINE# #TAB# model = None #LINE# #TAB# if path and args['model']: #LINE# #TAB# #TAB# model = getattr(args['model'], path) #LINE# #TAB# #TAB# del args['model'] #LINE# #TAB# return model"
"#LINE# #TAB# base_dir = os.path.abspath(base_dir) #LINE# #TAB# df = red_ds(ddf_id, base_dir=base_dir) #LINE# #TAB# return df"
"#LINE# #TAB# proc = psutil.Process(os.getpid(), stdout=subprocess.PIPE, stderr= #LINE# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# has_cose = False #LINE# #TAB# while not has_cose: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# proc.stdin.close() #LINE# #TAB# #TAB# except Exception as ex: #LINE# #TAB# #TAB# #TAB# if hasattr(ex, 'fileno'): #LINE# #TAB# #TAB# #TAB# #TAB# has_cose = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if fd is not None: #LINE# #TAB# #TAB# proc.stdin.close() #LINE# #TAB# #TAB# proc.stdout.flush() #LINE# #TAB# return has_cose"
#LINE# try: #LINE# #TAB# return obj.has_variable(variable) #LINE# except AttributeError: #LINE# #TAB# return False
#LINE# #TAB# if cls._tbale_profile is None: #LINE# #TAB# #TAB# cls._tbale_profile = cls.get_profile_name() #LINE# #TAB# schema = cls.get_schema() #LINE# #TAB# if schema: #LINE# #TAB# #TAB# return schema.table_name #LINE# #TAB# return cls._tbale_profile
"#LINE# #TAB# code3 = max(code1, ord('a')) #LINE# #TAB# code4 = min(code2, ord('z') + 1) #LINE# #TAB# if code3 < code4: #LINE# #TAB# #TAB# d = ord('A') - ord('a') #LINE# #TAB# #TAB# return code3 + d - 1 #LINE# #TAB# elif code3 < code4: #LINE# #TAB# #TAB# return code4 #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# mapping = [] #LINE# #TAB# for item in list_: #LINE# #TAB# #TAB# if isinstance(item, tuple): #LINE# #TAB# #TAB# #TAB# mapping.extend(ast_mapping(item)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# index = ast_arg(item) #LINE# #TAB# #TAB# #TAB# mapping.append((index, item)) #LINE# #TAB# return mapping"
#LINE# #TAB# parsed = statement.split('\n') #LINE# #TAB# return [s for s in parsed if s!= '']
"#LINE# #TAB# pick = """" #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = q_get() #LINE# #TAB# #TAB# #TAB# if obj is not None: #LINE# #TAB# #TAB# #TAB# #TAB# pick = str(obj) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return pick"
#LINE# #TAB# client = boto3.client('ec2') #LINE# #TAB# services = client.list_services() #LINE# #TAB# for service in services['items']: #LINE# #TAB# #TAB# if service['name'] == name: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# res = socket.AF_INET #LINE# #TAB# try: #LINE# #TAB# #TAB# res = socket.AF_INET6 #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res = socket.AF_INET #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if address.startswith('[') and address.endswith(']') or not address. #LINE# #TAB# #TAB# startswith('['): #LINE# #TAB# #TAB# res = socket.AF_INET6 #LINE# #TAB# return res
"#LINE# #TAB# url = ( #LINE# #TAB# #TAB# method #LINE# #TAB# #TAB# + urlencode({ #LINE# #TAB# #TAB# #TAB#'method': method, #LINE# #TAB# #TAB# #TAB# 'args': args #LINE# #TAB# #TAB# }) #LINE# #TAB# if args: #LINE# #TAB# #TAB# params = urlencode(args) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return hash(params) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# return params #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return url"
#LINE# #TAB# tree = OdfTree() #LINE# #TAB# tree.parse(infile) #LINE# #TAB# return tree
"#LINE# #TAB# with open(_config_file, 'r') as f: #LINE# #TAB# #TAB# key = None #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# key = f.read() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if key is not None: #LINE# #TAB# #TAB# return key[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# t.value = float(t.value) #LINE# #TAB# return t
"#LINE# #TAB# if _PY2 and not isinstance(s, str): #LINE# #TAB# #TAB# return s #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse(s) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(num) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# return num"
#LINE# #TAB# if val is True: #LINE# #TAB# #TAB# return '1' #LINE# #TAB# elif val is False: #LINE# #TAB# #TAB# return '0' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '0'
#LINE# #TAB# write_svg_filename = ai_filename #LINE# #TAB# for i_ol in ai_figure: #LINE# #TAB# #TAB# if type(i_ol) == list: #LINE# #TAB# #TAB# #TAB# for r_ol in i_ol: #LINE# #TAB# #TAB# #TAB# #TAB# write_svg_filename = '{:s}.svg'.format(r_ol) #LINE# #TAB# #TAB# #TAB# #TAB# write_svg_filename = '{:s}.svg'.format(ai_filename) #LINE# #TAB# #TAB# #TAB# #TAB# write_svg_filename = '{:s}.svg'.format(ai_filename) #LINE# #TAB# return write_svg_filename
#LINE# #TAB# try: #LINE# #TAB# #TAB# cls.get_required_code_templates(template) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValidationError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# _, src = os.path.split(filename) #LINE# #TAB# #TAB# dst = os.path.join(dst, src) #LINE# #TAB# #TAB# unlink(src, dst) #LINE# #TAB# return"
"#LINE# #TAB# a = r * 2.0 #LINE# #TAB# b = np.zeros((a.shape[0], a.shape[1], 1)) #LINE# #TAB# a[0] = 0.0 #LINE# #TAB# b[1] = 1.0 #LINE# #TAB# return a"
"#LINE# #TAB# here = Path.home() / '.bcml' #LINE# #TAB# if not here.exists(): #LINE# #TAB# #TAB# here.mkdir(parents=True, exist_ok=True) #LINE# #TAB# _report_files = here / 'bcml' #LINE# #TAB# return _report_files"
"#LINE# #TAB# freqs = np.arange(0, length + 1) * delta_f #LINE# #TAB# mask = freqs > low_freq_cutoff #LINE# #TAB# return freqs[mask]"
#LINE# #TAB# rate = ET.Element('rate') #LINE# #TAB# rate.text = fps #LINE# #TAB# return rate
"#LINE# #TAB# cls = this_class() #LINE# #TAB# setattr(cls, '_group_name', group_name) #LINE# #TAB# return cls"
"#LINE# #TAB# string = string.replace('_', '') #LINE# #TAB# string = string.replace(' ', '_') #LINE# #TAB# for char in string: #LINE# #TAB# #TAB# string = string.replace(char, '_') #LINE# #TAB# return string"
"#LINE# #TAB# if re.match('^[a-z]+$', geometry): #LINE# #TAB# #TAB# return 'polygon' #LINE# #TAB# if re.match('^[a-z]+$', geometry): #LINE# #TAB# #TAB# return 'polygon' #LINE# #TAB# return None"
"#LINE# #TAB# data = data[0:num_epochs] #LINE# #TAB# for batch in range(num_epochs): #LINE# #TAB# #TAB# train_data = mx.nd.array(data[batch:batch_size]) #LINE# #TAB# #TAB# test_data = mx.nd.array(data[batch:batch_size + 1]) #LINE# #TAB# #TAB# test_seq = mx.nd.array(data[batch:batch_size + 2]) #LINE# #TAB# #TAB# train_seq = mx.nd.concatenate((train_data, test_seq)) #LINE# #TAB# #TAB# test_seq = mx.nd.array(test_seq) #LINE# #TAB# #TAB# yield train_seq, test_seq"
#LINE# #TAB# sr = [s] #LINE# #TAB# cur = 0 #LINE# #TAB# while cur < len(s): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# #TAB# sr.append(s[cur:i]) #LINE# #TAB# #TAB# #TAB# cur += 1 #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return sr
"#LINE# #TAB# config, args, params = gt_quote_resolve_file(file_name) #LINE# #TAB# return config, args, params"
#LINE# #TAB# if letter in _letter_table: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = db.get_session() #LINE# #TAB# ret = session.query(obj).filter_by(**skip).first() #LINE# #TAB# return ret
"#LINE# #TAB# V, D = G #LINE# #TAB# edge_lists = ((u, v) for u, v in V) #LINE# #TAB# incoming_edges = ((u, v) for u, v in D) #LINE# #TAB# return edge_lists, incoming_edges"
"#LINE# #TAB# result = [] #LINE# #TAB# n = 0 #LINE# #TAB# for start, quantity in sorted(d.items()): #LINE# #TAB# #TAB# if width > n: #LINE# #TAB# #TAB# #TAB# pad_str = b'\x00' * (width - n) #LINE# #TAB# #TAB# #TAB# result.append((start, end, quantity)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pad_str = b'\x00' * (width - n) #LINE# #TAB# #TAB# #TAB# result.append((start, pad_str, quantity)) #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# return result"
"#LINE# #TAB# if reponame == reposave and path: #LINE# #TAB# #TAB# return os.path.join(path, 'issues') #LINE# #TAB# elif reponame == None and reposave == None: #LINE# #TAB# #TAB# return os.path.join(path, 'issues') #LINE# #TAB# elif reponame == None and reposave == None: #LINE# #TAB# #TAB# return os.path.join(path, 'issues') #LINE# #TAB# elif reponame and reposave: #LINE# #TAB# #TAB# return os.path.join(path, 'issues') #LINE# #TAB# else: #LINE# #TAB# #TAB# return os.path.join(path, 'issues') + os.path.join(reposave, '.' + #LINE# #TAB# #TAB# #TAB# reponame) + os.path.join(path, 'issues')"
"#LINE# #TAB# result = {} #LINE# #TAB# for item in value.split('#'): #LINE# #TAB# #TAB# parts = item.split('=') #LINE# #TAB# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# #TAB# result[parts[0]] = int(parts[1], 16) #LINE# #TAB# #TAB# elif len(parts) == 3: #LINE# #TAB# #TAB# #TAB# key = parts[0] #LINE# #TAB# #TAB# #TAB# val = parts[1] #LINE# #TAB# #TAB# #TAB# result[key] = val #LINE# #TAB# return result"
#LINE# #TAB# if not'milestone_properties' in holder: #LINE# #TAB# #TAB# holder['milestone_properties'] = MilestoneProperties() #LINE# #TAB# return holder
"#LINE# #TAB# for child in walk(node): #LINE# #TAB# #TAB# if isinstance(child, astroid.Field): #LINE# #TAB# #TAB# #TAB# if not child.docstring: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if contains_docstring(child): #LINE# #TAB# #TAB# #TAB# #TAB# yield child"
#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shell = get_ipython().__class__.__name__ #LINE# #TAB# #TAB# #TAB# if shell == 'ZMQInteractiveShell': #LINE# #TAB# #TAB# #TAB# #TAB# return Truecolor #LINE# #TAB# #TAB# except NameError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if shell == 'TerminalInteractiveShell': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shell = get_ipython().__class__.__name__ #LINE# #TAB# #TAB# #TAB# if shell == 'TerminalInteractiveShell': #LINE# #TAB# #TAB# #TAB# #TAB# return Truecolor #LINE# #TAB# #TAB# except NameError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False
"#LINE# #TAB# if cls._validator_condition is None: #LINE# #TAB# #TAB# raise ImproperlyConfigured( #LINE# #TAB# #TAB# #TAB# ""Must specify a valid condition for an Amazon S3 Bucket"") #LINE# #TAB# elif cls._validator_condition == 'aws4_request': #LINE# #TAB# #TAB# raise ImproperlyConfigured( #LINE# #TAB# #TAB# #TAB# ""Must specify a aws4_request object (e.g. 'aws4_response.aws4_request_object')"" #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# elif cls._validator_condition == 'aws_secret_access_key': #LINE# #TAB# #TAB# raise ImproperlyConfigured( #LINE# #TAB# #TAB# #TAB# ""Must specify either a valid aws_secret_access_key (e.g. 'aws4_secret_access_key' or 'aws_secret_secret_access_key')"" #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return cls._validator_condition"
"#LINE# #TAB# if mod_name is None: #LINE# #TAB# #TAB# mod_name = filepath #LINE# #TAB# module = imp.new_module(mod_name) #LINE# #TAB# try: #LINE# #TAB# #TAB# exec_(module.__file__, os.stat(filepath)) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# out = {} #LINE# #TAB# for k, v in src.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# out[k] = merge_dit_keys_encode_bytes(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out[k] = v #LINE# #TAB# return out"
"#LINE# #TAB# subparser.add_argument( #LINE# #TAB# #TAB# ""--download"", #LINE# #TAB# #TAB# type=str, #LINE# #TAB# #TAB# required=False, #LINE# #TAB# #TAB# choices=['jpg', 'jpeg', 'png', 'tiff', 'gif']) #LINE# #TAB# subparser.add_argument( #LINE# #TAB# #TAB# ""--duration"", #LINE# #TAB# #TAB# type=int, #LINE# #TAB# #TAB# default=None, #LINE# #TAB# #TAB# choices=['1s', '2s', '2s', '3s', '4s']) #LINE# #TAB# return None"
#LINE# #TAB# artile_name_list = [] #LINE# #TAB# artile_name = raw_parser.artile_name(soup) #LINE# #TAB# if artile_name: #LINE# #TAB# #TAB# artile_name_list.append(artile_name.text) #LINE# #TAB# return artile_name_list
"#LINE# #TAB# languages = getattr(settings, 'LANGUAGES', None) #LINE# #TAB# if languages: #LINE# #TAB# #TAB# app_settings = apps.get_app_config(force=True) #LINE# #TAB# #TAB# languages_list = app_settings.get('languags', []) #LINE# #TAB# #TAB# for l in languages_list: #LINE# #TAB# #TAB# #TAB# lang = l.split(':')[0] #LINE# #TAB# #TAB# #TAB# settings.LANGUAGES[lang] = lang #LINE# #TAB# return"
#LINE# #TAB# try: #LINE# #TAB# #TAB# winreg.CloseKey(key) #LINE# #TAB# except WindowsError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# try: #LINE# #TAB# #TAB# return request.query['access_token'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if 'wit_link' in json: #LINE# #TAB# #TAB# json['wit_link'] = Link(json['wit_link']) #LINE# #TAB# #TAB# return json #LINE# #TAB# elif isinstance(json, dict): #LINE# #TAB# #TAB# return {k: gen_ojects_wit_link(v) for k, v in json.items()} #LINE# #TAB# elif isinstance(json, list): #LINE# #TAB# #TAB# return [gen_ojects_wit_link(v) for v in json] #LINE# #TAB# else: #LINE# #TAB# #TAB# return json"
"#LINE# #TAB# if header in ('CONTENT_TYPE', 'CONTENT_LENGTH', 'CONTENT_LENGTH'): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if not isinstance(inp, numbers.Number): #LINE# #TAB# #TAB# check_value(inp) #LINE# #TAB# #TAB# raise error.BASICError(err) #LINE# #TAB# return inp"
#LINE# #TAB# cid = int(cid) #LINE# #TAB# if cid not in _reference_read_child_map: #LINE# #TAB# #TAB# return '' #LINE# #TAB# label = _reference_read_child_map[cid]['label'] #LINE# #TAB# if label is not None: #LINE# #TAB# #TAB# return label #LINE# #TAB# return ''
"#LINE# #TAB# bnds_path = os.getenv(envkey, '') #LINE# #TAB# bigdataset_path = os.path.join(bnds_path, 'bigdata') #LINE# #TAB# files = os.listdir(bigdataset_path) #LINE# #TAB# if len(files) == 0: #LINE# #TAB# #TAB# raise ValueError('No big data found.') #LINE# #TAB# bigdataset_folder = os.path.split(bigdataset_path)[0] #LINE# #TAB# nearest_big_datasets = [] #LINE# #TAB# for file in files: #LINE# #TAB# #TAB# if os.path.splitext(file)[1] == '.fastq': #LINE# #TAB# #TAB# #TAB# nearest_big_datasets.append(os.path.relpath(os.path.join(bigdataset_path, #LINE# #TAB# #TAB# #TAB# #TAB# file), bigdataset_path)) #LINE# #TAB# return nearest_big_datasets"
#LINE# #TAB# assert cfg.name not in _reference_cfg_registry #LINE# #TAB# _reference_cfg_registry[cfg.name] = cfg #LINE# #TAB# return cfg
"#LINE# #TAB# try: #LINE# #TAB# #TAB# c = compile(source, name, 'eval') #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# c = compile(source, name, 'exec') #LINE# #TAB# if c.co_consts: #LINE# #TAB# #TAB# return c.co_consts[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return c.co_expr"
"#LINE# #TAB# global _dtype_spellceck_initial #LINE# #TAB# _dtype_spellceck_initial = np.dtype(int(os.path.basename(build_dir)).replace( #LINE# #TAB# #TAB# '\\', '/')) #LINE# #TAB# return _dtype_spellceck_initial"
#LINE# #TAB# i = 0 #LINE# #TAB# o = '' #LINE# #TAB# while i < len(s): #LINE# #TAB# #TAB# o += s[i] #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# return o
"#LINE# #TAB# d = {} #LINE# #TAB# for k, v in yw.items(): #LINE# #TAB# #TAB# d[k] = v #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# d[k] = container_depth(v) #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, tuple): #LINE# #TAB# #TAB# #TAB# d[k] = container_depth(v) #LINE# #TAB# return d"
"#LINE# #TAB# if registry is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# key = registry.CreateKey(registry.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Valve\\Steam') #LINE# #TAB# return registry.QueryValueEx(key, 'SteamURL')[0]"
#LINE# #TAB# #TAB# el.value = value #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del el.selinux #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# for var_name, var_type in program_ast.variables.items(): #LINE# #TAB# #TAB# if var_type == 'Name': #LINE# #TAB# #TAB# #TAB# return var_name"
"#LINE# #TAB# if prev_consensus_hashes is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# ake_snapsht_hash = cls._generate_snapsht_hash( record_root_hash ) #LINE# #TAB# for state_hash, consensus_hash in prev_consensus_hashes.items(): #LINE# #TAB# #TAB# ake_snapsht_hash = ake_snapsht_hash.encode(encoding='utf-8') #LINE# #TAB# #TAB# if ake_snapsht_hash!= record_root_hash: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return ake_snapsht_hash"
#LINE# #TAB# if a.name!= b.name: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# tool_dir = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# json_file = os.path.join(tool_dir, 'data.json') #LINE# #TAB# with open(json_file, 'r') as json_file: #LINE# #TAB# #TAB# data = json.load(json_file) #LINE# #TAB# return data"
"#LINE# #TAB# return {'condition_button': button.condition, 'button_text': #LINE# #TAB# #TAB# button.text, 'dismiss_button': button.dismiss_button, 'condition': #LINE# #TAB# #TAB# button.condition}"
"#LINE# #TAB# if name not in app.apps: #LINE# #TAB# #TAB# app.apps[name] = {} #LINE# #TAB# for k, v in app.apps.items(): #LINE# #TAB# #TAB# if isinstance(v, App): #LINE# #TAB# #TAB# #TAB# app.apps[k] = flatten_using_site(v, name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# app.apps[k] = v"
"#LINE# #TAB# win = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) #LINE# #TAB# win.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# win.bind(('', 0)) #LINE# #TAB# return win"
"#LINE# #TAB# store_regons = [] #LINE# #TAB# alt_file = tz.get_in([""config"", ""algorithm"", ""store_contigs""], data) #LINE# #TAB# if not alt_file: #LINE# #TAB# #TAB# return store_regons #LINE# #TAB# for chrom in data: #LINE# #TAB# #TAB# start = tz.get_in([""config"", ""algorithm"", ""store_contigs""], data) #LINE# #TAB# #TAB# end = tz.get_in([""config"", ""algorithm"", ""store_contigs""], data) #LINE# #TAB# #TAB# if start and end: #LINE# #TAB# #TAB# #TAB# store_regons.append(""{0}:{1}"".format(chrom, end)) #LINE# #TAB# return store_regons"
#LINE# #TAB# if 'ndarray' not in sys.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# if np.nanmax(np.abs(np.fft.fftfreq(1)) - np.fft.fftfreq(0.1)) > np.finfo( #LINE# #TAB# #TAB# np.float64).eps: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# path = os.path.join(git_folder, 'HEAD') #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(['git','rev-parse', '--show-toplevel'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# shell=True) #LINE# #TAB# #TAB# return p.communicate()[0].strip() #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if isinstance(x, str): #LINE# #TAB# #TAB# return int(x) #LINE# #TAB# try: #LINE# #TAB# #TAB# return datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return x"
"#LINE# #TAB# ret = [] #LINE# #TAB# totalLength = len(fileContent) #LINE# #TAB# if totalLength!= len(fileContent): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Incorrect length of data provided. Must be less than {} bytes.' #LINE# #TAB# #TAB# #TAB#.format(totalLength)) #LINE# #TAB# for i in range(0, totalLength): #LINE# #TAB# #TAB# temp = bytearray(fileContent[i:i + 2]) #LINE# #TAB# #TAB# temp[0:2] = temp[2:2 + 2] #LINE# #TAB# #TAB# temp[2:4] = temp[4:8] #LINE# #TAB# #TAB# ret.append(temp) #LINE# #TAB# return ret"
"#LINE# #TAB# import os #LINE# #TAB# path = os.path.dirname(__file__) #LINE# #TAB# points_file = os.path.join(path, 'EDS_Points.csv') #LINE# #TAB# data = pd.read_csv(points_file, sep='\t', index_col=0) #LINE# #TAB# data.columns = ['Point1', 'Point2', 'Point3', 'Point4', 'Point5', #LINE# #TAB# #TAB# 'Point6', 'Point7', 'Point8', 'Point9', 'Point10', 'Point11', 'Point12'] #LINE# #TAB# return data"
"#LINE# #TAB# found_class = None #LINE# #TAB# for c in module.__dict__.values(): #LINE# #TAB# #TAB# if c[1] == class_string: #LINE# #TAB# #TAB# #TAB# found_class = c #LINE# #TAB# if found_class is None: #LINE# #TAB# #TAB# raise Exception('Class ""' + class_string + '"" not found in'+ module #LINE# #TAB# #TAB# #TAB#.__name__) #LINE# #TAB# return found_class"
"#LINE# #TAB# basename = os.path.basename(path) #LINE# #TAB# if grayscale: #LINE# #TAB# #TAB# basename = os.path.splitext(basename)[0] + '.bed' #LINE# #TAB# if target_size: #LINE# #TAB# #TAB# basename = os.path.splitext(basename)[0] + '.min.bed' #LINE# #TAB# if target_size!= basename: #LINE# #TAB# #TAB# basename = os.path.splitext(basename)[0] + '.bed' #LINE# #TAB# img = pybedtools.BedTool(basename).build(target_size, grayscale=grayscale) #LINE# #TAB# return img"
"#LINE# #TAB# data = yaml.load(open(input_file, 'r')) #LINE# #TAB# if not isinstance(data, dict): #LINE# #TAB# #TAB# raise ValueError('Configuration not a dictionary.') #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# if key.startswith('_'): #LINE# #TAB# #TAB# #TAB# del data[key] #LINE# #TAB# return data"
#LINE# #TAB# countries = [] #LINE# #TAB# for language in get_language_list(): #LINE# #TAB# #TAB# language_id = request.matchdict['language_id'] #LINE# #TAB# #TAB# if language_id: #LINE# #TAB# #TAB# #TAB# countries.append(Language(language_id)) #LINE# #TAB# return countries
"#LINE# #TAB# try: #LINE# #TAB# #TAB# acl_url = urljoin(_acl_url(), 'users/{}'.format(uid)) #LINE# #TAB# #TAB# r = http.post(acl_url) #LINE# #TAB# #TAB# assert r.status_code == 201 #LINE# #TAB# except DCOSHTTPException as e: #LINE# #TAB# #TAB# if e.response.status_code!= 409: #LINE# #TAB# #TAB# #TAB# raise"
#LINE# #TAB# word = word.lower() #LINE# #TAB# for c in correction: #LINE# #TAB# #TAB# if word.isupper(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if word.startswith(c): #LINE# #TAB# #TAB# #TAB# return 'H' + c #LINE# #TAB# #TAB# if word.islower(): #LINE# #TAB# #TAB# #TAB# word = 'A' + word #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# word = 'T' + word #LINE# #TAB# return word
"#LINE# #TAB# if not names: #LINE# #TAB# #TAB# return [] #LINE# #TAB# old_names = REGISTERED_NAMES.get(names[0], []) #LINE# #TAB# for name in names[1:]: #LINE# #TAB# #TAB# if name in old_names: #LINE# #TAB# #TAB# #TAB# del REGISTERED_NAMES[name] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# REGISTERED_NAMES[name] += 1 #LINE# #TAB# return old_names"
"#LINE# #TAB# discovery_objects = {} #LINE# #TAB# for k, v in dict_of_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# discovery_objects[k] = generate_discovery_objects(v) #LINE# #TAB# return discovery_objects"
#LINE# #TAB# radar = np.asarray(radar) #LINE# #TAB# assert radar.shape[0] == 2 #LINE# #TAB# for sweep in radar[1:]: #LINE# #TAB# #TAB# assert sweep % 2 == 0 #LINE# #TAB# return radar
"#LINE# #TAB# retval = None #LINE# #TAB# if isinstance(mylist, float): #LINE# #TAB# #TAB# retval = float(mylist) #LINE# #TAB# elif mylist is None: #LINE# #TAB# #TAB# return retval #LINE# #TAB# for i, v in enumerate(mylist): #LINE# #TAB# #TAB# retval[i] = v #LINE# #TAB# return retval"
#LINE# #TAB# if not os.path.isfile(folder): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# return '\n' #LINE# #TAB# if flask.request.method == 'OPTIONS': #LINE# #TAB# #TAB# return u'\n' #LINE# #TAB# if flask.request.environ.get('HTTP_HOST'): #LINE# #TAB# #TAB# return u'%s: %s' % (flask.request.environ['HTTP_HOST'], flask. #LINE# #TAB# #TAB# #TAB# request.environ['HTTP_PORT']) #LINE# #TAB# return u'\n'"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return np.sqrt(np.sum(s ** 2, axis=0)) #LINE# #TAB# except: #LINE# #TAB# #TAB# return np.nan"
"#LINE# #TAB# h = sha256() #LINE# #TAB# with open(file, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# buf = f.read(VRepository.PAGE_SIZE) #LINE# #TAB# #TAB# #TAB# if len(buf) > VRepository.PAGE_SIZE: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# h.update(buf) #LINE# #TAB# return h.hexdigest()[0:VRepository.PAGE_SIZE]"
#LINE# #TAB# #TAB# element = cls(name) #LINE# #TAB# #TAB# element.exporter = 'get_table_table_name' #LINE# #TAB# #TAB# element.output_name = output_name #LINE# #TAB# #TAB# return element
"#LINE# #TAB# with open(fasta_name, 'r') as handle: #LINE# #TAB# #TAB# for line in handle: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# store = gt_locate_tree_store(identifier, store_id=store_id) #LINE# #TAB# graph = gt_tree_to_graph(store) #LINE# #TAB# if graph.open(None, create=create)!= VALID_STORE: #LINE# #TAB# #TAB# raise ValueError(""The store {} doesn't exist"".format(store_id)) #LINE# #TAB# return graph"
"#LINE# #TAB# t1 = set(g1.triangles) #LINE# #TAB# t2 = set(g2.triangles) #LINE# #TAB# for i in range(len(t1)): #LINE# #TAB# #TAB# for j in range(len(t2)): #LINE# #TAB# #TAB# #TAB# t1.add(tuple([t1[j]])) #LINE# #TAB# for i in range(len(g1)): #LINE# #TAB# #TAB# for j in range(len(t2)): #LINE# #TAB# #TAB# #TAB# t2.add(tuple([t1[j]])) #LINE# #TAB# return t1, t2"
#LINE# #TAB# old_keys = set(previous_props.keys()) #LINE# #TAB# new_keys = set(next_props.keys()) #LINE# #TAB# result = {} #LINE# #TAB# for key in old_keys: #LINE# #TAB# #TAB# if key in new_keys: #LINE# #TAB# #TAB# #TAB# result[key] = next_props[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[key] = previous_props[key] #LINE# #TAB# for key in new_keys: #LINE# #TAB# #TAB# if key not in old_keys: #LINE# #TAB# #TAB# #TAB# result[key] = next_props[key] #LINE# #TAB# return result
#LINE# #TAB# if message.startswith(UNiversal_definition_message_prefix): #LINE# #TAB# #TAB# return _UNiversal_definition_message_prefix + message[len( #LINE# #TAB# #TAB# #TAB# UNiversal_definition_message_prefix):] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# for device in all.Device.objects.all(): #LINE# #TAB# #TAB# if hasattr(device,'supported_languages'): #LINE# #TAB# #TAB# #TAB# for language in device.supported_languages: #LINE# #TAB# #TAB# #TAB# #TAB# if search_text in language.search_text: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield device"
"#LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# return [decode_hapoxy_model(escape_char(c)) for c in value] #LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return [decode_hapoxy_model(item) for item in value] #LINE# #TAB# return value"
#LINE# #TAB# text = memory_image(text) #LINE# #TAB# lines = text.split('\n') #LINE# #TAB# highlght = 0 #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# part = line.split('#')[0].split() #LINE# #TAB# #TAB# if len(part) > 2: #LINE# #TAB# #TAB# #TAB# if part[1] in highlght: #LINE# #TAB# #TAB# #TAB# #TAB# highlght += len(part[1]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# highlght += len(part[0]) + len(part[1]) #LINE# #TAB# return highlght
#LINE# #TAB# try: #LINE# #TAB# #TAB# return m.__path__[0] #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return m.__name__
"#LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib #LINE# #TAB# #TAB# if not hasattr(matplotlib, 'rcParams'): #LINE# #TAB# #TAB# #TAB# matplotlib.rcParams = {} #LINE# #TAB# #TAB# if 'text' not in matplotlib.rcParams: #LINE# #TAB# #TAB# #TAB# matplotlib.rcParams['text'] = '' #LINE# #TAB# #TAB# if 'font_size' not in matplotlib.rcParams: #LINE# #TAB# #TAB# #TAB# matplotlib.rcParams['font_size'] = '14pt'"
"#LINE# #TAB# if igs is None: #LINE# #TAB# #TAB# igs = range(len(conns)) #LINE# #TAB# yaml = {'name': name, #LINE# #TAB# #TAB# #TAB# 'coors': coors, #LINE# #TAB# #TAB# #TAB# 'ngroups': ngroups, #LINE# #TAB# #TAB# #TAB# 'conns': conns, #LINE# #TAB# #TAB# #TAB#'mat_ids': mat_ids, #LINE# #TAB# #TAB# #TAB# 'descs': descs, #LINE# #TAB# #TAB# } #LINE# #TAB# return yaml"
"#LINE# #TAB# r_sources = [] #LINE# #TAB# for obj in builder.get_objects(): #LINE# #TAB# #TAB# if isinstance(obj, gtk.Widget): #LINE# #TAB# #TAB# #TAB# r_sources.append(obj) #LINE# #TAB# if len(r_sources) > 0: #LINE# #TAB# #TAB# return extract_component_r_sources(r_sources[0]) #LINE# #TAB# return r_sources"
"#LINE# #TAB# global object_cache #LINE# #TAB# new_obj = None #LINE# #TAB# if not summary: #LINE# #TAB# #TAB# return new_obj, objects #LINE# #TAB# for obj in objects: #LINE# #TAB# #TAB# new_obj = _flatten_bject(obj, summary) #LINE# #TAB# #TAB# if new_obj is None: #LINE# #TAB# #TAB# #TAB# new_obj = obj #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# object_cache[new_obj] = new_obj #LINE# #TAB# return new_obj, objects"
"#LINE# #TAB# return _load_yaml_timestamp(timestamp=True, default_flow_style=False #LINE# #TAB# #TAB# ) or {}"
"#LINE# #TAB# if bgcolor is not None: #LINE# #TAB# #TAB# img = Image.new('RGB', (bgcolor, bgcolor), img) #LINE# #TAB# else: #LINE# #TAB# #TAB# img = img #LINE# #TAB# return img"
#LINE# #TAB# df_f = df[groupby].groupby(fld) #LINE# #TAB# for row in df_f.iterrows(): #LINE# #TAB# #TAB# result = df_f[row].count() #LINE# #TAB# #TAB# if result > 0: #LINE# #TAB# #TAB# #TAB# return result #LINE# #TAB# return None
#LINE# #TAB# log_path = get_log_path() #LINE# #TAB# handler = logging.StreamHandler(sys.stderr) #LINE# #TAB# log_path.setLevel(logging.DEBUG) #LINE# #TAB# handler.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# '%(name)s - %(levelname)s - %(message)s')) #LINE# #TAB# log_path.addHandler(handler) #LINE# #TAB# log_path.setLevel(logging.INFO) #LINE# #TAB# return log_path
"#LINE# #TAB# import os #LINE# #TAB# path = os.path.join(os.path.dirname(__file__),'services', name + #LINE# #TAB# #TAB# '_weights.yaml') #LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# return path #LINE# #TAB# init_file = os.path.join(os.path.dirname(__file__), 'weights.yaml') #LINE# #TAB# if os.path.isfile(init_file): #LINE# #TAB# #TAB# return init_file #LINE# #TAB# weight_path = os.path.join(os.path.dirname(__file__), 'weights.json') #LINE# #TAB# if os.path.isfile(weight_path): #LINE# #TAB# #TAB# return weight_path"
"#LINE# #TAB# pop_no_diff = max(latest_config.keys(), current_config.keys()) #LINE# #TAB# return diff( #LINE# #TAB# #TAB# latest_config, #LINE# #TAB# #TAB# current_config, #LINE# #TAB# #TAB# ignore_order=True #LINE# #TAB# ) > 0"
#LINE# #TAB# param = ge_get_params(r) #LINE# #TAB# try: #LINE# #TAB# #TAB# if 'charset' in param: #LINE# #TAB# #TAB# #TAB# return param['charset'] #LINE# #TAB# #TAB# elif'replace' in param: #LINE# #TAB# #TAB# #TAB# return param['replace'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return param
#LINE# #TAB# amount = item.amount #LINE# #TAB# if amount > max_amount: #LINE# #TAB# #TAB# amount = 0 #LINE# #TAB# while amount < item.amount: #LINE# #TAB# #TAB# amount += 1 #LINE# #TAB# item.amount = amount #LINE# #TAB# return item
"#LINE# #TAB# import pysam #LINE# #TAB# try: #LINE# #TAB# #TAB# pysam.Samfile(bam_file, ""rb"") #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# return"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(name, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# logi = LOGI_TABLE[name] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# logi = int(logi) #LINE# #TAB# #TAB# if logi is not None: #LINE# #TAB# #TAB# #TAB# return logi #LINE# #TAB# return None"
"#LINE# #TAB# del value, frame #LINE# #TAB# return 'OBJECT_HAS_NO_FUNC'"
#LINE# #TAB# result = numpy.zeros(3) #LINE# #TAB# for tag in taglist: #LINE# #TAB# #TAB# result[tag] = node.getElementsByTagName(tag)[1].value #LINE# #TAB# return result
#LINE# #TAB# s = str(string) #LINE# #TAB# if s[0].isdigit(): #LINE# #TAB# #TAB# return [s] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# combined_headers = {} #LINE# #TAB# for name in fnames: #LINE# #TAB# #TAB# header = _generate_header(name, blend) #LINE# #TAB# #TAB# if header is not None: #LINE# #TAB# #TAB# #TAB# combined_headers[name] = header #LINE# #TAB# return combined_headers"
"#LINE# #TAB# ecc_names = [] #LINE# #TAB# for group in os.listdir(ec_path): #LINE# #TAB# #TAB# with open(os.path.join(ec_path, group), 'r') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# if line.startswith('ec:'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ecc_names.append(line.strip()) #LINE# #TAB# return ecc_names"
#LINE# #TAB# if model_mae is None or naive_mae is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# model_mae_sum = float(model_mae) #LINE# #TAB# naive_mae_sum = float(naive_mae) #LINE# #TAB# if model_mae_sum!= naive_mae_sum: #LINE# #TAB# #TAB# model_mae = model_mae_sum + (naive_mae - model_mae_sum) #LINE# #TAB# else: #LINE# #TAB# #TAB# model_mae = model_mae_sum #LINE# #TAB# return model_mae
"#LINE# #TAB# file = [] #LINE# #TAB# if not os.path.isdir(directory): #LINE# #TAB# #TAB# return file #LINE# #TAB# for root, dirs, files in os.walk(directory): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# file.append(os.path.join(root, f)) #LINE# #TAB# return file"
"#LINE# #TAB# width, height = entity.properties['width'], entity.properties['height'] #LINE# #TAB# if width is not None and height is not None: #LINE# #TAB# #TAB# dx = xy[0] - width #LINE# #TAB# #TAB# dy = xy[1] - height #LINE# #TAB# else: #LINE# #TAB# #TAB# dx = xy[0] - width #LINE# #TAB# #TAB# dy = xy[1] - height #LINE# #TAB# return dx, dy"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return eval(obj) #LINE# #TAB# except: #LINE# #TAB# #TAB# return obj
#LINE# #TAB# forwardng = UwEmailForwarding(source=netid) #LINE# #TAB# forwardng.save() #LINE# #TAB# return forwardng
#LINE# #TAB# labels_list = [] #LINE# #TAB# for item in series: #LINE# #TAB# #TAB# if item in df.columns: #LINE# #TAB# #TAB# #TAB# labels_list.append(df[item]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# labels_list.append(item) #LINE# #TAB# df = pd.DataFrame(labels_list) #LINE# #TAB# df.columns = labels_list.index #LINE# #TAB# return df
"#LINE# #TAB# if isinstance(string, str): #LINE# #TAB# #TAB# tree = ET.parse(string) #LINE# #TAB# else: #LINE# #TAB# #TAB# tree = ET.parse(string) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# return root"
#LINE# #TAB# r = sum(p * scale for p in polygon) #LINE# #TAB# return [polygon[i] * scale for i in range(len(polygon))]
#LINE# #TAB# shape = data.shape #LINE# #TAB# if len(shape) == 1: #LINE# #TAB# #TAB# return data[0] #LINE# #TAB# elif len(shape) == 2: #LINE# #TAB# #TAB# return data[0] #LINE# #TAB# elif len(shape) == 3: #LINE# #TAB# #TAB# return data[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# logging.info('Applying _ainex_restore generator:'#LINE# #TAB# #TAB# #TAB# #TAB# 'Building aainex index object for each record.') #LINE# #TAB# aainex_index_objects = {} #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# aaindex_object = AAIndexObject() #LINE# #TAB# #TAB# aaindex_object.deserialize(record) #LINE# #TAB# #TAB# aainex_index_objects[record.id] = aaindex_object #LINE# #TAB# logging.info( #LINE# #TAB# #TAB# 'Finished aainex_restore generator:'#LINE# #TAB# #TAB# ) #LINE# #TAB# return aainex_index_objects
"#LINE# #TAB# s = str(u) #LINE# #TAB# for symbol in VALID_SYMBOLS: #LINE# #TAB# #TAB# s = s.replace(symbol, '_') #LINE# #TAB# while s.isdigit(): #LINE# #TAB# #TAB# s = s.replace('e', 'e') #LINE# #TAB# while s.isdigit(): #LINE# #TAB# #TAB# s = s.replace('d', 'd') #LINE# #TAB# while s.isdigit(): #LINE# #TAB# #TAB# s = s.replace('e', 'f') #LINE# #TAB# return s"
#LINE# #TAB# filename_dict = dict() #LINE# #TAB# for filename in object.GetFileList(): #LINE# #TAB# #TAB# if filename.endswith('.gz'): #LINE# #TAB# #TAB# #TAB# filename_dict[filename.split('.')[0]] = filename #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# for filename in filename_dict: #LINE# #TAB# #TAB# if filename.endswith('.json'): #LINE# #TAB# #TAB# #TAB# filename = filename[:-5] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return filename_dict
"#LINE# #TAB# if scale == 1: #LINE# #TAB# #TAB# return matrix #LINE# #TAB# min_val = matrix[0, 0] + shift #LINE# #TAB# max_val = matrix[1, 0] + shift #LINE# #TAB# conv_matrix = np.zeros_like(matrix) #LINE# #TAB# for i in range(1, scale): #LINE# #TAB# #TAB# conv_matrix[i, 0] = min_val #LINE# #TAB# #TAB# conv_matrix[i, 1] = max_val #LINE# #TAB# conv_matrix[matrix > shift] = shift #LINE# #TAB# return conv_matrix"
#LINE# #TAB# if mimetype is None: #LINE# #TAB# #TAB# return #LINE# #TAB# mimetype = mimetype.lower() #LINE# #TAB# if mimetype not in RET_TYPES: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'Invalid mimetype: {mimetype}') #LINE# #TAB# return mimetype
#LINE# #TAB# try: #LINE# #TAB# #TAB# s = _view_ype_mapping[mypath] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return 'directory' #LINE# #TAB# else: #LINE# #TAB# #TAB# return s
#LINE# #TAB# if response.ok: #LINE# #TAB# #TAB# return response.text #LINE# #TAB# return 'No response received'
"#LINE# #TAB# result = {} #LINE# #TAB# for x in data: #LINE# #TAB# #TAB# result[x] = { #LINE# #TAB# #TAB# #TAB# 'name': x['name'], #LINE# #TAB# #TAB# #TAB#'version': x['version'], #LINE# #TAB# #TAB# #TAB# 'created_at': x['created_at'], #LINE# #TAB# #TAB# #TAB# 'updated_at': x['updated_at'], #LINE# #TAB# #TAB# } #LINE# #TAB# return result"
"#LINE# #TAB# if exn is not None: #LINE# #TAB# #TAB# state = exn.state #LINE# #TAB# #TAB# if state == 'WARNING': #LINE# #TAB# #TAB# #TAB# return ('WARNING', 'warning') #LINE# #TAB# #TAB# if state == 'CONTINUE': #LINE# #TAB# #TAB# #TAB# return ('CONTINUE', 'continue') #LINE# #TAB# return None, None"
#LINE# #TAB# curve_name = cls.fmap[name] #LINE# #TAB# obj = cls(curve_name) #LINE# #TAB# obj.generate() #LINE# #TAB# return obj
#LINE# #TAB# tmp = stream.read(8192) #LINE# #TAB# return tmp
"#LINE# #TAB# for binding in generate_ipv4_bindings(model, predicate): #LINE# #TAB# #TAB# yield binding"
"#LINE# #TAB# alpha = 0 #LINE# #TAB# beta = 0 #LINE# #TAB# for i in range(A.shape[0]): #LINE# #TAB# #TAB# for j in range(A.shape[1]): #LINE# #TAB# #TAB# #TAB# alpha += A[i, j] * beta #LINE# #TAB# for i in range(A.shape[0]): #LINE# #TAB# #TAB# for j in range(B.shape[0]): #LINE# #TAB# #TAB# #TAB# beta += B[i, j] * beta #LINE# #TAB# divergence = math.log(alpha) #LINE# #TAB# return divergence"
"#LINE# #TAB# if isinstance(structure, dict): #LINE# #TAB# #TAB# return dict((key, extend_deserialize(value)) for key, value in structure.items()) #LINE# #TAB# elif isinstance(structure, list): #LINE# #TAB# #TAB# return [extend_deserialize(element) for element in structure] #LINE# #TAB# else: #LINE# #TAB# #TAB# return structure"
"#LINE# #TAB# return {'tree_factor': obj.tree_factor, 'capakey': obj.capakey, #LINE# #TAB# #TAB# 'percid': obj.percid}"
"#LINE# #TAB# t = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# t = datetime.fromtimestamp(float(f.readline().split()[0])) #LINE# #TAB# #TAB# return t #LINE# #TAB# except (OSError, IOError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return None"
"#LINE# #TAB# days, remainder = divmod(duration_in_seconds, 86400) #LINE# #TAB# total_seconds = remainder * 86400 #LINE# #TAB# hours, remainder = divmod(total_seconds, 3600) #LINE# #TAB# minutes, seconds = divmod(remainder, 60) #LINE# #TAB# return f'{days}:{hours:02d}:{minutes:02d}:{seconds:02d}'"
"#LINE# #TAB# from.versioneer import VersioneerConfig #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = '' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source ='src/pymor/versioneer.yaml' #LINE# #TAB# cfg.read(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB#'src/pymor/versioneer.yaml')) #LINE# #TAB# return cfg"
"#LINE# #TAB# axisNames = ['lon', 'lat', 'crs'] #LINE# #TAB# for item in searchList: #LINE# #TAB# #TAB# name = item[0] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if name not in axisNames: #LINE# #TAB# #TAB# #TAB# #TAB# axisNames.append(name) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return axisNames"
#LINE# #TAB# if not tag_name: #LINE# #TAB# #TAB# return False #LINE# #TAB# element = AppElement(tag_name) #LINE# #TAB# if push: #LINE# #TAB# #TAB# element.push() #LINE# #TAB# return element
"#LINE# #TAB# keymaps = Keymap() #LINE# #TAB# for old_keymap in raw_keymaps: #LINE# #TAB# #TAB# old_keymap.name = raw_keymaps[old_keymap['name']].name #LINE# #TAB# #TAB# new_keymap = Keymap(raw_keymaps[old_keymap['name']], old_keymap= #LINE# #TAB# #TAB# #TAB# old_keymap) #LINE# #TAB# #TAB# new_keymap.save() #LINE# #TAB# return keymaps"
"#LINE# #TAB# from os.path import basename, expanduser #LINE# #TAB# for line in open(fp): #LINE# #TAB# #TAB# if basename.lower().endswith(('.dat', '.mrc')): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('SNAPSHOT'): #LINE# #TAB# #TAB# #TAB# #TAB# yield os.path.basename(fp).replace('.dat', '').split('.')[0] #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if not os.path.exists(fp): #LINE# #TAB# #TAB# raise ValueError(""No equivalent snapshot channel found at {}"". #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# format(fp)) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield fp"
#LINE# #TAB# global _enabl_scope_enabled #LINE# #TAB# _enabl_scope_enabled = False
"#LINE# #TAB# parts = string.split('/') #LINE# #TAB# author = parts[0] #LINE# #TAB# name = parts[1] #LINE# #TAB# return author, name"
"#LINE# #TAB# if len(regexps) == 0: #LINE# #TAB# #TAB# return content #LINE# #TAB# for regex, replace in regexps: #LINE# #TAB# #TAB# if not isinstance(regex, tuple): #LINE# #TAB# #TAB# #TAB# regex = re.compile(regex) #LINE# #TAB# #TAB# #TAB# content = regex.sub(replace, content) #LINE# #TAB# return content"
"#LINE# #TAB# if id_cols == None: #LINE# #TAB# #TAB# id_cols = df.columns.values[0] #LINE# #TAB# invalid_ids = [n for n in df.index.values if n not in id_cols] #LINE# #TAB# df.loc[:, (invalid_ids)] = df.loc[:, (invalid_ids)].apply(lambda n: np.isnan) #LINE# #TAB# df.dropna(inplace=True, axis=1, inplace=True) #LINE# #TAB# return df"
#LINE# #TAB# conf = configparser.SectionProxy() #LINE# #TAB# conf.add_section('serer-results') #LINE# #TAB# return conf
"#LINE# #TAB# result = [] #LINE# #TAB# if not os.path.isdir(directory): #LINE# #TAB# #TAB# return result #LINE# #TAB# for root, dirs, files in os.walk(directory): #LINE# #TAB# #TAB# for fname in files: #LINE# #TAB# #TAB# #TAB# if fname.endswith('.txt'): #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(root, fname) #LINE# #TAB# #TAB# #TAB# #TAB# result.append(filename) #LINE# #TAB# return result"
"#LINE# #TAB# if expiration_ms is None: #LINE# #TAB# #TAB# expiration_ms = 0 #LINE# #TAB# if isinstance(field, str): #LINE# #TAB# #TAB# field = pd.to_datetime(field) #LINE# #TAB# if isinstance(field, pd.Timestamp): #LINE# #TAB# #TAB# field = field.to_datetime() #LINE# #TAB# field['date'] = pd.to_datetime(field).strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# else: #LINE# #TAB# #TAB# field = pd.to_datetime(field) #LINE# #TAB# return field"
"#LINE# #TAB# postfixes = [] #LINE# #TAB# first = None #LINE# #TAB# for calendar in calendars: #LINE# #TAB# #TAB# if first is not None: #LINE# #TAB# #TAB# #TAB# if first.lstrip().endswith('/calendar'): #LINE# #TAB# #TAB# #TAB# #TAB# first = first.lstrip() #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# postfixes.append((first, None)) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# elif calendar is not None and first.lstrip().endswith('/calendar'): #LINE# #TAB# #TAB# #TAB# postfixes.append((first.lstrip(), calendar)) #LINE# #TAB# #TAB# #TAB# first = None #LINE# #TAB# for postfix in postfixes: #LINE# #TAB# #TAB# yield postfix"
"#LINE# #TAB# if start > end: #LINE# #TAB# #TAB# begidx = get_begidx(text, start) #LINE# #TAB# #TAB# endidx = get_endidx(text, end) #LINE# #TAB# else: #LINE# #TAB# #TAB# begidx = get_begidx(text, start) #LINE# #TAB# #TAB# endidx = end #LINE# #TAB# return [(begidx, endidx)]"
"#LINE# #TAB# table = formatting.dedent( #LINE# #TAB# #TAB# ) #LINE# #TAB# if not os.path.exists(METADATA_TABLE_PATH): #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(METADATA_TABLE_PATH, 'r') as f: #LINE# #TAB# #TAB# api_key = f.readline().strip() #LINE# #TAB# #TAB# while api_key!= current_app.config['API_KEY']: #LINE# #TAB# #TAB# #TAB# key = input('Enter your API key: ') #LINE# #TAB# #TAB# #TAB# if not key: #LINE# #TAB# #TAB# #TAB# #TAB# sys.stderr.write('Invalid API key.\n') #LINE# #TAB# #TAB# #TAB# #TAB# sys.exit(1) #LINE# #TAB# return table"
"#LINE# #TAB# assert theta_units in ['radians', 'degrees'],\ #LINE# #TAB# #TAB# ""kwarg theta_units must specified in radians or degrees"" #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.degrees(np.arctan2(y, x)) #LINE# #TAB# elif theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.arcsin(np.sin(theta)) #LINE# #TAB# return theta, x, y"
"#LINE# #TAB# method = request.method.lower() #LINE# #TAB# if method == ""GET"": #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# ""name"": request.path, #LINE# #TAB# #TAB# #TAB# ""required"": True, #LINE# #TAB# #TAB# #TAB# ""http_method"": request.method, #LINE# #TAB# #TAB# #TAB# ""resource"": parent_resource, #LINE# #TAB# #TAB# } #LINE# #TAB# if method == ""GET"": #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# ""name"": request.path, #LINE# #TAB# #TAB# #TAB# ""required"": True, #LINE# #TAB# #TAB# #TAB# ""http_method"": request.method #LINE# #TAB# #TAB# } #LINE# #TAB# return { #LINE# #TAB# #TAB# #TAB# ""resource"": resource, #LINE# #TAB# #TAB# }"
"#LINE# #TAB# if renderer is None: #LINE# #TAB# #TAB# renderer = 'agg' #LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib #LINE# #TAB# #TAB# renderer_module = importlib.import_module('matplotlib.backends.{}'.format(renderer)) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# raise ImportError('renderer {} not found'.format(renderer)) #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# renderer_module = importlib.import_module('matplotlib.backends.{}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# renderer)) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# raise ImportError('renderer {} not found'.format(renderer)) #LINE# #TAB# return renderer_module,"
"#LINE# #TAB# x = x[(...), ::-1] #LINE# #TAB# x[..., 0] -= 103.939 #LINE# #TAB# x[..., 1] -= 116.779 #LINE# #TAB# x[..., 2] -= 123.68 #LINE# #TAB# return x"
"#LINE# #TAB# for e in G: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# u = e[0] #LINE# #TAB# #TAB# #TAB# if u not in G.edges: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# if u not in G.nodes: #LINE# #TAB# #TAB# #TAB# #TAB# G.remove_node(u) #LINE# #TAB# #TAB# #TAB# #TAB# G.add_edge(u, v) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# G.add_edge(u, v) #LINE# #TAB# #TAB# #TAB# #TAB# break"
"#LINE# #TAB# ts = None #LINE# #TAB# with open(fn) as f: #LINE# #TAB# #TAB# for i, _ in enumerate(f): #LINE# #TAB# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# ts = None #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return float(f.readline().split()[0]) #LINE# #TAB# #TAB# except (IOError, IndexError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return None"
"#LINE# #TAB# table = [] #LINE# #TAB# default_val = defaults[0] #LINE# #TAB# for i, arg in enumerate(args): #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# table.append((arg, default_val)) #LINE# #TAB# #TAB# elif arg == '-': #LINE# #TAB# #TAB# #TAB# table.append((arg, default_val)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# table.append((arg, default_val)) #LINE# #TAB# return table"
#LINE# #TAB# value = data.extracted if data.extracted else None #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# compound = Compound.objects.get(slug=value) #LINE# #TAB# except Compound.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return compound
#LINE# #TAB# #TAB# epr_str = str(obj) #LINE# #TAB# #TAB# if pretty: #LINE# #TAB# #TAB# #TAB# epr_str = pretty_string(epr_str) #LINE# #TAB# #TAB# #TAB# if indent > 1: #LINE# #TAB# #TAB# #TAB# #TAB# epr_str = '\n' + epr_str #LINE# #TAB# #TAB# #TAB# return epr_str #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return epr_str
#LINE# #TAB# tokens = [] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# oen_token = elastic_client.indices.get(index)['tokens'] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# tokens.append(index) #LINE# #TAB# return tokens
#LINE# #TAB# mod = types.ModuleType(name) #LINE# #TAB# mod.__dict__.update(code or {}) #LINE# #TAB# mod.__loader__ = None #LINE# #TAB# mod.__package__ = '' #LINE# #TAB# return mod
"#LINE# #TAB# xml_dct = [] #LINE# #TAB# for obj in object_list: #LINE# #TAB# #TAB# for key, value in obj.obj_list.items(): #LINE# #TAB# #TAB# #TAB# xml_dct.append(_xml_o_knx(key)) #LINE# #TAB# return xml_dct"
"#LINE# #TAB# angles = np.asarray(angles, dtype=np.float64) #LINE# #TAB# if angles.ndim == 2: #LINE# #TAB# #TAB# theta = np.pi / 2 * np.pi #LINE# #TAB# elif angles.ndim == 3: #LINE# #TAB# #TAB# theta = np.arccos(angles / np.pi) #LINE# #TAB# return theta"
#LINE# #TAB# params['model_id'] = 0 #LINE# #TAB# return params
#LINE# #TAB# sstem_inputs = [] #LINE# #TAB# for column in columns: #LINE# #TAB# #TAB# sstem_inputs.append(column) #LINE# #TAB# return sstem_inputs
#LINE# #TAB# step = epoch * factor #LINE# #TAB# numba = epoch * (step / factor) ** factor #LINE# #TAB# return numba
"#LINE# #TAB# cm = np.zeros_like(tpm) #LINE# #TAB# for i in range(tpm.shape[0]): #LINE# #TAB# #TAB# for j in range(tpm.shape[1]): #LINE# #TAB# #TAB# #TAB# cm[i, j] = 1 #LINE# #TAB# return cm"
"#LINE# #TAB# properties = {} #LINE# #TAB# for key, value in network.properties.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# properties[key] = create_client_properties(network, key) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# properties[key] = [create_client_properties(network, key)] #LINE# #TAB# for key in ('source_url', 'target_url'): #LINE# #TAB# #TAB# if key in network: #LINE# #TAB# #TAB# #TAB# properties[key] = network[key]['source_url'] #LINE# #TAB# return properties"
#LINE# #TAB# msg = 'Invalid configuration file: {}'.format(file_dict) #LINE# #TAB# try: #LINE# #TAB# #TAB# validate(msg) #LINE# #TAB# except ValidationError as exc: #LINE# #TAB# #TAB# log.error(msg) #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# return file_dict
"#LINE# #TAB# try: #LINE# #TAB# #TAB# vals = sql['values'] #LINE# #TAB# except: #LINE# #TAB# #TAB# vals = [sql] #LINE# #TAB# metrics = Metrics() #LINE# #TAB# for val in vals: #LINE# #TAB# #TAB# if isinstance(val, datetime): #LINE# #TAB# #TAB# #TAB# val = val.isoformat() #LINE# #TAB# #TAB# metrics.append(Metrics(val)) #LINE# #TAB# return metrics"
#LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = ContentType.objects.get(name=ct_name) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return content_type
#LINE# #TAB# target_col = transform.col[0] #LINE# #TAB# target_row = transform.row[1] #LINE# #TAB# while target_col!= transform.col[0]: #LINE# #TAB# #TAB# if target_row == transform.col[0]: #LINE# #TAB# #TAB# #TAB# transform = transform.pop() #LINE# #TAB# #TAB# elif target_row == transform.row[1]: #LINE# #TAB# #TAB# #TAB# transform = transform.pop() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# transform = transform.pop() #LINE# #TAB# return transform
"#LINE# #TAB# if isinstance(obj, Promise): #LINE# #TAB# #TAB# return obj #LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# return [signed_iterate(i) for i in obj] #LINE# #TAB# if hasattr(obj, '__iter__') and not isinstance(obj, str): #LINE# #TAB# #TAB# return [signed_iterate(i) for i in obj] #LINE# #TAB# return obj"
"#LINE# #TAB# return [os.path.expandvars(os.path.expanduser(file_path)) for file_path in #LINE# #TAB# #TAB# os.listdir(os.getcwd()) if os.path.isdir(os.path.join(file_path, #LINE# #TAB# #TAB# module_name,'setup.py'))]"
#LINE# #TAB# return [tag for tag in os.listdir(exension_dn_image_path) if os.path. #LINE# #TAB# #TAB# isfile(exension_dn_image_path[0]) and tag!= 'dn']
#LINE# #TAB# s = form.get(key) #LINE# #TAB# if s is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# out = {} #LINE# #TAB# for e in element: #LINE# #TAB# #TAB# out[e.tag] = e.text.encode('utf-8') #LINE# #TAB# return out
"#LINE# #TAB# out = {} #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# fields = line.strip().split() #LINE# #TAB# #TAB# #TAB# if len(fields) == 2: #LINE# #TAB# #TAB# #TAB# #TAB# chrom, start, end = fields #LINE# #TAB# #TAB# #TAB# #TAB# out[chrom] = int(start), int(end) - 1 #LINE# #TAB# return out"
"#LINE# #TAB# mac = os.environ.get('MAC', '') #LINE# #TAB# if not mac: #LINE# #TAB# #TAB# return None #LINE# #TAB# for line in open('/etc/machine-id', 'r'): #LINE# #TAB# #TAB# if line.startswith('d'): #LINE# #TAB# #TAB# #TAB# v = line.split() #LINE# #TAB# #TAB# #TAB# mac = v[1] #LINE# #TAB# #TAB# #TAB# if len(mac) > 2: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mac = mac[0].replace(' ', '_') #LINE# #TAB# return mac"
#LINE# #TAB# if not context: #LINE# #TAB# #TAB# return False #LINE# #TAB# if context.user_id is None or not context.project_id: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# profile_arn = profile.get('arn') #LINE# #TAB# if profile_arn: #LINE# #TAB# #TAB# aws_account = profile_arn.split(':')[0] #LINE# #TAB# #TAB# aws_secret = profile_arn.split(':')[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# aws_account = None #LINE# #TAB# #TAB# aws_secret = None #LINE# #TAB# return aws_account, aws_secret"
"#LINE# #TAB# data = data.copy() #LINE# #TAB# peak_index = data.index.copy() #LINE# #TAB# peak_index[peak_index == 0] = np.inf #LINE# #TAB# new_index = pd.Index(peak_index) #LINE# #TAB# new_index[peak_index == 0] = np.inf #LINE# #TAB# data.loc[new_index, 'peak_index'] -= peak_index[peak_index] #LINE# #TAB# new_index = new_index.rename(columns={'peak_index': 'peak_index'}) #LINE# #TAB# data.loc[new_index, 'peak_index'] = peak_index #LINE# #TAB# return data"
"#LINE# #TAB# result = [] #LINE# #TAB# for operand in operand_list: #LINE# #TAB# #TAB# result.append(gen_entry_blockchain(operand, operand_name_list)) #LINE# #TAB# return result"
"#LINE# #TAB# rest_host = '/rest/services/rst' #LINE# #TAB# logger.debug('Gotrst service to get rest host') #LINE# #TAB# service_file = open(os.path.join(os.getcwd(), rest_host), 'w') #LINE# #TAB# response = requests.get(service_file) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# rst_service = response.content #LINE# #TAB# return rst_service"
"#LINE# #TAB# import pyspark #LINE# #TAB# if type(filename) == str: #LINE# #TAB# #TAB# filename = open(filename, 'r') #LINE# #TAB# lc_input_data = [] #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.rstrip('\n') #LINE# #TAB# #TAB# #TAB# lc_input_data.append(float(line.split()[0])) #LINE# #TAB# lc_input_data = np.array(lc_input_data) #LINE# #TAB# return lc_input_data"
#LINE# #TAB# if ':' in signature: #LINE# #TAB# #TAB# signature = signature.split(':')[0] #LINE# #TAB# return signature
"#LINE# #TAB# for b in blueprints: #LINE# #TAB# #TAB# obj = app.extensions[b] #LINE# #TAB# #TAB# if hasattr(obj,'static_folder'): #LINE# #TAB# #TAB# #TAB# obj.static_folder = b.static_folder.rstrip('/') #LINE# #TAB# #TAB# if hasattr(obj, 'collect_groups'): #LINE# #TAB# #TAB# #TAB# for k in obj.collect_groups.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# if k.startswith('collect_'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# del obj.collect_groups[k]"
"#LINE# #TAB# return [[container_bytes_gen_tc_pixel_internal(table, utc_hour) for table in #LINE# #TAB# #TAB# _container_iter(table_name)] for container in _container_iter( #LINE# #TAB# #TAB# table_name)]"
"#LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in fromDic: #LINE# #TAB# #TAB# #TAB# if not isinstance(fromDic[key], dict) and not isinstance(toDic[key], #LINE# #TAB# #TAB# #TAB# #TAB# dict): #LINE# #TAB# #TAB# #TAB# #TAB# toDic[key] = fromDic[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# toDic[key] = fromDic[key]"
#LINE# #TAB# return [Source.from_text(line) for line in sourcelist if len(line #LINE# #TAB# #TAB# ) > 3]
"#LINE# #TAB# candidates = [] #LINE# #TAB# for root, dirs, files in os.walk(top): #LINE# #TAB# #TAB# dirs[:] = [r for r in dirs if r.startswith(package)] #LINE# #TAB# #TAB# if dirs: #LINE# #TAB# #TAB# #TAB# yield root, dirs, files #LINE# #TAB# else: #LINE# #TAB# #TAB# for root, dirs, files in os.walk(top): #LINE# #TAB# #TAB# #TAB# yield root, dirs, files"
"#LINE# #TAB# biopax = None #LINE# #TAB# if not isinstance(model, BioPAXLevel): #LINE# #TAB# #TAB# raise TypeError(""Unknown BioPAX level {}"".format(model.getLevel())) #LINE# #TAB# if model.isComposite(): #LINE# #TAB# #TAB# biopax = BiopaxProcessorComposite(model) #LINE# #TAB# else: #LINE# #TAB# #TAB# biopax = BiopaxProcessor(model) #LINE# #TAB# return biopax"
#LINE# #TAB# opener = urllib.request.build_opener(*get_handlers()) #LINE# #TAB# urllib.request.install_opener(opener) #LINE# #TAB# return opener
"#LINE# #TAB# prob = {} #LINE# #TAB# exemplar_followers = set() #LINE# #TAB# for followers in exemplars.values(): #LINE# #TAB# #TAB# exemplar_followers |= followers #LINE# #TAB# for brand, followers in brands: #LINE# #TAB# #TAB# prob[brand] = _prob(followers, exemplar_followers) #LINE# #TAB# return prob"
"#LINE# #TAB# df = pd.DataFrame({'year': year,'month': month, 'day': day}) #LINE# #TAB# if month!= 20: #LINE# #TAB# #TAB# df['month'] = month + 1 #LINE# #TAB# return df"
"#LINE# #TAB# error_dict = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# body = response.json() #LINE# #TAB# #TAB# if ""error_description"" in body: #LINE# #TAB# #TAB# #TAB# error_dict = body['error_description'] #LINE# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if not error_dict: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# error_dict = json.loads(error_dict) #LINE# #TAB# except json.decoder.JSONDecodeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return error_dict"
"#LINE# #TAB# lexer, extension = os.path.splitext(file_name) #LINE# #TAB# if extension == '.pdf': #LINE# #TAB# #TAB# return _parse_pdf(text) #LINE# #TAB# elif extension == '.docx': #LINE# #TAB# #TAB# return _parse_docx(text) #LINE# #TAB# elif extension == '.py': #LINE# #TAB# #TAB# return _parse_pydoc(text) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# module = __import__(module) #LINE# #TAB# #TAB# return module #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# index = pad_index(index, count) #LINE# #TAB# if index == '': #LINE# #TAB# #TAB# return '0' * count #LINE# #TAB# return f'0{index}{count}'"
"#LINE# #TAB# key = rsa_key.RSAKey() #LINE# #TAB# serialized_pub_key = base64.b64decode(privkeystring) #LINE# #TAB# pub_key = rsa_key.decrypt(serialized_pub_key) #LINE# #TAB# inp = inp.replace('-----BEGIN PRIVATE KEY-----', '\n') #LINE# #TAB# inp = inp.replace('-----END PRIVATE KEY-----', '\n') #LINE# #TAB# return pub_key"
"#LINE# #TAB# match = RE_VER_PATTERN.match(version_string) #LINE# #TAB# if not match: #LINE# #TAB# #TAB# raise ValueError(""Version string '{}' is not valid."".format(version_string)) #LINE# #TAB# major, minor = match.groups() #LINE# #TAB# return major, minor, version_string"
"#LINE# #TAB# cores = min(multiprocessing.cpu_count(), cores) #LINE# #TAB# if six.PY3: #LINE# #TAB# #TAB# cores = 2 ** cores #LINE# #TAB# return cores"
#LINE# #TAB# args['state_scope'] = 'Trading' #LINE# #TAB# return args['state_scope']
"#LINE# #TAB# stmt = 'UPDATE'#LINE# #TAB# for key, value in dictionary.items(): #LINE# #TAB# #TAB# if isinstance(value, datetime.datetime): #LINE# #TAB# #TAB# #TAB# stmt +='WHERE'#LINE# #TAB# #TAB# #TAB# stmt += '{0} = {1}'.format(key, value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# stmt += '{0} = {1}'.format(key, value) #LINE# #TAB# stmt += ')' #LINE# #TAB# return stmt"
#LINE# #TAB# newline = line.rstrip(';') #LINE# #TAB# if newline == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# commands = line.split(';') #LINE# #TAB# message = commands[0] #LINE# #TAB# if len(commands) > 1: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# message = commands[1].strip() #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if message == '': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return message #LINE# #TAB# else: #LINE# #TAB# #TAB# return message
"#LINE# #TAB# if isinstance(nmea_degrees, str): #LINE# #TAB# #TAB# nmea_degrees = [nmea_degrees] #LINE# #TAB# hex_nme_degrees = hex(nmea_degrees[0]) #LINE# #TAB# hex_nme_degrees = [int(nmea_degrees[1:], 16) for nmea_degrees in #LINE# #TAB# #TAB# hex_nme_degrees] #LINE# #TAB# if len(hex_nme_degrees) == 6: #LINE# #TAB# #TAB# hex_nme_degrees = hex_nme_degrees[0:2] #LINE# #TAB# return hex_nme_degrees"
"#LINE# #TAB# xray_info = et_get_xray(team_id, gameweek) #LINE# #TAB# picks = [] #LINE# #TAB# for k in xray_info: #LINE# #TAB# #TAB# if k['game'] == game: #LINE# #TAB# #TAB# #TAB# picks.append(xray_info[k]) #LINE# #TAB# if 'inserts' in xray_info: #LINE# #TAB# #TAB# for i in range(5): #LINE# #TAB# #TAB# #TAB# for j in range(5): #LINE# #TAB# #TAB# #TAB# #TAB# picks.append(xray_info[i]) #LINE# #TAB# return {'xray_info': xray_info, 'picks': picks}"
#LINE# #TAB# radius = str(event.parameter.radius) #LINE# #TAB# if radius == 'none': #LINE# #TAB# #TAB# obj.signupsheet.disable = True #LINE# #TAB# else: #LINE# #TAB# #TAB# obj.signupsheet.enable = False
#LINE# #TAB# if 'basic_auth' in request.headers: #LINE# #TAB# #TAB# auth = request.headers['basic_auth'] #LINE# #TAB# #TAB# if auth: #LINE# #TAB# #TAB# #TAB# return 'username:%s' % auth #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# weekday_string = vehicle_journey_element.get('weekday') #LINE# #TAB# if weekday_string is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# translation_template = None #LINE# #TAB# if weekday_string == 'TransXChange': #LINE# #TAB# #TAB# return translation_template #LINE# #TAB# for element in vehicle_journey_element: #LINE# #TAB# #TAB# weekday_string = element.get('weekday') #LINE# #TAB# #TAB# if weekday_string: #LINE# #TAB# #TAB# #TAB# translation_template = get_translation_template_from_element( #LINE# #TAB# #TAB# #TAB# #TAB# weekday_string) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return translation_template
"#LINE# #TAB# errorCode = c_uint() #LINE# #TAB# try: #LINE# #TAB# #TAB# cdb = conf.lib.clang_CompilationDatabase_from_string(buildDir.encode( #LINE# #TAB# #TAB# #TAB# 'utf-8'), byref(errorCode)) #LINE# #TAB# except CompilationDatabaseError as e: #LINE# #TAB# #TAB# raise CompilationDatabaseError(int(errorCode.value), #LINE# #TAB# #TAB# #TAB# 'CompilationDatabase loading failed') #LINE# #TAB# return cdb"
"#LINE# #TAB# dirs = [os.path.join(os.path.dirname(os.path.realpath(__file__)), 'lib') for #LINE# #TAB# #TAB# _ in os.listdir(os.path.dirname(os.path.realpath(__file__))) if #LINE# #TAB# #TAB# not os.path.isdir(os.path.join(os.path.dirname(os.path. #LINE# #TAB# #TAB# realpath(__file__)), 'lib'))] #LINE# #TAB# if os.path.isdir(os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# 'lang_longest')): #LINE# #TAB# #TAB# dirs.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# #TAB# 'lang_longest')) #LINE# #TAB# return dirs"
#LINE# #TAB# if tag.HasAttribute('nmesOfrExtension'): #LINE# #TAB# #TAB# return tag.get('nmesOfrExtension').split(' ') #LINE# #TAB# if tag.HasAttribute('nmesShort'): #LINE# #TAB# #TAB# return tag.get('nmesShort').split(' ') #LINE# #TAB# return []
#LINE# #TAB# global _namespace #LINE# #TAB# _namespace[prefix] = ns
#LINE# #TAB# if 'ColdStart' in _cld_strt: #LINE# #TAB# #TAB# return 'True' #LINE# #TAB# return 'False'
#LINE# #TAB# beta = gb / 100.0 #LINE# #TAB# theta = np.arccos(1.0 - beta) #LINE# #TAB# w = np.zeros_like(gl) #LINE# #TAB# w[0] = gl[0] + beta * np.cos(theta) #LINE# #TAB# w[1] = gl[1] + beta * np.sin(theta) #LINE# #TAB# w[2] = gl[2] - beta * np.cos(theta) #LINE# #TAB# w[3] = gl[0] + beta * np.sin(theta) #LINE# #TAB# return w
#LINE# #TAB# old_logger_level = logger.getEffectiveLevel() #LINE# #TAB# if old_logger_level is None: #LINE# #TAB# #TAB# logger.setLevel(level) #LINE# #TAB# yield #LINE# #TAB# logger.setLevel(old_logger_level) #LINE# #TAB# yield
#LINE# #TAB# start_activities = set() #LINE# #TAB# for node in dfg: #LINE# #TAB# #TAB# if node.subject: #LINE# #TAB# #TAB# #TAB# start_activities.add(node.subject['id']) #LINE# #TAB# #TAB# if node.action: #LINE# #TAB# #TAB# #TAB# start_activities.add(node.action['id']) #LINE# #TAB# return start_activities
#LINE# #TAB# extension = os.path.splitext(file_path)[1] #LINE# #TAB# return extension
"#LINE# #TAB# return [name for name, val in inspect.signature(func).parameters.items() if #LINE# #TAB# #TAB# isinstance(val, gt_rule)]"
#LINE# #TAB# global OUTPUT_HTL #LINE# #TAB# OUTPUT_HTL = OUTPUT_HTL + str(uuid.uuid4()) #LINE# #TAB# return OUTPUT_HTL
#LINE# #TAB# if is_forward_reference(typ): #LINE# #TAB# #TAB# return typ #LINE# #TAB# else: #LINE# #TAB# #TAB# return typ
"#LINE# #TAB# pairs = [] #LINE# #TAB# m = re_url.match(value) #LINE# #TAB# if m: #LINE# #TAB# #TAB# urlparts = m.groupdict() #LINE# #TAB# #TAB# if 'doi' in urlparts: #LINE# #TAB# #TAB# #TAB# doi = urlparts['doi'] #LINE# #TAB# #TAB# #TAB# pairs.append((urlparts['scheme'], doi)) #LINE# #TAB# #TAB# elif 'name' in urlparts: #LINE# #TAB# #TAB# #TAB# pairs.append(urlparts['name']) #LINE# #TAB# return pairs"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if t.value.count('.') == 0: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = int(t.value) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('[%d]: Number %s is not valid!' % (t.lineno, t.value)) #LINE# #TAB# #TAB# t.value = 0 #LINE# #TAB# return t"
"#LINE# #TAB# result = [] #LINE# #TAB# for i in range(0, len(_bytes)): #LINE# #TAB# #TAB# res.append(uint16_t(_bytes[i * 2:i * 2 + 2])) #LINE# #TAB# return result"
#LINE# #TAB# Tr = 298.15 #LINE# #TAB# return abc[0] + abc[1] * (T - Tr) + abc[2] * (T - Tr) ** 2
"#LINE# #TAB# code = str(code) #LINE# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# crs = {} #LINE# #TAB# for entry in proj4.split("";""): #LINE# #TAB# #TAB# if "":"" in entry: #LINE# #TAB# #TAB# #TAB# key, value = entry.split("":"") #LINE# #TAB# #TAB# #TAB# crs[key] = value.strip() #LINE# #TAB# return crs"
"#LINE# #TAB# ang1 = np.arctan2(v1[1], v2[1]) #LINE# #TAB# ang2 = np.arctan2(v1[2], v2[2]) #LINE# #TAB# r = np.arccos(ang1) #LINE# #TAB# return r * 180 / pi"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return import_module(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# from bs4 import BeautifulSoup #LINE# #TAB# from yacms.core.request import current_request #LINE# #TAB# request = current_request() #LINE# #TAB# if request is not None: #LINE# #TAB# #TAB# dom = BeautifulSoup(html, ""html.parser"") #LINE# #TAB# #TAB# for tag, attr in ABSOLUTE_URL_TAGS.items(): #LINE# #TAB# #TAB# #TAB# for node in dom.findAll(tag): #LINE# #TAB# #TAB# #TAB# #TAB# url = node.get(attr, """") #LINE# #TAB# #TAB# #TAB# #TAB# if url: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# node[attr] = request.build_absolute_uri(url) #LINE# #TAB# #TAB# html = str(dom) #LINE# #TAB# return html"
"#LINE# #TAB# for i in classes: #LINE# #TAB# #TAB# logger.info('on_vs_logger_fuc(%s): %s:%s:%s', i, class_name, TP[i], #LINE# #TAB# #TAB# #TAB# TN[i], FP[i], FN[i]) #LINE# #TAB# pass"
"#LINE# #TAB# response = {} #LINE# #TAB# if keys is None: #LINE# #TAB# #TAB# keys = [] #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# key = str(key) #LINE# #TAB# #TAB# if key not in orig_dict: #LINE# #TAB# #TAB# #TAB# response[key] = default #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# response[key] = orig_dict.get(key, default) #LINE# #TAB# return response"
#LINE# #TAB# winlen = int(winlen) #LINE# #TAB# count = 0 #LINE# #TAB# res = [] #LINE# #TAB# for occ in concept: #LINE# #TAB# #TAB# if min_c <= occ[0] and min_z <= occ[1]: #LINE# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# #TAB# if min_c >= occ[0]: #LINE# #TAB# #TAB# #TAB# res.append(concept[count]) #LINE# #TAB# #TAB# if max_c >= occ[0]: #LINE# #TAB# #TAB# #TAB# res.append(concept[count]) #LINE# #TAB# #TAB# if max_z <= occ[1]: #LINE# #TAB# #TAB# #TAB# res.append(concept[count]) #LINE# #TAB# return res
"#LINE# #TAB# bins = Bins() #LINE# #TAB# for i in range(num_bins): #LINE# #TAB# #TAB# p =probs[i] #LINE# #TAB# #TAB# c = np.cumsum(p) #LINE# #TAB# #TAB# bins.put(c, bins.get_value(i, min=0)) #LINE# #TAB# return bins"
"#LINE# #TAB# files = list(files) #LINE# #TAB# to_remove = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# md5sum = hashlib.md5(f.read()).hexdigest() #LINE# #TAB# #TAB# for i in range(0, len(md5sum), psize): #LINE# #TAB# #TAB# #TAB# to_remove.append(md5sum[i:i + psize]) #LINE# #TAB# return to_remove"
"#LINE# #TAB# string = str(string) #LINE# #TAB# initial = True #LINE# #TAB# if not string: #LINE# #TAB# #TAB# return initial #LINE# #TAB# while initial: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return bool(eval(string)) #LINE# #TAB# #TAB# except (SyntaxError, NameError, ValueError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
#LINE# #TAB# locae = None #LINE# #TAB# if request.accept_language: #LINE# #TAB# #TAB# locae = request.accept_language.best_match(LANGUAGES) #LINE# #TAB# return locae
"#LINE# #TAB# size_one = len(array_one) #LINE# #TAB# size_two = len(array_two) #LINE# #TAB# exp_diff = 0 #LINE# #TAB# for i in range(size_one): #LINE# #TAB# #TAB# exp_diff += ranksu_avail_one(array_one[i], array_two[i]) #LINE# #TAB# return exp_diff"
"#LINE# #TAB# median = median_filter(grid, rectangle) #LINE# #TAB# if not median: #LINE# #TAB# #TAB# return False, 'invalid rectangle' #LINE# #TAB# return True, median"
"#LINE# #TAB# #TAB# if format: #LINE# #TAB# #TAB# #TAB# format = format.lower() #LINE# #TAB# #TAB# lines = [] #LINE# #TAB# #TAB# if not os.path.isfile(filename): #LINE# #TAB# #TAB# #TAB# return lines #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# first_line = f.readline() #LINE# #TAB# #TAB# lines.append(first_line) #LINE# #TAB# #TAB# rst = '\n\n' #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# rst += '#TAB#'+ line + '\n' #LINE# #TAB# #TAB# rst += '\n' #LINE# #TAB# #TAB# return rst"
"#LINE# #TAB# return type(annotation) is str and getattr(annotation, '_name', None #LINE# #TAB# #TAB# ) == 'Dict'"
#LINE# #TAB# doc = event.get('doc') #LINE# #TAB# extra = {} #LINE# #TAB# extra['_id'] = event.get('id') #LINE# #TAB# event_dict = dict(event) #LINE# #TAB# event_dict['_id'] = str(event.get('id')) #LINE# #TAB# event_dict['op_name'] = event.get('op_name') #LINE# #TAB# event_dict['op_message_body'] = event.get('op_message_body') #LINE# #TAB# event_dict['access_token'] = event.get('access_token') #LINE# #TAB# if extra: #LINE# #TAB# #TAB# event_dict['extra'] = extra #LINE# #TAB# return event_dict
#LINE# #TAB# if len(skipUserInput) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# password = getpass.getpass('Enter password: ') #LINE# #TAB# if password == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# return password
#LINE# #TAB# if error_message is None: #LINE# #TAB# #TAB# error_message = 'Must provide a datetime value' #LINE# #TAB# val = row.get(name) #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# return val.isoformat() #LINE# #TAB# elif error_message is not None: #LINE# #TAB# #TAB# return error_message #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# version_split = split_version(args) #LINE# #TAB# if not version_split: #LINE# #TAB# #TAB# return {'status': 'not supported', 'changes': {}} #LINE# #TAB# elif len(version_split) == 1: #LINE# #TAB# #TAB# return {'status': 'running', 'changes': set()} #LINE# #TAB# return {'status': 'running', 'changes': set()}"
"#LINE# #TAB# address = socket.gethostname() #LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# try: #LINE# #TAB# #TAB# s.connect(address) #LINE# #TAB# #TAB# domain = s.getsockname()[0] #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# s.close() #LINE# #TAB# if domain == '127.0.0.1': #LINE# #TAB# #TAB# domain = '127.0.0.1' #LINE# #TAB# else: #LINE# #TAB# #TAB# domain = domain.split('.')[0] #LINE# #TAB# return domain"
"#LINE# #TAB# r = subprocess.run(['febrl2', '-v'], stdout=subprocess.PIPE, stderr= #LINE# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# stdout, stderr = r.communicate() #LINE# #TAB# return stdout.decode('utf-8').rstrip().split()[-1]"
#LINE# #TAB# shapes = _shapes_q1(geo) #LINE# #TAB# if not shapes: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [shape for shape in shapes if shape.type!='shape']
#LINE# #TAB# content_type = magic.from_bytes(input) #LINE# #TAB# return content_type
#LINE# #TAB# if text: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return DateTime(*parse_date(text)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass
#LINE# #TAB# blueprint = Blueprint(__name__) #LINE# #TAB# for dependency in dependencies: #LINE# #TAB# #TAB# blueprint.add_dependency(dependency) #LINE# #TAB# return blueprint
"#LINE# #TAB# url = url.strip('/') #LINE# #TAB# url = url.split('/')[-1] #LINE# #TAB# config = {'type': 'feature', 'url': url} #LINE# #TAB# if url.endswith('/'): #LINE# #TAB# #TAB# config['url'] = url[:-1] #LINE# #TAB# return config"
#LINE# #TAB# if 'cv_free_topic' not in store: #LINE# #TAB# #TAB# columns = store['cv_free_topic'] #LINE# #TAB# else: #LINE# #TAB# #TAB# columns = store['cv_free_topic'][chromosome_plot_info['chromosome']] #LINE# #TAB# topic = ColumnDataSource(columns) #LINE# #TAB# topic.name = solution['name'] #LINE# #TAB# return topic
"#LINE# #TAB# value = kwargs.get(param.name) #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# if prameter_type == 'collection': #LINE# #TAB# #TAB# #TAB# value = list() #LINE# #TAB# #TAB# elif prameter_type =='record': #LINE# #TAB# #TAB# #TAB# value.append(serialize_handler_paramete(path, kwargs, param, prameter_type)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# value = serialize_handler_paramete(path, kwargs, param, prameter_type) #LINE# #TAB# return value"
#LINE# #TAB# import re #LINE# #TAB# prefix = 'cche_loction_base_' #LINE# #TAB# if postfix: #LINE# #TAB# #TAB# prefix += '_' + str(postfix) #LINE# #TAB# h = re.compile(prefix) #LINE# #TAB# m = h.match(url) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return m.group(1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# log.debug('set_pid() called') #LINE# #TAB# if entry: #LINE# #TAB# #TAB# entry = get_entry(entry) #LINE# #TAB# if not entry: #LINE# #TAB# #TAB# return #LINE# #TAB# if username and prompt: #LINE# #TAB# #TAB# return getpass.getpass(prompt) #LINE# #TAB# elif always_ask: #LINE# #TAB# #TAB# return pwd.getpwuid(username) #LINE# #TAB# return None
#LINE# #TAB# bitmask = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# real_fid += 1 #LINE# #TAB# #TAB# #TAB# bitmask |= 1 #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return real_fid
"#LINE# #TAB# if not isinstance(value, basestring): #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.Ack(BaseMsgCode.HasTupleT)) #LINE# #TAB# return packet
#LINE# #TAB# if key not in _parents: #LINE# #TAB# #TAB# raise KeyError() #LINE# #TAB# return _parents[key]
#LINE# #TAB# return_text = '' #LINE# #TAB# for line in text.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if remove_url and line.startswith('{') and line.endswith('}'): #LINE# #TAB# #TAB# #TAB# line = line[1:-1] #LINE# #TAB# #TAB# return return_text += line + '\n' #LINE# #TAB# return return_text
"#LINE# #TAB# if isinstance(x, str) and re.match(r'^[a-zA-Z0-9\.\+_-]+@[a-zA-Z0-9\._-]+\.[a-zA-Z]{2,4}', x): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# ret = salt.utils.mac_utils.execute_return_result( #LINE# #TAB# #TAB#'systemsetup -getannotationsfontsqueue') #LINE# #TAB# return salt.utils.mac_utils.validate_enabled( #LINE# #TAB# #TAB# salt.utils.mac_utils.parse_return(ret)) == 'on'
#LINE# #TAB# try: #LINE# #TAB# #TAB# return socket.gethostbyname(hostname) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# a, b, c, d, e, f = m #LINE# #TAB# q, r = create_adapter_jrik(a, b, c, d, g, s) #LINE# #TAB# return q, r"
#LINE# #TAB# #TAB# eps = 1e-05 #LINE# #TAB# #TAB# Tmax = T #LINE# #TAB# #TAB# if Tmax < 0: #LINE# #TAB# #TAB# #TAB# r = 0 #LINE# #TAB# #TAB# elif Tmax > 0: #LINE# #TAB# #TAB# #TAB# r = Tmax #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# r = Tmax / (Rmax + 1e-05) #LINE# #TAB# #TAB# #TAB# if r < 0: #LINE# #TAB# #TAB# #TAB# #TAB# r = 0 #LINE# #TAB# #TAB# return r
"#LINE# #TAB# out = np.zeros(x2ys.shape[0]) #LINE# #TAB# for i in range(n_trans): #LINE# #TAB# #TAB# x = x2ys[i] #LINE# #TAB# #TAB# tmp = 0.0 #LINE# #TAB# #TAB# for j in range(i + 1, n_trans): #LINE# #TAB# #TAB# #TAB# x2ys_tmp = x2ys[j] #LINE# #TAB# #TAB# #TAB# tmp += co_occurrences(x2ys_tmp, x2ys) #LINE# #TAB# #TAB# out[i] = tmp / (n_trans - i) #LINE# #TAB# return out"
"#LINE# #TAB# intercept = SDL_do_intercept(X, y, sample_weight) #LINE# #TAB# d_beta = safe_sparse_dot(X.T, intercept) / X.shape[0] #LINE# #TAB# G = len(np.unique(group_index)) #LINE# #TAB# return intercept, d_beta, G"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return not peer.is_admin #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# col_dict = {} #LINE# #TAB# col_dict['R'] = colour.R #LINE# #TAB# col_dict['G'] = colour.G #LINE# #TAB# col_dict['B'] = colour.B #LINE# #TAB# col_dict['X'] = colour.X #LINE# #TAB# col_dict['Y'] = colour.Y #LINE# #TAB# col_dict['Z'] = colour.Z #LINE# #TAB# return col_dict
#LINE# #TAB# if not is_iem_availale_available(item_pid): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if filename.endswith('.pyc'): #LINE# #TAB# #TAB# return -len(os.path.getmtime(filename)) #LINE# #TAB# elif filename.endswith('.pyo'): #LINE# #TAB# #TAB# return -len(os.path.getmtime(filename)) #LINE# #TAB# return None
"#LINE# #TAB# curr_dir = os.path.dirname(nested_path) #LINE# #TAB# while curr_dir!= os.path.dirname(os.getcwd()): #LINE# #TAB# #TAB# nested_path = os.path.join(curr_dir, nested_path) #LINE# #TAB# #TAB# curr_dir = os.path.dirname(os.path.abspath(nested_path)) #LINE# #TAB# return nested_path"
#LINE# #TAB# if entry['type'] == 'collection': #LINE# #TAB# #TAB# collection = remote.collection(UseCurrent.collection) #LINE# #TAB# elif entry['type'] == 'file': #LINE# #TAB# #TAB# collection = remote.file(UseCurrent.file) #LINE# #TAB# return collection
"#LINE# #TAB# masking_row = 0 #LINE# #TAB# masking_col = 0 #LINE# #TAB# for row in m: #LINE# #TAB# #TAB# if numpy.isnan(row[masking_row]) or numpy.isinf(row[masking_row]): #LINE# #TAB# #TAB# #TAB# masking_row += 1 #LINE# #TAB# #TAB# #TAB# masking_col += 1 #LINE# #TAB# return masking_row, masking_col"
#LINE# #TAB# return abs(p[0] - rect[0]) <= rect[0] and abs(p[1] - rect[1]) <= rect[1 #LINE# #TAB# #TAB# ] and p[2] - rect[2] <= rect[3]
"#LINE# #TAB# A = np.array(vertices) #LINE# #TAB# B = np.array(pole) #LINE# #TAB# C = np.cos(np.deg2rad(np.arctan2(A, B)) * np.pi / 180) #LINE# #TAB# D = -width * np.log(np.abs(C)) #LINE# #TAB# verices = np.zeros(3) #LINE# #TAB# for i in range(0, 3): #LINE# #TAB# #TAB# x = np.cos(np.deg2rad(A)) #LINE# #TAB# #TAB# y = np.sin(np.deg2rad(np.arctan2(B, C))) #LINE# #TAB# #TAB# verices[i] = pole[0] * x + pole[1] * y #LINE# #TAB# return verices"
"#LINE# #TAB# if is_module: #LINE# #TAB# #TAB# good_word = word.replace('__mod__', '__') #LINE# #TAB# else: #LINE# #TAB# #TAB# good_word = word #LINE# #TAB# return good_word"
"#LINE# #TAB# undeclared = set() #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# undeclared |= has_recursive(nodes, name) #LINE# #TAB# return undeclared"
#LINE# #TAB# global log_server #LINE# #TAB# log_server = port #LINE# #TAB# global loop #LINE# #TAB# if loop: #LINE# #TAB# #TAB# loop.run_until_complete(_log_server_accept) #LINE# #TAB# #TAB# log_server = None #LINE# #TAB# #TAB# return #LINE# #TAB# log_server.listen(10) #LINE# #TAB# loop = asyncio.get_event_loop() #LINE# #TAB# asyncio.set_event_loop(loop) #LINE# #TAB# log_server.listen(10) #LINE# #TAB# loop.run_until_complete(_log_server_accept) #LINE# #TAB# return
#LINE# #TAB# ip_address = None #LINE# #TAB# try: #LINE# #TAB# #TAB# ip_address = socket.gethostbyname(dns_name) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# pass #LINE# #TAB# if ip_address is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ip_address = socket.inet_ntoa(ip_address) #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if ip_address is not None: #LINE# #TAB# #TAB# repository_address = ip_address #LINE# #TAB# return repository_address
#LINE# #TAB# functions = set() #LINE# #TAB# if '--pymodule' in options and options['--pymodule']: #LINE# #TAB# #TAB# modules = options['--pymodule'] #LINE# #TAB# #TAB# for module in modules: #LINE# #TAB# #TAB# #TAB# if module.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# functions.add(module) #LINE# #TAB# if options['--pymodule']: #LINE# #TAB# #TAB# functions.add('pymodule') #LINE# #TAB# return functions
"#LINE# #TAB# _arse_seeds = [] #LINE# #TAB# for seed in seeds: #LINE# #TAB# #TAB# if re.match('[^@]+@[^@]+', seed): #LINE# #TAB# #TAB# #TAB# _arse_seeds.append(seed) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for user in _arse_seeds: #LINE# #TAB# #TAB# #TAB# if re.match('[^@]+@[^@]+', user): #LINE# #TAB# #TAB# #TAB# #TAB# _arse_seeds.append(user) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# _arse_seeds.append(seed) #LINE# #TAB# return _arse_seeds"
"#LINE# #TAB# files_param = {} #LINE# #TAB# if os.path.exists(directory) and os.path.isfile(directory): #LINE# #TAB# #TAB# files_param['filepath'] = directory #LINE# #TAB# #TAB# files_param['sha256'] = hashlib.sha256(open(directory, 'rb').read()).hexdigest() #LINE# #TAB# return files_param"
"#LINE# #TAB# if image.pixeltype!= 'unsigned char': #LINE# #TAB# #TAB# image = image.clone('unsigned char') #LINE# #TAB# idim = image.dimension #LINE# #TAB# vimage = iio.ANTsImage(pixeltype=image.pixeltype, dimension=image.dimension, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# components=image.components) #LINE# #TAB# vimage.data = image.data.reshape(image.shape, image.pixeltype) #LINE# #TAB# return vimage"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return context.player_security_glot_version #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# flatten = base_dict.copy() #LINE# #TAB# for k, v in extra_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# flatten.update(flatten_weights(v)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# flatten[k] = v #LINE# #TAB# return flatten"
#LINE# #TAB# vaulttype = opt.vault_type #LINE# #TAB# if vaulttype == 'azure': #LINE# #TAB# #TAB# return vault_client.transform_azure(opt) #LINE# #TAB# elif vaulttype == 'cyberark': #LINE# #TAB# #TAB# return vault_client.transform_cyberark(opt) #LINE# #TAB# elif vaulttype =='redhat': #LINE# #TAB# #TAB# return vault_client.transform_redhat(opt) #LINE# #TAB# elif vaulttype == 'azure': #LINE# #TAB# #TAB# return vault_client.transform_azure(opt) #LINE# #TAB# return []
"#LINE# #TAB# id, version = get_id_n_version(ident_hash) #LINE# #TAB# if id == 0: #LINE# #TAB# #TAB# return {} #LINE# #TAB# params = {} #LINE# #TAB# session = cnx_session() #LINE# #TAB# params['id'] = id #LINE# #TAB# params['version'] = version #LINE# #TAB# params['created_at'] = time.strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# params['updated_at'] = time.strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# params['version'] = version #LINE# #TAB# session.add(params) #LINE# #TAB# session.commit() #LINE# #TAB# return params"
"#LINE# #TAB# data = get_raw_blast(pdb_id, chain_id, output_form) #LINE# #TAB# return data"
#LINE# #TAB# global _xdg_guess #LINE# #TAB# _xdg_guess = guess
#LINE# #TAB# end = start + 5 #LINE# #TAB# while True: #LINE# #TAB# #TAB# ch = data_stream.read(1) #LINE# #TAB# #TAB# if not ch: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# start += 5 #LINE# #TAB# #TAB# if ch == '\x00': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return start
"#LINE# #TAB# fields = {'from': batch.sender, 'to': sorted(batch.recipients)} #LINE# #TAB# if batch.delivery_report: #LINE# #TAB# #TAB# fields['delivery_report'] = batch.delivery_report #LINE# #TAB# if batch.send_at: #LINE# #TAB# #TAB# fields['send_at'] = _html_send_at(batch.send_at) #LINE# #TAB# if batch.expire_at: #LINE# #TAB# #TAB# fields['expire_at'] = _html_expire_at(batch.expire_at) #LINE# #TAB# if batch.tags: #LINE# #TAB# #TAB# fields['tags'] = sorted(batch.tags) #LINE# #TAB# return fields"
"#LINE# #TAB# if power not in wg: #LINE# #TAB# #TAB# p1, p2 = power #LINE# #TAB# #TAB# if p1 =='m': #LINE# #TAB# #TAB# #TAB# return 0.0 #LINE# #TAB# #TAB# if p2 == 'g': #LINE# #TAB# #TAB# #TAB# return wg[0, 1] #LINE# #TAB# #TAB# if p3 == 'a': #LINE# #TAB# #TAB# #TAB# return wg[1, 2] #LINE# #TAB# #TAB# return wg[2, 0]"
"#LINE# #TAB# if url not in cache: #LINE# #TAB# #TAB# return None #LINE# #TAB# response = {} #LINE# #TAB# response_code, response_text = ge_rom_cache.get(url) #LINE# #TAB# if response_code: #LINE# #TAB# #TAB# response = json.loads(response_text) #LINE# #TAB# #TAB# cache[url] = response #LINE# #TAB# return response"
#LINE# #TAB# str_dic = '' #LINE# #TAB# for key in dic: #LINE# #TAB# #TAB# str_dic += str(key) +'' #LINE# #TAB# return str_dic
"#LINE# #TAB# slot = GetSlot(name, module) #LINE# #TAB# if not slot: #LINE# #TAB# #TAB# raise ValueError('No slot named %s' % name) #LINE# #TAB# return slot"
"#LINE# #TAB# body_backbone_names = [f.name for f in body_backbone_CNN] #LINE# #TAB# image_backbone_names = [f.name for f in image_backbone_CNN] #LINE# #TAB# weights_filenames = ['{}-weights.csv'.format(body_backbone_names[0]) for #LINE# #TAB# #TAB# body_backbone_name in body_backbone_names] #LINE# #TAB# csvlogger_filenames = ['{}-image-backbone-{}'.format(body_backbone_names[1], #LINE# #TAB# #TAB# image_backbone_names[0]) for image_backbone_name in image_backbone_names] #LINE# #TAB# return weights_filenames, csvlogger_filenames"
"#LINE# #TAB# data = sock.recv(1024) #LINE# #TAB# m = struct.unpack('>i', data) #LINE# #TAB# result = m[0] #LINE# #TAB# while len(data) < 10: #LINE# #TAB# #TAB# data += m[0] #LINE# #TAB# return result"
"#LINE# #TAB# n, m = covmatrix.shape #LINE# #TAB# nv = np.shape(covmatrix)[0] #LINE# #TAB# corrmatrix = np.zeros((n, n)) #LINE# #TAB# for i in range(m): #LINE# #TAB# #TAB# for j in range(n): #LINE# #TAB# #TAB# #TAB# corrmatrix[(i), :] = hd_corrat1(covmatrix[(i), :], covmatrix[(j), #LINE# #TAB# #TAB# #TAB# #TAB# :]) #LINE# #TAB# return corrmatrix"
#LINE# #TAB# pr_list_str = '' #LINE# #TAB# for entry in pr_list: #LINE# #TAB# #TAB# pr_list_str += entry + '\n' #LINE# #TAB# return pr_list_str
"#LINE# #TAB# default = default or entry.get(prop, default) #LINE# #TAB# if not raw: #LINE# #TAB# #TAB# return default #LINE# #TAB# return entry[prop]"
"#LINE# #TAB# if datatype in ['int8', 'uint8', 'uint16', 'uint32', 'uint64']: #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# elif datatype in ['int16', 'uint32', 'uint64']: #LINE# #TAB# #TAB# dbrcode = 2 #LINE# #TAB# elif datatype in ['uint8', 'uint16', 'uint32', 'uint64']: #LINE# #TAB# #TAB# dbrcode = 3 #LINE# #TAB# elif datatype in ['uint16', 'uint32', 'uint64']: #LINE# #TAB# #TAB# dbrcode = 4 #LINE# #TAB# return dbrcode, dtype"
"#LINE# #TAB# if mono: #LINE# #TAB# #TAB# return np.hanning(n) + 1e-05 #LINE# #TAB# else: #LINE# #TAB# #TAB# return np.array([1, 1]).T"
#LINE# #TAB# if not cls.uploa_allowed: #LINE# #TAB# #TAB# cls.uploa_allowed = False #LINE# #TAB# elif cls.user_model and cls.user_model.objects.filter(active=True #LINE# #TAB# #TAB# ).exists(): #LINE# #TAB# #TAB# cls.uploa_allowed = True #LINE# #TAB# else: #LINE# #TAB# #TAB# cls.uploa_allowed = False #LINE# #TAB# return cls.uploa_allowed
"#LINE# #TAB# if cls._argument_cache is None: #LINE# #TAB# #TAB# cls._argument_cache = dict() #LINE# #TAB# #TAB# for arg_name, arg_parser in inspect.getgetargs(cls): #LINE# #TAB# #TAB# #TAB# cls._argument_cache[arg_name] = arg_parser #LINE# #TAB# return cls._argument_cache"
#LINE# #TAB# for i in indices: #LINE# #TAB# #TAB# yield sequence[i]
#LINE# #TAB# global _MOLGENIS_CLASS #LINE# #TAB# if _MOLGENIS_CLASS is None: #LINE# #TAB# #TAB# _MOLGENIS_CLASS = find_module('mlgenis') #LINE# #TAB# return _MOLGENIS_CLASS
"#LINE# #TAB# event_bytes = np.zeros((len(events), len(slots))) #LINE# #TAB# slot_bytes = np.zeros((len(slots), len(events))) #LINE# #TAB# for row, event_slot in enumerate(events): #LINE# #TAB# #TAB# slot_bytes[row, slot] = 1 #LINE# #TAB# return event_bytes, slot_bytes"
"#LINE# #TAB# nd = _find_nd_read(name) #LINE# #TAB# if not nd: #LINE# #TAB# #TAB# nd = import_ #LINE# #TAB# if not os.path.exists(nd): #LINE# #TAB# #TAB# return None #LINE# #TAB# if os.path.isdir(nd): #LINE# #TAB# #TAB# return nd #LINE# #TAB# file_path = os.path.dirname(nd) #LINE# #TAB# spec = importlib.util.spec_from_file_location(os.path.join(file_path, import_)) #LINE# #TAB# nd = importlib.util.module_from_spec(spec) #LINE# #TAB# spec.loader.exec_module(nd) #LINE# #TAB# return nd"
#LINE# #TAB# ret = msg #LINE# #TAB# if cfg.profile.auth: #LINE# #TAB# #TAB# ret['auth'] = cfg.profile.auth.get('name') #LINE# #TAB# return ret
"#LINE# #TAB# path = os.path.join(_ROOT, 'items.json') #LINE# #TAB# items = read_json_file(path) #LINE# #TAB# version = items.get(item_name) #LINE# #TAB# if version: #LINE# #TAB# #TAB# return version #LINE# #TAB# else: #LINE# #TAB# #TAB# return '0.0'"
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if value > lower: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# round_ = round_ or (int(round_) * 100.0) #LINE# #TAB# if round_ is not None: #LINE# #TAB# #TAB# used = round_(used, round_) #LINE# #TAB# #TAB# total = round_(total, round_) #LINE# #TAB# return used / total"
"#LINE# #TAB# if enabled: #LINE# #TAB# #TAB# return enabled #LINE# #TAB# if sys.version_info < (3, 0): #LINE# #TAB# #TAB# return False #LINE# #TAB# if enabled and sys.version_info < (3, 0): #LINE# #TAB# #TAB# return False #LINE# #TAB# if enabled and hasattr(sys, 'getfilesystemencoding'): #LINE# #TAB# #TAB# globl_strip_relative_old = os.path.normcase(os.path.join(sys. #LINE# #TAB# #TAB# #TAB# getfilesystemencoding(), 'globl')).startswith('/') #LINE# #TAB# #TAB# if globl_strip_relative_old: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return next(modules) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# modules = modules[:] #LINE# #TAB# #TAB# for module in modules: #LINE# #TAB# #TAB# #TAB# if exists_cain(module): #LINE# #TAB# #TAB# #TAB# #TAB# return module #LINE# #TAB# #TAB# importlib.import_module('.'.join(modules)) #LINE# #TAB# #TAB# return None
"#LINE# #TAB# result = [] #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if k == which_key: #LINE# #TAB# #TAB# #TAB# result.append(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append((k, knx_generate(v, which_key))) #LINE# #TAB# return result"
#LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# with open(f) as f: #LINE# #TAB# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# #TAB# for key in data: #LINE# #TAB# #TAB# #TAB# #TAB# obj[key] = data[key] #LINE# #TAB# #TAB# return obj
#LINE# #TAB# import os #LINE# #TAB# base_module = os.path.splitext(os.path.basename(sys.argv[0]))[0] #LINE# #TAB# if base_module!= 'base': #LINE# #TAB# #TAB# base_module = os.path.splitext(base_module)[0] #LINE# #TAB# return base_module
"#LINE# #TAB# N = len(waves) #LINE# #TAB# M = np.zeros((N, N)) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# W = waves[i] #LINE# #TAB# #TAB# for g in gaps: #LINE# #TAB# #TAB# #TAB# if W[g]!= 0: #LINE# #TAB# #TAB# #TAB# #TAB# W = W[:g] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# W = W[:-1] #LINE# #TAB# #TAB# M[:, i] = W.real() #LINE# #TAB# return M"
#LINE# #TAB# if not depends: #LINE# #TAB# #TAB# return data #LINE# #TAB# res = [] #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# req = depends.get(item) #LINE# #TAB# #TAB# if req is None: #LINE# #TAB# #TAB# #TAB# res.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if req in res: #LINE# #TAB# #TAB# #TAB# #TAB# res.remove(req) #LINE# #TAB# #TAB# #TAB# res.append(item) #LINE# #TAB# res.sort() #LINE# #TAB# return res
#LINE# #TAB# finder = InSPIREFinder(ot=ot) #LINE# #TAB# txt = finder.extract_txt(search) #LINE# #TAB# if resultformat == 'brief': #LINE# #TAB# #TAB# text = finder.extract_txt_frm_inpire(txt) #LINE# #TAB# elif resultformat == 'list': #LINE# #TAB# #TAB# text = finder.extract_txt_frm_inpire(search) #LINE# #TAB# return text
"#LINE# #TAB# return [goea_res[0], goea_res[1], goea_res[2], goea_res[3], goea_res[4]]"
"#LINE# #TAB# response = client.list_buckets(Bucket=bucket_name) #LINE# #TAB# more_results = True #LINE# #TAB# while more_results: #LINE# #TAB# #TAB# response = client.get_bucket(Bucket=bucket_name) #LINE# #TAB# #TAB# if bucket_name in response['Buckets']: #LINE# #TAB# #TAB# #TAB# logger.debug('Bucket %s found in bucket %s', bucket_name, #LINE# #TAB# #TAB# #TAB# #TAB# response['Buckets'][0]['Name']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# more_results = False #LINE# #TAB# return more_results"
#LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# df['o_' + col] = df[col] * 100 #LINE# #TAB# return df
"#LINE# #TAB# sdm = getsdm(sdmname) #LINE# #TAB# sourcelist = sdm['Source'] #LINE# #TAB# gcddict = {} #LINE# #TAB# for s in sourcelist: #LINE# #TAB# #TAB# gcd = gcd_details(sdm, s) #LINE# #TAB# #TAB# gcddict['source'] = s #LINE# #TAB# #TAB# gcddict['dec'] = gcd_details(sdm, s) #LINE# #TAB# return gcddict"
"#LINE# #TAB# s = duration_to_seconds(duration) #LINE# #TAB# cstring = '{0:02d}:{1:02d}:{2:02d}'.format(s[0], s[1], s[2], s[3]) #LINE# #TAB# return cstring"
#LINE# #TAB# if db is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
#LINE# #TAB# try: #LINE# #TAB# #TAB# return s.index('-') + 1 #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return s
#LINE# #TAB# for request_addition in requested_additions: #LINE# #TAB# #TAB# if request_addition.tea_slug == tea_slug: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# yield 'runtime' #LINE# #TAB# yield 'original' #LINE# #TAB# yield 'frozen' #LINE# #TAB# yield 'frozen' #LINE# #TAB# yield'modules' #LINE# #TAB# yield 'frozen' #LINE# #TAB# for pattern, action in find_fil_reference_choice(): #LINE# #TAB# #TAB# if pattern == 'original': #LINE# #TAB# #TAB# #TAB# yield 'frozen' #LINE# #TAB# #TAB# elif pattern == 'frozen': #LINE# #TAB# #TAB# #TAB# yield 'original' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield action"
"#LINE# #TAB# ts = datetime.datetime.now() #LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# for output in cell.outputs: #LINE# #TAB# #TAB# #TAB# #TAB# if'segments' in output: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# output['segments'] = seg_commit(output['segments'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# timeout=timeout) #LINE# #TAB# #TAB# #TAB# #TAB# cell.metadata['commit'] = 'commit' #LINE# #TAB# return nb"
"#LINE# #TAB# if not isinstance(dir_name, str): #LINE# #TAB# #TAB# msg = '{0} is not a string'.format(dir_name) #LINE# #TAB# #TAB# raise ArgumentTypeError(msg) #LINE# #TAB# return dir_name"
"#LINE# #TAB# if hasattr(filterset_class, 'base_fields'): #LINE# #TAB# #TAB# return et_filterin_rgs_read_nested(filterset_class.base_fields, type) #LINE# #TAB# return {}"
#LINE# #TAB# default_value = [] #LINE# #TAB# for i in range(len(initialparameters)): #LINE# #TAB# #TAB# default_value.append(initialparameters[i]) #LINE# #TAB# return default_value
#LINE# #TAB# xml = ElementTree.fromstring(tree) #LINE# #TAB# for child in xml.iter(): #LINE# #TAB# #TAB# if child.tag == 'header': #LINE# #TAB# #TAB# #TAB# headers = headers_o_list(child) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# text = oLine.line.replace('# ', '') #LINE# #TAB# oLine.set_comment() #LINE# #TAB# return text"
#LINE# #TAB# symbols = set() #LINE# #TAB# for paradigm in paradigms: #LINE# #TAB# #TAB# symbols.update(paradigm.symbols) #LINE# #TAB# return symbols
#LINE# #TAB# global VIRTUAL_DISPATCHER #LINE# #TAB# if VIRTUAL_DISPATCHER: #LINE# #TAB# #TAB# VIRTUAL_DISPATCHER.terminate() #LINE# #TAB# VIRTUAL_DISPATCHER = None
"#LINE# #TAB# epoch = int(current_step / steps_per_epoch) #LINE# #TAB# loop = int(current_step % steps_per_loop) #LINE# #TAB# return epoch, loop, steps_per_epoch, steps_per_loop"
#LINE# #TAB# if not reserved_tokens: #LINE# #TAB# #TAB# return [] #LINE# #TAB# coords = [] #LINE# #TAB# for t in reserved_tokens: #LINE# #TAB# #TAB# if t == '.': #LINE# #TAB# #TAB# #TAB# coords.append(None) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# parts = (t.strip() + t.split('.')) #LINE# #TAB# #TAB# #TAB# coords[-1] = parts[-1].rstrip() #LINE# #TAB# coords = '[{}]'.format('|'.join(parts)) #LINE# #TAB# return coords
"#LINE# #TAB# for title in os.listdir(path): #LINE# #TAB# #TAB# fullpath = os.path.join(path, title) #LINE# #TAB# #TAB# if os.path.isdir(fullpath): #LINE# #TAB# #TAB# #TAB# rm_creation(fullpath) #LINE# #TAB# #TAB# elif os.path.isfile(fullpath): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.remove(fullpath) #LINE# #TAB# #TAB# #TAB# except OSError as e: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls._response_queue.get_nowait() #LINE# #TAB# except Empty: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# normvectpart = np.sqrt(quat[0] ** 2 + quat[1] ** 2 + quat[2] ** 2 + #LINE# #TAB# #TAB# quat[3] ** 2) #LINE# #TAB# angle = np.arccos(quat[3] / normvectpart) * 2.0 #LINE# #TAB# unitvec = np.array(quat[:3]) / np.sin(angle / 2) / normvectpart #LINE# #TAB# return unitvec, angle"
"#LINE# #TAB# with open(filepath) as f: #LINE# #TAB# #TAB# configuration = yaml.safe_load(f) #LINE# #TAB# config = OrderedDict() #LINE# #TAB# for item in configuration: #LINE# #TAB# #TAB# if isinstance(item, list): #LINE# #TAB# #TAB# #TAB# key = item[0] #LINE# #TAB# #TAB# #TAB# val = item[1] #LINE# #TAB# #TAB# #TAB# config[key] = val #LINE# #TAB# return config"
"#LINE# #TAB# tags = [] #LINE# #TAB# for item in ds.Image: #LINE# #TAB# #TAB# if item.HasField(""PRIVATE_TAG""): #LINE# #TAB# #TAB# #TAB# private_tags = [] #LINE# #TAB# #TAB# #TAB# for tag in item.Image.tags: #LINE# #TAB# #TAB# #TAB# #TAB# if tag!= ""PRIVATE_TAG"": #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# private_tags.append(tag) #LINE# #TAB# #TAB# if len(private_tags) > 0: #LINE# #TAB# #TAB# #TAB# tags.extend(private_tags) #LINE# #TAB# return tags"
"#LINE# #TAB# for oligo, pkg in oligo_dict.items(): #LINE# #TAB# #TAB# if oligo.startswith('NM'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for line in contents: #LINE# #TAB# #TAB# #TAB# if line.startswith('ATOM') or line.startswith('HETATM'): #LINE# #TAB# #TAB# #TAB# #TAB# s = line.split(' ')[1].strip() #LINE# #TAB# #TAB# #TAB# #TAB# oligo_status = s[0] #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return oligo_status"
"#LINE# #TAB# outer_keys, inner_keys = collect_nested_keys(nested_dict) #LINE# #TAB# compare_nested_dict = {} #LINE# #TAB# for k1 in outer_keys: #LINE# #TAB# #TAB# compare_nested_dict[k1] = nested_dict[k1] #LINE# #TAB# #TAB# for k2 in inner_keys: #LINE# #TAB# #TAB# #TAB# compare_nested_dict[k2] = nested_dict[k2] #LINE# #TAB# return compare_nested_dict"
"#LINE# #TAB# resp = requests.get(BASE_URL.format(game_id)) #LINE# #TAB# try: #LINE# #TAB# #TAB# resp.raise_for_status() #LINE# #TAB# except HTTPError as e: #LINE# #TAB# #TAB# if e.response.status_code == 404: #LINE# #TAB# #TAB# #TAB# return {'game_id': game_id,'status': 'not found'} #LINE# #TAB# #TAB# elif e.response.status_code!= 200: #LINE# #TAB# #TAB# #TAB# resp.raise_for_status() #LINE# #TAB# #TAB# return {'game_id': game_id,'status': 'ok'} #LINE# #TAB# return resp"
#LINE# #TAB# status = data['headers']['status'] #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# Memory.leave_room = data['room']
"#LINE# #TAB# parser.add_argument('--os-cache-api-version', metavar= #LINE# #TAB# #TAB# '<cache-api-version>', default=utils.env('OS_CACHE_API_VERSION', #LINE# #TAB# #TAB# default=DEFAULT_CACHE_API_VERSION), help=_( #LINE# #TAB# #TAB# 'Cache API version, default=%s (Env: OS_CACHE_API_VERSION)') % #LINE# #TAB# #TAB# DEFAULT_CACHE_API_VERSION) #LINE# #TAB# return parser"
"#LINE# #TAB# box_list = [] #LINE# #TAB# with open(path, 'r') as box_file: #LINE# #TAB# #TAB# for line in box_file: #LINE# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# #TAB# if line[0] == 'S': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# box_list.append(parse_candidates_boxfie(line)) #LINE# #TAB# #TAB# #TAB# #TAB# elif line[0] == 'P': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# box_list.append(parse_candidates_boxfie(line)) #LINE# #TAB# return box_list"
"#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif isinstance(value, six.binary_type): #LINE# #TAB# #TAB# return value.strip() #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# mdel = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(model_path, 'rb') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.decode('utf-8') #LINE# #TAB# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# #TAB# line = line.split('\t') #LINE# #TAB# #TAB# #TAB# #TAB# mdel[line[0]] = line[1] #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return mdel"
"#LINE# #TAB# v = archive_table.loc[(archive_table['version_column'] == 'v_id'), ( #LINE# #TAB# #TAB# archive_table['version_column'] == 'v_id_lat'), (archive_table['version_column'] == #LINE# #TAB# #TAB# 'v_id_lon'), (archive_table['version_column'] == 'v_id_lat'), ( #LINE# #TAB# #TAB# archive_table['version_column'] == 'v_id_lon'), (archive_table[ #LINE# #TAB# #TAB#'version_column'] == 'v_id_lat'), (archive_table['version_column'] == #LINE# #TAB# #TAB# 'v_id_lon'), (archive_table['version_column'] == 'v_id_lat'), ( #LINE# #TAB# #TAB# archive_table['version_column'] == 'v_id_lon'), (archive_table['version_column'] == #LINE# #TAB# #TAB# 'v_id')] #LINE# #TAB# return v"
#LINE# #TAB# ctype = ContentType.objects.get_for_model(cls) #LINE# #TAB# return cls.objects.filter(id__in=ctype.objects.all().annotate( #LINE# #TAB# #TAB# content_type=ctype.model_name)).annotate(content_type=ctype. #LINE# #TAB# #TAB# content_type.objects.filter(content_type__isnull=False)).order_by('-content_type')[: #LINE# #TAB# #TAB# -1]
"#LINE# tasks = {} #LINE# fname = pkg_resources.resource_filename(__name__,'resources/TaskNumbers.csv') #LINE# with open(fname, 'rU') as csvfile: #LINE# #TAB# reader = csv.reader(csvfile, delimiter = ',') #LINE# #TAB# for row in reader: #LINE# #TAB# tasks[row[0]] = row[1] #LINE# return tasks"
"#LINE# #TAB# x = [(i, 1) for i in p] #LINE# #TAB# y = [(i, 2) for i in p] #LINE# #TAB# return x, y"
#LINE# #TAB# if scenario_uuid == 'SCenarios': #LINE# #TAB# #TAB# return'scenarios/' #LINE# #TAB# elif scenario_uuid == 'Network': #LINE# #TAB# #TAB# return 'network/' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '/scenarios/' + scenario_uuid + '/'
#LINE# #TAB# if params: #LINE# #TAB# #TAB# regex = re.compile('(?P<name>[A-Za-z_-]+)') #LINE# #TAB# #TAB# doc = 'Parameters\n\n' #LINE# #TAB# #TAB# for line in params.splitlines(): #LINE# #TAB# #TAB# #TAB# match = regex.match(line) #LINE# #TAB# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# #TAB# doc +='' #LINE# #TAB# #TAB# #TAB# #TAB# doc += match.group('name') #LINE# #TAB# #TAB# #TAB# doc += '\n' #LINE# #TAB# #TAB# return doc #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
"#LINE# #TAB# itsdir = os.path.join('pyauth', 'istdir') #LINE# #TAB# return itsdir"
"#LINE# #TAB# model_list = set() #LINE# #TAB# status, output = call((git_path, 'log', '-1', '--pretty=%H'], cwd=os.getcwd(), #LINE# #TAB# #TAB# stdout=PIPE, stderr=PIPE)) #LINE# #TAB# if status: #LINE# #TAB# #TAB# model_list.add((status, output.decode('utf-8'))) #LINE# #TAB# else: #LINE# #TAB# #TAB# model_list.remove((status, output.decode('utf-8'))) #LINE# #TAB# return model_list"
"#LINE# #TAB# bone_name, extension = os.path.splitext(name) #LINE# #TAB# if topping: #LINE# #TAB# #TAB# bone_name += '_topping' #LINE# #TAB# return bone_name + extension"
#LINE# #TAB# cls.show_arser = True #LINE# #TAB# return cls
#LINE# #TAB# result = '' #LINE# #TAB# for attr in attribute_set: #LINE# #TAB# #TAB# result += attr +'' #LINE# #TAB# return result
"#LINE# #TAB# resurce_name = request.GET.get('kinto.core', None) #LINE# #TAB# resource_id = request.GET.get('resource', None) #LINE# #TAB# viewset_name = request.GET.get('viewset', None) #LINE# #TAB# if viewset_name: #LINE# #TAB# #TAB# return '%s.%s' % (resource_name, viewset_name) #LINE# #TAB# return resource_id"
#LINE# #TAB# p = request.json['action']['detailParameters'] #LINE# #TAB# return p
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# use = use.strip() #LINE# #TAB# if use and not isinstance(use, str): #LINE# #TAB# #TAB# use = ""'{}'"".format(use) #LINE# #TAB# if kid and not isinstance(kid, str): #LINE# #TAB# #TAB# kid = kid #LINE# #TAB# new_key = Key(kid, key) #LINE# #TAB# new_key.use = use #LINE# #TAB# new_key.kid = kid #LINE# #TAB# return new_key"
"#LINE# #TAB# size = len(list1) #LINE# #TAB# sum1 = sum(list1) #LINE# #TAB# sum2 = sum(list2) #LINE# #TAB# sum_sq1 = sum([pow(l, 2) for l in list1]) #LINE# #TAB# sum_sq2 = sum([pow(l, 2) for l in list2]) #LINE# #TAB# conv = sum1 * sum2 / (sum1 + sum2) #LINE# #TAB# return conv"
"#LINE# #TAB# rewriteParams = {} #LINE# #TAB# for head in allHeadersContent: #LINE# #TAB# #TAB# if head.startswith('HTTP_'): #LINE# #TAB# #TAB# #TAB# head = head.replace('HTTP_', '') #LINE# #TAB# #TAB# if ':' in head: #LINE# #TAB# #TAB# #TAB# head = head.replace(':', '') #LINE# #TAB# #TAB# for param in allHeadersContent: #LINE# #TAB# #TAB# #TAB# if param.startswith('$'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if param in rewriteParams: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# rewriteParams[param] = '' #LINE# #TAB# #TAB# rewriteParams.update(head) #LINE# #TAB# return rewriteParams"
"#LINE# #TAB# if cause_string: #LINE# #TAB# #TAB# cause_list = ','.join(cause_string.split(',')) #LINE# #TAB# #TAB# queryset = queryset.filter(**{'%s__icontains' % cause_list: True}) #LINE# #TAB# return queryset"
"#LINE# #TAB# total = [] #LINE# #TAB# points = [] #LINE# #TAB# for subcell_content in row_content: #LINE# #TAB# #TAB# subcell_points = [] #LINE# #TAB# #TAB# for subcell in subcell_content: #LINE# #TAB# #TAB# #TAB# if isinstance(subcell, LTTextLine): #LINE# #TAB# #TAB# #TAB# #TAB# subcell_points.append(subcell) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# subcell_points.append(subcell) #LINE# #TAB# #TAB# total.append(points) #LINE# #TAB# return total"
"#LINE# #TAB# def_lo = u'#define-lo-definition\n\n' #LINE# #TAB# for line in open(os.devnull, 'w'): #LINE# #TAB# #TAB# f = open(os.devnull, 'w') #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line.startswith(u'#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith(u'#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# def_lo += u'#define-lo-definition\n' #LINE# #TAB# with open(os.devnull, 'w') as f: #LINE# #TAB# #TAB# f.write(def_lo) #LINE# #TAB# os.unlink(def_lo) #LINE# #TAB# return def_lo"
#LINE# #TAB# global _batcher #LINE# #TAB# if _batcher is None: #LINE# #TAB# #TAB# _batcher = cls() #LINE# #TAB# return _batcher
"#LINE# #TAB# b0 = 0.0641 * 3 / 4 #LINE# #TAB# b1 = 2.255 * 3 / 4 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.0 * 3 / 2 ** (5 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Na'] * i2c['S2O3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
#LINE# #TAB# cmdline = [] #LINE# #TAB# for iface in intfspec: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cmdline.append(iface['cmdline']) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return cmdline
#LINE# #TAB# try: #LINE# #TAB# #TAB# version = db.session.query(models.Version).filter_by(id=model_id).one()[0][0] #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# return None #LINE# #TAB# if len(version) == 1: #LINE# #TAB# #TAB# return version[0] #LINE# #TAB# return None
#LINE# #TAB# number_length = len(buffer) #LINE# #TAB# if field_type == FloatType: #LINE# #TAB# #TAB# return 'Float' #LINE# #TAB# elif field_type == BooleanType: #LINE# #TAB# #TAB# return 'Boolean' #LINE# #TAB# elif field_type == IntegerType: #LINE# #TAB# #TAB# return 'Integer' #LINE# #TAB# elif field_type == FloatType: #LINE# #TAB# #TAB# return 'Float' #LINE# #TAB# elif is_string: #LINE# #TAB# #TAB# return 'String' #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# parser = argparse.ArgumentParser() #LINE# #TAB# parser.add_argument('--prse-version', action='version', version=__version__) #LINE# #TAB# args = parser.parse_args() #LINE# #TAB# return args"
"#LINE# #TAB# #TAB# checkpoint = os.path.join(checkpointDir,'model.checkpoint') #LINE# #TAB# #TAB# model = cls.readFromCheckpoint(checkpoint) #LINE# #TAB# #TAB# model.checkpoint = checkpoint #LINE# #TAB# #TAB# return model"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return os.kill(pid, 0) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# decoder = BitPackedDecoder(data) #LINE# #TAB# msg = decoder.decode() #LINE# #TAB# return msg
"#LINE# #TAB# indents = [] #LINE# #TAB# add_start = False #LINE# #TAB# for i, char in enumerate(code): #LINE# #TAB# #TAB# if include_start: #LINE# #TAB# #TAB# #TAB# indents.append(char) #LINE# #TAB# #TAB# elif char == '\n': #LINE# #TAB# #TAB# #TAB# add_start = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# indents.append(char) #LINE# #TAB# if add_start: #LINE# #TAB# #TAB# return ''.join(indents) #LINE# #TAB# else: #LINE# #TAB# #TAB# return indents"
#LINE# #TAB# if not repo: #LINE# #TAB# #TAB# return False #LINE# #TAB# if repo == 'fe.buildtimetrend': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# sig = inspect.signature(func) #LINE# #TAB# n_inputs = getattr(sig, '_n_inputs', 0) #LINE# #TAB# if n_inputs is None: #LINE# #TAB# #TAB# n_inputs = 1 #LINE# #TAB# for input_type in input_type: #LINE# #TAB# #TAB# if not isinstance(input_type, InputType): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if input_type.size > n_inputs: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return sig"
#LINE# #TAB# entry_read_tags = [] #LINE# #TAB# for tag in html: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# entry_read_tags.append(tag) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return entry_read_tags
"#LINE# #TAB# frozen_pool = _get_frozen_pool() #LINE# #TAB# enabled = False #LINE# #TAB# for key, val in six.iteritems(kwargs): #LINE# #TAB# #TAB# if not key in frozen_pool: #LINE# #TAB# #TAB# #TAB# frozen_pool[key] = val #LINE# #TAB# #TAB# #TAB# enabled = True #LINE# #TAB# #TAB# elif val is None: #LINE# #TAB# #TAB# #TAB# del frozen_pool[key] #LINE# #TAB# if frozen_pool: #LINE# #TAB# #TAB# _confiure_shell(frozen_pool) #LINE# #TAB# return enabled"
"#LINE# #TAB# expr = expression.Expression('v{} {}'.format(vid, expr)) #LINE# #TAB# cols = dtable.columns(vid) #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# col = col.name #LINE# #TAB# #TAB# mask = expr.evaluate(dtable, {vid: col}) #LINE# #TAB# #TAB# dtable[mask, col] = np.nan"
"#LINE# #TAB# y = poly_hartree(a, k1 * (x - x1) + k2 * (x - x2)) #LINE# #TAB# return y"
"#LINE# #TAB# for d in contexts(): #LINE# #TAB# #TAB# sock = d.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# if sock.connect_ex((addr,)): #LINE# #TAB# #TAB# #TAB# return sock #LINE# #TAB# return None"
#LINE# #TAB# new_row = {} #LINE# #TAB# new_row['id'] = row['id'] #LINE# #TAB# new_row['collection_name'] = collection #LINE# #TAB# new_row['version'] = row['version'] #LINE# #TAB# return new_row
#LINE# #TAB# try: #LINE# #TAB# #TAB# return importlib.import_module(module) is not None #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# path = os.path.join(ge_env_dir(), name) #LINE# #TAB# return path"
#LINE# #TAB# erm_j = j1 - j2 #LINE# #TAB# return erm_j
"#LINE# #TAB# assert tp.is_function_type() #LINE# #TAB# result = [] #LINE# #TAB# for member in tp.type_members: #LINE# #TAB# #TAB# if not isinstance(member, TypeVarType): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if member.type.fullname not in {'FunctionDef', 'FunctionDef'}: #LINE# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# return result"
#LINE# #TAB# url = urlparse(path) #LINE# #TAB# if url.scheme!= 'file': #LINE# #TAB# #TAB# return f'{url.scheme}://{url.netloc}'
#LINE# #TAB# try: #LINE# #TAB# #TAB# return value == 1.0 #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# resolve_win(port) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# phi = np.arccos(vectors[0]) #LINE# #TAB# theta = np.arccos(vectors[1]) #LINE# #TAB# return phi, theta"
"#LINE# #TAB# parser = HyperKittyArgumentParser() #LINE# #TAB# parser.add_argument('--config-file', dest='config_file', help= #LINE# #TAB# #TAB# 'Path to the HyperKitty configuration file.') #LINE# #TAB# parser.add_argument('--listen', dest='listen_port', help= #LINE# #TAB# #TAB# 'Port to listen on.') #LINE# #TAB# return parser"
#LINE# #TAB# arr_view = generate_arra_parts_fro(vnl_vector) #LINE# #TAB# ndarr_view = arr_view.view(np.ndarray) #LINE# #TAB# ndarr_view[:] = vnl_vector #LINE# #TAB# return arr_view
"#LINE# #TAB# if LOG_ENABLED: #LINE# #TAB# #TAB# return #LINE# #TAB# formatter = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(levelname)s - %(module)s - %(message)s', datefmt='%H:%M:%S' #LINE# #TAB# #TAB# ) #LINE# #TAB# if LOG_ENABLED: #LINE# #TAB# #TAB# logger = logging.getLogger(LOG_NAME) #LINE# #TAB# #TAB# for handler in logger.handlers: #LINE# #TAB# #TAB# #TAB# if isinstance(handler, logging.StreamHandler): #LINE# #TAB# #TAB# #TAB# #TAB# line = formatter.format_list([handler]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# line = formatter.format_list([handler]) #LINE# #TAB# #TAB# logger.info(line) #LINE# #TAB# return formatter"
"#LINE# #TAB# head, msg = messageString.split('\r\n', 1) #LINE# #TAB# recData = [] #LINE# #TAB# for l in msg: #LINE# #TAB# #TAB# if l!= '\n': #LINE# #TAB# #TAB# #TAB# if len(l) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# recData.append(l[1:]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# recData.append(l) #LINE# #TAB# return recData"
"#LINE# #TAB# app_vesion_name = getattr(sys.modules[__name__], 'get_app_vesion_name_nested', None) #LINE# #TAB# if app_vesion_name: #LINE# #TAB# #TAB# app_version_name = app_vesion_name.split('.')[-1] #LINE# #TAB# #TAB# current_version = '.'.join(app_version_name.split('.')) #LINE# #TAB# #TAB# if current_version == '': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return current_version #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if target['position'] is None: #LINE# #TAB# #TAB# cursor = connection.cursor() #LINE# #TAB# #TAB# target['position'] = connection.execute( #LINE# #TAB# #TAB# #TAB# ).fetchone()[0] + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# for row in connection.execute( #LINE# #TAB# #TAB# #TAB# ).fetchall(): #LINE# #TAB# #TAB# #TAB# if row['position'] == target['position']: #LINE# #TAB# #TAB# #TAB# #TAB# row['position'] = connection.execute( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ).fetchone()[0] #LINE# #TAB# #TAB# #TAB# target['position'] = cursor.fetchone()[0]
"#LINE# #TAB# kstr = kstr.strip() #LINE# #TAB# while kstr: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return conn.keysym.lookup(bytes(kstr, 'utf-8')) #LINE# #TAB# #TAB# except pyhsm.exception.KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# kstr = kstr.encode('utf-8') #LINE# #TAB# return None"
#LINE# #TAB# out = [] #LINE# #TAB# for line in txt.splitlines(): #LINE# #TAB# #TAB# found = False #LINE# #TAB# #TAB# for t in line.split(): #LINE# #TAB# #TAB# #TAB# if t == 'TAF': #LINE# #TAB# #TAB# #TAB# #TAB# found = True #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# elif found and t!= 'TAF': #LINE# #TAB# #TAB# #TAB# #TAB# out.append(t) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if found: #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# return [hex_frm_dict(prefix, suffix) for prefix, suffix in zip( #LINE# #TAB# #TAB# prefix, suffix)]"
"#LINE# #TAB# node_choices = [] #LINE# #TAB# if model.is_leaf(): #LINE# #TAB# #TAB# node_choices.append(('--root-node', '--root')) #LINE# #TAB# if for_node: #LINE# #TAB# #TAB# nodes = model.get_terminals(for_node) #LINE# #TAB# #TAB# for node in nodes: #LINE# #TAB# #TAB# #TAB# node_choices.append((node.id, str(node))) #LINE# #TAB# else: #LINE# #TAB# #TAB# for node in model.get_terminals(): #LINE# #TAB# #TAB# #TAB# for choice in cls.compress_drodown_requirements(node, for_node): #LINE# #TAB# #TAB# #TAB# #TAB# node_choices.append((node.id, choice)) #LINE# #TAB# return node_choices"
#LINE# #TAB# from sicor import symbols #LINE# #TAB# gt_seqs = [] #LINE# #TAB# for st in body: #LINE# #TAB# #TAB# if st.location == location: #LINE# #TAB# #TAB# #TAB# gt_seqs.append(st.astronomic) #LINE# #TAB# #TAB# elif st.location == date: #LINE# #TAB# #TAB# #TAB# gt_seqs.append(st.azimuth) #LINE# #TAB# #TAB# elif st.location == location: #LINE# #TAB# #TAB# #TAB# gt_seqs.append(st.altitude) #LINE# #TAB# if len(gt_seqs) > 1: #LINE# #TAB# #TAB# return '\n'.join(gt_seqs) #LINE# #TAB# else: #LINE# #TAB# #TAB# return '\n'.join(gt_seqs) + '\n'
"#LINE# #TAB# script_location = os.path.dirname(os.path.abspath(sys.argv[0])) #LINE# #TAB# script_path = os.path.join(script_location, name) #LINE# #TAB# if os.path.isfile(script_path): #LINE# #TAB# #TAB# return os.path.abspath(os.path.join(script_path, name)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return name"
"#LINE# #TAB# return cls.build_send_payload('delete_directory_disabed', {'cacheDisabled': #LINE# #TAB# #TAB# cacheDisabled}), None"
"#LINE# #TAB# for root, subdirs, files in os.walk(dir): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# yield root, file #LINE# #TAB# #TAB# for subdir in subdirs: #LINE# #TAB# #TAB# #TAB# for file in path_iterate(subdir): #LINE# #TAB# #TAB# #TAB# #TAB# yield file"
#LINE# #TAB# fields = name.split('_') #LINE# #TAB# if len(fields) == 2: #LINE# #TAB# #TAB# return fields[1] #LINE# #TAB# elif len(fields) == 1: #LINE# #TAB# #TAB# return fields[0]
"#LINE# #TAB# canonical, https, httpswww = (domain.canonical, domain.https, domain. #LINE# #TAB# #TAB# httpswww) #LINE# #TAB# if canonical.host == '127.0.0.1': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# for i in source_iter: #LINE# #TAB# #TAB# if i >= n: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# return [i]
"#LINE# #TAB# frame = inspect.currentframe().f_back #LINE# #TAB# while frame.f_back and frame.f_globals.get(name, None) is not None: #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# if name == '__locals__' and frame.f_code.co_filename == '__main__': #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# results = [] #LINE# #TAB# if isinstance(pat, str): #LINE# #TAB# #TAB# pat = re.compile(pat) #LINE# #TAB# for key, op in _assign_dict.items(): #LINE# #TAB# #TAB# if op in pat: #LINE# #TAB# #TAB# #TAB# results.append(key) #LINE# #TAB# if isinstance(pat, dict): #LINE# #TAB# #TAB# for pat_type, pat_list in _assign_dict.items(): #LINE# #TAB# #TAB# #TAB# for pat_list in pat_list: #LINE# #TAB# #TAB# #TAB# #TAB# if pat_type == pat.type: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# results.append(key) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return results"
"#LINE# #TAB# if not isinstance(apps_list, (list, tuple)): #LINE# #TAB# #TAB# apps_list = [apps_list] #LINE# #TAB# labels = [] #LINE# #TAB# for app in apps_list: #LINE# #TAB# #TAB# app = apps.get_app_config(app) #LINE# #TAB# #TAB# if app not in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# #TAB# labels.append(app) #LINE# #TAB# labels = sorted(labels) #LINE# #TAB# return labels"
"#LINE# #TAB# conn = sqlite3.connect(':memory:') #LINE# #TAB# c = conn.cursor() #LINE# #TAB# c.execute('SELECT * FROM {};'.format('*')) #LINE# #TAB# c.execute('SELECT * FROM {};'.format('*')) #LINE# #TAB# c.execute('SELECT * FROM {}'.format('*')) #LINE# #TAB# path = c.fetchone()[0] #LINE# #TAB# c.close() #LINE# #TAB# conn.commit() #LINE# #TAB# query = ( #LINE# #TAB# #TAB# 'SELECT * FROM {};'.format('*')) #LINE# #TAB# for row in c.fetchall(): #LINE# #TAB# #TAB# match = re.search(query, row) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# conn.close() #LINE# #TAB# #TAB# #TAB# return match.group(1) #LINE# #TAB# return ''"
"#LINE# #TAB#uer_ifo = type('UserClass', (User,), {}) #LINE# #TAB#uer_ifo['email'] = user.email #LINE# #TAB# user_ifo['first_name'] = user.first_name #LINE# #TAB# user_ifo['last_name'] = user.last_name #LINE# #TAB# user_ifo['email'] = user.email #LINE# #TAB# user_ifo['is_staff'] = user.is_staff #LINE# #TAB# user_ifo['is_superuser'] = user.is_superuser #LINE# #TAB# user_ifo['is_staff'] = user.is_staff #LINE# #TAB# user_ifo['user_id'] = str(user.id) #LINE# #TAB# return user_ifo"
"#LINE# #TAB# th = threading.Thread(target=worker.run, args=(parent, worker)) #LINE# #TAB# th.daemon = True #LINE# #TAB# th.start() #LINE# #TAB# if deleteWorkerLater: #LINE# #TAB# #TAB# th.daemon = False #LINE# #TAB# return th"
"#LINE# #TAB# template_dir = get_template_dir() #LINE# #TAB# index_file = os.path.join(template_dir, ""index"") #LINE# #TAB# textfsm_obj = clitable.CliTable(index_file, template_dir) #LINE# #TAB# try: #LINE# #TAB# #TAB# entries = textfsm_obj.get_entries(platform, command) #LINE# #TAB# #TAB# structured_data = [] #LINE# #TAB# #TAB# for index, entry in enumerate(entries): #LINE# #TAB# #TAB# #TAB# if entry['platform']!= platform: #LINE# #TAB# #TAB# #TAB# #TAB# structured_data.append(dict(index=index, command=command, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# raw_output=raw_output)) #LINE# #TAB# #TAB# return structured_data #LINE# #TAB# except CliTableError: #LINE# #TAB# #TAB# return raw_output"
"#LINE# #TAB# if 'keys' in src: #LINE# #TAB# #TAB# for k in src['keys']: #LINE# #TAB# #TAB# #TAB# if k in tgt: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(tgt[k], dict) and isinstance(src[k], dict): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# json_update_val(tgt[k], src[k]) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# tgt[k] = src[k] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tgt[k] = src.get(k) #LINE# #TAB# return tgt"
#LINE# #TAB# rv = np.zeros(3) #LINE# #TAB# m1 = mass1 / mass2 #LINE# #TAB# m2 = mass2 / mass1 #LINE# #TAB# xi = xi2 / mass2 #LINE# #TAB# a = (m1 * mass2) ** 2 / (m2 * mass1) #LINE# #TAB# b = (m1 * mass2) ** 2 / (m2 * mass1) #LINE# #TAB# sina = np.sin(a) #LINE# #TAB# cosa = np.cos(a) #LINE# #TAB# sinb = np.sin(b) #LINE# #TAB# rv[a < 0] = 0 #LINE# #TAB# rv[b > 0] = 1 #LINE# #TAB# return rv
#LINE# #TAB# if eps == 0.0: #LINE# #TAB# #TAB# return alpha #LINE# #TAB# sign = np.sign(alpha) #LINE# #TAB# num = 0 #LINE# #TAB# for k in alpha: #LINE# #TAB# #TAB# if sign * k < eps: #LINE# #TAB# #TAB# #TAB# num += 1 #LINE# #TAB# return num
#LINE# #TAB# ConversionFinder): #LINE# #TAB# object_parser = MultifileObjectParser(parser_finder) #LINE# #TAB# object_parser.finder = parser_finder #LINE# #TAB# conversion_finder = conversion_finder #LINE# #TAB# return object_parser
"#LINE# #TAB# log = '<html>\n<head>\n' #LINE# #TAB# if hasattr(e, 'errorString'): #LINE# #TAB# #TAB# log += '<h1>%s</h1>' % e.errorString #LINE# #TAB# if hasattr(e, 'description'): #LINE# #TAB# #TAB# log += '<p>%s</p>' % e.description #LINE# #TAB# else: #LINE# #TAB# #TAB# log += '<p>%s</p>' % e.description #LINE# #TAB# return log"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# old_title = tkinter.Tk() #LINE# #TAB# #TAB# old_message = tkinter.Tk() #LINE# #TAB# #TAB# tkinter.Tk() #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# tkinter.Tk() #LINE# #TAB# #TAB# if old_title is None: #LINE# #TAB# #TAB# #TAB# del old_title #LINE# #TAB# #TAB# elif old_message is not None: #LINE# #TAB# #TAB# #TAB# tkinter.Tk() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield old_title, old_message"
"#LINE# #TAB# sign, number = str(number), int(number) #LINE# #TAB# if number == 0: #LINE# #TAB# #TAB# return '++' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '-' + sign"
#LINE# #TAB# num_sets = len(sets) #LINE# #TAB# covered = 0 #LINE# #TAB# for s in sets: #LINE# #TAB# #TAB# for v in s: #LINE# #TAB# #TAB# #TAB# if v in covered: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# covered += 1 #LINE# #TAB# return num_sets & covered
"#LINE# #TAB# func_name = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# new_scope_name = relative_scope_name #LINE# #TAB# while new_scope_name!= '__main__': #LINE# #TAB# #TAB# new_scope_name = '{}.{}'.format(func_name, new_scope_name) #LINE# #TAB# return new_scope_name"
#LINE# #TAB# #TAB# if not silent: #LINE# #TAB# #TAB# #TAB# c = cls.get_action_subject(uri) #LINE# #TAB# #TAB# #TAB# if c is not None: #LINE# #TAB# #TAB# #TAB# #TAB# return c #LINE# #TAB# #TAB# c.load() #LINE# #TAB# #TAB# return None
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return [sta_win.win, getattr(sta_win, 'name', None), getattr(sta_win, #LINE# #TAB# #TAB# #TAB# 'channel_count', None)] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return []"
"#LINE# #TAB# lambda_1 = lambda_fiel(a, n) #LINE# #TAB# lambda_2 = lambda_fiel(a, n - 1) #LINE# #TAB# return lambda_1 + lambda_2"
"#LINE# #TAB# template = """""" #LINE# #TAB# #TAB# ip_proxy: #LINE# #TAB# #TAB# ip_proxy.netloc#TAB# #TAB# #TAB# :{ #LINE# #TAB# #TAB# #TAB# }#TAB# #TAB# :{ #LINE# #TAB# #TAB# #TAB# http_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# https_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# http_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# https_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# http_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'http_proxy: { #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# } """""" #LINE# #TAB"
"#LINE# #TAB# if password is None or not is_password_usable(encoded): #LINE# #TAB# #TAB# return False #LINE# #TAB# preferred = get_hasher(preferred) #LINE# #TAB# hasher = identify_hasher(encoded) #LINE# #TAB# must_update = hasher.algorithm!= preferred.algorithm #LINE# #TAB# is_correct = hasher.verify(password, encoded) #LINE# #TAB# if setter and is_correct and must_update: #LINE# #TAB# #TAB# setter(password) #LINE# #TAB# return is_correct"
#LINE# #TAB# if request.authorization and request.authorization[0] == 'Basic': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# global _swif_task_discovered #LINE# #TAB# _swif_task_discovered = True
#LINE# #TAB# base = os.path.basename(file) #LINE# #TAB# prodct = base[:-3].split('-')[0] #LINE# #TAB# return prodct
"#LINE# #TAB# global networkjson #LINE# #TAB# if networkjson is None: #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# #TAB# 'network.json'), 'r') as f: #LINE# #TAB# #TAB# #TAB# networkjson = json.load(f) #LINE# #TAB# except: #LINE# #TAB# #TAB# return #LINE# #TAB# if os.path.exists(networkjson.__file__): #LINE# #TAB# #TAB# with open(networkjson.__file__, 'w') as f: #LINE# #TAB# #TAB# #TAB# json.dump(networkjson, f, indent=2) #LINE# #TAB# return networkjson"
"#LINE# #TAB# pars = [] #LINE# #TAB# pattern = re.compile('^\\s*?\\[(.*)\\]\\s*$') #LINE# #TAB# for line in re.finditer(pattern, ing): #LINE# #TAB# #TAB# name, _, comment = line.groups() #LINE# #TAB# #TAB# match = re.match(pattern, name) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# pars.append(match.group(1)) #LINE# #TAB# pars = re.sub('\\s+','', pars) #LINE# #TAB# return pars"
#LINE# #TAB# with open(filepath) as hff_file: #LINE# #TAB# #TAB# json_data = json.load(hff_file) #LINE# #TAB# variable = json_data[property] #LINE# #TAB# return variable
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('tmpdir', dtype='string', direction=function.OUT, #LINE# #TAB# #TAB# description='name of the temporary directory') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# Current value was set #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# Directory does not exist #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
"#LINE# #TAB# client = _get_client() #LINE# #TAB# client.db_url = db_url #LINE# #TAB# client.db_name = db_name #LINE# #TAB# keys = ['url', 'user', 'password', 'host', 'port', 'db'] #LINE# #TAB# for k in keys: #LINE# #TAB# #TAB# setattr(client, k, os.environ[k]) #LINE# #TAB# client.headers = {'Content-Type': 'application/json', 'Accept': #LINE# #TAB# #TAB# 'application/json'}"
#LINE# #TAB# demux_samples = get_demux_samples() #LINE# #TAB# found = [] #LINE# #TAB# for sample in demux_samples: #LINE# #TAB# #TAB# if sample['id'] == sample_id: #LINE# #TAB# #TAB# #TAB# found.append(sample) #LINE# #TAB# if len(found) > 0: #LINE# #TAB# #TAB# return found[0]
"#LINE# #TAB# return get_block_overview(block_representation=block_representation, #LINE# #TAB# #TAB# #TAB# coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['hsh']"
#LINE# #TAB# dF = Ft / Fo #LINE# #TAB# return dF
"#LINE# #TAB# user = request.user #LINE# #TAB# if user.is_authenticated(): #LINE# #TAB# #TAB# username = user.username #LINE# #TAB# else: #LINE# #TAB# #TAB# username = 'instagram' #LINE# #TAB# pid = username + '-' + password + '-' + request.session.get( #LINE# #TAB# #TAB# 'instagram_auth_token', None) #LINE# #TAB# return pid"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if isinstance(field, list) and len(field) == 1: #LINE# #TAB# #TAB# #TAB# return field[0], field[0] #LINE# #TAB# #TAB# elif isinstance(field, tuple): #LINE# #TAB# #TAB# #TAB# return tuple(field) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None, None #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None, None"
"#LINE# #TAB# results = [] #LINE# #TAB# for series in seriesList: #LINE# #TAB# #TAB# series.name = ""boundary_belw(%s)"" % (series.name) #LINE# #TAB# #TAB# val = safeMin(series) #LINE# #TAB# #TAB# if val is not None and val >= n: #LINE# #TAB# #TAB# #TAB# results.append(series) #LINE# #TAB# return results"
#LINE# #TAB# sets = [None] * depth #LINE# #TAB# for cmdset in cmdsets: #LINE# #TAB# #TAB# h5c = dict() #LINE# #TAB# #TAB# for job1 in job: #LINE# #TAB# #TAB# #TAB# h5c[job1] = set(cmdset) #LINE# #TAB# #TAB# #TAB# for job2 in job: #LINE# #TAB# #TAB# #TAB# #TAB# h5c[job2].add(job1) #LINE# #TAB# #TAB# for job2 in job: #LINE# #TAB# #TAB# #TAB# h5c[job2] = set(h5c[job2]) #LINE# #TAB# #TAB# sets[depth] = h5c #LINE# #TAB# return sets
"#LINE# #TAB# width, height = img1.shape[:2] #LINE# #TAB# b = img2.shape[:2] #LINE# #TAB# if width > height: #LINE# #TAB# #TAB# return False #LINE# #TAB# if padding > 0: #LINE# #TAB# #TAB# left = (width - padding) / 2 #LINE# #TAB# #TAB# right = (height - padding) / 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# left = 0 #LINE# #TAB# if padding > 0: #LINE# #TAB# #TAB# right = (width - padding) / 2 #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# resname = node1.get('resname') #LINE# #TAB# resname_abs = '' if resname == '' else resname #LINE# #TAB# PTM_atom = node2.get('PCM_atom') #LINE# #TAB# if PTM_atom: #LINE# #TAB# #TAB# for n in range(0, len(node2)): #LINE# #TAB# #TAB# #TAB# if PTM_atom[n] == '*': #LINE# #TAB# #TAB# #TAB# #TAB# n = node2 #LINE# #TAB# #TAB# #TAB# elif PTM_atom[n] == '*': #LINE# #TAB# #TAB# #TAB# #TAB# n = node1 #LINE# #TAB# return resname, PTM_atom"
"#LINE# #TAB# types = set() #LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if isinstance(val, h5py.Dataset): #LINE# #TAB# #TAB# #TAB# types.add(val.dtype) #LINE# #TAB# #TAB# elif isinstance(val, list): #LINE# #TAB# #TAB# #TAB# for v in val: #LINE# #TAB# #TAB# #TAB# #TAB# types.add(v.dtype) #LINE# #TAB# #TAB# elif isinstance(val, h5py.Dataset): #LINE# #TAB# #TAB# #TAB# for v in val: #LINE# #TAB# #TAB# #TAB# #TAB# types.add(v.dtype) #LINE# #TAB# return types"
"#LINE# #TAB# base_url = base_config()['base_url'] #LINE# #TAB# session = generic_session(session) #LINE# #TAB# headers = base_config()['headers'] #LINE# #TAB# params = {'base_url': base_url, 'headers': headers, 'user_agent': base_config[ #LINE# #TAB# #TAB# 'user_agent'],'remote_addr': base_config['remote_addr'],'remote_name': #LINE# #TAB# #TAB# base_config['remote_name']} #LINE# #TAB# return params"
"#LINE# #TAB# ret = [] #LINE# #TAB# if a[0]: #LINE# #TAB# #TAB# ret.append(a[0]) #LINE# #TAB# #TAB# for i in range(1, len(a)): #LINE# #TAB# #TAB# #TAB# if a[i] == '.': #LINE# #TAB# #TAB# #TAB# #TAB# ret.append(a[i]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ret.append(a[i]) #LINE# #TAB# return ret"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return schema[path] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# in_bytes = in_int.to_bytes(1, byteorder='big') #LINE# #TAB# in_bytes = in_bytes.zfill(2) #LINE# #TAB# in_bytes = in_bytes.reshape((new_dim_length, max_bits) + in_bytes.shape[1:]) #LINE# #TAB# return in_bytes"
"#LINE# #TAB# name = class_name.rsplit('.', 1)[0] #LINE# #TAB# try: #LINE# #TAB# #TAB# cache_result = cache_user_build_version_cache.get(name) #LINE# #TAB# except TemplateDoesNotExist: #LINE# #TAB# #TAB# cache_result = None #LINE# #TAB# if not cache_result: #LINE# #TAB# #TAB# return None #LINE# #TAB# return cache_result[0]"
"#LINE# #TAB# if scale == 0.0: #LINE# #TAB# #TAB# return 1.0 #LINE# #TAB# elif isinstance(scale, np.ndarray): #LINE# #TAB# #TAB# if copy: #LINE# #TAB# #TAB# #TAB# scale = scale.copy() #LINE# #TAB# #TAB# scale[scale == 1.0] = 1.0 #LINE# #TAB# return scale"
"#LINE# #TAB# results = [] #LINE# #TAB# with open(csvFile, 'r', encoding='utf-8-sig') as csvFile: #LINE# #TAB# #TAB# data = csvFile.readlines() #LINE# #TAB# #TAB# for line in data: #LINE# #TAB# #TAB# #TAB# results.append(red_text_line(line)) #LINE# #TAB# return results"
#LINE# #TAB# if description: #LINE# #TAB# #TAB# swagger_meta = {'description': description} #LINE# #TAB# if resource: #LINE# #TAB# #TAB# swagger_meta['resource'] = resource #LINE# #TAB# if options: #LINE# #TAB# #TAB# swagger_meta['options'] = options #LINE# #TAB# return swagger_meta
#LINE# #TAB# if method in Lan.serviceTypeLookup.keys(): #LINE# #TAB# #TAB# return Lan.serviceTypeLookup[method] #LINE# #TAB# return None
#LINE# #TAB# cmd.restrict_id = '' #LINE# #TAB# return cmd.restrict_id
"#LINE# #TAB# T = zeros((len(chimerics), len(msa))) #LINE# #TAB# for i, c in enumerate(chimerics): #LINE# #TAB# #TAB# cell = msa[(i), :] #LINE# #TAB# #TAB# cell[(0), :] = c #LINE# #TAB# #TAB# cell[(1), :] = np.nan #LINE# #TAB# temp_mat = np.stack(T, axis=1) #LINE# #TAB# temp_mat = np.delete(temp_mat, np.where(np.isnan(temp_mat), 0), axis=1) #LINE# #TAB# temp_mat = np.delete(temp_mat, np.where(np.isnan(temp_mat), 1), axis=1) #LINE# #TAB# return temp_mat"
#LINE# #TAB# symbol = random.choice(string) #LINE# #TAB# return symbol
#LINE# #TAB# frag = cls() #LINE# #TAB# frag.content = pods['content'] #LINE# #TAB# frag._resources = [FragmentResource(**d) for d in pods['resources']] #LINE# #TAB# frag.js_init_fn = pods['js_init_fn'] #LINE# #TAB# frag.js_init_version = pods['js_init_version'] #LINE# #TAB# frag.json_init_args = pods['json_init_args'] #LINE# #TAB# return frag
"#LINE# #TAB# output = subprocess.check_output(['pip','reset-branch', '-v'], #LINE# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# return output.decode('utf-8').split('\n') #LINE# #TAB# outdated = [] #LINE# #TAB# for pkg in output.splitlines(): #LINE# #TAB# #TAB# if pkg not in outdated: #LINE# #TAB# #TAB# #TAB# outdated.append(pkg) #LINE# #TAB# return outdated"
"#LINE# #TAB# if not folder: #LINE# #TAB# #TAB# return #LINE# #TAB# for g in groups: #LINE# #TAB# #TAB# files=[] #LINE# #TAB# #TAB# for fname in g.keys(): #LINE# #TAB# #TAB# #TAB# if fname.endswith(folder): #LINE# #TAB# #TAB# #TAB# #TAB# files.append(os.path.join(folder,fname)) #LINE# #TAB# return files"
"#LINE# #TAB# accuacy = [] #LINE# #TAB# for i, data in enumerate(data_iterator): #LINE# #TAB# #TAB# data = data.astype(float) #LINE# #TAB# #TAB# accuacy.append(net(data).get_accuacy()) #LINE# #TAB# return accuacy"
"#LINE# #TAB# pat_dict = {} #LINE# #TAB# fname = os.path.join(os.path.dirname(__file__), 'data', 'pos_pattens.txt') #LINE# #TAB# with open(fname) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# match = re.search('^(\\w+)\\s+(\\w+)$', line) #LINE# #TAB# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# #TAB# pat_dict[match.group(1)] = match.group(2) #LINE# #TAB# return pat_dict"
#LINE# #TAB# for host in tree: #LINE# #TAB# #TAB# for component in host.components: #LINE# #TAB# #TAB# #TAB# if component.vulnerability == cve: #LINE# #TAB# #TAB# #TAB# #TAB# yield component
#LINE# #TAB# global _BACKEND_READ_Dango #LINE# #TAB# if _BACKEND_READ_Dango is None: #LINE# #TAB# #TAB# _BACKEND_READ_Dango = True #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with db.session.begin_nested(): #LINE# #TAB# #TAB# #TAB# #TAB# emails = db.session.query(Email).all() #LINE# #TAB# #TAB# #TAB# #TAB# _BACKEND_READ_Dango = True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# mod_list = find_clas_by_module_list() #LINE# #TAB# clas = [c for c in mod_list if c.__name__ == name] #LINE# #TAB# if not clas: #LINE# #TAB# #TAB# raise ValueError(""No class named '%s' could be found"" % name) #LINE# #TAB# elif len(clas) > 1: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Multiple classes named '%s' could be found in the system.' % name #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# return clas[0]"
"#LINE# #TAB# if re.isfile(s): #LINE# #TAB# #TAB# f = open(s, 'r') #LINE# #TAB# #TAB# return f.read() #LINE# #TAB# return s"
#LINE# #TAB# tree = ET.parse(stats_xml) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# total_tokens = int(root.find('size/total/tokens').text) #LINE# #TAB# diff = int(root.find('diff').get('text')) #LINE# #TAB# expected_lemmas = int(abs(diff)) #LINE# #TAB# return expected_lemmas / total_tokens
"#LINE# #TAB# projecion = np.zeros((NOBSERVATIONS, 3)) #LINE# #TAB# projecion[:, :, (0)] = np.exp(-0.5 * (projection[:, :, (1)]) #LINE# #TAB# #TAB# ) #LINE# #TAB# projecion[:, :, (1)] = np.exp(-0.5 * (projection[:, :, (2)]) #LINE# #TAB# projecion[:, :, (3)] = np.exp(-0.5 * (projection[:, :, (2)]) #LINE# #TAB# #TAB# ) #LINE# #TAB# return projecion"
"#LINE# #TAB# fig = plt.figure(figsize=(10, 10)) #LINE# #TAB# ax = fig.add_subplot(111) #LINE# #TAB# ax.set_title('Plotting') #LINE# #TAB# plot = ax.get_plot() #LINE# #TAB# position = ax.get_position() #LINE# #TAB# plot.set_position(position) #LINE# #TAB# ax.get_yaxis().set_rotation(90) #LINE# #TAB# ax.get_xaxis().set_rotation(90) #LINE# #TAB# ax.get_yaxis().set_rotation(90) #LINE# #TAB# velocity = plot.get_velocity() #LINE# #TAB# return velocity"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return RequestInfo._find_read_args() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# info = RequestInfo() #LINE# #TAB# #TAB# info.process_request(request) #LINE# #TAB# #TAB# RequestInfo._find_read_args = info #LINE# #TAB# #TAB# return info
#LINE# #TAB# result = list() #LINE# #TAB# stack = [()] #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# selected_obj = dialog.get_selected_object() #LINE# #TAB# #TAB# #TAB# stack.append(selected_obj) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# for obj in dialog.get_objects(): #LINE# #TAB# #TAB# #TAB# #TAB# stack.append(obj) #LINE# #TAB# #TAB# #TAB# #TAB# selected_obj = None #LINE# #TAB# return stack
#LINE# #TAB# #TAB# resolved_packages = set() #LINE# #TAB# #TAB# for package in include_packages: #LINE# #TAB# #TAB# #TAB# resolved_packages.add(package) #LINE# #TAB# #TAB# #TAB# cls._validate_packages(resolved_packages) #LINE# #TAB# #TAB# return resolved_packages
"#LINE# #TAB# summary = '' #LINE# #TAB# for cmd_name, summary in command_summaries: #LINE# #TAB# #TAB# if cmd_name == 'clean': #LINE# #TAB# #TAB# #TAB# for subcommand_summary in summary: #LINE# #TAB# #TAB# #TAB# #TAB# if subcommand_summary!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# summary += subcommand_summary #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for subcommand_summary in summary: #LINE# #TAB# #TAB# #TAB# #TAB# if subcommand_summary!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# summary += subcommand_summary #LINE# #TAB# return summary"
#LINE# #TAB# if response not in _element_image_map: #LINE# #TAB# #TAB# _element_image_map[response] = {} #LINE# #TAB# #TAB# content_type = response.headers.get('Content-Type') #LINE# #TAB# #TAB# if content_type and 'image/png' in content_type: #LINE# #TAB# #TAB# #TAB# _element_image_map[response.headers['Content-Type']] = 'image/png' #LINE# #TAB# #TAB# elif content_type and 'image/gif' in content_type: #LINE# #TAB# #TAB# #TAB# _element_image_map[response.headers['Content-Type']] = 'image/gif' #LINE# #TAB# return _element_image_map[response.headers['Content-Type']]
"#LINE# #TAB# return cls.build_send_payload('delete_reverse_breakpoit', {'eventName': #LINE# #TAB# #TAB# eventName}), None"
"#LINE# #TAB# for role in model.permissions.all(): #LINE# #TAB# #TAB# if issubclass(perm_model, role): #LINE# #TAB# #TAB# #TAB# yield role"
#LINE# #TAB# if seq.upper().startswith('AT'): #LINE# #TAB# #TAB# return 'AT' #LINE# #TAB# elif seq.upper().startswith('BB'): #LINE# #TAB# #TAB# return 'BB' #LINE# #TAB# elif seq.upper().startswith('A'): #LINE# #TAB# #TAB# return 'A' #LINE# #TAB# elif seq.upper().startswith('AD'): #LINE# #TAB# #TAB# return 'AD' #LINE# #TAB# elif seq.upper().startswith('R'): #LINE# #TAB# #TAB# return 'R'
"#LINE# #TAB# arg1_star_sorted_ratio = lib.znp_expand_star_sorted_ratio(arg1, arg2) #LINE# #TAB# arg2_star_sorted_ratio = lib.znp_expand_star_sorted_ratio(arg1, arg2) #LINE# #TAB# return arg1_star_sorted_ratio, arg2_star_sorted_ratio"
"#LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# return run('docker status {}'.format(node))!= ''"
"#LINE# #TAB# module = bottle.request.form.get('module', None) #LINE# #TAB# if not module: #LINE# #TAB# #TAB# return None #LINE# #TAB# country = None #LINE# #TAB# if not hasattr(module, 'country') or not module.country: #LINE# #TAB# #TAB# return None #LINE# #TAB# if key: #LINE# #TAB# #TAB# countries = module.country.all() #LINE# #TAB# #TAB# if len(countries) == 1: #LINE# #TAB# #TAB# #TAB# country = countries[0] #LINE# #TAB# #TAB# elif len(countries) > 1: #LINE# #TAB# #TAB# #TAB# country = countries[0] #LINE# #TAB# return {'country': country}"
"#LINE# #TAB# image = imageClass() #LINE# #TAB# for a, d in stretchDim.items(): #LINE# #TAB# #TAB# image = interp1d(image, a, d, scheme=scheme) #LINE# #TAB# return image"
#LINE# #TAB# if response['Error']['Code'] == 100: #LINE# #TAB# #TAB# return 'FINISHED' #LINE# #TAB# elif response['Error']['Code'] == 101: #LINE# #TAB# #TAB# return 'FAILED' #LINE# #TAB# logging.warning('Batch status: {}'.format(response['Error']['Code'])) #LINE# #TAB# if success is True: #LINE# #TAB# #TAB# return 'FINISHED' #LINE# #TAB# else: #LINE# #TAB# #TAB# logging.warning('Batch status: {}'.format(response['Error']['Code'])) #LINE# #TAB# #TAB# return 'FINISHED'
"#LINE# #TAB# file_ext = '.'.join(file_ext.split('.')[:-1]) #LINE# #TAB# file_name = file_name.split('.')[-1] #LINE# #TAB# config = {'file_name': file_name, 'ext': file_ext, 'data': data} #LINE# #TAB# if len(config['file_name']) > 0: #LINE# #TAB# #TAB# config['file_name'] = file_name #LINE# #TAB# return config"
"#LINE# #TAB# if 'T108_En_O_pick_nae' in filename: #LINE# #TAB# #TAB# pos = logical_line.find('-') #LINE# #TAB# #TAB# if pos > -1: #LINE# #TAB# #TAB# #TAB# yield 0, 'T108: Call player_at_en_o_pick_nae(logical_line, filename) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield 0, 'T108: Call player_at_en_o_pick_nae(logical_line, filename) #LINE# #TAB# if 'T108_En_O_pick_nae' in filename: #LINE# #TAB# #TAB# yield 0, 'T108: Call player_at_en_o_pick_nae(logical_line, filename) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield 0, 'T108: Use seed_en_o_pick_nae() instead.'"
"#LINE# #TAB# help_msg = hparams.get(""help_msg"") #LINE# #TAB# if not help_msg: #LINE# #TAB# #TAB# help_msg = hparams.get(""verbose_help_msg"") #LINE# #TAB# return [help_msg, hparams.hidden_size]"
#LINE# #TAB# session = requests.Session() #LINE# #TAB# response = session.get(course_url) #LINE# #TAB# if response.status_code == 200: #LINE# #TAB# #TAB# tree = ElementTree.fromstring(response.content) #LINE# #TAB# #TAB# pare_coi_courses = {} #LINE# #TAB# #TAB# for child in tree.findall('a'): #LINE# #TAB# #TAB# #TAB# if child.tag == 'h1': #LINE# #TAB# #TAB# #TAB# #TAB# pare_coi_courses['name'] = child.text #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# pare_coi_courses['name'] = course_url #LINE# #TAB# #TAB# return pare_coi_courses #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# resource_id = cls.resource_id() #LINE# #TAB# with db.session.begin_nested(): #LINE# #TAB# #TAB# cls.resource_id = resource_id #LINE# #TAB# #TAB# cls.resource_attributes = {'schema_version': '1.0.0', #LINE# #TAB# #TAB# #TAB# 'uuid': cls.uuid} #LINE# #TAB# #TAB# cursor = db.session.query(cls).filter(cls.resource_id == cls. #LINE# #TAB# #TAB# #TAB# resource_id).first() #LINE# #TAB# #TAB# if not cursor: #LINE# #TAB# #TAB# #TAB# raise RuntimeError('no such resource: %s' % cls.resource_id) #LINE# #TAB# return cls.resource_attributes"
"#LINE# #TAB# if not isinstance(pn1, set) or not isinstance(pn2, set): #LINE# #TAB# #TAB# raise ValueError('Invalid public numbers') #LINE# #TAB# if len(pn1)!= len(pn2): #LINE# #TAB# #TAB# raise ValueError('Invalid public numbers') #LINE# #TAB# for i in range(len(pn1)): #LINE# #TAB# #TAB# if not pn1[i] == pn2[i]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# d = uuid.uuid4().hex #LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# d = d[:8] #LINE# #TAB# return d
"#LINE# #TAB# if isinstance(transaction, cls._cls_tx_base): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(transaction, tuple): #LINE# #TAB# #TAB# return any(cls.i_part_convert_2D_applied(x, y) for x, y in #LINE# #TAB# #TAB# #TAB# transaction if y in cls._cls_tx_base and cls.i_part_convert_2D_applied(x, y)) #LINE# #TAB# elif isinstance(transaction, dict): #LINE# #TAB# #TAB# return any(cls.i_part_convert_2D_applied(x, y) for x in transaction #LINE# #TAB# #TAB# #TAB#.keys()) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# r = [] #LINE# #TAB# if num_seqs > 0: #LINE# #TAB# #TAB# for _ in range(start_at): #LINE# #TAB# #TAB# #TAB# r.append(base_name + '_' + str(seq_id(r))) #LINE# #TAB# else: #LINE# #TAB# #TAB# for _ in range(num_seqs): #LINE# #TAB# #TAB# #TAB# s = '%s%d' % (base_name, start_at) #LINE# #TAB# #TAB# #TAB# unique_name = '%s_%d' % (base_name, start_at) #LINE# #TAB# #TAB# #TAB# r.append(s) #LINE# #TAB# return r"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return str(a) == str(b) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# out_filename = None #LINE# #TAB# for classname in classnames: #LINE# #TAB# #TAB# out_filename = os.path.join(out_filename, '{}.h5'.format(classname)) #LINE# #TAB# return out_filename"
#LINE# #TAB# #TAB# file_name = cls.get_widget_names_xml_file(update=update) #LINE# #TAB# #TAB# widget_names = cls.read_xml_file(file_name) #LINE# #TAB# #TAB# cls.delete_xml_file(file_name) #LINE# #TAB# #TAB# return widget_names
"#LINE# #TAB# if not isinstance(morfs, list): #LINE# #TAB# #TAB# orifs = [morfs] #LINE# #TAB# items = [] #LINE# #TAB# for item in orifs: #LINE# #TAB# #TAB# out = CodeUnit(item, file_locator=file_locator) #LINE# #TAB# #TAB# items.append(out) #LINE# #TAB# return items"
#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(payload) as f: #LINE# #TAB# #TAB# #TAB# f.seek(0) #LINE# #TAB# #TAB# #TAB# return json.loads(f.read()) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return {}
#LINE# #TAB# parser = DspParser() #LINE# #TAB# parser.parse(filename) #LINE# #TAB# return parser.results
#LINE# #TAB# global _table_sve_weights_ix #LINE# #TAB# old_maxblock = _table_sve_weights_ix #LINE# #TAB# _table_sve_weights_ix = maxblock #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# _table_sve_weights_ix = old_maxblock
"#LINE# #TAB# if context is None: #LINE# #TAB# #TAB# context = get_global_context() #LINE# #TAB# llvmir = _encode_string(llvmir) #LINE# #TAB# with ffi.OutputString() as errmsg: #LINE# #TAB# #TAB# mod = ModuleRef(ffi.lib.LLVMPY_CreateFromLLVMIR(llvmir, context, #LINE# #TAB# #TAB# #TAB# errmsg), context) #LINE# #TAB# #TAB# if errmsg: #LINE# #TAB# #TAB# #TAB# mod.close() #LINE# #TAB# #TAB# #TAB# raise RuntimeError('LLVM IR parsing error\n{0}'.format(errmsg)) #LINE# #TAB# return mod"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return y * psd #LINE# #TAB# except NotImplementedError: #LINE# #TAB# #TAB# return y * psd
#LINE# #TAB# total = 0 #LINE# #TAB# edges = nxg.edges(data=True) #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if 'label' in edge[2]: #LINE# #TAB# #TAB# #TAB# total += edge[2]['label'] #LINE# #TAB# return total
"#LINE# #TAB# global DJANGO_SETTINGS_MODULE #LINE# #TAB# if not DJANGO_SETTINGS_MODULE: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# from django.conf import settings #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# dango_folder = os.path.abspath(settings.DJANGO_SETTINGS_MODULE) #LINE# #TAB# #TAB# paths = [os.path.join(dango_folder, f) for f in os.listdir(dango_folder) if #LINE# #TAB# #TAB# #TAB# #TAB# '.dango' in f and not f.startswith('.')] #LINE# #TAB# #TAB# #TAB# return [os.path.join(dango_folder, f) for f in paths] #LINE# #TAB# return None"
"#LINE# #TAB# if include_expired: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM zone_members WHERE owner_block =? AND block_number >=?;"" #LINE# #TAB# else: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM zone_members WHERE owner_block =?;"" #LINE# #TAB# args = (current_block,) if current_block else () #LINE# #TAB# if cur is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return cur.execute(query, args)['COUNT']"
#LINE# #TAB# if residue.residue_type == 'union': #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# get_canonical_form(residue) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# context = {} #LINE# #TAB# for name, plugin in obj._plugins.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# context[name] = getattr(plugin, 'validate', None) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return context"
"#LINE# #TAB# return [ #LINE# #TAB# #TAB# r.url_pattern #LINE# #TAB# #TAB# for r in r.handlers #LINE# #TAB# #TAB# if hasattr(r, 'url_pattern') #LINE# #TAB# ]"
#LINE# #TAB# long_event = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open('README.rst') as f: #LINE# #TAB# #TAB# #TAB# readme = f.read() #LINE# #TAB# #TAB# #TAB# long_event = readme.split('\n')[0] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return long_event
"#LINE# #TAB# try: #LINE# #TAB# #TAB# query_string = event[""context""][""request""][""query""] #LINE# #TAB# #TAB# event[""context""][""request""][""query""] = _sanitize_string(query_string, ""="") #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return event"
"#LINE# #TAB# #TAB# metadata = [] #LINE# #TAB# #TAB# for opcode, value in data.items(): #LINE# #TAB# #TAB# #TAB# metadata.append((opcode, value)) #LINE# #TAB# #TAB# return metadata"
#LINE# #TAB# clear_cache() #LINE# #TAB# SharedInstance.instance = bitshares.BitShares(**SharedInstance.config) #LINE# #TAB# return SharedInstance.instance
#LINE# #TAB# doc = etree.fromstring(xmlcontent) #LINE# #TAB# for el in doc: #LINE# #TAB# #TAB# el.text = el.text.strip() #LINE# #TAB# return doc
"#LINE# #TAB# with h5py.File(h5, 'r') as h: #LINE# #TAB# #TAB# dictionary = unwrapArray(h, recursive=True, readH5pyDataset=readH5pyDataset) #LINE# #TAB# return dictionary"
"#LINE# #TAB# out = [] #LINE# #TAB# out.append(row[int(row[0])]) #LINE# #TAB# for i in range(len(row)): #LINE# #TAB# #TAB# if row[i][0] > dem_pth: #LINE# #TAB# #TAB# #TAB# out.append(round(row[i][0] * dem_adjustment, 3)) #LINE# #TAB# #TAB# elif row[i][1] > dem_pth: #LINE# #TAB# #TAB# #TAB# out.append(round(row[i][1] * dem_adjustment, 3)) #LINE# #TAB# return out"
#LINE# #TAB# signal[np.isnan(signal)] = np.nan #LINE# #TAB# return signal
"#LINE# #TAB# xml_schemas = [] #LINE# #TAB# for root, dirs, filenames in os.walk(os.path.join(os.path.dirname(os. #LINE# #TAB# #TAB# path.abspath(__file__)),'schemas')): #LINE# #TAB# #TAB# for filename in filenames: #LINE# #TAB# #TAB# #TAB# schema = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# if os.path.isfile(schema): #LINE# #TAB# #TAB# #TAB# #TAB# xml_schemas.append(schema) #LINE# #TAB# return xml_schemas"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return _moving_energy_matrix(size, smooth_factor) #LINE# #TAB# except NotImplementedError: #LINE# #TAB# #TAB# return np.nan"
#LINE# #TAB# seq = '' #LINE# #TAB# for l in lines: #LINE# #TAB# #TAB# if l.startswith('ID='): #LINE# #TAB# #TAB# #TAB# if len(l) > 1 and l[0].strip(): #LINE# #TAB# #TAB# #TAB# #TAB# seq += l[1].strip() #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if seq: #LINE# #TAB# #TAB# return seq #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
"#LINE# #TAB# if not rows: #LINE# #TAB# #TAB# return None #LINE# #TAB# return [pseudo_named_tuple_row(colname, row) for colname, row in zip( #LINE# #TAB# #TAB# colnames, rows)]"
#LINE# #TAB# for attr in attributes_set: #LINE# #TAB# #TAB# if not log.attributes[attr]: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# all_attributes = set() #LINE# #TAB# #TAB# for trace in log.traces: #LINE# #TAB# #TAB# #TAB# if attr not in all_attributes: #LINE# #TAB# #TAB# #TAB# #TAB# all_attributes.add(attr) #LINE# #TAB# #TAB# if len(all_attributes) > 0: #LINE# #TAB# #TAB# #TAB# attributes_set.remove(all_attributes) #LINE# #TAB# return attributes_set
"#LINE# #TAB# res_name = str(name).rsplit(""-"")[0] #LINE# #TAB# numer_str = None #LINE# #TAB# if residue: #LINE# #TAB# #TAB# res_name = str(residue[0]) #LINE# #TAB# for numer in range(1, len(res_name)): #LINE# #TAB# #TAB# if res_name[numer_str].isdigit(): #LINE# #TAB# #TAB# #TAB# numer_str = res_name[numer_str] #LINE# #TAB# if numer_str == '': #LINE# #TAB# #TAB# numer_str = ""No number"" #LINE# #TAB# return numer_str"
"#LINE# #TAB# account = get_object_or_404(Branch, account_id=accountid) #LINE# #TAB# if not request.user.can_access(account): #LINE# #TAB# #TAB# raise PermDeniedException() #LINE# #TAB# library_path = settings.BRANCH_LIB_PATH #LINE# #TAB# if not os.path.exists(library_path): #LINE# #TAB# #TAB# raise PermissionDenied() #LINE# #TAB# library = BranchLibrary.query.filter_by(account_id=account.id).first() #LINE# #TAB# if not library: #LINE# #TAB# #TAB# raise PermissionDenied() #LINE# #TAB# return library"
"#LINE# #TAB# if isinstance(tree, NameCollector): #LINE# #TAB# #TAB# return tree.collect_namespaces(ctx) #LINE# #TAB# if isinstance(tree, (list, tuple)): #LINE# #TAB# #TAB# return [populate_info(n, ctx) for n in tree] #LINE# #TAB# if isinstance(tree, dict): #LINE# #TAB# #TAB# return {k: populate_info(v, ctx) for k, v in tree.items()} #LINE# #TAB# return tree"
"#LINE# #TAB# command_args_package_name = [cls.find_command_args(file_path, #LINE# #TAB# #TAB# 'Package'), cls.find_command_args(file_path, 'Version')] #LINE# #TAB# command_args_version = [cls.find_command_args(file_path, 'Package'), #LINE# #TAB# #TAB# cls.find_command_args(file_path, 'Version')] #LINE# #TAB# command_args_package_version = [cls.find_command_args(file_path, #LINE# #TAB# #TAB# 'Package')] #LINE# #TAB# command_args_package_name.append(command_args_package_name[0]) #LINE# #TAB# command_args_version.append(command_args_version[0]) #LINE# #TAB# package_info = PackageInfo() #LINE# #TAB# package_info.package = command_args_package_name #LINE# #TAB# package_info.version = command_args_version[0] #LINE# #TAB#"
#LINE# #TAB# lines = [] #LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if len(l[i]) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if l[i] == 1: #LINE# #TAB# #TAB# #TAB# lines.append(l[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# diff = l[i] - l[i - 1] #LINE# #TAB# #TAB# #TAB# if diff > t: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return lines
#LINE# #TAB# roles = [] #LINE# #TAB# status = client.list_account_roles() #LINE# #TAB# if status['account']!= '': #LINE# #TAB# #TAB# roles = status['account_roles'] #LINE# #TAB# else: #LINE# #TAB# #TAB# roles = client.list_account_roles() #LINE# #TAB# #TAB# if status['account']!= '': #LINE# #TAB# #TAB# #TAB# roles = status['account_roles'] #LINE# #TAB# return roles
#LINE# #TAB# if confirm == 'y': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# for i in range(32): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(x) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break
"#LINE# #TAB# df_out = df.copy() #LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# s = [s] #LINE# #TAB# for col in s: #LINE# #TAB# #TAB# df_out = df_out.dropna(how='all', axis=1) #LINE# #TAB# return df_out"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if isinstance(param, list): #LINE# #TAB# #TAB# #TAB# return param #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return [param]"
#LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += calculate_size_data(new_value) #LINE# #TAB# return data_size
"#LINE# #TAB# fname = fname.split('/')[-1] #LINE# #TAB# if not fname.endswith('.frm'): #LINE# #TAB# #TAB# fname += '.frm' #LINE# #TAB# suffix = os.path.splitext(fname)[1] #LINE# #TAB# if suffix in ['h5', 'txt']: #LINE# #TAB# #TAB# return 'h5' #LINE# #TAB# if suffix == 'zip': #LINE# #TAB# #TAB# return 'zip' #LINE# #TAB# return None"
"#LINE# #TAB# engineio_server.routes[engineio_endpoint] = app #LINE# #TAB# app.router.add_route('GET', view_func=engineio_server.get) #LINE# #TAB# app.router.add_route('POST', view_func=engineio_endpoint, callback= #LINE# #TAB# #TAB# _on_request) #LINE# #TAB# app.router.add_route('GET', view_func=engineio_server.get) #LINE# #TAB# app.router.add_route('POST', view_func=engineio_endpoint, callback= #LINE# #TAB# #TAB# _on_request) #LINE# #TAB# return app"
#LINE# #TAB# pass_gather = [] #LINE# #TAB# for string in string_set: #LINE# #TAB# #TAB# pass_gather.append(string) #LINE# #TAB# return pass_gather
#LINE# #TAB# for x in range(len(my_list)): #LINE# #TAB# #TAB# if my_list[x].__class__ == my_element.__class__: #LINE# #TAB# #TAB# #TAB# my_list.append(x) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
#LINE# #TAB# out = {} #LINE# #TAB# for item in dico: #LINE# #TAB# #TAB# out[item.id] = item #LINE# #TAB# return out
#LINE# #TAB# if str1[0].isdigit(): #LINE# #TAB# #TAB# return '-' + str1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return str1
#LINE# #TAB# if options.input: #LINE# #TAB# #TAB# sys.stdout.write('Extracting files from {}'.format(options.input)) #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# parts = string.split('=') #LINE# #TAB# #TAB# #TAB# if len(parts) > 2: #LINE# #TAB# #TAB# #TAB# #TAB# parts[0] = parts[0].strip() #LINE# #TAB# #TAB# #TAB# if len(parts) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# yield parts[1] #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string
#LINE# #TAB# lik_score = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(links): #LINE# #TAB# #TAB# if 0 <= links[i] < 10: #LINE# #TAB# #TAB# #TAB# lik_score.append(links[i]) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# elif len(links) > 10: #LINE# #TAB# #TAB# if 0 <= links[i] < 10: #LINE# #TAB# #TAB# #TAB# lik_score.append(links[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# return lik_score
"#LINE# #TAB# ""for json output add a full stop if ends in et al"" #LINE# #TAB# view = soup.find('div', {'class': 'row-container'}) #LINE# #TAB# for child in view: #LINE# #TAB# #TAB# if child.get('class') == 'row': #LINE# #TAB# #TAB# #TAB# child.set('start', child.get('start')) #LINE# #TAB# #TAB# #TAB# child.set('end', child.get('end')) #LINE# #TAB# return soup"
"#LINE# #TAB# joystick_c = unbox(joystick, 'SDL_Joystick *') #LINE# #TAB# rc = lib.sdl_joysticknumhats(joystick_c) #LINE# #TAB# return rc"
"#LINE# #TAB# r = response.content #LINE# #TAB# if isinstance(r, (list, dict)): #LINE# #TAB# #TAB# return ResourceList(r) #LINE# #TAB# if isinstance(r, dict): #LINE# #TAB# #TAB# return Resource(r) #LINE# #TAB# return r"
"#LINE# #TAB# values = [] #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return values #LINE# #TAB# for prop, value in value.items(): #LINE# #TAB# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# if value.lower() == 'true': #LINE# #TAB# #TAB# #TAB# #TAB# values.append(True) #LINE# #TAB# #TAB# #TAB# elif value.lower() == 'false': #LINE# #TAB# #TAB# #TAB# #TAB# values.append(False) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# values.append((prop, value)) #LINE# #TAB# return values"
#LINE# #TAB# tool = XsdValidator(require=require) #LINE# #TAB# return tool
#LINE# #TAB# if chid in ChannelType.enums: #LINE# #TAB# #TAB# return [str(chid) for ch in ChannelType.enums[chid]] #LINE# #TAB# return None
#LINE# #TAB# sbformat = SBFormat() #LINE# #TAB# sbformat.id_type = id_type #LINE# #TAB# sbformat.id_value = id_value #LINE# #TAB# sbformat.quality = quality #LINE# #TAB# return sbformat
"#LINE# #TAB# attributes = {attr: value for attr, value in attrs.items() if not attr. #LINE# #TAB# #TAB# startswith('_')} #LINE# #TAB# for key, value in attributes.items(): #LINE# #TAB# #TAB# if not key.startswith('pandoc.'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if callable(value): #LINE# #TAB# #TAB# #TAB# attributes[key] = value() #LINE# #TAB# return attributes"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# params['parasm'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# paras = [] #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# key = key.replace('-', '_') #LINE# #TAB# #TAB# if type(value) == dict: #LINE# #TAB# #TAB# #TAB# paras.extend(collect_parasm(value)) #LINE# #TAB# #TAB# elif type(value) == list: #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# paras.extend(collect_parasm(item)) #LINE# #TAB# #TAB# paras.append('\n') #LINE# #TAB# return paras"
"#LINE# #TAB# if 'type' not in data_dict: #LINE# #TAB# #TAB# raise ValueError(f""'{type_name}' not found in dict."") #LINE# #TAB# for key in data_dict.keys(): #LINE# #TAB# #TAB# if data_dict[key].get('type') == type_name: #LINE# #TAB# #TAB# #TAB# data_dict[key]['type'] = type_name #LINE# #TAB# return data_dict"
"#LINE# #TAB# threshold = abs(threshold) #LINE# #TAB# new_scores = OrderedDict() #LINE# #TAB# for position, basescore in enumerate(scores): #LINE# #TAB# #TAB# if basescore >= threshold: #LINE# #TAB# #TAB# #TAB# new_scores[position] = basescore - threshold #LINE# #TAB# return new_scores"
"#LINE# #TAB# model = KNeighborsClassifier(n_components=1) #LINE# #TAB# model.read(fname) #LINE# #TAB# lookup_table = np.asarray(model.lookup_table) #LINE# #TAB# return model, lookup_table"
"#LINE# #TAB# outdir = expandvars(outdir) #LINE# #TAB# plotpath = os.path.join(outdir, 'plot') #LINE# #TAB# if not os.path.exists(plotpath): #LINE# #TAB# #TAB# os.makedirs(plotpath) #LINE# #TAB# return plotpath"
"#LINE# #TAB# if isinstance(value, tuple) and len(value) == 2: #LINE# #TAB# #TAB# return b'__s3key__={}'.format(value[0]) #LINE# #TAB# return value"
"#LINE# #TAB# r = subprocess.run([path, '--returncode'], stdout=subprocess.PIPE, #LINE# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# if r.returncode == 0: #LINE# #TAB# #TAB# return r.stdout.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# return r.returncode"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return PICCOLO_ID_MAP[txt] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return PICCOLO_ID_MAP[int(txt)] #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return txt
"#LINE# #TAB# objects = [] #LINE# #TAB# while len(content): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj, content = berdecoder(content) #LINE# #TAB# #TAB# #TAB# objects.append(obj) #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return objects"
"#LINE# #TAB# bibtex_t = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# item = sys.stdin.readline() #LINE# #TAB# #TAB# if item == '\n': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# index = int(item) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# bibtex_t.append((-1, index)) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# for i in range(len(bibtex_t)): #LINE# #TAB# #TAB# bibtex_t[i] = int(bibtex_t[i]) #LINE# #TAB# return bibtex_t"
"#LINE# #TAB# cell = requests.get(""{0}/series-prepare-cell.xml"".format( #LINE# #TAB# #TAB# tmp_dir)) #LINE# #TAB# cell.raise_for_status() #LINE# #TAB# files = [] #LINE# #TAB# with open(""{0}/series-prepare-cell.xml"".format( #LINE# #TAB# #TAB# tmp_dir #LINE# #TAB# #TAB# ), ""r"") as f: #LINE# #TAB# #TAB# for chunk in zip( #LINE# #TAB# #TAB# #TAB# f.readlines(), #LINE# #TAB# #TAB# #TAB# f.readlines() #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# #TAB# files.append(chunk) #LINE# #TAB# return files"
"#LINE# #TAB# if isinstance(val, bytes): #LINE# #TAB# #TAB# return val.decode('utf-8') #LINE# #TAB# elif hasattr(val, 'decode'): #LINE# #TAB# #TAB# return val.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# return val"
"#LINE# #TAB# result = [] #LINE# #TAB# for l in left_bytes: #LINE# #TAB# #TAB# r = hammng_distance(l, r) #LINE# #TAB# #TAB# if r > 0: #LINE# #TAB# #TAB# #TAB# result.append(r) #LINE# #TAB# return result"
"#LINE# #TAB# details = {'amount': card.amount, 'type': card.type, 'debit': card. #LINE# #TAB# #TAB# debit, 'amount_int': card.amount, 'currency': card.currency, #LINE# #TAB# #TAB# 'debit_details': card.debit_details} #LINE# #TAB# return details"
#LINE# #TAB# global _sfe_analysis_has #LINE# #TAB# return _sfe_analysis_has
#LINE# #TAB# global ota_corners #LINE# #TAB# if ota is None: #LINE# #TAB# #TAB# ota_corners = read_edit_corners() #LINE# #TAB# elif ota.startswith('3D'): #LINE# #TAB# #TAB# ota_corners = read_edit_corners() #LINE# #TAB# elif ota.startswith('V3D'): #LINE# #TAB# #TAB# ota_corners = read_edit_corners() #LINE# #TAB# elif ota.startswith('V4D'): #LINE# #TAB# #TAB# ota_corners = read_edit_corners() #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception('unknown OTA: %s' % ota) #LINE# #TAB# return ota_corners
#LINE# #TAB# entities = [] #LINE# #TAB# text = text.strip() #LINE# #TAB# while text and text[0]!= '&': #LINE# #TAB# #TAB# entities.append('&') #LINE# #TAB# #TAB# text = text[1:] #LINE# #TAB# return entities
#LINE# #TAB# #TAB# global DepotManagerStatus #LINE# #TAB# #TAB# DepotManagerStatus.active = False
#LINE# #TAB# key = _cache.get(lock) #LINE# #TAB# if key is None: #LINE# #TAB# #TAB# return _default_lock #LINE# #TAB# return _cache[key]
"#LINE# #TAB# node = nodes.reference(target_id, env, lineno) #LINE# #TAB# node.attr = 'config' #LINE# #TAB# return node"
"#LINE# #TAB# if isinstance(obj, sqlalchemy.Model): #LINE# #TAB# #TAB# return dict((k, filename_cast_bytes(v)) for k, v in obj.all_fields.items()) #LINE# #TAB# elif isinstance(obj, (list, tuple)): #LINE# #TAB# #TAB# return [filename_cast_bytes(v) for v in obj] #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# try: #LINE# #TAB# #TAB# has_slice_access = fn.__has_slice__ #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# has_slice_access = False #LINE# #TAB# return has_slice_access
"#LINE# #TAB# if isinstance(vals, str): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(vals, (list, tuple, set)): #LINE# #TAB# #TAB# return all(check_unicode_o_stings(x) for x in vals) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if len(X)!= len(y): #LINE# #TAB# #TAB# raise ValueError('X and y must be the same length') #LINE# #TAB# w = coef[:, (0)] #LINE# #TAB# w_flip = weights[1:] - weights[:-1] #LINE# #TAB# y_pred = y * w_flip #LINE# #TAB# d_pred = np.sum(y_pred, axis=0) #LINE# #TAB# if link: #LINE# #TAB# #TAB# y_pred = y_pred / d_pred #LINE# #TAB# elif link: #LINE# #TAB# #TAB# y_pred = y_pred - d_pred #LINE# #TAB# return y_pred, d_pred"
"#LINE# #TAB# if asse_equal_type_re.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'SL317: assertEqual(type(A), B) sentences not allowed'"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ip = ipaddress.ip_address(u'' + ip_address) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if ip.is_loopback or ip.is_link_local or ip.is_loopback: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# regions = gt_logger_list() #LINE# #TAB# for region in regions: #LINE# #TAB# #TAB# print(region) #LINE# #TAB# return regions[-1] / regions[-2]
#LINE# #TAB# nb_path = normalize_path(path) #LINE# #TAB# if not config.is_valid_notebook(nb_path): #LINE# #TAB# #TAB# raise ValueError('Invalid notebook path: {}'.format(nb_path)) #LINE# #TAB# return nb_path
"#LINE# #TAB# output = {} #LINE# #TAB# for key, val in value.items(): #LINE# #TAB# #TAB# if isinstance(val, dict): #LINE# #TAB# #TAB# #TAB# output[key] = fmt_dict(val, max_width=max_width) #LINE# #TAB# #TAB# elif isinstance(val, (list, tuple)): #LINE# #TAB# #TAB# #TAB# for item in val: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = fmt_list(item, max_width=max_width) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output[key] = fmt_str(val) #LINE# #TAB# return output"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = _sections[section].get(option) #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# return datetime.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%f') #LINE# #TAB# #TAB# elif raise_exception and default is not None: #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# #TAB# return None #LINE# #TAB# except (ValueError, KeyError): #LINE# #TAB# #TAB# if raise_exception and default is not None: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# return default"
#LINE# #TAB# bit_arr = [] #LINE# #TAB# for i in range(len(arr)): #LINE# #TAB# #TAB# for j in range(8): #LINE# #TAB# #TAB# #TAB# bit_arr.append(arr[i][j]) #LINE# #TAB# return bit_arr
"#LINE# #TAB# if not retries: #LINE# #TAB# #TAB# retries = 1 #LINE# #TAB# #TAB# sleep = 2 #LINE# #TAB# conn = mysql.connect(host=host, db=db, user=user, password=password) #LINE# #TAB# conn.row_factory = mysql.Row #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield conn #LINE# #TAB# #TAB# except (socket.error, mysql.OperationalError): #LINE# #TAB# #TAB# #TAB# time.sleep(sleep) #LINE# #TAB# #TAB# #TAB# retry = 0 #LINE# #TAB# #TAB# #TAB# if retry > retries: #LINE# #TAB# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# #TAB# conn = mysql.connect(host=host, db=db, user=user, password=password) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB"
"#LINE# #TAB# inputs = [] #LINE# #TAB# for item in example_command: #LINE# #TAB# #TAB# key = item[0] #LINE# #TAB# #TAB# if key!= 'context': #LINE# #TAB# #TAB# #TAB# if isinstance(item[1], dict): #LINE# #TAB# #TAB# #TAB# #TAB# inputs.append(item[1]) #LINE# #TAB# #TAB# #TAB# elif isinstance(item[1], list): #LINE# #TAB# #TAB# #TAB# #TAB# for item2 in item[1]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# inputs.append(item2) #LINE# #TAB# return inputs"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# setting_type = get_resource_field_rl_schema(app_name, model_name) #LINE# #TAB# #TAB# return setting_type #LINE# #TAB# except: #LINE# #TAB# #TAB# raise Http404"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# c = models.ContentType.objects.get(app_name=app_name, model_name= #LINE# #TAB# #TAB# #TAB# model_name) #LINE# #TAB# except models.ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# clean_key_headers = c.content_type.split('/') #LINE# #TAB# if len(clean_key_headers)!= 1: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# return clean_key_headers[0]"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return env['stream_proxy'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return ''
#LINE# #TAB# _logger.error(error) #LINE# #TAB# raise error
#LINE# #TAB# if direction =='read': #LINE# #TAB# #TAB# name = f'{name}_read' #LINE# #TAB# elif direction == 'write': #LINE# #TAB# #TAB# name = f'{name}_write' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError( #LINE# #TAB# #TAB# #TAB# f'Unsupported direction {direction!r}') #LINE# #TAB# return name
"#LINE# #TAB# dst_ns = int((extent[2] - extent[0])/res + 0.99) #LINE# #TAB# dst_nl = int((extent[3] - extent[1])/res + 0.99) #LINE# #TAB# dst_nl = int((extent[5] - extent[3])/res + 0.99) #LINE# #TAB# m = gdal.GetGeoTransform() #LINE# #TAB# m.SetGeoTransform(dst_ns, dst_nl) #LINE# #TAB# ds = gdal.GetDataset(res, dst_ns, dtype) #LINE# #TAB# if srs is not None: #LINE# #TAB# #TAB# m.SetProjection(srs.ExportToWkt()) #LINE# #TAB# return ds"
"#LINE# #TAB# page = Page(filename) #LINE# #TAB# page.read() #LINE# #TAB# colnames = page.columns #LINE# #TAB# indices = {} #LINE# #TAB# for i, name in enumerate(page.names): #LINE# #TAB# #TAB# colnames[name] = i #LINE# #TAB# #TAB# indices[name] = i #LINE# #TAB# return colnames, indices"
"#LINE# #TAB# eigvalue = np.zeros((esys_mapdata[0].shape[0], esys_mapdata[0].shape[1])) #LINE# #TAB# eigenvectors = np.zeros((esys_mapdata[0].shape[1], esys_mapdata[0].shape[1])) #LINE# #TAB# for i in range(esys_mapdata[0].shape[1]): #LINE# #TAB# #TAB# esys_mapdata[i][0] = esys_mapdata[0][i] #LINE# #TAB# #TAB# for j in range(esys_mapdata[1].shape[1]): #LINE# #TAB# #TAB# #TAB# eigvalue[i][j] = esys_mapdata[1][j] / np.sqrt(esys_mapdata[1][i] * #LINE# #TAB# #TAB# #TAB# #TAB# esys_mapdata[1][j]) #LINE# #TAB# return eigvalue, eigenvectors"
#LINE# #TAB# grains = __salt__['grains.get'](_GLOBUS_KEY) #LINE# #TAB# if not grains: #LINE# #TAB# #TAB# if proxy and salt.utils.napalm.is_proxy(__opts__): #LINE# #TAB# #TAB# #TAB# grains = salt.utils.napalm.get_grains(__opts__) #LINE# #TAB# #TAB# if not proxy and salt.utils.napalm.is_minion(__opts__): #LINE# #TAB# #TAB# #TAB# proxy = salt.utils.napalm.get_proxy(__opts__) #LINE# #TAB# #TAB# grains = salt.utils.napalm.get_grains(__opts__) #LINE# #TAB# if not proxy and salt.utils.napalm.is_minion(__opts__): #LINE# #TAB# #TAB# grains = salt.utils.napalm.get_grains(__opts__) #LINE# #TAB# return grains
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return re.search(regex, 'ecma') is not None #LINE# #TAB# except: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# parsed = OrderedDict(type=get_type_prser(arg)) #LINE# #TAB# parsed['name'] = arg.__name__ #LINE# #TAB# parsed['signature'] = str(signature_func(arg)) #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getfullargspec(arg)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getargspec(arg)) #LINE# #TAB# parsed['isbuiltin'] = inspect.isbuiltin(arg) #LINE# #TAB# return parsed
"#LINE# #TAB# path = resource_filename(Requirement.parse('s3s3'), template) #LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# return path #LINE# #TAB# else: #LINE# #TAB# #TAB# return path"
#LINE# #TAB# ret = ReviewConfig() #LINE# #TAB# ret.set_ini_file(ini_config) #LINE# #TAB# ret.set_app_config(app_config) #LINE# #TAB# return ret
"#LINE# #TAB# template = """""" #LINE# #TAB# #TAB# <option> [name] #LINE# #TAB# #TAB# </option>\ #LINE# #TAB# #TAB# <value> #LINE# #TAB# #TAB# </option>\ #LINE# #TAB# #TAB# </option>\ #LINE# #TAB# #TAB# </option>\ #LINE# #TAB# """""" #LINE# #TAB# return template"
#LINE# #TAB# compression_method = decompressor.COMPRESSION_METHOD.lower() #LINE# #TAB# if compression_method not in cls._decompresors: #LINE# #TAB# #TAB# raise KeyError('No decompressor for compression method: {0:s}'.format( #LINE# #TAB# #TAB# #TAB# decompressor)) #LINE# #TAB# cls._decompresors[compression_method] = decompressor
#LINE# #TAB# sum = 0 #LINE# #TAB# for chunk in data: #LINE# #TAB# #TAB# sum += chunk[0] + chunk[1] #LINE# #TAB# return sum % 256
#LINE# #TAB# z = x.copy() #LINE# #TAB# z.update(y) #LINE# #TAB# return z
#LINE# #TAB# method_n = url_dict[start_int:total_int] #LINE# #TAB# if authn_subj_list is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# for authn_subj in authn_subj_list: #LINE# #TAB# #TAB# if authn_subj not in method_n: #LINE# #TAB# #TAB# #TAB# method_n += 1 #LINE# #TAB# return method_n
"#LINE# #TAB# psi = -0.0094 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
#LINE# #TAB# code_object = _replace_circle(code_object) #LINE# #TAB# code_object = _insert_side_around_circle(code_object) #LINE# #TAB# return code_object
"#LINE# #TAB# for pattern in matches: #LINE# #TAB# #TAB# if fnmatch.fnmatch(name, pattern): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# mib_max = np.max(mib) #LINE# #TAB# xi_complement_max = np.max(xi_complement) #LINE# #TAB# new_sdas = apply_reachability_plot(sdas, mib_max, xi_complement_max, #LINE# #TAB# #TAB# reachability_plot) #LINE# #TAB# return new_sdas"
"#LINE# #TAB# username = env.get('CI_USERNAME') #LINE# #TAB# password = env.get('CI_PASSWORD') #LINE# #TAB# if not username: #LINE# #TAB# #TAB# return None #LINE# #TAB# result = subprocess.Popen(commands, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# shell=True, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# env=env, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# input=username, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# env=env) #LINE# #TAB# return result"
#LINE# #TAB# class _set_codes_scheme(value): #LINE# #TAB# #TAB# pass #LINE# #TAB# return _set_codes_scheme
"#LINE# #TAB# fname = op.join(op.dirname(op.abspath(__file__)), 'data', name) #LINE# #TAB# if op.isfile(fname): #LINE# #TAB# #TAB# f = open(fname, 'rb') #LINE# #TAB# #TAB# f.read() #LINE# #TAB# #TAB# return f.read() #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# x = next(gen) #LINE# #TAB# #TAB# #TAB# yield x #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# tags = [x[1] for x in generate_dns_xml(section, min_paragraph_length= #LINE# #TAB# #TAB# min_paragraph_length)] #LINE# #TAB# paragraphs = [] #LINE# #TAB# for tag in tags: #LINE# #TAB# #TAB# if tag['type'] == 'paragraph': #LINE# #TAB# #TAB# #TAB# paragraph = tag['text'] #LINE# #TAB# #TAB# #TAB# paragraphs.append(paragraph) #LINE# #TAB# return paragraphs"
#LINE# #TAB# if width == 1: #LINE# #TAB# #TAB# return str(data) #LINE# #TAB# if width == 2: #LINE# #TAB# #TAB# return 'B' + str(data) #LINE# #TAB# if width == 3: #LINE# #TAB# #TAB# return 'C' + str(data) #LINE# #TAB# return data
"#LINE# #TAB# field.setText(str(field.text()).replace(',','')) #LINE# #TAB# for i in range(num): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if len(field) == i: #LINE# #TAB# #TAB# #TAB# #TAB# _add_item(field, _of_xml_encoded_port_text(i, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# message)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# _add_item(field, _of_xml_encoded_port_text(field, #LINE# #TAB# #TAB# #TAB# #TAB# message)) #LINE# #TAB# #TAB# return 1"
"#LINE# #TAB# if not base: #LINE# #TAB# #TAB# return [] #LINE# #TAB# stabs = [_detect_version(base) for _ in gens] #LINE# #TAB# ucs = [] #LINE# #TAB# for n in stabs: #LINE# #TAB# #TAB# ucs.append(generate_uoa(base, n)) #LINE# #TAB# ucs.sort() #LINE# #TAB# transversals = [] #LINE# #TAB# for uu in ucs: #LINE# #TAB# #TAB# transversals.append(_detect_transformation(uu, gens)) #LINE# #TAB# return transversals"
"#LINE# #TAB# path, filename = os.path.split(fullpath) #LINE# #TAB# filename, ext = os.path.splitext(filename) #LINE# #TAB# sys.path.insert(0, path) #LINE# #TAB# module = importlib.import_module(filename, path) #LINE# #TAB# importlib.reload(module) #LINE# #TAB# del sys.path[0] #LINE# #TAB# return module"
"#LINE# #TAB# slackbuilds_dir = join(location,'slackbuilds') #LINE# #TAB# if not isdir(slackbuilds_dir): #LINE# #TAB# #TAB# return [name] #LINE# #TAB# matches = fnmatch.filter(name, '*') #LINE# #TAB# matches = [x for x in matches if x!= '*'] #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return [name] #LINE# #TAB# for f in matches: #LINE# #TAB# #TAB# filepath = join(slackbuilds_dir, f) #LINE# #TAB# #TAB# if isfile(filepath) and size == getsize(filepath): #LINE# #TAB# #TAB# #TAB# return [name] #LINE# #TAB# #TAB# elif size == unsize: #LINE# #TAB# #TAB# #TAB# return [name] #LINE# #TAB# return []"
"#LINE# #TAB# task_config = cls.build_task_from_config(config) #LINE# #TAB# sequence = cls.build_sequence(config, dependencies) #LINE# #TAB# return task_config, sequence"
"#LINE# #TAB# match = False #LINE# #TAB# if model_a.name == model_b.name: #LINE# #TAB# #TAB# match = True #LINE# #TAB# if model_a.module_name and model_b.module_name: #LINE# #TAB# #TAB# match = version_up(model_a.module_name, model_b.module_name) #LINE# #TAB# return match"
"#LINE# #TAB# container_ile = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# path = base_path + '/' + f #LINE# #TAB# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# container_ile.append(os.path.join(path, zip_filename)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# container_ile.append(os.path.join(path, zip_filename)) #LINE# #TAB# return container_ile"
"#LINE# #TAB# s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open('/proc/%s/fd' % mount_point, 'rb') as f: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# data = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# if data: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# f.close() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# os.unlink(data[0]) #LINE# #TAB# except (OSError, IOError): #LINE# #TAB# #TAB# pass"
#LINE# #TAB# now = datetime.datetime.now() #LINE# #TAB# elapsed += lease_time - now #LINE# #TAB# return elapsed
"#LINE# #TAB# y, m, s = dt.timetuple() #LINE# #TAB# h, m = divmod(m, 60) #LINE# #TAB# d = (y + m / 24.0 + h) / 24.0 #LINE# #TAB# 24.0 = h #LINE# #TAB# m -= 60.0 #LINE# #TAB# s += s / 60.0 #LINE# #TAB# return d"
"#LINE# #TAB# if node.kids: #LINE# #TAB# #TAB# nde = node.kids[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# nde = node #LINE# #TAB# if nde.value is None: #LINE# #TAB# #TAB# nde.value = value #LINE# #TAB# elif not isinstance(nde.value, list): #LINE# #TAB# #TAB# raise TypeError('Value must be a list or tuple') #LINE# #TAB# if nde.child is None: #LINE# #TAB# #TAB# nde.child = Node(value) #LINE# #TAB# else: #LINE# #TAB# #TAB# nde.child.value = value"
#LINE# #TAB# try: #LINE# #TAB# #TAB# v = _class_map[tag] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# v = tag #LINE# #TAB# return v
"#LINE# #TAB# query_params = query.as_dict() #LINE# #TAB# qs_params = {} #LINE# #TAB# for key in query_params.keys(): #LINE# #TAB# #TAB# if key == 'exclude': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(qs_params[key], dict): #LINE# #TAB# #TAB# #TAB# qs_params[key] = list(qs_params[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# qs_params[key] = qs_params[key] #LINE# #TAB# return qs_params"
"#LINE# #TAB# vertices_neighbours = _get_vertices_neighbours(nets) #LINE# #TAB# for subgraph_vertices in _get_connected_subgraphs(vertices_resources, #LINE# #TAB# #TAB# vertices_neighbours): #LINE# #TAB# #TAB# slug = _evaluate_v1_slug_subgraph(subgraph_vertices, vertices_neighbours) #LINE# #TAB# #TAB# yield slug #LINE# #TAB# if len(nets) == 1: #LINE# #TAB# #TAB# return #LINE# #TAB# for subgraph in _get_connected_subgraphs(nets): #LINE# #TAB# #TAB# for subgraph in _get_connected_subgraphs(subgraph_vertices, vertices_neighbours): #LINE# #TAB# #TAB# #TAB# for vertex in subgraph: #LINE# #TAB# #TAB# #TAB# #TAB# yield vertex"
#LINE# #TAB# cursor.execute('PRAGMA foreign_keys = ON') #LINE# #TAB# return [row.decode('utf-8') for row in cursor]
#LINE# #TAB# parser = yaml.SafeConfigParser() #LINE# #TAB# parser.optionxform = str #LINE# #TAB# return parser
"#LINE# #TAB# installed_dists_by_name = {} #LINE# #TAB# for installed_dist in installed_dists: #LINE# #TAB# #TAB# installed_dists_by_name[installed_dist.project_name] = installed_dist #LINE# #TAB# for requirement in dist.requires(): #LINE# #TAB# #TAB# present_dist = installed_dists_by_name.get(requirement.project_name) #LINE# #TAB# #TAB# if present_dist: #LINE# #TAB# #TAB# #TAB# yield requirement, present_dist"
#LINE# #TAB# if text.endswith(suffix): #LINE# #TAB# #TAB# return text[:-len(suffix)] #LINE# #TAB# return text
#LINE# #TAB# rgb = [(int(x) * 255) for x in xyz] #LINE# #TAB# clipped = False #LINE# #TAB# for i in range(len(rgb)): #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# clipped = True #LINE# #TAB# #TAB# rgb[i] = rgb[i][0] #LINE# #TAB# #TAB# if clipped: #LINE# #TAB# #TAB# #TAB# rgb[i] = rgb[i][1] #LINE# #TAB# return rgb
#LINE# #TAB# if i >= 2 ** 28: #LINE# #TAB# #TAB# raise ValueError('value of {} is too large'.format(i)) #LINE# #TAB# elif i < 0: #LINE# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# return i
"#LINE# #TAB# new_ip_allocation = {} #LINE# #TAB# allocation_file = module.params['allocation_file'] #LINE# #TAB# with open(allocation_file, 'r') as f: #LINE# #TAB# #TAB# content = f.read() #LINE# #TAB# for line in content.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# new_ip_allocation[line[0]] = line[1] #LINE# #TAB# return new_ip_allocation"
"#LINE# #TAB# sock = DDPSocket() #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, DDPSocket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, DDPSocket.SO_BROADCAST, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, DDPSocket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setblocking(0) #LINE# #TAB# return sock"
"#LINE# #TAB# root_models = [] #LINE# #TAB# for root_path in get_models(directory): #LINE# #TAB# #TAB# if '__init__' in root_path: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if root_path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# root_models.append((root_path, '__init__.py')) #LINE# #TAB# #TAB# elif flush_local_modules: #LINE# #TAB# #TAB# #TAB# root_models.append((root_path, os.path.relpath(root_path, #LINE# #TAB# #TAB# #TAB# #TAB# directory))) #LINE# #TAB# return root_models"
"#LINE# #TAB# if comm.strip() == '': #LINE# #TAB# #TAB# return None, {} #LINE# #TAB# try: #LINE# #TAB# #TAB# obj = json.loads(comm) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# obj = comm #LINE# #TAB# cmd_name = comm.split(' ')[0] #LINE# #TAB# args = [] #LINE# #TAB# if obj['args']: #LINE# #TAB# #TAB# args = json.loads(obj['args']) #LINE# #TAB# return cmd_name, args"
#LINE# #TAB# et_default_port_config = copy.deepcopy(pg_default_port_config) #LINE# #TAB# return et_default_port_config[pg_name]['outShapingPolicy']
"#LINE# #TAB# found = find_profile_build_ul(url) #LINE# #TAB# if found: #LINE# #TAB# #TAB# username = found.group('username') #LINE# #TAB# #TAB# password = found.group('password') #LINE# #TAB# else: #LINE# #TAB# #TAB# username = 'anonymous' #LINE# #TAB# #TAB# password = 'anonymous' #LINE# #TAB# return username, password"
"#LINE# #TAB# matches = [] #LINE# #TAB# with open(filename) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line_split = line.split() #LINE# #TAB# #TAB# #TAB# if line_split[0] == '----': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# matches.append((int(line_split[1]), float(line_split[2]))) #LINE# #TAB# matches.sort() #LINE# #TAB# return matches, []"
#LINE# #TAB# if type(elements) is int: #LINE# #TAB# #TAB# elements = [elements] #LINE# #TAB# while elements: #LINE# #TAB# #TAB# middle = elements % 2 #LINE# #TAB# #TAB# elements = elements // 2 #LINE# #TAB# #TAB# while elements > 1: #LINE# #TAB# #TAB# #TAB# elements -= 1 #LINE# #TAB# #TAB# while elements > 1: #LINE# #TAB# #TAB# #TAB# elements -= 1 #LINE# #TAB# return elements
"#LINE# #TAB# #TAB# gol = '' #LINE# #TAB# #TAB# start = 0 #LINE# #TAB# #TAB# for idx, s in enumerate(mol2_lst): #LINE# #TAB# #TAB# #TAB# if start == 0: #LINE# #TAB# #TAB# #TAB# #TAB# start = idx + 1 #LINE# #TAB# #TAB# #TAB# if start == len(mol2_lst) - 1: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# gol += s #LINE# #TAB# #TAB# #TAB# start = idx + 1 #LINE# #TAB# #TAB# return gol"
"#LINE# #TAB# for key, value in dictnode_tree.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# dictnode_tree[key].update(set_requirements_keys(value, src)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dictnode_tree[key] = ''.join(value) #LINE# #TAB# return dictnode_tree"
#LINE# #TAB# try: #LINE# #TAB# #TAB# returning = options['returning'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return base_context.returning if base_context else ReturnType.Records #LINE# #TAB# else: #LINE# #TAB# #TAB# if type(returning) is str: #LINE# #TAB# #TAB# #TAB# return ReturnType(returning) #LINE# #TAB# #TAB# return returning
"#LINE# #TAB# data = [] #LINE# #TAB# if use_ordered_dict: #LINE# #TAB# #TAB# data = OrderedDict(sorted(df[index_col].unique())) #LINE# #TAB# for row in df: #LINE# #TAB# #TAB# for key in row: #LINE# #TAB# #TAB# #TAB# data.append(cstring_node_ow_string(row[key], use_ordered_dict= #LINE# #TAB# #TAB# #TAB# #TAB# use_ordered_dict)) #LINE# #TAB# return data"
#LINE# #TAB# #TAB# if not now: #LINE# #TAB# #TAB# #TAB# now = datetime.datetime.now() #LINE# #TAB# #TAB# offset = timestamp - now #LINE# #TAB# #TAB# return offset * 1000
"#LINE# #TAB# _has_dt(nicknames, relations, name, format, program, directed) #LINE# #TAB# if directed: #LINE# #TAB# #TAB# return _has_dt_directed(nicknames, relations, name, format, program) #LINE# #TAB# else: #LINE# #TAB# #TAB# return _has_dt_native(nicknames, relations, name, format, program)['_has_dt']"
"#LINE# #TAB# all_columns = list(filter(lambda el: el[-2:] == '_I', df.columns)) #LINE# #TAB# for column in all_columns: #LINE# #TAB# #TAB# del df[column]"
"#LINE# #TAB# for key, val in in_dict.items(): #LINE# #TAB# #TAB# if val is not None: #LINE# #TAB# #TAB# #TAB# if has_recursive(val): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif isinstance(val, dict): #LINE# #TAB# #TAB# #TAB# for sub_key, sub_val in has_recursive(val).items(): #LINE# #TAB# #TAB# #TAB# #TAB# if sub_key is not None: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# del in_dict[sub_key] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return False"
#LINE# #TAB# handler = logging.NullHandler() #LINE# #TAB# if debug: #LINE# #TAB# #TAB# handler.setLevel(logging.DEBUG) #LINE# #TAB# return handler
"#LINE# #TAB# new_d = {} #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, Sequence): #LINE# #TAB# #TAB# #TAB# v = v[0] #LINE# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# new_d[k] = v[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_d[k] = v #LINE# #TAB# return new_d"
"#LINE# #TAB# frontend_modules = _load_frontend_modules(config.get(""MODULE_PATHS""), config.get(""MODULE_DEFAULTS""), callback, internal_attributes) #LINE# #TAB# vocab = { #LINE# #TAB# #TAB# config[""MODULE_NAME""]: { #LINE# #TAB# #TAB# #TAB# ""attributes"": frontend_modules, #LINE# #TAB# #TAB# } #LINE# #TAB# } #LINE# #TAB# for frontend_module in frontend_modules: #LINE# #TAB# #TAB# vocab[frontend_module] = generate_vocab(frontend_module, config, #LINE# #TAB# #TAB# #TAB# internal_attributes, callback) #LINE# #TAB# return vocab"
#LINE# #TAB# data = {} #LINE# #TAB# data['file'] = func.__file__ #LINE# #TAB# data['lineno'] = func.__code__.co_firstlineno #LINE# #TAB# data['function'] = func #LINE# #TAB# data['source'] = inspect.getsourcelines(func)[1] #LINE# #TAB# return data
#LINE# #TAB# return LooseVersion(obj_class.__version__) >= LooseVersion('1.8.0' #LINE# #TAB# #TAB# ) and obj_class.__version__ >= LooseVersion('2.0.0') and obj_class.__version__ < LooseVersion( #LINE# #TAB# #TAB# '3.0.2' #LINE# #TAB# #TAB# ) <= LooseVersion('1.8.0') and obj_class.__version__ < LooseVersion('1.16.0')
"#LINE# #TAB# #TAB# metadata = [] #LINE# #TAB# #TAB# for db_ref in entry.iterfind(""./dbreference""): #LINE# #TAB# #TAB# #TAB# model_ref = models.DbReference(reference=db_ref) #LINE# #TAB# #TAB# #TAB# metadata.append(model_ref) #LINE# #TAB# #TAB# return metadata"
#LINE# #TAB# rotated_histories = [] #LINE# #TAB# for histories_per_epoch in histories: #LINE# #TAB# #TAB# for epoch in histories_per_epoch: #LINE# #TAB# #TAB# #TAB# rotated_histories.append(namedb.RotatedMetric(epoch=epoch)) #LINE# #TAB# return rotated_histories
"#LINE# #TAB# digits = sys.getmaxsize() #LINE# #TAB# log_format = '%(lineno)d: %(name)s\t%(message)s' #LINE# #TAB# return log_format % {'lineno': sys.lineno, 'name': sys.name,'message': #LINE# #TAB# #TAB# traceback.format_exception(*sys.exc_info()[2])}"
"#LINE# #TAB# r = [] #LINE# #TAB# for name in os.listdir(dirpath): #LINE# #TAB# #TAB# full_name = os.path.join(dirpath, name) #LINE# #TAB# #TAB# if os.path.isdir(full_name): #LINE# #TAB# #TAB# #TAB# r.extend(edit_template(full_name, cond)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# r.append(full_name) #LINE# #TAB# return r"
#LINE# #TAB# out = 0 #LINE# #TAB# if value & 1: #LINE# #TAB# #TAB# out = value | 2147483648 #LINE# #TAB# elif value & 2: #LINE# #TAB# #TAB# out = (value >> 1) & 4294967295 #LINE# #TAB# elif value & 4: #LINE# #TAB# #TAB# out = (value >> 2) & 4294967295 #LINE# #TAB# return out
"#LINE# #TAB# with open(jsonParams) as f: #LINE# #TAB# #TAB# jsonFile = f.read() #LINE# #TAB# parsedJSON = json.loads(jsonFile) #LINE# #TAB# return parsedJSON[blockNames.ControlFileParams.generalParams], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.spawningBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.simulationBlockname]"
"#LINE# #TAB# ports = [] #LINE# #TAB# for i in dir(obj): #LINE# #TAB# #TAB# if callable(getattr(obj, i)) and not i.startswith('_'): #LINE# #TAB# #TAB# #TAB# ports.append((i, getattr(obj, i))) #LINE# #TAB# return ports"
#LINE# #TAB# _stdout_ski = _vocab_helper(format=format) #LINE# #TAB# return _stdout_ski
"#LINE# #TAB# #TAB# keys = p[0] #LINE# #TAB# #TAB# values = p[1] #LINE# #TAB# #TAB# for k in keys: #LINE# #TAB# #TAB# #TAB# yield k, values"
"#LINE# #TAB# theta = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
#LINE# #TAB# if match.end >= len(chars): #LINE# #TAB# #TAB# return True #LINE# #TAB# return match.input_string[match.end] in chars
"#LINE# #TAB#dsn = cls.client.creae_dsn(subnet_id, router_id, name=name) #LINE# #TAB# return dsn"
"#LINE# #TAB# lines = [] #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# row = OrderedDict() #LINE# #TAB# #TAB# row['Name'] = rule['name'] #LINE# #TAB# #TAB# row['Resource Group'] = rule['resourceGroup'] #LINE# #TAB# #TAB# row['Resource Group Size'] = str(len(rule['resourceGroups'])) #LINE# #TAB# #TAB# lines.append(row) #LINE# #TAB# #TAB# if rule['resourceGroups']: #LINE# #TAB# #TAB# #TAB# for group in rule['resourceGroups']: #LINE# #TAB# #TAB# #TAB# #TAB# row['Resource Group Size'] = str(group['resourceGroup']) #LINE# #TAB# #TAB# lines.append(row) #LINE# #TAB# table = Table(lines, title='Rules') #LINE# #TAB# return table"
"#LINE# #TAB# from schmea.orm import ICachedItemMapper #LINE# #TAB# df = schema.to_pandas() #LINE# #TAB# if isinstance(df, DataFrame): #LINE# #TAB# #TAB# return df #LINE# #TAB# elif isinstance(df, dict): #LINE# #TAB# #TAB# columns = [df.keys()[0]] #LINE# #TAB# #TAB# df.columns = columns #LINE# #TAB# #TAB# return ICachedItemMapper(columns) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if re.match('^[a-zA-Z][0-9a-zA-Z+\\-\\.]*://', uri): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# urls = [] #LINE# #TAB# full_path = resource #LINE# #TAB# while os.path.isdir(full_path): #LINE# #TAB# #TAB# full_path = os.path.join(full_path, resource) #LINE# #TAB# #TAB# if os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# urls += walk_common(full_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# urls += walk_common(full_path) #LINE# #TAB# return urls"
#LINE# #TAB# context = os.environ.copy() #LINE# #TAB# if 'PATH' in context: #LINE# #TAB# #TAB# os.environ['PATH'] = context['PATH'] #LINE# #TAB# if not os.path.isdir(context['PATH']): #LINE# #TAB# #TAB# os.makedirs(context['PATH']) #LINE# #TAB# if command in os.listdir(context['PATH']): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# value_not_found = fget.apply(fdel) #LINE# #TAB# while value_not_found is None and value_not_found!= '': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = apply_func(fget) #LINE# #TAB# #TAB# #TAB# if result!= '': #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# logger.warning('Applying function %s failed with error: %s', #LINE# #TAB# #TAB# #TAB# #TAB# apply_func, value_not_found) #LINE# #TAB# #TAB# #TAB# value_not_found = e #LINE# #TAB# return value_not_found"
"#LINE# #TAB# q = q.copy() #LINE# #TAB# for i in range(q.shape[1]): #LINE# #TAB# #TAB# if q[i, 0]!= x[i]: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# add_v(q, x, i) #LINE# #TAB# return q"
"#LINE# #TAB# f_num = normalise_f(f) #LINE# #TAB# f_den = normalise_f(f) #LINE# #TAB# if not df_num and not df_den: #LINE# #TAB# #TAB# return f_num, f_den #LINE# #TAB# elif not df_num and not df_den: #LINE# #TAB# #TAB# return f_num, f_den #LINE# #TAB# return f_num, f_den"
#LINE# #TAB# request_handler.is_valid = validator is not None #LINE# #TAB# return request_handler
"#LINE# #TAB# if isinstance(path, Path): #LINE# #TAB# #TAB# path = path.paths #LINE# #TAB# if not isinstance(path, (list, tuple)): #LINE# #TAB# #TAB# path = [path] #LINE# #TAB# data = [] #LINE# #TAB# for p in path: #LINE# #TAB# #TAB# if isinstance(p, str): #LINE# #TAB# #TAB# #TAB# p = Path(p) #LINE# #TAB# #TAB# data.append(p) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data.append(list(p)) #LINE# #TAB# if not data: #LINE# #TAB# #TAB# raise ValueError('invalid path') #LINE# #TAB# return data"
"#LINE# #TAB# N = X.shape[0] #LINE# #TAB# ac = np.zeros((N, N)) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# for j in range(N): #LINE# #TAB# #TAB# #TAB# ac[i, j] = ac_cell(X[i, j], cells) #LINE# #TAB# return ac"
"#LINE# #TAB# return {'trun': trun, 'axes': []} #LINE# #TAB# for axis in tsuite['axes'].keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f = getattr(trun, axis) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# trun['axes'].pop(axis) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f = getattr(trun, axis) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# trun['axes'].append((axis, f)) #LINE# #TAB# return {'trun': trun, 'axes': []}"
"#LINE# #TAB# for file_name in glob.glob(os.path.join(path, '*.yaml')): #LINE# #TAB# #TAB# yield file_name"
"#LINE# #TAB# if nsname is None and nspid is None: #LINE# #TAB# #TAB# return nspath #LINE# #TAB# elif nspath is not None: #LINE# #TAB# #TAB# return nspath #LINE# #TAB# elif nsname is not None and nspid is not None: #LINE# #TAB# #TAB# return '%s/%s' % (nspath, nspid) #LINE# #TAB# else: #LINE# #TAB# #TAB# return nspath"
#LINE# #TAB# if not groupids: #LINE# #TAB# #TAB# return '' #LINE# #TAB# for item in items: #LINE# #TAB# #TAB# if item['groupid'] in groupids: #LINE# #TAB# #TAB# #TAB# yield item
#LINE# #TAB# if 'type' in schema: #LINE# #TAB# #TAB# return schema['type'] #LINE# #TAB# elif 'enum' in schema: #LINE# #TAB# #TAB# return [x.strip() for x in schema['enum']] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# while hasattr(node, 'children'): #LINE# #TAB# #TAB# node = node.children[0] #LINE# #TAB# yield node"
#LINE# #TAB# if running_tm_ver == 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# samples = np.asarray(samples) #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# samples = np.log(samples) #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# samples = np.log(samples) #LINE# #TAB# return samples
"#LINE# #TAB# for key, value in cls.__dict__.items(): #LINE# #TAB# #TAB# if isinstance(value, tuple): #LINE# #TAB# #TAB# #TAB# yield key, value #LINE# #TAB# #TAB# elif isinstance(value, property): #LINE# #TAB# #TAB# #TAB# yield key, value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield key, value"
#LINE# #TAB# has = False #LINE# #TAB# for col in state.result: #LINE# #TAB# #TAB# if col_names is not None and state.result[col].get(match): #LINE# #TAB# #TAB# #TAB# has = True #LINE# #TAB# if sort: #LINE# #TAB# #TAB# for row in state.rows: #LINE# #TAB# #TAB# #TAB# if row[col].sort()!= state.result[col].get(match): #LINE# #TAB# #TAB# #TAB# #TAB# has = True #LINE# #TAB# return has
"#LINE# #TAB# key = variants['key'] #LINE# #TAB# variant['key'] = base64.b64decode(key).decode('utf8') #LINE# #TAB# append_fields(variants, key) #LINE# #TAB# return variants"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return requests.head(url).headers #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# return {}
"#LINE# #TAB# predicted_risk_input = riskinputs.copy() #LINE# #TAB# predicted_risk_model = riskmodel.copy() #LINE# #TAB# X = np.dot(risk_inputs, predicted_risk_input) #LINE# #TAB# monitor.update( #LINE# #TAB# #TAB# [x for x in X] #LINE# #TAB# #TAB# [monitor.update(x) for x in X] #LINE# #TAB# ) #LINE# #TAB# return X"
#LINE# #TAB# plugin_name = plugin_class.NAME.lower() #LINE# #TAB# if plugin_name in cls._plugin_classes: #LINE# #TAB# raise KeyError('Plugin class already set for name: {0:s}.'.format( #LINE# #TAB# #TAB# plugin_class.NAME)) #LINE# #TAB# cls._plugin_classes[plugin_name] = plugin_class
"#LINE# #TAB# data = dxpy.info(app, env=env, region=region) #LINE# #TAB# data['deployment'] = {'name': data['deployment']['name'],'version': data[ #LINE# #TAB# #TAB# 'deployment']['version'], 'docker': data['docker']['url'], #LINE# #TAB# #TAB# 'docker_env': env} #LINE# #TAB# return data"
"#LINE# #TAB# tmp = V * dx #LINE# #TAB# for i in range(2): #LINE# #TAB# #TAB# for j in range(i + 1, len(tmp)): #LINE# #TAB# #TAB# #TAB# tmp[i] = interp1d(tmp[j], tmp[i + 1], eps) #LINE# #TAB# return tmp"
#LINE# #TAB# body = cls.client.update_compute(external_network_id=external_network_id) #LINE# #TAB# floating_ip = body['floating_ip'] #LINE# #TAB# cls.ip_to_compute.append(floating_ip) #LINE# #TAB# return floating_ip
#LINE# #TAB# for row in rows: #LINE# #TAB# #TAB# for column in range(len(row)): #LINE# #TAB# #TAB# #TAB# if left: #LINE# #TAB# #TAB# #TAB# #TAB# yield column[0] #LINE# #TAB# #TAB# #TAB# yield column[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield column[0]
"#LINE# #TAB# decoded = '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# if key == 'extra': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# decoded += f'{key}={value}' #LINE# #TAB# return decoded"
"#LINE# #TAB# outdir = Path(outdir) #LINE# #TAB# df.to_csv(outdir, sep='\t', index=False) #LINE# #TAB# return outdir"
#LINE# #TAB# config.LOGGER.info('Downloading files to different formats') #LINE# #TAB# files_to_diff = list(set(files_to_diff)) #LINE# #TAB# if config.SUSHI_BAR_CLIENT: #LINE# #TAB# #TAB# config.SUSHI_BAR_CLIENT.download_files(files_to_diff) #LINE# #TAB# else: #LINE# #TAB# #TAB# config.SUSHI_BAR_CLIENT.download_files(files_to_diff) #LINE# #TAB# return files_to_diff
#LINE# #TAB# stream_connection = 'psutil-stream' #LINE# #TAB# return stream_connection
"#LINE# #TAB# size = ts.ts.size #LINE# #TAB# if start < ts.ts.size: #LINE# #TAB# #TAB# start = ts.ts.size - pad #LINE# #TAB# if end > ts.ts.size: #LINE# #TAB# #TAB# end = ts.ts.size - pad #LINE# #TAB# for i in range(int(start) + pad, int(end) + pad): #LINE# #TAB# #TAB# yield ts[i:i + pad] #LINE# #TAB# return"
#LINE# #TAB# global _gt_context #LINE# #TAB# if _gt_context is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# with io.StringIO() as stream: #LINE# #TAB# #TAB# token = stream.readline().strip() #LINE# #TAB# #TAB# if not token: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# _gt_context = token.encode('utf-8') #LINE# #TAB# return _gt_context
#LINE# #TAB# assert Nu > 1 #LINE# #TAB# assert Nc > 1 #LINE# #TAB# clust = 0 #LINE# #TAB# for clust_idx in range(Nu): #LINE# #TAB# #TAB# if clust == 0 and Nc == 1: #LINE# #TAB# #TAB# #TAB# clust = 1 #LINE# #TAB# #TAB# elif Nc > clust_idx: #LINE# #TAB# #TAB# #TAB# clust = 0 #LINE# #TAB# return clust
"#LINE# #TAB# max_size = powerline.segment_conf('cwd','max_dir_size') #LINE# #TAB# if max_size: #LINE# #TAB# #TAB# return name[:max_size] #LINE# #TAB# return name"
#LINE# #TAB# if experiment.metadata.letter is experiment.metadata.new_title: #LINE# #TAB# #TAB# return experiment.metadata.letter +'' + experiment.metadata.title #LINE# #TAB# else: #LINE# #TAB# #TAB# return experiment.metadata.blank
"#LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif isinstance(data, dict): #LINE# #TAB# #TAB# return {key: shorten_object(value) for key, value in data.items()} #LINE# #TAB# elif isinstance(data, list): #LINE# #TAB# #TAB# return [shorten_object(x) for x in data] #LINE# #TAB# elif isinstance(data, dict): #LINE# #TAB# #TAB# return {key: shorten_object(v) for key, v in data.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return data"
"#LINE# #TAB# if isinstance(obj, datetime): #LINE# #TAB# #TAB# obj = obj.isoformat() #LINE# #TAB# return obj"
#LINE# #TAB# if lock_base_model: #LINE# #TAB# #TAB# return _dn_registry_ag_cache #LINE# #TAB# else: #LINE# #TAB# #TAB# return _dn_registry_ag_cache
#LINE# #TAB# data = bytes(data) #LINE# #TAB# im = Image.open(BytesIO(data)) #LINE# #TAB# return im
#LINE# #TAB# has = False #LINE# #TAB# for ips_marker in fhpatch.get_markers(): #LINE# #TAB# #TAB# if ips_marker == EOF: #LINE# #TAB# #TAB# #TAB# has = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return has
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('ln', dtype='int32', direction=function.IN) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"#LINE# #TAB# actions = [] #LINE# #TAB# for func_name in dir(cls): #LINE# #TAB# #TAB# func = getattr(cls, func_name) #LINE# #TAB# #TAB# if not hasattr(func, '__call__') or getattr(func, #LINE# #TAB# #TAB# #TAB# '__daemonocle_exposed__', False) is not True: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# action = func_name.replace('_', '-') #LINE# #TAB# #TAB# if action not in actions: #LINE# #TAB# #TAB# #TAB# actions.append(action) #LINE# #TAB# return actions"
#LINE# #TAB# accounting_tags = set() #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line!= '': #LINE# #TAB# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# accounting_tags.add(line.split(' ')[0]) #LINE# #TAB# return accounting_tags
"#LINE# #TAB# #TAB# java_obj = javabridge.get_object(clazz.getFileName(), javabridge.get_instance_class( #LINE# #TAB# #TAB# #TAB# clazz.getInstance())) #LINE# #TAB# #TAB# return java_obj"
"#LINE# #TAB# if not rate: #LINE# #TAB# #TAB# return None #LINE# #TAB# factor, scale = rate.split(':', 1) #LINE# #TAB# scale = int(scale) #LINE# #TAB# if scale < 1: #LINE# #TAB# #TAB# raise ValueError('Invalid rate: %r' % rate) #LINE# #TAB# rae = 10 ** factor * rate #LINE# #TAB# return rae"
"#LINE# #TAB# if job.task =='search': #LINE# #TAB# #TAB# basedir = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# #TAB# output = os.path.join(basedir,'result.txt') #LINE# #TAB# #TAB# if os.path.exists(output): #LINE# #TAB# #TAB# #TAB# with open(output, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# text = f.read() #LINE# #TAB# #TAB# #TAB# job.result = text #LINE# #TAB# #TAB# return job #LINE# #TAB# elif job.task == 'open' and os.path.exists(output): #LINE# #TAB# #TAB# with open(output, 'r') as f: #LINE# #TAB# #TAB# #TAB# job.result = json.load(f) #LINE# #TAB# return job"
"#LINE# #TAB# text = text.replace('&lt;', '<') #LINE# #TAB# text = text.replace('&gt;', '>') #LINE# #TAB# text = text.replace('&quot;', '""') #LINE# #TAB# return text"
"#LINE# #TAB# if isinstance(obj, collections.Mapping): #LINE# #TAB# #TAB# return dict((k, shuffle_strins(v)) for k, v in obj.items()) #LINE# #TAB# elif isinstance(obj, collections.Iterable): #LINE# #TAB# #TAB# return type(obj)(shuffle_strins(x) for x in obj) #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# prop = model_cls._get_prop(prop_name) #LINE# #TAB# if prop: #LINE# #TAB# #TAB# mdel_errors = prop.get_mdel_errors() #LINE# #TAB# else: #LINE# #TAB# #TAB# mdel_errors = None #LINE# #TAB# return mdel_errors
#LINE# #TAB# with db.session.begin_nested(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del LizardInbox #LINE# #TAB# #TAB# #TAB# del LizardInbox.query #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# annotations = IAnnotations(context) #LINE# #TAB# return {'googl_anlytics_describe': annotations,'max_retries': #LINE# #TAB# #TAB# context.max_retries,'metrics_per_task': context.metrics_per_task, #LINE# #TAB# #TAB#'max_count': context.max_count}"
#LINE# #TAB# new_gmt_datadir = os.environ.get('GMT_DATADIR') #LINE# #TAB# if not os.path.exists(new_gmt_datadir): #LINE# #TAB# #TAB# os.environ['GMT_DATADIR'] = old_gmt_datadir #LINE# #TAB# elif new_gmt_datadir!= old_gmt_datadir: #LINE# #TAB# #TAB# os.environ['GMT_DATADIR'] = new_gmt_datadir
"#LINE# #TAB# files = [] #LINE# #TAB# if os.path.exists(default_config_path): #LINE# #TAB# #TAB# for dirpath, dirnames, filenames in os.walk(default_config_path): #LINE# #TAB# #TAB# #TAB# for filename in filenames: #LINE# #TAB# #TAB# #TAB# #TAB# if filename.endswith('.json'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# json_config_path = os.path.join(dirpath, filename) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# with open(json_config_path) as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# content = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# files.append(content) #LINE# #TAB# return files"
#LINE# #TAB# #TAB# if not file_name: #LINE# #TAB# #TAB# #TAB# raise ValueError('{0} is not a valid file name'.format(file_name)) #LINE# #TAB# #TAB# if model_exists(file_name): #LINE# #TAB# #TAB# #TAB# return file_name #LINE# #TAB# #TAB# model = model_read(file_name) #LINE# #TAB# #TAB# if model: #LINE# #TAB# #TAB# #TAB# return model #LINE# #TAB# #TAB# if model.exists(): #LINE# #TAB# #TAB# #TAB# logger.debug('{0} is included in model.'.format(file_name)) #LINE# #TAB# #TAB# #TAB# return model #LINE# #TAB# #TAB# return None
#LINE# #TAB# client = context['client'] #LINE# #TAB# col = None #LINE# #TAB# try: #LINE# #TAB# #TAB# client = client.clusters[cluster] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# if show_progress: #LINE# #TAB# #TAB# #TAB# click.echo('Cluster %s not found.' % cluster) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# click.echo('Cluster %s not found.' % cluster) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# col = cluster #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# click.echo('Unable to create cluster %s.' % cluster) #LINE# #TAB# return col
"#LINE# #TAB# encoded = {} #LINE# #TAB# for key, value in to_encode.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# value = [ecode_to_dict(e) for ecode_dict in value] #LINE# #TAB# #TAB# encoded[key] = value #LINE# #TAB# return encoded"
"#LINE# #TAB# x = ZeroPadding2D(((1, 0), (1, 0)))(x) #LINE# #TAB# for i in range(num_blocks): #LINE# #TAB# #TAB# y = compose(DarknetConvolution2D(num_filters // 2, (1, 1)), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# DarknetConvolution2D(num_filters // 2, (1, 1)))(x) #LINE# #TAB# #TAB# x = Add()([x, y]) #LINE# #TAB# return x"
#LINE# #TAB# c['si_anulus_plant'] = moving_si_anulus_plant(c) #LINE# #TAB# return c
"#LINE# #TAB# fractional_coordinates = deepcopy(frac_coordinates) #LINE# #TAB# for coord in range(frac_coordinates.shape[0]): #LINE# #TAB# #TAB# fractional_coordinates[coord] = sentry_coordinates_from_fractional( #LINE# #TAB# #TAB# #TAB# fractional_coordinates[coord], lattice_array) #LINE# #TAB# return fractional_coordinates"
"#LINE# #TAB# if prefix is None: #LINE# #TAB# #TAB# prefix = _expnum_to_prefix(expnum) #LINE# #TAB# key = _expnum_to_key(expnum, ccd, version) #LINE# #TAB# if key in ccd: #LINE# #TAB# #TAB# return ccd[key] #LINE# #TAB# else: #LINE# #TAB# #TAB# aperture = generate_pci(expnum, ccd, version, ext='aperture') #LINE# #TAB# #TAB# if prefix: #LINE# #TAB# #TAB# #TAB# key = ""%s-%s"" % (prefix, key) #LINE# #TAB# #TAB# #TAB# aperture = aperture[key] #LINE# #TAB# #TAB# return aperture"
#LINE# #TAB# txt = b'' #LINE# #TAB# for item in stack: #LINE# #TAB# #TAB# txt += _ye_image(item) #LINE# #TAB# return txt
#LINE# #TAB# if s.startswith(start): #LINE# #TAB# #TAB# i = s.find(start) #LINE# #TAB# #TAB# if i < 0: #LINE# #TAB# #TAB# #TAB# raise ValueError('Value did not start with %s' % start) #LINE# #TAB# else: #LINE# #TAB# #TAB# return s[:i]
"#LINE# #TAB# value = float(value) #LINE# #TAB# if value < 0.0: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Glyph left margin must be non-negative, got %s.' % value) #LINE# #TAB# return value"
"#LINE# #TAB# if counts.sum() <= n: #LINE# #TAB# #TAB# return counts #LINE# #TAB# cumsum = np.cumsum(counts, dtype=dtype) #LINE# #TAB# nz = counts.nonzero()[0] #LINE# #TAB# result = cumsum[nz] #LINE# #TAB# counts[nz] = result #LINE# #TAB# return counts"
"#LINE# #TAB# out = {} #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# out[u'.'.join(k)] = decoed_array(v) #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# out[u'.join(k)] = [decoed_array(v) for v in v] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out[u'.join(k)] = v #LINE# #TAB# return out"
"#LINE# #TAB# if is_pyparon_model(o): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return json.dumps(o, indent=2) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return o"
"#LINE# #TAB# method = request.method.upper() #LINE# #TAB# url = reverse(method) #LINE# #TAB# headers = { #LINE# #TAB# #TAB# 'X-Requested-With': 'XMLHttpRequest' #LINE# #TAB# } #LINE# #TAB# resp = request.response #LINE# #TAB# resp.status_code = 200 #LINE# #TAB# resp.content = json.dumps(resp.read(), indent=4) #LINE# #TAB# return resp"
#LINE# #TAB# for s in structures: #LINE# #TAB# #TAB# params = s.get_param_lattice() #LINE# #TAB# #TAB# for s2 in params: #LINE# #TAB# #TAB# #TAB# s.set_param_lattice(s2.get_param_lattice()) #LINE# #TAB# flat_structures = [] #LINE# #TAB# for s in structures: #LINE# #TAB# #TAB# if len(flat_structures) == 1: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# flat_structures.append(s) #LINE# #TAB# return flat_structures
#LINE# #TAB# message = message.encode() #LINE# #TAB# h = hashlib.sha256() #LINE# #TAB# h.update(message) #LINE# #TAB# response_hash = h.hexdigest() #LINE# #TAB# return response_hash
"#LINE# #TAB# day_in_month = _get_day_in_month(year, month) #LINE# #TAB# if day_in_month < 0: #LINE# #TAB# #TAB# day_in_month = 1 #LINE# #TAB# closest_days = 0 #LINE# #TAB# for i in range(31): #LINE# #TAB# #TAB# if day_in_month > day_in_month: #LINE# #TAB# #TAB# #TAB# closest_days += 1 #LINE# #TAB# return closest_days"
"#LINE# #TAB# filename = url.replace('\\', '/') #LINE# #TAB# m = re.search('^\\s*__\\w+\\s*$', filename) #LINE# #TAB# if m: #LINE# #TAB# #TAB# filename = m.group(1) #LINE# #TAB# #TAB# filename = filename.replace('\\n', '\n') #LINE# #TAB# #TAB# filename = filename.replace('\\t', '\t') #LINE# #TAB# return filename"
#LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if l[i]!= l[i + 1] and l[i]!= l[i + 2]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if len(set(l)) == len(L) - 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# prime = False #LINE# #TAB# #TAB# for x in l: #LINE# #TAB# #TAB# #TAB# if not prime: #LINE# #TAB# #TAB# #TAB# #TAB# prime = False #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if prime: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# updat_conrol_chain_opts_ = update_conrol_chain_opts_py(control_board, #LINE# #TAB# #TAB# fitted_params) #LINE# #TAB# return updat_conrol_chain_opts_"
"#LINE# #TAB# country_deaths = {} #LINE# #TAB# for instance in instance.countries.values(): #LINE# #TAB# #TAB# for country in instance.countries.keys(): #LINE# #TAB# #TAB# #TAB# country_deaths[country] = ge_axes_deaths_per_country(instance, #LINE# #TAB# #TAB# #TAB# #TAB# country_deaths[country]) #LINE# #TAB# return country_deaths"
#LINE# #TAB# game = {} #LINE# #TAB# for elem in tree.iter('.//game'): #LINE# #TAB# #TAB# name = elem.attrib['name'] #LINE# #TAB# #TAB# if name == 'order': #LINE# #TAB# #TAB# #TAB# direction = 'left' #LINE# #TAB# #TAB# elif name == 'leap': #LINE# #TAB# #TAB# #TAB# direction = 'right' #LINE# #TAB# #TAB# elif direction == 'left': #LINE# #TAB# #TAB# #TAB# direction = 'left' #LINE# #TAB# #TAB# game[name] = {'order': direction} #LINE# #TAB# return game
#LINE# #TAB# paths = _find_requirements_unique_paths(search_string) #LINE# #TAB# if not paths: #LINE# #TAB# #TAB# return #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if path[-1]!= replacement_line: #LINE# #TAB# #TAB# #TAB# path = path[:-1] #LINE# #TAB# #TAB# return path + replacement_line
#LINE# #TAB# FN_TYPE = type(p) #LINE# #TAB# if FN_TYPE == type('S'): #LINE# #TAB# #TAB# return 1 #LINE# #TAB# elif FN_TYPE == type('N'): #LINE# #TAB# #TAB# return 2 #LINE# #TAB# elif FN_TYPE == type('R'): #LINE# #TAB# #TAB# return 3 #LINE# #TAB# elif FN_TYPE == type('S'): #LINE# #TAB# #TAB# return 4 #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# num_bytes = ord(base[0]) #LINE# #TAB# if align & 1: #LINE# #TAB# #TAB# num_bytes += 1 #LINE# #TAB# elif align & 2: #LINE# #TAB# #TAB# num_bytes += align - 2 #LINE# #TAB# if base[0] & 192!= 128: #LINE# #TAB# #TAB# num_bytes += 192 #LINE# #TAB# #TAB# base[0] = 128 #LINE# #TAB# base[1] <<= num_bytes #LINE# #TAB# if align & 3: #LINE# #TAB# #TAB# num_bytes += 3 #LINE# #TAB# return num_bytes
#LINE# #TAB# unassigned = [] #LINE# #TAB# for analysis in analysis_request.getAnalyses(): #LINE# #TAB# #TAB# analysis_status = api.get_workflow_status_of(analysis) #LINE# #TAB# #TAB# if analysis_status in unassigned: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if api.get_workflow_status_of(analysis)!= 'unassigned': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if not host: #LINE# #TAB# #TAB# return False #LINE# #TAB# rpg_hosts = ['127.0.0.1', '127.0.0.1', '127.0.0.1'] #LINE# #TAB# for rpg_host in rpg_hosts: #LINE# #TAB# #TAB# if rpg_host == host: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if isinstance(arg, (list, tuple)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res = arg[0] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# res = lookup_dict.get(arg[0], None) #LINE# #TAB# #TAB# if res is not None: #LINE# #TAB# #TAB# #TAB# return res #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return lookup_dict[arg] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# elif isinstance(arg, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return lookup_dict[arg] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return arg"
"#LINE# #TAB# fields = [] #LINE# #TAB# index_options = get_index_options(connection, db, tbl, index) #LINE# #TAB# if index_options is not None: #LINE# #TAB# #TAB# for key in index_options: #LINE# #TAB# #TAB# #TAB# fields.append(key) #LINE# #TAB# return fields"
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise
#LINE# #TAB# projec_repositry = dict() #LINE# #TAB# projec_repositry['name'] = project_element.attrib['name'] #LINE# #TAB# projec_repositry['url'] = project_element.attrib['url'] #LINE# #TAB# projec_repositry['project_label'] = project_element.attrib['project_label'] #LINE# #TAB# projec_repositry['repository_name'] = project_element.attrib['repository_name' #LINE# #TAB# #TAB# ] #LINE# #TAB# return projec_repositry
"#LINE# #TAB# bool=False) ->Union[int, str]: #LINE# #TAB# if table_code == 3: #LINE# #TAB# #TAB# return msc_rom_schema_code[table_name] #LINE# #TAB# elif table_code == 2: #LINE# #TAB# #TAB# return 3 #LINE# #TAB# elif table_code == 4: #LINE# #TAB# #TAB# return 5 #LINE# #TAB# elif table_code == 3: #LINE# #TAB# #TAB# return 6 #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# recipes_exists = [] #LINE# #TAB# for recipes in node.run_list: #LINE# #TAB# #TAB# for option in recipes: #LINE# #TAB# #TAB# #TAB# recipes_exists.append(option) #LINE# #TAB# return recipes_exists
"#LINE# #TAB# base, ext = os.path.splitext(name) #LINE# #TAB# out = {'base': base, 'ext': ext} #LINE# #TAB# return out"
"#LINE# #TAB# mean = [0.485, 0.456, 0.406] #LINE# #TAB# std = [0.229, 0.224, 0.225] #LINE# #TAB# image = image.convert('RGB') #LINE# #TAB# arr = np.array(image) #LINE# #TAB# arr = crop_and_resize(arr, 224) #LINE# #TAB# arr = arr / 255.0 #LINE# #TAB# arr[..., 0] -= mean[0] #LINE# #TAB# arr[..., 1] -= mean[1] #LINE# #TAB# arr[..., 2] -= mean[2] #LINE# #TAB# arr[..., 0] /= std[0] #LINE# #TAB# arr[..., 1] /= std[1] #LINE# #TAB# return arr"
"#LINE# #TAB# import tempfile #LINE# #TAB# tmpfile = tempfile.NamedTemporaryFile(delete=False) #LINE# #TAB# with open(tmpfile.name, 'w') as xlsxf: #LINE# #TAB# #TAB# xlsxf.write(template_file) #LINE# #TAB# rewrite_env(xlsxf) #LINE# #TAB# os.unlink(tmpfile.name) #LINE# #TAB# return xlsxf"
#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# return _DEFAULT_PAH #LINE# #TAB# else: #LINE# #TAB# #TAB# return _DEFAULT_ROOT
"#LINE# #TAB# tokens = line.strip().split('\t') #LINE# #TAB# prop_str = unquote(tokens[0]) #LINE# #TAB# if prop_str == '': #LINE# #TAB# #TAB# return #LINE# #TAB# elif len(tokens) < 2: #LINE# #TAB# #TAB# return #LINE# #TAB# for token in tokens[1:]: #LINE# #TAB# #TAB# name, value = token.split('=') #LINE# #TAB# #TAB# prop_dict[name] = value"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(s) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return s
"#LINE# #TAB# summary = {'LOGNAME': os.environ['LOGNAME'], 'USER': os.environ['USER'], #LINE# #TAB# #TAB# 'INFLUXDB_URL': os.environ['INFLUXDB_URL']} #LINE# #TAB# for k, v in summary.items(): #LINE# #TAB# #TAB# if k in ['LOGNAME', 'USER', 'INFLUXDB_URL']: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# del summary[k] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# return summary"
"#LINE# #TAB# ret = [] #LINE# #TAB# for k in dir(cls): #LINE# #TAB# #TAB# if k.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not hasattr(cls, k): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# f = getattr(cls, k) #LINE# #TAB# #TAB# if _is_persistent_attribute(f): #LINE# #TAB# #TAB# #TAB# ret.append(k) #LINE# #TAB# return ret"
"#LINE# #TAB# device_tuple_list = get_tuple_evices_current_hstname(database_name) #LINE# #TAB# tuple_evices = [] #LINE# #TAB# for device in device_tuple_list: #LINE# #TAB# #TAB# if search_string in device.get('hostName', []): #LINE# #TAB# #TAB# #TAB# tuple_evices.append(device) #LINE# #TAB# return tuple_evices"
"#LINE# #TAB# if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)): #LINE# #TAB# #TAB# dtend_float = obj.dtend #LINE# #TAB# elif isinstance(obj, (datetime.timedelta,)): #LINE# #TAB# #TAB# dtend_float = obj.duration #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError('unsupported type: %r' % type(obj)) #LINE# #TAB# if not hasattr(obj, 'dtend'): #LINE# #TAB# #TAB# obj.dtend = dtend_float #LINE# #TAB# return obj.dtend"
#LINE# #TAB# alleles = {} #LINE# #TAB# for alias in typealiases: #LINE# #TAB# #TAB# if '-' in alias: #LINE# #TAB# #TAB# #TAB# alias = alias.split('-')[0] #LINE# #TAB# #TAB# #TAB# alleles[alias] = True #LINE# #TAB# return alleles
#LINE# #TAB# try: #LINE# #TAB# #TAB# return globals()[name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if default is not None: #LINE# #TAB# #TAB# return default
#LINE# #TAB# key = state.prompt_keys[state.cur_key] #LINE# #TAB# try: #LINE# #TAB# #TAB# subkeys = key.split('_') #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return '' #LINE# #TAB# if subkeys[0] == 'X': #LINE# #TAB# #TAB# return state.safe_subkeys(subkeys[1:]) #LINE# #TAB# elif subkeys[0] == 'P': #LINE# #TAB# #TAB# return state.safe_subkeys(subkeys[1:]) #LINE# #TAB# return ''
#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# return logger
#LINE# #TAB# try: #LINE# #TAB# #TAB# child.related_ignore_alias = alias_map[child.related_ignore_alias] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# bidi = request_parameters.get('oidc_bidi') #LINE# #TAB# if bidi is None: #LINE# #TAB# #TAB# raise OIDCRequestParameterMissing( #LINE# #TAB# #TAB# #TAB# 'oidc_bidi is required') #LINE# #TAB# return bidi
"#LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# return (f.read(4) + f.read(8)) // 8"
"#LINE# #TAB# fig, axes = plt.subplots(3, 1, figsize=(18, 6)) #LINE# #TAB# ax1 = axes[0] #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# ax1[:, (i)] = swim_speed[0] * np.cos(np.radians(exp_ind[i])) #LINE# #TAB# #TAB# ax1[:, (i)] = -ax1[:, (i)] * swim_speed[1] #LINE# #TAB# ax1.set_xlim(0, 2 * np.pi) #LINE# #TAB# ax1.set_ylim(0, 2 * np.pi) #LINE# #TAB# return fig, axes"
"#LINE# #TAB# cert = cls(token=api_token, id=cert_id) #LINE# #TAB# cert.load() #LINE# #TAB# return cert"
"#LINE# #TAB# check_path(path) #LINE# #TAB#ile_path = os.path.abspath(os.path.join(doc_root, path)) #LINE# #TAB# returnile_path"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(txt) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
#LINE# #TAB# try: #LINE# #TAB# #TAB# return type(value) is None #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# _LOGGER.info('Reset password') #LINE# #TAB# global _password #LINE# #TAB# _password = None
"#LINE# #TAB# for ind, x in enumerate(indices): #LINE# #TAB# #TAB# full_path = os.path.join(indir, x) #LINE# #TAB# #TAB# if os.path.exists(full_path): #LINE# #TAB# #TAB# #TAB# return full_path"
#LINE# #TAB# global paths #LINE# #TAB# if path is None or path in paths: #LINE# #TAB# #TAB# return DEFAULT_PATH #LINE# #TAB# else: #LINE# #TAB# #TAB# if path not in paths: #LINE# #TAB# #TAB# #TAB# paths.append(path) #LINE# #TAB# #TAB# return DEFAULT_PATH
"#LINE# #TAB# tmp_executable_only_modified = False #LINE# #TAB# for port in _ports.values(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if os.access(str(port), os.R_OK): #LINE# #TAB# #TAB# #TAB# #TAB# tmp_executable_only_modified = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except OSError as e: #LINE# #TAB# #TAB# #TAB# if e.errno!= errno.ENOENT: #LINE# #TAB# #TAB# #TAB# #TAB# raise #LINE# #TAB# return tmp_executable_only_modified"
"#LINE# #TAB# old_mtime = None #LINE# #TAB# mtime = 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# import os #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# import time #LINE# #TAB# #TAB# time.sleep(1) #LINE# #TAB# #TAB# old_mtime = time.time() #LINE# #TAB# file_mtime = os.path.getmtime(file_path) #LINE# #TAB# os.chmod(file_path, os.stat(file_path).st_mtime) #LINE# #TAB# return old_mtime, mtime"
"#LINE# #TAB# dir_path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# page_path = os.path.join(dir_path, 'project_com', name) #LINE# #TAB# if not os.path.exists(page_path): #LINE# #TAB# #TAB# return False #LINE# #TAB# code_path = os.path.join(dir_path, 'code.py') #LINE# #TAB# with open(code_path, 'w') as f: #LINE# #TAB# #TAB# f.write('<?xml version=""1.0""?>') #LINE# #TAB# #TAB# f.write('<?xml version=""1.0""?>') #LINE# #TAB# #TAB# f.write('<?xml version=""1.0""?>') #LINE# #TAB# return True"
"#LINE# #TAB# return { #LINE# #TAB# #TAB# 'filename': DEFAULT_CONF_FILENAME, #LINE# #TAB# #TAB# 'ipython_dir': ipython_dir, #LINE# #TAB# #TAB# 'defalt_confg': CONFIG, #LINE# #TAB# }"
"#LINE# #TAB# as_dict = {} #LINE# #TAB# for option in sorted(options): #LINE# #TAB# #TAB# key = option[0] #LINE# #TAB# #TAB# value = option[1] #LINE# #TAB# #TAB# if key in as_dict: #LINE# #TAB# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# as_dict[key] = as_settings(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# as_dict[key] = value #LINE# #TAB# return as_dict"
#LINE# #TAB# s = os.stat(0) #LINE# #TAB#ino = s.st_ino #LINE# #TAB# s.close() #LINE# #TAB# returnino
#LINE# #TAB# new_label = record.label #LINE# #TAB# new_label.start = 0 #LINE# #TAB# for _ in range(record.length): #LINE# #TAB# #TAB# new_label = next(new_label) #LINE# #TAB# #TAB# if type(new_label) == type(''): #LINE# #TAB# #TAB# #TAB# new_label = list(new_label)[0] #LINE# #TAB# #TAB# record.label = new_label #LINE# #TAB# return new_label
"#LINE# #TAB# mask = zeros_like(points) #LINE# #TAB# if priority is not None: #LINE# #TAB# #TAB# threshold = priority #LINE# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# mask[i] = _one_denity(point, radius, threshold) #LINE# #TAB# else: #LINE# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# mask[i] = 1 #LINE# #TAB# return mask"
#LINE# #TAB# word_require = f'{inst_img_id}' #LINE# #TAB# aminame = f'{word_require}{inst_img_id}' #LINE# #TAB# return aminame
#LINE# #TAB# keys = response.split(' ') #LINE# #TAB# if len(keys) == 1: #LINE# #TAB# #TAB# return keys[0] #LINE# #TAB# if len(keys) == 2: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return keys[1] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# return 'Unknown error code {}'.format(keys[1]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Unknown error code {}'.format(keys[1])
"#LINE# #TAB# for function in node.body: #LINE# #TAB# #TAB# if isinstance(function, ast.FunctionDef): #LINE# #TAB# #TAB# #TAB# return function"
"#LINE# #TAB# return {'blueprint_bundle_paths': list(app.blueprint_bundle_paths) if #LINE# #TAB# #TAB# isinstance(app.blueprint_bundle_paths, list)}"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
#LINE# #TAB# resource_groups = [unicode_extension_group(service) for service in #LINE# #TAB# #TAB# unicode_extension_group_iter(admin_required)] #LINE# #TAB# return resource_groups
"#LINE# #TAB# from django.forms.widgets import BSEIput #LINE# #TAB# try: #LINE# #TAB# #TAB# iput = BSEIput() #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# raise ImproperlyConfigured(""django.forms.widgets is not installed"") #LINE# #TAB# if test: #LINE# #TAB# #TAB# raise ImproperlyConfigured(""testing forms.widgets module is not installed"") #LINE# #TAB# return iput"
"#LINE# #TAB# out_files = [] #LINE# #TAB# for in_file in in_files: #LINE# #TAB# #TAB# match_groups = generate_match_groups(in_file, metadata) #LINE# #TAB# #TAB# if match_groups: #LINE# #TAB# #TAB# #TAB# out_files.extend(match_groups) #LINE# #TAB# return out_files"
"#LINE# #TAB# import sqlite3 #LINE# #TAB# from django.db import connection #LINE# #TAB# connection.connect(database_url=os.environ['DB_URL'], #LINE# #TAB# #TAB# username=os.environ['USERNAME'], password=os.environ['PASSWORD']) #LINE# #TAB# yield"
"#LINE# #TAB# resolv_annotations = raw_annotations.copy() #LINE# #TAB# if module_name is not None: #LINE# #TAB# #TAB# resolv_annotations.update({ #LINE# #TAB# #TAB# #TAB#'module': module_name, #LINE# #TAB# #TAB# }) #LINE# #TAB# return resolv_annotations"
#LINE# #TAB# if not rule: #LINE# #TAB# #TAB# return #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# if rule.from_name == name: #LINE# #TAB# #TAB# #TAB# rule.to_disable = True #LINE# #TAB# return
"#LINE# #TAB# et_rot = None #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# et_rot = et_rot_package_inner() #LINE# #TAB# #TAB# except (AttributeError, OSError): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if et_rot is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return et_rot"
"#LINE# #TAB# currency_unique = os.path.join(testcases_root, 'ark-currency-unique-settings.yaml') #LINE# #TAB# if not os.path.exists(currency_unique): #LINE# #TAB# #TAB# with open(currency_unique, 'w') as f: #LINE# #TAB# #TAB# #TAB# yaml.safe_dump(config, f, default_flow_style=False, allow_unicode=False) #LINE# #TAB# elif os.path.exists(testcases_root): #LINE# #TAB# #TAB# with open(testcases_root, 'r') as f: #LINE# #TAB# #TAB# #TAB# yaml.safe_dump(config, f, default_flow_style=False, allow_unicode=False) #LINE# #TAB# return currency_unique"
"#LINE# #TAB# file_name = 'game_logs.txt' #LINE# #TAB# z = get_zip_file(seson_wrapper_url(season), file_name) #LINE# #TAB# data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='""') #LINE# #TAB# data.columns = gamelog_columns #LINE# #TAB# return data"
"#LINE# #TAB# if isinstance(value, dict) or isinstance(value, list): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# input_names[0]: _format_link_mode_vriables(v, input_names) #LINE# #TAB# #TAB# #TAB# for v in value #LINE# #TAB# #TAB# } #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return {} #LINE# #TAB# return { #LINE# #TAB# #TAB# input_names[0]: value, #LINE# #TAB# #TAB# input_names[1:] #LINE# #TAB# }"
#LINE# #TAB# network_fro = {} #LINE# #TAB# all_keys = get_relevant_event_keys(mod) #LINE# #TAB# for key in all_keys: #LINE# #TAB# #TAB# if key in mod.logged_events: #LINE# #TAB# #TAB# #TAB# network_fro[key] = mod.logged_events[key] #LINE# #TAB# return network_fro
#LINE# #TAB# result = {} #LINE# #TAB# for boxed_key in boxed_type: #LINE# #TAB# #TAB# if boxed_key in data: #LINE# #TAB# #TAB# #TAB# boxed_value = data[boxed_key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# boxed_value = data[boxed_key] #LINE# #TAB# #TAB# result[boxed_key] = boxed_value #LINE# #TAB# return result
#LINE# #TAB# unique_repo_list = list() #LINE# #TAB# for repo in repo_list: #LINE# #TAB# #TAB# if repo.uniue_command not in unique_repo_list: #LINE# #TAB# #TAB# #TAB# unique_repo_list.append(repo) #LINE# #TAB# return unique_repo_list
#LINE# #TAB# verbose = [] #LINE# #TAB# lines = text.split('\n') #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# #TAB# print('\t'.join(verbose)) #LINE# #TAB# #TAB# #TAB# verbose.append(line) #LINE# #TAB# return verbose
#LINE# #TAB# for suject_label in vicon.suject_streams: #LINE# #TAB# #TAB# if suject_label == name: #LINE# #TAB# #TAB# #TAB# return suject_label #LINE# #TAB# return None
"#LINE# #TAB# if not isinstance(a, np.ndarray): #LINE# #TAB# #TAB# raise TypeError('Input array type must be of type {}'.format(dtype)) #LINE# #TAB# if a.dtype!= dtype: #LINE# #TAB# #TAB# raise TypeError('Input array type must be of type {}'.format(dtype)) #LINE# #TAB# return a"
"#LINE# #TAB# ret = [] #LINE# #TAB# if isinstance(mem_size_str, str): #LINE# #TAB# #TAB# size = int(mem_size_str) #LINE# #TAB# elif isinstance(mem_size_str, int) and reserve_time is not None: #LINE# #TAB# #TAB# size = int(reserve_time) #LINE# #TAB# #TAB# while size > 0: #LINE# #TAB# #TAB# #TAB# ret.append(size) #LINE# #TAB# #TAB# #TAB# size -= 1 #LINE# #TAB# elif mem_size_str == '1GB': #LINE# #TAB# #TAB# ret.append('1MB') #LINE# #TAB# #TAB# ret.append('1KB') #LINE# #TAB# else: #LINE# #TAB# #TAB# ret.append('0') #LINE# #TAB# return 'on_auto(%s)' % ret"
"#LINE# #TAB# c = pd.read_table(fn, index_col=None, engine='python') #LINE# #TAB# chanel_d = {} #LINE# #TAB# for chanel in c.itervalues(): #LINE# #TAB# #TAB# chanel_d[chanel[0]] = chanel[1] #LINE# #TAB# if usecols is not None: #LINE# #TAB# #TAB# while chanel[0].dtype in usecols: #LINE# #TAB# #TAB# #TAB# chanel_d[chanel[0]].name = chanel[0] #LINE# #TAB# #TAB# chanel_d[chanel[1]] = chanel[1] #LINE# #TAB# return chanel_d"
#LINE# #TAB# if len(cloud) < 4: #LINE# #TAB# #TAB# return cloud #LINE# #TAB# if not cloud[3].isdigit(): #LINE# #TAB# #TAB# if cloud[3] == 'O': #LINE# #TAB# #TAB# #TAB# return cloud[:3] + '0' + cloud[4:] #LINE# #TAB# #TAB# elif cloud[3] == 'O': #LINE# #TAB# #TAB# #TAB# return cloud[:3] + '0' + cloud[4:] #LINE# #TAB# if not cloud[3].isdigit(): #LINE# #TAB# #TAB# if cloud[3] == 'U': #LINE# #TAB# #TAB# #TAB# return cloud[:3] + '0' + cloud[4:] #LINE# #TAB# #TAB# return cloud #LINE# #TAB# return cloud
"#LINE# #TAB# log.info('Applying _maping_convert_step generator:'+ str( #LINE# #TAB# #TAB# mapping)) #LINE# #TAB# res = {} #LINE# #TAB# for k, v in mapping.items(): #LINE# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# v = v.split(':') #LINE# #TAB# #TAB# if len(v) == 1: #LINE# #TAB# #TAB# #TAB# res[k] = set([v]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res[k] = set([v]) #LINE# #TAB# return res"
"#LINE# #TAB# start = s.find(char, 0) #LINE# #TAB# if start == -1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return start"
"#LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# px4_state = DCM_State(ATT.Roll, ATT.Pitch, ATT.Yaw) #LINE# #TAB# px4_state.update(IMU) #LINE# #TAB# return px4_state"
#LINE# #TAB# if _use_internal.flag is True: #LINE# #TAB# #TAB# return #LINE# #TAB# pwduid = _getpwuid() #LINE# #TAB# if pwduid is None: #LINE# #TAB# #TAB# return #LINE# #TAB# if'sh' not in pwduid: #LINE# #TAB# #TAB# return #LINE# #TAB# _use_internal.flag = True #LINE# #TAB# pwduid = pwd.getpwuid() #LINE# #TAB# try: #LINE# #TAB# #TAB# _use_internal.flag = True #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# _use_internal.flag = False
#LINE# #TAB# gabeway_json = {} #LINE# #TAB# for gateway in vpc.vpcs.all(): #LINE# #TAB# #TAB# for gateway_name in gateway.nics: #LINE# #TAB# #TAB# #TAB# gabeway = {'name': gateway_name} #LINE# #TAB# #TAB# #TAB# gabeway_json[gateway_name] = gabeway #LINE# #TAB# return gabeway_json
#LINE# #TAB# if k not in o: #LINE# #TAB# #TAB# o[k] = {} #LINE# #TAB# o = o[k] #LINE# #TAB# if k == 'type': #LINE# #TAB# #TAB# if v == True: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# l = o[k].get('label') #LINE# #TAB# #TAB# if l is not None: #LINE# #TAB# #TAB# #TAB# o[k]['label'] = l #LINE# #TAB# #TAB# elif k == 'group': #LINE# #TAB# #TAB# #TAB# o[k]['group'] = {'label': l} #LINE# #TAB# #TAB# elif k == 'label-value': #LINE# #TAB# #TAB# #TAB# o[k]['label-value'] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# header = QLabel() #LINE# #TAB# h = header.find(name) #LINE# #TAB# if h is None: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# extension = h.extension() #LINE# #TAB# #TAB# if extension == ""png"": #LINE# #TAB# #TAB# #TAB# return h.loadImage(name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return h.loadImage(name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return default"
"#LINE# #TAB# if p.dimension!= 1: #LINE# #TAB# #TAB# f = item.font(0) #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# #TAB# item.setFont(0, f) #LINE# #TAB# if p.shadow: #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# return 1"
"#LINE# #TAB# parts = path.split('.') #LINE# #TAB# for part in parts[:-1]: #LINE# #TAB# #TAB# obj = extend_document(obj, part, ext) #LINE# #TAB# return obj"
#LINE# #TAB# d = [] #LINE# #TAB# for y in x: #LINE# #TAB# #TAB# if type(y) == list: #LINE# #TAB# #TAB# #TAB# d.extend(y) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d.append(y[0]) #LINE# #TAB# #TAB# #TAB# d.append(y[1]) #LINE# #TAB# return d
"#LINE# #TAB# if textfile!= 'trip.txt': #LINE# #TAB# #TAB# raise ValueError('{} is not a valid file name'.format(textfile)) #LINE# #TAB# df = pd.read_csv(os.path.join(textfile_path, textfile), dtype={ #LINE# #TAB# #TAB# 'trip_id': object}, low_memory=False) #LINE# #TAB# if len(df) == 0: #LINE# #TAB# #TAB# raise ValueError('{} has no records'.format(os.path.join( #LINE# #TAB# #TAB# #TAB# textfile_path, textfile))) #LINE# #TAB# df['trip_id'] = df['trip_id'].map(str.strip) #LINE# #TAB# df.set_index('trip_id', inplace=True) #LINE# #TAB# df = df.sort_index(inplace=True) #LINE# #TAB# return df"
"#LINE# #TAB# directory = os.path.join(os.path.abspath(os.path.dirname(__file__)), #LINE# #TAB# #TAB#'shared-templates') #LINE# #TAB# try: #LINE# #TAB# #TAB# return os.path.dirname(directory) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(directory, exist_ok=True) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None"
"#LINE# #TAB# matches = [] #LINE# #TAB# for c in cls.__subclasses__(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if issubclass(c, ResponseMicroService): #LINE# #TAB# #TAB# #TAB# #TAB# matches.append(c) #LINE# #TAB# #TAB# #TAB# elif issubclass(c, RequestMicroService): #LINE# #TAB# #TAB# #TAB# #TAB# matches.append(c) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return matches"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if node.body: #LINE# #TAB# #TAB# #TAB# return max(posorder_imports(node.body[0]), posorder_imports(node.body[-1])) #LINE# #TAB# #TAB# elif node.head: #LINE# #TAB# #TAB# #TAB# return max(posorder_imports(node.head), posorder_imports(node.tail)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return 1"
"#LINE# #TAB# plugin_context = getattr(dashboard_entry, 'plugin_context', {}) #LINE# #TAB# if plugin_context and 'pug_dta' in plugin_context: #LINE# #TAB# #TAB# return plugin_context['pug_dta'] #LINE# #TAB# context = request #LINE# #TAB# if not hasattr(plugin_context, 'pug_dta_clone'): #LINE# #TAB# #TAB# plugin_context.pug_dta_clone = {} #LINE# #TAB# plugin_context.pug_dta = plugin_context.pug_dta_clone #LINE# #TAB# return plugin_context"
"#LINE# #TAB# coder_coords = [] #LINE# #TAB# coder_params = [] #LINE# #TAB# for hdu in fits: #LINE# #TAB# #TAB# if hdu.name == extname: #LINE# #TAB# #TAB# #TAB# if 'CENTER' in hdu.header: #LINE# #TAB# #TAB# #TAB# #TAB# coder_coords.append(hdu.header['CENTER']) #LINE# #TAB# #TAB# #TAB# elif 'CENTER_PARAMS' in hdu.header: #LINE# #TAB# #TAB# #TAB# #TAB# coder_params.append(hdu.header['CENTER_PARAMS']) #LINE# #TAB# return coder_coords, coder_params"
"#LINE# #TAB# theta = 0.12 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"#LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_card_server_url(), school_id) #LINE# #TAB# response = requests.get(url) #LINE# #TAB# code = response.status_code #LINE# #TAB# data = response.data.decode('utf-8') #LINE# #TAB# if code!= 200: #LINE# #TAB# #TAB# raise Exception('Error response: %s, code: %s' % (code, #LINE# #TAB# #TAB# #TAB# response.status_code)) #LINE# #TAB# return data"
"#LINE# #TAB# blueprint = blueprint_from_string('loan_repace_tree_structure', #LINE# #TAB# #TAB#'repace_tree') #LINE# #TAB# blueprint.from_object(LoanItem) #LINE# #TAB# return blueprint"
"#LINE# #TAB# for key, value in ns.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# shorten_integer(value) #LINE# #TAB# #TAB# elif isinstance(value, Number): #LINE# #TAB# #TAB# #TAB# ns[key] = int(value) #LINE# #TAB# #TAB# elif isinstance(value, str): #LINE# #TAB# #TAB# #TAB# if value.lower() == 'true': #LINE# #TAB# #TAB# #TAB# #TAB# ns[key] = True #LINE# #TAB# #TAB# #TAB# elif value.lower() == 'false': #LINE# #TAB# #TAB# #TAB# #TAB# ns[key] = False #LINE# #TAB# return ns"
#LINE# #TAB# if obj is not None: #LINE# #TAB# #TAB# return prefix + ';' + obj + suffix #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
"#LINE# #TAB# r_item, _ = config #LINE# #TAB# exit_code = 0 #LINE# #TAB# if exception is not None: #LINE# #TAB# #TAB# raise Exception( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# if not r_item: #LINE# #TAB# #TAB# r_item = 1 #LINE# #TAB# return exit_code, r_item"
"#LINE# #TAB# diff_dict = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, #LINE# #TAB# #TAB# 'H': 7, 'I': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12} #LINE# #TAB# solved_vec = vectors_to_isi(input_dict, diff_dict) #LINE# #TAB# return solved_vec"
#LINE# #TAB# if alphabet is None: #LINE# #TAB# #TAB# alphabet = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ' #LINE# #TAB# num = '' #LINE# #TAB# if len(base_num) < 2: #LINE# #TAB# #TAB# return num #LINE# #TAB# for i in xrange(base_num.bit_length()): #LINE# #TAB# #TAB# num += alphabet[i] * base_num[i] #LINE# #TAB# base_num = str(int(base_num)) #LINE# #TAB# return num
#LINE# #TAB# for p in entity_predictions: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p.detach_errors() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return entity_predictions
"#LINE# #TAB# if not hasattr(cls, 'G'): #LINE# #TAB# #TAB# return {} #LINE# #TAB# attrs = cls.G #LINE# #TAB# values = {} #LINE# #TAB# for k in dir(cls): #LINE# #TAB# #TAB# if k.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# k = k.lower() #LINE# #TAB# #TAB# values[k] = getattr(settings, k) #LINE# #TAB# return attrs, values"
#LINE# #TAB# buff = nd4j_array.ctypes_data() #LINE# #TAB# timestamp_type = buff.dtype.name #LINE# #TAB# if timestamp_type =='string': #LINE# #TAB# #TAB# return bytes_to_cstring(buff) #LINE# #TAB# else: #LINE# #TAB# #TAB# return buff
#LINE# #TAB# if name: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return gts_net_unique_users[name] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return None
#LINE# #TAB# value = data[attribute_name] #LINE# #TAB# return value
#LINE# #TAB# global _print_actions #LINE# #TAB# if _print_actions is None: #LINE# #TAB# #TAB# _print_actions = True #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# req = auth(url) #LINE# #TAB# #TAB# #TAB# _print_actions = req.status_code == 200 #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# _print_actions = False #LINE# #TAB# #TAB# return _print_actions #LINE# #TAB# return False
"#LINE# #TAB# filepath = os.path.dirname(os.path.abspath(base_file)) #LINE# #TAB# html_file = os.path.join(filepath, csv_name + sep) #LINE# #TAB# if convert_float: #LINE# #TAB# #TAB# html_float = format_float(html_file) #LINE# #TAB# html = '<csv>' + html_float + '</csv>' #LINE# #TAB# return html"
"#LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.host, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# out = numpy.empty(vector_length) #LINE# #TAB# for i in range(0, vector_length): #LINE# #TAB# #TAB# out[i] = vector[i + shift] #LINE# #TAB# return out"
"#LINE# #TAB# out = [x.capitalize() for x in name.split('_')] #LINE# #TAB# for i in range(1, len(out)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# out[i] = out[i].upper() #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return out"
#LINE# #TAB# if cls._decode_move_classma is None: #LINE# #TAB# #TAB# cls._decode_move_classma = {} #LINE# #TAB# return cls._decode_move_classma
"#LINE# #TAB# if value is not None: #LINE# #TAB# #TAB# if value not in [""true"", ""false""]: #LINE# #TAB# #TAB# #TAB# raise ArgumentTypeError(""Expected True or False, not %s."" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# % value) #LINE# #TAB# return value"
#LINE# #TAB# if type_string == 'list': #LINE# #TAB# #TAB# list_type_string = 'item' #LINE# #TAB# elif type_string == 'bool': #LINE# #TAB# #TAB# list_type_string = 'boolean' #LINE# #TAB# elif type_string == 'datetime': #LINE# #TAB# #TAB# list_type_string = 'datetime' #LINE# #TAB# else: #LINE# #TAB# #TAB# list_type_string = type_string #LINE# #TAB# if list_type_string not in row_object_clean_image_dict: #LINE# #TAB# #TAB# row_object_clean_image_dict[list_type_string] = 'null' #LINE# #TAB# return row_object_clean_image_dict[type_string]
"#LINE# #TAB# eigenvalues = [] #LINE# #TAB# values = [] #LINE# #TAB# contactMapContact = np.array(contactMap) #LINE# #TAB# for c in contactMap: #LINE# #TAB# #TAB# eigv = calculate_eigenvectors_for_contact(c) #LINE# #TAB# #TAB# eigenvalues.append(eigv) #LINE# #TAB# #TAB# values.append(eigv) #LINE# #TAB# return eigenvalues, values"
"#LINE# #TAB#packed_nodes = [] #LINE# #TAB# if points is not None: #LINE# #TAB# #TAB# for i, p in enumerate(points): #LINE# #TAB# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# packed_nodes.append(byte_o_pack_ponts(nodes[i], p, dtype)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# packed_nodes.append(byte_o_pack_ponts(nodes[i], p, dtype, deep)) #LINE# #TAB# else: #LINE# #TAB# #TAB# for i, node in enumerate(nodes): #LINE# #TAB# #TAB# #TAB# packed_nodes.append(byte_o_pack_ponts(node, dtype, deep)) #LINE# #TAB# return packed_nodes"
"#LINE# #TAB# global IP_FOUND #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.socket(socket.AF_INET, socket.SOCK_DGRAM).close() #LINE# #TAB# #TAB# IP_FOUND = True #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# pass #LINE# #TAB# if not IP_FOUND: #LINE# #TAB# #TAB# print('ip_address not found') #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.remove(ip_address) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('ip address not found') #LINE# #TAB# return"
"#LINE# #TAB# if ext == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# for parser, keyword in cls.suggestParsers: #LINE# #TAB# #TAB# if parser == text: #LINE# #TAB# #TAB# #TAB# return keyword #LINE# #TAB# return None"
"#LINE# #TAB# source_list = cls() #LINE# #TAB# for source_file in glob.glob(os.path.join(path, '*.sdetect')): #LINE# #TAB# #TAB# if source_file.is_file(): #LINE# #TAB# #TAB# #TAB# source_list.append(source_file) #LINE# #TAB# return source_list"
#LINE# #TAB# images = [] #LINE# #TAB# preload_images = soup.find_all('preload_images') #LINE# #TAB# for img in preload_images: #LINE# #TAB# #TAB# if img.get('href') and img.get('src'): #LINE# #TAB# #TAB# #TAB# images.append(img.get('href')) #LINE# #TAB# ips_rls = [] #LINE# #TAB# for img in images: #LINE# #TAB# #TAB# ips_rls.append(img.get('src')) #LINE# #TAB# return ips_rls
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return all([isinstance(point, list) for point in pointlist]) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# t = time.gps() #LINE# #TAB# s = t / 3600.0 #LINE# #TAB# return s
#LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# os.rmdir(path) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# name = name.lower() #LINE# #TAB# name = ''.join(c for c in name if c in string.ascii_letters) #LINE# #TAB# if name.startswith('A'): #LINE# #TAB# #TAB# name = 'A' + name #LINE# #TAB# if name.endswith('-'): #LINE# #TAB# #TAB# name = name[:-1] #LINE# #TAB# return name
"#LINE# #TAB# cost = None #LINE# #TAB# if slice_obj.start is None and slice_obj.stop is None: #LINE# #TAB# #TAB# cost = None #LINE# #TAB# elif slice_obj.start and slice_obj.stop: #LINE# #TAB# #TAB# cost = max(cost, slice_obj.start.stop - 1) #LINE# #TAB# if slice_obj.stop and slice_obj.stop < slice_obj.start.stop: #LINE# #TAB# #TAB# cost = max(cost, slice_obj.stop.start - 1) #LINE# #TAB# if cost is None: #LINE# #TAB# #TAB# cost = None #LINE# #TAB# return cost"
"#LINE# #TAB# global last_update_time #LINE# #TAB# date = str() #LINE# #TAB# last_update_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #LINE# #TAB# return date"
"#LINE# #TAB# with _cache_lock: #LINE# #TAB# #TAB# if package_name in _CLASS_CACHE: #LINE# #TAB# #TAB# #TAB# return _CLASS_CACHE[package_name] #LINE# #TAB# #TAB# old_version = _CLASS_CACHE.get(package_name, None) #LINE# #TAB# #TAB# if old_version is None: #LINE# #TAB# #TAB# #TAB# return old_version #LINE# #TAB# #TAB# new_version = _CLASS_CACHE[package_name] #LINE# #TAB# #TAB# _CLASS_CACHE[package_name] = new_version #LINE# #TAB# #TAB# return new_version"
"#LINE# #TAB# aic = np.zeros(yk.size) #LINE# #TAB# aic[-1] = 0.0 #LINE# #TAB# for K in range(yk.size - 2, -1, -1): #LINE# #TAB# #TAB# aic[K] = aic[K + 1] + 2 * aic[K + 1] / (NF - K - 2) #LINE# #TAB# return aic"
"#LINE# #TAB# k = np.argmin(normalisation_parameters) #LINE# #TAB# M = np.shape(normalisation_parameters) #LINE# #TAB# new_y = np.dot(y, M) #LINE# #TAB# if k == 0: #LINE# #TAB# #TAB# new_y = np.zeros_like(y) #LINE# #TAB# elif k == 1: #LINE# #TAB# #TAB# new_y = np.dot(normalisation_parameters[0], y) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_y = y / k #LINE# #TAB# return new_y"
"#LINE# #TAB# from ctypes import NamedTemporaryFile #LINE# #TAB# dir_array = NamedTemporaryFile(dir=os.getcwd()) #LINE# #TAB# dir_array.append(var_instance) #LINE# #TAB# dir_array.seek(0) #LINE# #TAB# while True: #LINE# #TAB# #TAB# bl = NamedTemporaryFile(dir=dir_array) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cstring_copy(var_instance, bl, bl) #LINE# #TAB# #TAB# except EnvironmentError as e: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# dir_array.seek(0) #LINE# #TAB# #TAB# dir_array.append(b.read()) #LINE# #TAB# #TAB# if bl.has_next(): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# dir_array.close() #LINE# #TAB# #TAB# break #LINE# #TAB# return dir_array"
"#LINE# #TAB# with open(os.devnull, 'w') as devnull: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# subprocess.check_call(['cmake', cmake_with_sdist], stdout= #LINE# #TAB# #TAB# #TAB# #TAB# devnull, stderr=devnull) #LINE# #TAB# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# n = float(n) #LINE# #TAB# Ht_est = sum(Ht_est) / float(n) #LINE# #TAB# Hs_est = sum(Hs_est) / float(n) #LINE# #TAB# if (n - 1.0) * Ht_est == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# Hs_est_ = sum(Hs_est) / float(n) #LINE# #TAB# d_est = (Ht_est - Hs_est_) / float(n) #LINE# #TAB# return d_est
"#LINE# #TAB# b0 = 0.903 * 2 / 3 #LINE# #TAB# b1 = 8.181 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.0909 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['SM'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# label_u = unique_labels(labels) #LINE# #TAB# label_u_line = [(i + '_line') for i in label_u] #LINE# #TAB# return label_u, label_u_line"
#LINE# #TAB# glob_result = glob.glob(ses_path.joinpath('*.ibl')) #LINE# #TAB# if not glob_result: #LINE# #TAB# #TAB# raise ValueError('No files found.') #LINE# #TAB# if len(glob_result) > 1: #LINE# #TAB# #TAB# raise ValueError('Multiple files found.') #LINE# #TAB# return glob_result
"#LINE# #TAB# psi = -0.0102 #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"#LINE# #TAB# config = get_default(based_on=based_on, filename=filename) #LINE# #TAB# lines = config_string.splitlines() #LINE# #TAB# ret = '' #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if '=' not in line: #LINE# #TAB# #TAB# #TAB# return ret #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# if '\n' in ret: #LINE# #TAB# #TAB# ret += '\n' #LINE# #TAB# return ret"
#LINE# #TAB# is_last_page = False #LINE# #TAB# try: #LINE# #TAB# #TAB# grab.fetch_one() #LINE# #TAB# #TAB# is_last_page = True #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return is_last_page
#LINE# #TAB# diss = [] #LINE# #TAB# for x in qs: #LINE# #TAB# #TAB# mea = mean_mea(x) #LINE# #TAB# #TAB# diss.append(mea) #LINE# #TAB# return diss
"#LINE# #TAB# fp = open(os.path.join(os.path.dirname(__file__), 'data/sparse_coods.csv'), 'r') #LINE# #TAB# sparse_coods = pd.read_csv(fp, index_col=0) #LINE# #TAB# fp.close() #LINE# #TAB# coods = sparse_coods[(0), :, :] #LINE# #TAB# fp.close() #LINE# #TAB# return coods"
"#LINE# #TAB# if'metadata_modified' not in data_dict or data_dict['metadata_modified' #LINE# #TAB# #TAB# ] == None: #LINE# #TAB# #TAB# return {'action': 'class_station','sorted': True,'metadata_modified': #LINE# #TAB# #TAB# #TAB# datetime.utcnow()} #LINE# #TAB# elif 'item_limit' not in data_dict: #LINE# #TAB# #TAB# return {'action': 'class_station','sorted': True,'metadata_modified': #LINE# #TAB# #TAB# #TAB# datetime.utcnow()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return data_dict"
"#LINE# #TAB# kernel = np.ones(shape, dtype=np.float64) #LINE# #TAB# for i in range(shape[0] - 1): #LINE# #TAB# #TAB# for j in range(shape[1] - i): #LINE# #TAB# #TAB# #TAB# kernel[i, j] = 1 #LINE# #TAB# return kernel"
#LINE# #TAB# labels = y.copy() #LINE# #TAB# edges = dict() #LINE# #TAB# for old_label in list(labels): #LINE# #TAB# #TAB# if old_label in edges: #LINE# #TAB# #TAB# #TAB# edges[old_label] = edges[old_label] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# edges[old_label] = list() #LINE# #TAB# #TAB# #TAB# edges[old_label].append(old_label) #LINE# #TAB# return edges
"#LINE# #TAB# result = depth_first_search(graph, root) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# chunks = [] #LINE# #TAB# for chunk in result: #LINE# #TAB# #TAB# chunks.append(graph.tuplet) #LINE# #TAB# return chunks"
#LINE# #TAB# if uri not in filter_list: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# framework_var = framework.Var(attr_name) #LINE# #TAB# return framework_var
"#LINE# #TAB# command =''.join(command) #LINE# #TAB# found_file = False #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# output = subprocess.check_output(command, shell=True) #LINE# #TAB# for line in output.splitlines(): #LINE# #TAB# #TAB# found_file = True #LINE# #TAB# #TAB# if found_file: #LINE# #TAB# #TAB# #TAB# output = subprocess.check_output(command, shell=True) #LINE# #TAB# #TAB# #TAB# output = subprocess.check_output(command, shell=True) #LINE# #TAB# return output"
"#LINE# #TAB# payload = {'datetime': data.get('datetime', None), 'payloa': data.get( #LINE# #TAB# #TAB# 'payloa', None)} #LINE# #TAB# return payload"
"#LINE# #TAB# metadata = {} #LINE# #TAB# if path.is_file(): #LINE# #TAB# #TAB# with path.open('r', encoding='utf-8') as f: #LINE# #TAB# #TAB# #TAB# metadata = yaml.safe_load(f) #LINE# #TAB# return metadata"
"#LINE# #TAB# for written_file in written_files: #LINE# #TAB# #TAB# filepath = written_file.split('/')[-1] #LINE# #TAB# #TAB# filepath = os.path.join(filepath, '__init__.py') #LINE# #TAB# #TAB# if os.path.exists(filepath): #LINE# #TAB# #TAB# #TAB# os.unlink(filepath) #LINE# #TAB# return written_files"
#LINE# #TAB# spark_form_model = dict() #LINE# #TAB# if memory: #LINE# #TAB# #TAB# spark_form_model['memory'] = memory #LINE# #TAB# return spark_form_model
#LINE# #TAB# s = arr.shape[index] #LINE# #TAB# li = [] #LINE# #TAB# for j in range(s): #LINE# #TAB# #TAB# temp = g[j] #LINE# #TAB# #TAB# ind = arr[j] #LINE# #TAB# #TAB# li.append(index - temp[j]) #LINE# #TAB# #TAB# s = arr[ind] #LINE# #TAB# return s
"#LINE# #TAB# new_A = A.copy() #LINE# #TAB# for i in range(A.shape[0]): #LINE# #TAB# #TAB# if A[i, i] < 0: #LINE# #TAB# #TAB# #TAB# new_A[i, i] = 0 #LINE# #TAB# return new_A"
#LINE# #TAB# for item_uuid in items: #LINE# #TAB# #TAB# item_uuid.index = mcs.calculate_location_uuid(item_uuid) #LINE# #TAB# return item_uuid
"#LINE# #TAB# return {'id': obj.id, 'naam': obj.naam, 'gewest': {'id': obj.gewest.id, 'naam': obj. #LINE# #TAB# #TAB# gewest.naam}}"
#LINE# #TAB# user_subject = user if user is not None else get_user_subject() #LINE# #TAB# if user_subject is None: #LINE# #TAB# #TAB# return ISecurityPolicy() #LINE# #TAB# return user_subject.subject
"#LINE# #TAB# halfwordArray = [0] * 256 #LINE# #TAB# for index, value in enumerate(byteData): #LINE# #TAB# #TAB# halfwordArray[index] = value #LINE# #TAB# return halfwordArray"
"#LINE# #TAB# n_sig = params[0] #LINE# #TAB# n_bkg = params[1] #LINE# #TAB# alpha = n_bkg / n_sig #LINE# #TAB# s = signal_2d[alpha] #LINE# #TAB# b = background_2d[alpha] #LINE# #TAB# sumlogl = np.sum(np.log(n_bkg * s + n_sig * b)) #LINE# #TAB# sumlogl -= np.sum(np.log(n_bkg * b)) #LINE# #TAB# sumlogl -= np.sum(np.log(np.arange(1, n_sig + 1))) #LINE# #TAB# return -sumlogl"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return open(flow_name) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if model.description!= '': #LINE# #TAB# #TAB# model.description += '\n' #LINE# #TAB# sbo = '' #LINE# #TAB# if model.objective_type == 'C': #LINE# #TAB# #TAB# sbo +='sbo-c' #LINE# #TAB# if model.objective_type == 'D': #LINE# #TAB# #TAB# sbo +='sbo-d' #LINE# #TAB# if model.objective_type == 'E': #LINE# #TAB# #TAB# sbo +='sbo-e' #LINE# #TAB# return sbo
"#LINE# #TAB# bbox = np.array([extent[0], extent[1], extent[2]], dtype=np.float64) #LINE# #TAB# _validate_extent(bbox) #LINE# #TAB# return bbox"
"#LINE# #TAB# module_scope = str(module_scope).replace('.', '/') #LINE# #TAB# model_scope = str(model_name).replace('.', '/') #LINE# #TAB# main_module_scope = module_scope + '.' + model_scope #LINE# #TAB# if os.path.exists(main_module_scope): #LINE# #TAB# #TAB# with open(main_module_scope) as f: #LINE# #TAB# #TAB# #TAB# env_element_main = f.read() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# env_element_main = module_scope + '.' + model_scope #LINE# #TAB# else: #LINE# #TAB# #TAB# env_element_main = None #LINE# #TAB# #TAB# with open(main_module_scope) as f: #LINE# #TAB# #TAB# #TAB# env_element_main = f.read() #LINE# #TAB# return env_element_main"
#LINE# #TAB# nparray = int(nparray) #LINE# #TAB# lib.SDL_core(nparray) #LINE# #TAB# return
"#LINE# #TAB# t_digits = str(epoch_time) #LINE# #TAB# digits = t_digits.rsplit('.', 1) #LINE# #TAB# if len(digits) == 1: #LINE# #TAB# #TAB# t_digits = t_digits[0] + 'D' #LINE# #TAB# elif len(digits) == 2: #LINE# #TAB# #TAB# t_digits = t_digits[0] + 'W' #LINE# #TAB# elif len(digits) == 3: #LINE# #TAB# #TAB# t_digits = t_digits[0] + 'Y' #LINE# #TAB# return digits"
"#LINE# #TAB# oo_path = os.path.join(custom_directory, 'dataset.environment_oot') #LINE# #TAB# if not os.path.exists(oo_path): #LINE# #TAB# #TAB# oo_path = os.path.join(os.path.dirname(custom_directory), #LINE# #TAB# #TAB# #TAB# 'data', 'environment_oot') #LINE# #TAB# if not os.path.exists(oo_path): #LINE# #TAB# #TAB# oo_path = os.path.join(os.path.dirname(custom_directory), #LINE# #TAB# #TAB# #TAB# 'environment_oot') #LINE# #TAB# return oo_path"
#LINE# #TAB# if target.is_unspecified(): #LINE# #TAB# #TAB# return None #LINE# #TAB# architecture = None #LINE# #TAB# host_name = target.GetName() #LINE# #TAB# if host_name is None: #LINE# #TAB# #TAB# architecture = target.GetArchitecture() #LINE# #TAB# #TAB# if architecture: #LINE# #TAB# #TAB# #TAB# host_name = architecture #LINE# #TAB# return host_name
#LINE# #TAB# if ordchr == '.': #LINE# #TAB# #TAB# return '.' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ordchr
#LINE# #TAB# try: #LINE# #TAB# #TAB# snapshot = root.get_child_by_name(name) #LINE# #TAB# #TAB# if snapshot: #LINE# #TAB# #TAB# #TAB# return [snapshot] #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
#LINE# #TAB# new_vec = np.zeros(3) #LINE# #TAB# new_vec[0] = vec1[0] * vec1[1] + vec1[2] * vec1[2] #LINE# #TAB# new_vec[1] = vec1[0] * vec1[1] - vec1[2] * vec1[2] #LINE# #TAB# return new_vec
#LINE# #TAB# input_class = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# input_class += str(number) #LINE# #TAB# #TAB# number += 1 #LINE# #TAB# return input_class
#LINE# #TAB# version = inflection.singularize(name) #LINE# #TAB# version = inflection.camelize(version) #LINE# #TAB# version = inflection.camelize(version) #LINE# #TAB# return version
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t') #LINE# #TAB# if isdatetime: #LINE# #TAB# #TAB# df = df.dt.to_datetime() #LINE# #TAB# return df"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return control.ha_static_hndler #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# result = function_name + '(' #LINE# #TAB# for arg in argument_list: #LINE# #TAB# #TAB# result +='(%s)' % arg #LINE# #TAB# return result
"#LINE# #TAB# is_windows = False #LINE# #TAB# if is_windows: #LINE# #TAB# #TAB# command = shlex.split(command) #LINE# #TAB# try: #LINE# #TAB# #TAB# shell = subprocess.Popen(command, stdout=subprocess.PIPE) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# is_windows = True #LINE# #TAB# #TAB# if not is_windows: #LINE# #TAB# #TAB# #TAB# shell = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# shell = True #LINE# #TAB# if is_windows: #LINE# #TAB# #TAB# terminal_output = shell.communicate()[0].decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# terminal_output = shell.communicate()[0].decode('utf-8') #LINE# #TAB# return terminal_output"
#LINE# #TAB# f_names = [] #LINE# #TAB# for from_column in from_table: #LINE# #TAB# #TAB# cols = from_table.c #LINE# #TAB# #TAB# if to_column in cols: #LINE# #TAB# #TAB# #TAB# if from_column.is_relation: #LINE# #TAB# #TAB# #TAB# #TAB# f_names.append(from_column.name) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# f_names.append(col.name) #LINE# #TAB# return f_names
#LINE# #TAB# if a == b: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# elif a > b: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return -1
"#LINE# #TAB# RQ = getattr(settings, 'RQ', {}) #LINE# #TAB# if job_class is None: #LINE# #TAB# #TAB# return RQ #LINE# #TAB# elif isinstance(job_class, six.string_types): #LINE# #TAB# #TAB# return import_module(job_class) #LINE# #TAB# else: #LINE# #TAB# #TAB# return job_class"
#LINE# #TAB# for req_country in request.country.all(): #LINE# #TAB# #TAB# if req_country in RESERVED_COUNTRIES: #LINE# #TAB# #TAB# #TAB# return req_country #LINE# #TAB# return None
"#LINE# #TAB# path = os.path.join(os.environ['HOME'], 'gridCAl') #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# return path"
#LINE# #TAB# content = open(path).read() #LINE# #TAB# try: #LINE# #TAB# #TAB# content = yaml.safe_load(content) #LINE# #TAB# except yaml.YAMLError: #LINE# #TAB# #TAB# content = json.safe_load(content) #LINE# #TAB# return content
"#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# response = request.json_body #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid request body') #LINE# #TAB# #TAB# if 400 <= response.status_code < 500: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid response code: %d - %d' % (response.status_code, #LINE# #TAB# #TAB# #TAB# #TAB# response.reason)) #LINE# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# data = pickle.load(f) #LINE# #TAB# #TAB# n = data.shape[0] #LINE# #TAB# #TAB# polygon_ata = [] #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# line = data[i] #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# polygon_ata.append(line) #LINE# #TAB# #TAB# data = np.array(polygon_ata) #LINE# #TAB# return data"
#LINE# #TAB# service_tuple = row[2] #LINE# #TAB# if service_tuple in _INVALID_SERVICE_TUPLE: #LINE# #TAB# #TAB# return _INVALID_SERVICE_TUPLE[service_tuple] #LINE# #TAB# else: #LINE# #TAB# #TAB# return service_tuple
#LINE# #TAB# import os #LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# current_path = os.path.abspath(os.curdir) #LINE# #TAB# else: #LINE# #TAB# #TAB# current_path = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# return current_path
#LINE# #TAB# with open(path) as cih_file: #LINE# #TAB# #TAB# info_dict = cih_file.read() #LINE# #TAB# return info_dict
"#LINE# #TAB# data = dolphin.search(label=label) #LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return {} #LINE# #TAB# else: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'id': data.id, #LINE# #TAB# #TAB# #TAB# 'bibtex': data.bibtex, #LINE# #TAB# #TAB# #TAB#'score': data.score, #LINE# #TAB# #TAB# #TAB# 'author': data.author, #LINE# #TAB# #TAB# #TAB# 'name': data.title, #LINE# #TAB# #TAB# #TAB# 'author_email': data.author_email, #LINE# #TAB# #TAB# #TAB# 'labels': data.labels #LINE# #TAB# #TAB# }"
#LINE# #TAB# try: #LINE# #TAB# #TAB# del _backends[library_name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# if isinstance(type_, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return type_(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return str(value) #LINE# #TAB# if isinstance(value, int): #LINE# #TAB# #TAB# return 'int' #LINE# #TAB# if isinstance(value, float): #LINE# #TAB# #TAB# return 'float' #LINE# #TAB# if isinstance(value, bool): #LINE# #TAB# #TAB# return 'bool' #LINE# #TAB# return value"
"#LINE# #TAB# text = text.strip('\n') #LINE# #TAB# sentences = text.split('\n') #LINE# #TAB# output = [] #LINE# #TAB# for sentence in sentences: #LINE# #TAB# #TAB# sentence = sentence.strip() #LINE# #TAB# #TAB# if sentence: #LINE# #TAB# #TAB# #TAB# output.append('\n'.join(process_html(sentence, end))) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output.append('\n') #LINE# #TAB# try: #LINE# #TAB# #TAB# return yaml.safe_dump(output, default_flow_style=False) #LINE# #TAB# except yaml.YAMLError: #LINE# #TAB# #TAB# return output"
"#LINE# #TAB# now = datetime.now().isoformat() #LINE# #TAB# for augment in augs: #LINE# #TAB# #TAB# if str(augment['AMI_ID']) in now: #LINE# #TAB# #TAB# #TAB# logger.debug('Deleting AMI %s', augment['AMI_ID']) #LINE# #TAB# #TAB# #TAB# shutil.rmtree(augment['AMI_ID']) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.debug('Deleting AMI %s', augment['AMI_ID']) #LINE# #TAB# #TAB# for augment in augs: #LINE# #TAB# #TAB# #TAB# if str(augment['AMI_ID']) in now: #LINE# #TAB# #TAB# #TAB# #TAB# logger.debug('Deleting AMI %s', augment['AMI_ID']) #LINE# #TAB# #TAB# #TAB# #TAB# del augs[augment['AMI_ID']]"
#LINE# #TAB# token_list = [x[0] for x in api.search_english(term)] #LINE# #TAB# if len(token_list) == 0: #LINE# #TAB# #TAB# raise ValueError('Couldn\'t find a word in English database.') #LINE# #TAB# elif len(token_list) > 1: #LINE# #TAB# #TAB# return''.join(token_list[1:]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return token_list[0]
#LINE# #TAB# tag_selling = True #LINE# #TAB# while tag_selling: #LINE# #TAB# #TAB# msg = mlog.recv_match(source) #LINE# #TAB# #TAB# if msg: #LINE# #TAB# #TAB# #TAB# tag_selling = False #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return tag_selling
"#LINE# #TAB# connection = redis.Connection(host=REDIS_HOST, port=REDIS_PORT) #LINE# #TAB# term = Term(connection) #LINE# #TAB# return term"
#LINE# #TAB# if cls.__ky_helper__ is None: #LINE# #TAB# #TAB# cls.__ky_helper__ = 'ky' #LINE# #TAB# return cls.__ky_helper__
"#LINE# #TAB# mn = float(np.sum((minval - minval) ** 2, axis=0)) #LINE# #TAB# mx = float(np.sum((maxval - maxval) ** 2, axis=0)) #LINE# #TAB# mx = float(np.sum((maxval - minval) ** 2, axis=0)) #LINE# #TAB# return mn, mx"
#LINE# #TAB# if window == 'hanning': #LINE# #TAB# #TAB# return impulse[::ntaps] #LINE# #TAB# else: #LINE# #TAB# #TAB# return impulse
#LINE# #TAB# if 'non' in options: #LINE# #TAB# #TAB# return options['non'] #LINE# #TAB# return 0
"#LINE# #TAB# count = term.count('\n') #LINE# #TAB# adj_offset = top_margin + offset #LINE# #TAB# if count > 2: #LINE# #TAB# #TAB# term.sub_year('', adj_offset) #LINE# #TAB# #TAB# term.sub_year('', adj_offset) #LINE# #TAB# #TAB# return adj_offset #LINE# #TAB# return adj_offset"
"#LINE# #TAB# if isinstance(obs_dict, dict): #LINE# #TAB# #TAB# return np.array(obs_dict) #LINE# #TAB# elif isinstance(obs_dict, list): #LINE# #TAB# #TAB# return np.tile(obs_dict, len(obs_dict)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return obs_dict"
"#LINE# #TAB# events = {} #LINE# #TAB# for event_name in dir(func): #LINE# #TAB# #TAB# method = getattr(func, event_name) #LINE# #TAB# #TAB# if not inspect.isfunction(method): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# event_globals = _event_annotations.get(method, ()) #LINE# #TAB# #TAB# if event_globals is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# events[event_name] = method, event_globals #LINE# #TAB# return events"
"#LINE# #TAB# if target is not None: #LINE# #TAB# #TAB# conf.setdefault('target', {}) #LINE# #TAB# conf['target']['project'] = project #LINE# #TAB# conf['target']['repository'] = repository"
#LINE# #TAB# ndices = [] #LINE# #TAB# local_states = basis.n_states #LINE# #TAB# for relation in correlations: #LINE# #TAB# #TAB# ndelt = 0 #LINE# #TAB# #TAB# for state in relation[0]: #LINE# #TAB# #TAB# #TAB# if state in local_states: #LINE# #TAB# #TAB# #TAB# #TAB# ndelt += 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ndelt += state #LINE# #TAB# #TAB# ndices.append(ndelt) #LINE# #TAB# return ndices
#LINE# #TAB# symbol = msgs[-1][-1] #LINE# #TAB# if len(symbol) > max_len: #LINE# #TAB# #TAB# while len(symbol) > max_len: #LINE# #TAB# #TAB# #TAB# symbol = symbol[:-1] #LINE# #TAB# return [symbol] + msgs[:-1]
"#LINE# #TAB# data = [] #LINE# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# data.append(line) #LINE# #TAB# return data"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return str(string.encode(fmt)) #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if isinstance(fmt, str): #LINE# #TAB# #TAB# return fmt(string) #LINE# #TAB# return string"
"#LINE# #TAB# bytes = bytearray() #LINE# #TAB# for key, value in ctx.items(): #LINE# #TAB# #TAB# if type(value) is int: #LINE# #TAB# #TAB# #TAB# bytes += rules_feld_bytes(value) #LINE# #TAB# #TAB# elif type(value) is str: #LINE# #TAB# #TAB# #TAB# bytes += rule_bytes(value) #LINE# #TAB# #TAB# elif type(value) is bytes: #LINE# #TAB# #TAB# #TAB# bytes += value #LINE# #TAB# return bytes"
"#LINE# #TAB# jobMaster = JobMaster(rootdir) #LINE# #TAB# authFile = get_authfile() #LINE# #TAB# if not os.path.exists(authFile): #LINE# #TAB# #TAB# return jobMaster #LINE# #TAB# try: #LINE# #TAB# #TAB# f = open(authFile, 'r') #LINE# #TAB# #TAB# token = f.readline().strip() #LINE# #TAB# #TAB# user = User(token) #LINE# #TAB# #TAB# jobMaster.addHandler(user) #LINE# #TAB# #TAB# f.close() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return jobMaster #LINE# #TAB# else: #LINE# #TAB# #TAB# return jobMaster"
"#LINE# #TAB# if is_typing_type(type_): #LINE# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(type_, cpptypes.volatile_t): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(type_, cpptypes.pointer_t): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# rep = grp.report() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# rep = grp.name #LINE# #TAB# return rep
"#LINE# #TAB# url = '{}/api/v1/schools/{}/class_device/{}/'.format(GTA_BASE, school_id, sn) #LINE# #TAB# resp = do_get_request(url) #LINE# #TAB# if resp.status_code == 200: #LINE# #TAB# #TAB# data = resp.json() #LINE# #TAB# #TAB# if data.get('school_id') == school_id: #LINE# #TAB# #TAB# #TAB# return data.get('class_device_info', {}) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'school_id': school_id, 'class_device_info': data}"
"#LINE# #TAB# meta = model_meta(instance.__class__) #LINE# #TAB# matches = [] #LINE# #TAB# for field in meta.get_fields(): #LINE# #TAB# #TAB# if not field.auto_created: #LINE# #TAB# #TAB# #TAB# if field.name == 'id': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# matches.append(field.name) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# if hasattr(instance, '_meta'): #LINE# #TAB# #TAB# #TAB# if instance._meta.get_data(): #LINE# #TAB# #TAB# #TAB# #TAB# matches = extract_field_match_model(instance._meta) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# matches.append(field.name) #LINE# #TAB# return matches"
"#LINE# #TAB# local_info = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# f = open(os.path.join(os.path.dirname(__file__), '../version.py')) #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# m = re.match(""__version__ = '([^']+)'"", line) #LINE# #TAB# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# #TAB# local_info[m.group(1)] = m.group(2) #LINE# #TAB# #TAB# return local_info #LINE# #TAB# except: #LINE# #TAB# #TAB# return local_info"
#LINE# #TAB# for row_model in results['values']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# headers = list(row_model['headers']) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# headers.append(row_model['name']) #LINE# #TAB# #TAB# for header in headers: #LINE# #TAB# #TAB# #TAB# yield header #LINE# #TAB# #TAB# yield row_model['values'][0]
"#LINE# #TAB# index = concept_cd.find(join_char) #LINE# #TAB# if index == -1: #LINE# #TAB# #TAB# category_cd = concept_cd #LINE# #TAB# #TAB# data_label = None #LINE# #TAB# else: #LINE# #TAB# #TAB# category_cd = concept_cd[:index] #LINE# #TAB# #TAB# data_label = concept_cd[index + 1:].strip() #LINE# #TAB# return category_cd, data_label"
"#LINE# #TAB# signature = b64encode(signature) #LINE# #TAB# xml = et.fromstring(xml) #LINE# #TAB# try: #LINE# #TAB# #TAB# rsa_fix(xml, signature, key, c14n_exc=c14n_exc) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if shape == 'gaus': #LINE# #TAB# #TAB# scale = 2.0 #LINE# #TAB# #TAB# weights = np.exp(sigma * scale) / 2.0 #LINE# #TAB# elif shape == 'fc': #LINE# #TAB# #TAB# weights = np.exp(sigma * 0.5) / 2.0 #LINE# #TAB# elif shape == 'fc': #LINE# #TAB# #TAB# weights = np.exp(sigma) / 2.0 #LINE# #TAB# return weights
"#LINE# #TAB# if len(name) > 20: #LINE# #TAB# #TAB# return '%s..%s' % (name[:20], '...') #LINE# #TAB# else: #LINE# #TAB# #TAB# return name"
"#LINE# #TAB# if hr_data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# with gzip.GzipFile(filename, 'wb') as f: #LINE# #TAB# #TAB# for entry in hr_data: #LINE# #TAB# #TAB# #TAB# f.write(entry[0] + '\n') #LINE# #TAB# #TAB# #TAB# f.write('\n') #LINE# #TAB# return filename"
"#LINE# #TAB# hash_obj = hashlib.sha512() #LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = f.read(size) #LINE# #TAB# #TAB# #TAB# hash_obj.update(data) #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return hash_obj.hexdigest()[:size]"
"#LINE# #TAB# if not config_dir: #LINE# #TAB# #TAB# config_dir = DEFAULT_CONFIG_DIR #LINE# #TAB# config_path = os.path.join(config_dir, DEFAULT_CONFIG_FILE) #LINE# #TAB# data = load_config(config_path) #LINE# #TAB# return data"
"#LINE# #TAB# results = [] #LINE# #TAB# for param in params: #LINE# #TAB# #TAB# key, value = param.split('=', 1) #LINE# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# value = normalize(value) #LINE# #TAB# #TAB# results.append((key, value)) #LINE# #TAB# return results"
#LINE# #TAB# out = [] #LINE# #TAB# for el in node.getElementsByTagName('D'): #LINE# #TAB# #TAB# if el.nodeType == 'ELEMENT': #LINE# #TAB# #TAB# #TAB# for child in el.childNodes: #LINE# #TAB# #TAB# #TAB# #TAB# if child.nodeType == 'ELEMENT': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# out.append(child) #LINE# #TAB# return out
"#LINE# #TAB# for directory in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# prefix = os.path.join(directory, 'gobal') #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(prefix, 'tasks')): #LINE# #TAB# #TAB# #TAB# yield prefix"
#LINE# #TAB# url = order_url + '/user/' + account + '/order/' #LINE# #TAB# data = requests.get(url) #LINE# #TAB# if data.status_code == 200: #LINE# #TAB# #TAB# if data.headers['Content-Type'] == 'application/json': #LINE# #TAB# #TAB# #TAB# return json.loads(data.text) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return data.text
"#LINE# #TAB# from ckanext.app import settings #LINE# #TAB# for entity_id, entity in settings.ENTITY_CHOICES.items(): #LINE# #TAB# #TAB# notification = Entity.query.filter_by(id=entity_id).first() #LINE# #TAB# #TAB# if not notification: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# settings.ENTITY_CHOICES[entity_id] = notification"
"#LINE# #TAB# #TAB# if isinstance(id, int): #LINE# #TAB# #TAB# #TAB# return cls.get(int(id)) #LINE# #TAB# #TAB# return -1"
"#LINE# #TAB# import socket #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.socket(socket.AF_INET, socket.SOCK_RAW) #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# external_name = '' #LINE# #TAB# for i in resource_uri.split('/'): #LINE# #TAB# #TAB# if i > 0 and i < len(resource_uri) - 1: #LINE# #TAB# #TAB# #TAB# external_name = resource_uri[i:i + 1] #LINE# #TAB# if external_name == '': #LINE# #TAB# #TAB# external_name ='resource' #LINE# #TAB# return external_name
#LINE# #TAB# if PyFunceble.CONFIGURATION.db_type =='mariadb': #LINE# #TAB# #TAB# from django.db import connection #LINE# #TAB# #TAB# PyFunceble.CONFIGURATION.db_type ='mariadb' #LINE# #TAB# elif PyFunceble.CONFIGURATION.db_type =='mysql': #LINE# #TAB# #TAB# from django.db import connection #LINE# #TAB# #TAB# PyFunceble.CONFIGURATION.db_type ='mysql'
"#LINE# #TAB# covariance_matrix = zeros((4, 4)) #LINE# #TAB# for point_i in range(4): #LINE# #TAB# #TAB# for point_j in range(4): #LINE# #TAB# #TAB# #TAB# covariance_matrix[point_i, point_j] = encaped_covariace(points[ #LINE# #TAB# #TAB# #TAB# #TAB# point_i], points[point_j]) #LINE# #TAB# return covariance_matrix"
"#LINE# #TAB# if dirpath is None: #LINE# #TAB# #TAB# return #LINE# #TAB# os.makedirs(dirpath, exist_ok=True) #LINE# #TAB# return dirpath"
"#LINE# #TAB# before = np.mean(A[permutation]) #LINE# #TAB# after = np.mean(A[permutation]) #LINE# #TAB# return {'before': before, 'after': after}"
"#LINE# #TAB# return {'year': date.strftime('%Y'),'month': date.strftime('%m'), #LINE# #TAB# #TAB# 'day': date.strftime('%d'), 'hour': date.strftime('%H'),'minute': #LINE# #TAB# #TAB# date.strftime('%M'),'second': date.strftime('%S')}"
"#LINE# #TAB# assert os.path.exists(fastq) #LINE# #TAB# cmd = ['git', 'qualiy_singl', '--infile', fastq, pkl] #LINE# #TAB# logger.info('Running command: %s', cmd) #LINE# #TAB# proc = subprocess.Popen(cmd, stdout=subprocess.PIPE) #LINE# #TAB# proc.wait() #LINE# #TAB# data = proc.communicate()[0] #LINE# #TAB# os.remove(pkl) #LINE# #TAB# return data"
#LINE# #TAB# if filename: #LINE# #TAB# #TAB# ext = os.path.splitext(filename)[1].lower() #LINE# #TAB# #TAB# if ext == 'global': #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 'global' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ext
"#LINE# #TAB# #TAB# if not val: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# ttype = val.split('/')[-1].lower() #LINE# #TAB# #TAB# if ttype == 'date': #LINE# #TAB# #TAB# #TAB# result['date'] = datetime.datetime.strptime(val, '%Y-%m-%dT%H:%M:%S.%f') #LINE# #TAB# #TAB# elif ttype == 'time': #LINE# #TAB# #TAB# #TAB# result['time'] = datetime.datetime.strptime(val, '%Y-%m-%dT%H:%M:%S') #LINE# #TAB# #TAB# elif ttype == 'period': #LINE# #TAB# #TAB# #TAB# result['period'] = int(val) #LINE# #TAB# #TAB# return result"
"#LINE# #TAB# return {'status': 'running','settings': {'task': {'name': 'task'}, #LINE# #TAB# #TAB# 'task_percent': '{0} seconds'.format(time.time()), 'running_at': datetime. #LINE# #TAB# #TAB# datetime.now().strftime('%Y-%m-%d %H:%M'), 'time': datetime.datetime.now(). #LINE# #TAB# #TAB# isoformat()}, 'details': {'task_percent': '{0} seconds'.format( #LINE# #TAB# #TAB# task_percent)}}"
"#LINE# #TAB# import json #LINE# #TAB# ret = {} #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(f, 'r') as handle: #LINE# #TAB# #TAB# #TAB# #TAB# j = json.load(handle) #LINE# #TAB# #TAB# #TAB# for k, v in j.items(): #LINE# #TAB# #TAB# #TAB# #TAB# template_name = '{0}.{1}'.format(k, v) #LINE# #TAB# #TAB# #TAB# #TAB# ret[template_name] = v #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return ret"
#LINE# #TAB# if not description: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not description.isupper(): #LINE# #TAB# #TAB# return description #LINE# #TAB# if not description.endswith('.'): #LINE# #TAB# #TAB# return description #LINE# #TAB# return description[:len(description) - 1]
#LINE# #TAB# for ws in nb.worksheets: #LINE# #TAB# #TAB# if ws.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# for cell in ws.cells: #LINE# #TAB# #TAB# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# cell.outputs = []
#LINE# #TAB# try: #LINE# #TAB# #TAB# return s.decode('utf8') #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# return s
#LINE# #TAB# orig = card_en() #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# card_en() #LINE# #TAB# #TAB# del orig
"#LINE# #TAB# #TAB# if sys.version_info >= (3, 5): #LINE# #TAB# #TAB# #TAB# return child_class #LINE# #TAB# #TAB# if not issubclass(child_class, this_abc): #LINE# #TAB# #TAB# #TAB# raise KappaError('Cannot inherit from class {} of type {}'.format(this_abc, #LINE# #TAB# #TAB# #TAB# #TAB# child_class)) #LINE# #TAB# #TAB# return child_class"
#LINE# #TAB# loc = [] #LINE# #TAB# for dir_str in dir_list: #LINE# #TAB# #TAB# if dir_str in DIR_STRS: #LINE# #TAB# #TAB# #TAB# if preprocess: #LINE# #TAB# #TAB# #TAB# #TAB# loc.append(dir_str) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# loc.append(dir_str) #LINE# #TAB# return loc
"#LINE# #TAB# global gblal_log_properties #LINE# #TAB# gblal_log_properties = glbal.glbal_get_log_properties() #LINE# #TAB# if not gblal_log_properties.has_key('general'): #LINE# #TAB# #TAB# gblal_log_properties['general'] = {} #LINE# #TAB# gblal_log_properties.setdefault('log_level', 'INFO') #LINE# #TAB# gblal_log_properties.setdefault('log_format', 'INFO') #LINE# #TAB# gblal_log_properties.setdefault('log_level', 'WARNING') #LINE# #TAB# gblal_log_properties.setdefault('display_name', '') #LINE# #TAB# gblal_log_properties.setdefault('loglevel', 'WARNING') #LINE# #TAB# return"
#LINE# #TAB# trim_strings = {} #LINE# #TAB# for model_object in references_json: #LINE# #TAB# #TAB# trim_strings[model_object.id] = model_object.to_json() #LINE# #TAB# return trim_strings
#LINE# #TAB# context = cls._context #LINE# #TAB# if not context: #LINE# #TAB# #TAB# raise ContextIsNotInitializedError #LINE# #TAB# return context
"#LINE# #TAB# num_pages = int(num) #LINE# #TAB# if num_pages > num: #LINE# #TAB# #TAB# idx = random.randint(0, num_pages - 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# idx = random.randint(0, num_pages - 1) #LINE# #TAB# return [idx]"
#LINE# #TAB# global _used_for_pysol_tieouts #LINE# #TAB# if not _used_for_pysol_tieouts: #LINE# #TAB# #TAB# _used_for_pysol_tieouts = [] #LINE# #TAB# #TAB# time.sleep(0.2) #LINE# #TAB# if stop: #LINE# #TAB# #TAB# _used_for_pysol_tieouts = []
"#LINE# #TAB# opts = 't' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# return [cmd, opts, archive]"
#LINE# #TAB# se_format_weights = np.array([[c[0][0] for c in connections] for c in #LINE# #TAB# #TAB# connections]) #LINE# #TAB# se_format_weights.sort() #LINE# #TAB# return se_format_weights
"#LINE# #TAB# actie_cols = np.empty(shape, dtype=int) #LINE# #TAB# for i in range(len(shape)): #LINE# #TAB# #TAB# x = np.arange(shape[i]) #LINE# #TAB# #TAB# y = np.arange(shape[i + 1]) #LINE# #TAB# #TAB# if node_status: #LINE# #TAB# #TAB# #TAB# actie_cols[i] = node_status #LINE# #TAB# if return_count: #LINE# #TAB# #TAB# return actie_cols, x, y #LINE# #TAB# else: #LINE# #TAB# #TAB# return actie_cols"
#LINE# #TAB# parent = tdict #LINE# #TAB# for name in path: #LINE# #TAB# #TAB# node = parent #LINE# #TAB# #TAB# if name not in tdict: #LINE# #TAB# #TAB# #TAB# tdict[name] = Node(name) #LINE# #TAB# #TAB# parent = parent.get_child(name) #LINE# #TAB# return tdict
"#LINE# #TAB# if not have_innodb_large_prefix(engine): #LINE# #TAB# #TAB# return False #LINE# #TAB# variables = dict(engine.execute( #LINE# #TAB# #TAB#'show variables where variable_name like ""innodb_large_prefix"" or variable_name like ""innodb_file_format"";' #LINE# #TAB# #TAB# ).fetchall()) #LINE# #TAB# return variables.get('innodb_file_format', 'Barracuda' #LINE# #TAB# #TAB# )!= 'Barracuda'"
#LINE# #TAB# global _omponent #LINE# #TAB# _omponent = None #LINE# #TAB# return _omponent
"#LINE# #TAB# x = np.asarray(x) #LINE# #TAB# if x.shape[0] == x.shape[1]: #LINE# #TAB# #TAB# x = np.sum(x, axis=0) #LINE# #TAB# return x"
#LINE# #TAB# r = dt.isoformat() #LINE# #TAB# if dt.microsecond: #LINE# #TAB# #TAB# r = r[:23] + r[26:] #LINE# #TAB# if r.endswith('+00:00'): #LINE# #TAB# #TAB# r = r[:-6] + 'Z' #LINE# #TAB# return r
"#LINE# #TAB# w0 = w / pagesize #LINE# #TAB# h0 = h / pagesize #LINE# #TAB# w = max(w0, w) #LINE# #TAB# while True: #LINE# #TAB# #TAB# dx = x - w0 #LINE# #TAB# #TAB# dy = y + h0 - w0 #LINE# #TAB# #TAB# if dx < 0: #LINE# #TAB# #TAB# #TAB# dx = 0 #LINE# #TAB# #TAB# if dy < 0: #LINE# #TAB# #TAB# #TAB# dy = 0 #LINE# #TAB# #TAB# W = w #LINE# #TAB# #TAB# X = x + dx / 2 #LINE# #TAB# #TAB# Y = y + dy / 2 #LINE# #TAB# #TAB# return X, Y"
"#LINE# #TAB# stream = io.StringIO(data) #LINE# #TAB# for step in parse_control_file(stream): #LINE# #TAB# #TAB# if 'name' in step.keys(): #LINE# #TAB# #TAB# #TAB# step['name'] = step['name'].replace('.', '_') #LINE# #TAB# #TAB# net_steps = [] #LINE# #TAB# #TAB# for line in stream.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# #TAB# steps = parse_steps(line, args) #LINE# #TAB# #TAB# #TAB# #TAB# net_steps.append(steps) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# log.debug('Skipping control file: %s' % line) #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB#"
"#LINE# #TAB# filename = os.path.abspath(filename) #LINE# #TAB# with open(os.devnull, 'w') as devnull: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# subprocess.check_call(['git', 'ls-files', '--ignore', filename], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=devnull, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=devnull) #LINE# #TAB# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# regex = '^[A-Z][a-z]+$' #LINE# #TAB# if not re.match(regex, code): #LINE# #TAB# #TAB# raise voluptuous.Invalid('Invalid author code.') #LINE# #TAB# return code"
"#LINE# #TAB# out_txt = op.join(out_txt, 'rate_ratio.txt') #LINE# #TAB# cmd ='samtools view -h -f 2 > {0} > {1}'.format(bam, out_txt) #LINE# #TAB# run(cmd) #LINE# #TAB# with open(out_txt, 'w') as out_file: #LINE# #TAB# #TAB# out_file.write('\n'.join(cmd)) #LINE# #TAB# return out_txt"
"#LINE# #TAB# dir_name = os.path.dirname(os.path.abspath(builder.__file__)) #LINE# #TAB# out_file_path = os.path.join(dir_name, '{}_due.filenam'.format(provider)) #LINE# #TAB# with open(out_file_path, 'w') as out_file: #LINE# #TAB# #TAB# with open(out_file_path, 'wb') as out_file: #LINE# #TAB# #TAB# #TAB# out_file.write(provider.name + '-due.filenam') #LINE# #TAB# return out_file_path"
"#LINE# #TAB# if isinstance(p[1], ast.Identifier): #LINE# #TAB# #TAB# arg = p[1] #LINE# #TAB# #TAB# return seen_union(arg) #LINE# #TAB# elif isinstance(p[1], ast.Tuple): #LINE# #TAB# #TAB# arg = p[1] #LINE# #TAB# #TAB# return elapsed_pow_delta(arg) #LINE# #TAB# else: #LINE# #TAB# #TAB# return p[1]"
#LINE# #TAB# cols = ((cols - 6) *.85) + 1 #LINE# #TAB# if shorten is False or len(url) < cols: #LINE# #TAB# #TAB# return url #LINE# #TAB# split = int(cols *.5) #LINE# #TAB# key = '' #LINE# #TAB# for _ in range(split): #LINE# #TAB# #TAB# key += url[-split[0]] #LINE# #TAB# #TAB# if len(key) > cols - 1: #LINE# #TAB# #TAB# #TAB# key = key[:cols - 1] #LINE# #TAB# return key
"#LINE# #TAB# response = request_handler.make_request('POST', '/reports/generate_b_nam', {'name': #LINE# #TAB# #TAB# name}) #LINE# #TAB# return response['report']"
"#LINE# #TAB# scheam = cls() #LINE# #TAB# scheam.build(tag, schema) #LINE# #TAB# return scheam"
"#LINE# #TAB# dimensions = graph.number_of_nodes() #LINE# #TAB# fig = plt.figure() #LINE# #TAB# ax = fig.add_subplot(111) #LINE# #TAB# for i in range(dimensions[0]): #LINE# #TAB# #TAB# j = 0 #LINE# #TAB# #TAB# for k in range(dimensions[1]): #LINE# #TAB# #TAB# #TAB# ax.add_subplot(i, j, k) #LINE# #TAB# #TAB# #TAB# ax.set_xlim(0, 2 * i + 1) #LINE# #TAB# #TAB# #TAB# ax.set_ylim(0, 2 * j + 1) #LINE# #TAB# #TAB# for k in range(dimensions[2]): #LINE# #TAB# #TAB# #TAB# ax.set_xlim(0, 2 * i + 1) #LINE# #TAB# #TAB# #TAB# ax.set_ylim(0, 2 * i + 1) #LINE# #TAB# return fig, ax"
#LINE# #TAB# assert sys.platform == 'win32' #LINE# #TAB# if sys.platform == 'darwin': #LINE# #TAB# #TAB# return True #LINE# #TAB# windir = os.environ.get('WINDIR') #LINE# #TAB# if windir: #LINE# #TAB# #TAB# if windir == 'hw': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# return {'type': 'LimitChain', 'catalog': {'name': 'LimitChain', #LINE# #TAB# #TAB#'version': '1.0.0'}, 'args': {'limit': None, 'type':'string'}, #LINE# #TAB# #TAB# 'kwargs': {'limit': None, 'catalog': {'name': 'LimitChainUser','version': #LINE# #TAB# #TAB# '1.0.0'}, 'catalog': {'name': 'LimitChainSystem', #LINE# #TAB# #TAB#'version': '1.0.0'}, 'args': {'limit': None, 'type':'string'}, #LINE# #TAB# #TAB# 'kwargs': {'limit': None, 'type':'string'}}}"
"#LINE# #TAB# root = os.path.dirname(config_file) #LINE# #TAB# config_dir = '.'.join(root) #LINE# #TAB# new_config_file = os.path.join(config_dir, 'netify.cfg') #LINE# #TAB# if os.path.isfile(new_config_file): #LINE# #TAB# #TAB# with open(new_config_file, 'w') as f: #LINE# #TAB# #TAB# #TAB# conf.write(f) #LINE# #TAB# elif os.path.isfile(config_dir): #LINE# #TAB# #TAB# new_config_file = os.path.join(config_dir, 'netify.conf') #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# load_config(new_config_file) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return config_dir"
#LINE# #TAB# status_code = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# for line in response.split('\n'): #LINE# #TAB# #TAB# #TAB# if line.startswith('Status'): #LINE# #TAB# #TAB# #TAB# #TAB# status_code = line.split(':')[1].strip() #LINE# #TAB# #TAB# #TAB# if status_code == 'PENDING': #LINE# #TAB# #TAB# #TAB# #TAB# status_code = 'pending' #LINE# #TAB# #TAB# #TAB# elif status_code == 'RUNNING': #LINE# #TAB# #TAB# #TAB# #TAB# status_code = 'running' #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return status_code
"#LINE# #TAB# edges = [] #LINE# #TAB# none_depth = [None] * len(ranges) #LINE# #TAB# for start, end in ranges: #LINE# #TAB# #TAB# edges.append((start, end)) #LINE# #TAB# #TAB# if none_depth[0] < start: #LINE# #TAB# #TAB# #TAB# edges[-1] += (none_depth[-1] - start) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# edges.append((start, end)) #LINE# #TAB# if none_depth: #LINE# #TAB# #TAB# return edges #LINE# #TAB# else: #LINE# #TAB# #TAB# return edges"
#LINE# #TAB# if label.startswith(LUT_PREFIX): #LINE# #TAB# #TAB# return label[len(LUT_PREFIX):] #LINE# #TAB# else: #LINE# #TAB# #TAB# return label
"#LINE# #TAB# if string is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# if isinstance(string, str): #LINE# #TAB# #TAB# #TAB# string = string.encode('utf8') #LINE# #TAB# #TAB# elif not isinstance(string, bytes): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# string = base64.b64encode(string) #LINE# #TAB# #TAB# #TAB# except (TypeError, binascii.Error): #LINE# #TAB# #TAB# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return string"
#LINE# #TAB# if abs(control[1]) > abs(test[1]): #LINE# #TAB# #TAB# return 0.5 #LINE# #TAB# if abs(control[0]) > abs(test[0]): #LINE# #TAB# #TAB# return 0.5 #LINE# #TAB# return 1.0
#LINE# #TAB# prm = {'cache': {}} #LINE# #TAB# return prm
"#LINE# #TAB# dvs_info = get_dvs_info(session, dvs_name) #LINE# #TAB# if dvs_info is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# portgroup_info = dvs_info[pg_name]['portgroup'] #LINE# #TAB# port_id_generic = portgroup_info['id'] #LINE# #TAB# if port_id_generic is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return port_id_generic"
"#LINE# #TAB# if option is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# if not isinstance(option, tuple): #LINE# #TAB# #TAB# raise TypeError('Expected a tuple but got %s instead' % type(option)) #LINE# #TAB# return option"
#LINE# #TAB# if not module_name: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# module = importlib.import_module(module_name) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# module = importlib.import_module(module_name) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not module: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# imp.find_module(module_name) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return module
"#LINE# #TAB# data_dir = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# file_path = os.path.join(data_dir, 'data', database_name + '.db') #LINE# #TAB# if not os.path.isfile(file_path): #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# primary_keys = list(f.read().keys()) #LINE# #TAB# #TAB# for key in primary_keys: #LINE# #TAB# #TAB# #TAB# if not os.path.exists(os.path.join(file_path, key)): #LINE# #TAB# #TAB# #TAB# #TAB# return None #LINE# #TAB# with open(file_path, 'w') as f: #LINE# #TAB# #TAB# f.write('\n') #LINE# #TAB# return database_name"
"#LINE# #TAB# if not song_name or not song_title: #LINE# #TAB# #TAB# return False #LINE# #TAB# song = Song(song_name, song_title) #LINE# #TAB# if not artist: #LINE# #TAB# #TAB# return False #LINE# #TAB# if song.artist!= artist: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# for dialect in dialects: #LINE# #TAB# #TAB# if not needs_defaults(dialect): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return dialect
#LINE# #TAB# if initialized == True: #LINE# #TAB# #TAB# target = get_initilized() #LINE# #TAB# elif initialized == False: #LINE# #TAB# #TAB# target = get_none() #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# global callback_positions #LINE# #TAB# callback_positions = run
"#LINE# #TAB# if not os.path.exists(filedir_pdf): #LINE# #TAB# #TAB# os.makedirs(filedir_pdf) #LINE# #TAB# filename, file_suffix = os.path.splitext(pdf_name) #LINE# #TAB# filename = os.path.join(filedir_pdf, filename) #LINE# #TAB# with open(filename, 'wb') as pdf_file: #LINE# #TAB# #TAB# pdf_file.write(pdf_bytes) #LINE# #TAB# return"
#LINE# #TAB# reduced_graph = SimpleGraph() #LINE# #TAB# for node in graph: #LINE# #TAB# #TAB# if node in reduced_graph.nodes(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# reduced_graph.add_node(node) #LINE# #TAB# for node in reduced_graph.nodes(): #LINE# #TAB# #TAB# if node in reduced_graph.test_keys(): #LINE# #TAB# #TAB# #TAB# del reduced_graph[node] #LINE# #TAB# for node in reduced_graph.nodes(): #LINE# #TAB# #TAB# del reduced_graph[node] #LINE# #TAB# return reduced_graph
"#LINE# #TAB# infile = os.listdir(dirPath) #LINE# #TAB# data = [] #LINE# #TAB# for f in infile: #LINE# #TAB# #TAB# if f.endswith('.csv'): #LINE# #TAB# #TAB# #TAB# data.append(load_csv_file(f, numLabels, modify=modify)) #LINE# #TAB# if modify: #LINE# #TAB# #TAB# return data"
"#LINE# #TAB# return { #LINE# #TAB# #TAB#'sgned': 1, #LINE# #TAB# #TAB# 'expiration': expiration, #LINE# #TAB# #TAB#'stringToSign': base64.b64encode(string_to_sign).decode('utf-8') #LINE# #TAB# }"
#LINE# #TAB# event = api.generate_selnux_event(domain) #LINE# #TAB# logger.info('selnux event: %s' % event) #LINE# #TAB# return event
"#LINE# #TAB# file_stats = None #LINE# #TAB# try: #LINE# #TAB# #TAB# st = os.stat(file_path) #LINE# #TAB# #TAB# if st: #LINE# #TAB# #TAB# #TAB# file_stats = st #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# logger.warning('failed to fsync: %s', err) #LINE# #TAB# return file_stats"
"#LINE# #TAB# if isinstance(obj, argparse.Namespace): #LINE# #TAB# #TAB# nsdict = {} #LINE# #TAB# #TAB# for k, v in obj.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# nsdict[k] = script_convert_string(v) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# nsdict[k] = v #LINE# #TAB# elif isinstance(obj, optparse.Values): #LINE# #TAB# #TAB# return {k: script_convert_string(v) for k, v in obj.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# details = False #LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# from socket import socket #LINE# #TAB# #TAB# #TAB# details = True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# elif os.name == 'posix': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# from file import open #LINE# #TAB# #TAB# #TAB# details = True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if details: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# global _lggers #LINE# #TAB# return _lggers
"#LINE# #TAB# try: #LINE# #TAB# #TAB# versions = sorted([os.path.join(directory, fname) for fname in os.listdir( #LINE# #TAB# #TAB# #TAB# directory) if fname.endswith('.egg')]) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# if len(versions) > 1: #LINE# #TAB# #TAB# return versions[0] #LINE# #TAB# return []"
"#LINE# #TAB# assert rois.shape[1] == df.shape[2] #LINE# #TAB# grouped_rois = np.zeros((df.shape[0], rois.shape[1])) #LINE# #TAB# for i in range(df.shape[0]): #LINE# #TAB# #TAB# tmp_rois = df.loc[rois[:, (i)].values == rois[:, (i)].values, axis=1) #LINE# #TAB# #TAB# grouped_rois[(i), :] = np.kron(tmp_rois, rois[:, (i)].values) #LINE# #TAB# return grouped_rois"
"#LINE# #TAB# fjac = infodic['fjac'] #LINE# #TAB# ipvt = infodic['ipvt'] #LINE# #TAB# n = len(p) #LINE# #TAB# scale_ratio = np.zeros((n, n)) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(i + 1, n): #LINE# #TAB# #TAB# #TAB# fjac = fjac[i, j] #LINE# #TAB# #TAB# #TAB# scale_ratio[i, j] = np.linalg.norm(fjac[(i), :] - fjac[(j), :]) #LINE# #TAB# return scale_ratio"
"#LINE# #TAB# if n <= 0: #LINE# #TAB# #TAB# raise ValueError('n must be greater than 0') #LINE# #TAB# if isinstance(iterable, Iterable) is False: #LINE# #TAB# #TAB# iterable = list(iterable) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# if i >= len(iterable): #LINE# #TAB# #TAB# #TAB# yield iterable[i] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield iterable[i]"
#LINE# #TAB# projects = list_installed(check_platform=check_platform) #LINE# #TAB# if name in projects: #LINE# #TAB# #TAB# return projects[name] #LINE# #TAB# return []
"#LINE# #TAB# global frob_coeffs #LINE# #TAB# i %= 2 #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return t_x #LINE# #TAB# return t_x[0], t_x[1] * frob_coeffs[2, i, 1] % Q"
"#LINE# #TAB# n = len(x) #LINE# #TAB# if fills[0] % 2 == 1: #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# x[i] += fills[i][0] #LINE# #TAB# #TAB# #TAB# y[i] += fills[i][1] #LINE# #TAB# else: #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# x[i] += fills[i][0] #LINE# #TAB# #TAB# #TAB# y[i] += fills[i][1] #LINE# #TAB# return x, y"
"#LINE# #TAB# q = np.arange(2) #LINE# #TAB# r = b * q + np.arange(3) #LINE# #TAB# while b!= 0: #LINE# #TAB# #TAB# q = q * r + np.arange(3) #LINE# #TAB# #TAB# r = r * r + np.arange(4) #LINE# #TAB# return q, r"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = sqlite3.connect(db_file) #LINE# #TAB# #TAB# return conn #LINE# #TAB# except Error as e: #LINE# #TAB# #TAB# print(bcolors.FAIL, e, bcolors.ENDC) #LINE# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# del headers_cache[cachefname] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False, None #LINE# #TAB# return True, None"
#LINE# #TAB# base_interval = [1] * cur_dev_size #LINE# #TAB# diff = cur_dev_size - thres_dev_size #LINE# #TAB# if diff < 0: #LINE# #TAB# #TAB# base_interval[diff] = thres_dev_size - diff #LINE# #TAB# return base_interval
"#LINE# #TAB# digits = {'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': #LINE# #TAB# #TAB# 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N'} #LINE# #TAB# return digits"
"#LINE# #TAB# kwargs = {'A': 0.0, 'C': 0.0, 'G': 0.0, 'D': 0.0} #LINE# #TAB# if variance is not None: #LINE# #TAB# #TAB# kwargs['A'] += variance * np.log(S) #LINE# #TAB# return kwargs"
#LINE# #TAB# if node is not None: #LINE# #TAB# #TAB# pubdate = node.find('pubdate') #LINE# #TAB# #TAB# if pubdate is None: #LINE# #TAB# #TAB# #TAB# log.debug('No pubdate found.') #LINE# #TAB# #TAB# #TAB# pubdate = {} #LINE# #TAB# #TAB# return pubdate #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'date': pubdate.text}
"#LINE# #TAB# attrs = set() #LINE# #TAB# for seq in SeqIO.parse(trimmed_aln, 'fasta'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# for a in seq: #LINE# #TAB# #TAB# #TAB# #TAB# if '-' not in a: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# a = '-' + a #LINE# #TAB# #TAB# #TAB# #TAB# attrs.add(a) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return attrs"
"#LINE# #TAB# client = ofxclient.Client() #LINE# #TAB# client.download_files('', accounts) #LINE# #TAB# account_list = [client.upload() for account in accounts] #LINE# #TAB# for i in range(0, len(account_list), days): #LINE# #TAB# #TAB# account = client.account(account_list[i]) #LINE# #TAB# #TAB# ofxclient.add_account(account) #LINE# #TAB# client.stop_upload(account_list) #LINE# #TAB# return account_list"
"#LINE# #TAB# if not isinstance(mats[0], sp.spmatrix): #LINE# #TAB# #TAB# raise ValueError('Input must be an iterable of sp.spmatrix.') #LINE# #TAB# if mats[0].format!= mats[1].format: #LINE# #TAB# #TAB# raise ValueError('Input must be an iterable of sp.spmatrix.') #LINE# #TAB# output = np.zeros((mats[0].shape[0], mats[0].shape[1])) #LINE# #TAB# for mat in mats: #LINE# #TAB# #TAB# output[mat.nonzero()[0]], mat.nonzero()[1] = mat.nonzero()[1] #LINE# #TAB# return output"
"#LINE# #TAB# check_profile(profile, geni_input, type_geni) #LINE# #TAB# if type_geni == 'g': #LINE# #TAB# #TAB# return profile #LINE# #TAB# elif type_geni == 'b': #LINE# #TAB# #TAB# return geni_input['b'] #LINE# #TAB# elif type_geni == 'a': #LINE# #TAB# #TAB# return profile + '/' + geni_input['b'] #LINE# #TAB# elif type_geni == 'h': #LINE# #TAB# #TAB# return profile + '/' + geni_input['h'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return profile"
"#LINE# #TAB# if mn is None and mx is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if np.issubdtype(dtype, np.integer): #LINE# #TAB# #TAB# if mn > mx: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True #LINE# #TAB# if np.issubdtype(mn, np.integer): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# when = when or time.time() #LINE# #TAB# nonce = str(time.time()).replace(':', '').replace('.', '').replace('-', '' #LINE# #TAB# #TAB# ).strip() #LINE# #TAB# if when: #LINE# #TAB# #TAB# nonce += '-' + str(when) #LINE# #TAB# return nonce"
#LINE# #TAB# if slot.type == 'parameter': #LINE# #TAB# #TAB# return f'<{slot.name}>' #LINE# #TAB# return f'<{slot.name}>'
"#LINE# #TAB# for ipaddr_type in [socket.AF_INET6, socket.AF_INET6]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# socket.inet_pton(ipaddr_type, value) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
"#LINE# #TAB# template = open(filename).read() #LINE# #TAB# for line in template.splitlines(): #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# fields = line.split() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# groups = fields[1].split() #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return fields[0], fields[1], groups[2]"
"#LINE# #TAB# lease = cls.get_in_combiation(lease_id, status) #LINE# #TAB# event_counts = cls.get_in_combiation(lease_id, status) #LINE# #TAB# return event_counts and event_counts[0] == event_counts[1]"
"#LINE# #TAB# global log_file #LINE# #TAB# log_file = open(LOG_FILE, 'a') #LINE# #TAB# ini_log = logging.StreamHandler(log_file) #LINE# #TAB# ini_log.setLevel(logging.INFO) #LINE# #TAB# ini_log.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# '[%(levelname)1.1s %(asctime)s %(name)s] %(message)s')) #LINE# #TAB# return log_file"
"#LINE# #TAB# p = pathlib.Path(filename) #LINE# #TAB# p.parent.mkdir(parents=True, exist_ok=True) #LINE# #TAB# p.touch() #LINE# #TAB# return"
"#LINE# #TAB# import logging #LINE# #TAB# logger = logging.getLogger('HapProject') #LINE# #TAB# handler = HTMapHandler(logging.StreamHandler(sys.stdout)) #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# logging.basicConfig(format='%(message)s', level=logging.INFO) #LINE# #TAB# return logger"
#LINE# #TAB# untrimmed_len = float(untrimmed_alignment_size) #LINE# #TAB# if untrimmed_len >= no_sites_trimmed: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# ret_min = float(untrimmed_len / no_sites_trimmed) #LINE# #TAB# #TAB# ret_max = float(untrimmed_len / no_sites_trimmed) #LINE# #TAB# #TAB# return ret_min / ret_max
"#LINE# #TAB# hex_string = '#' + color #LINE# #TAB# r = int(hex_string[0:2], 16) #LINE# #TAB# g = int(hex_string[2:4], 16) #LINE# #TAB# b = int(hex_string[4:6], 16) #LINE# #TAB# return r, g, b"
"#LINE# #TAB# contign = calculate_contign(k, v) #LINE# #TAB# return k, contign"
"#LINE# #TAB# if csv_content == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# assert type(csv_content) == list #LINE# #TAB# csv_content = [csv_content] #LINE# #TAB# for row in csv_content: #LINE# #TAB# #TAB# for col_ind, value in enumerate(row): #LINE# #TAB# #TAB# #TAB# if value == 'true': #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# response = fetch_service_build_manager_reference_object(code) #LINE# #TAB# if not response: #LINE# #TAB# #TAB# raise ValueError('Error fetching service build-manager reference from code: {}' #LINE# #TAB# #TAB# #TAB#.format(code)) #LINE# #TAB# return response
"#LINE# #TAB# n = len(tab) #LINE# #TAB# tmp = [] #LINE# #TAB# for i in range(0, n): #LINE# #TAB# #TAB# val = tab[i] #LINE# #TAB# #TAB# if val not in tmp: #LINE# #TAB# #TAB# #TAB# tmp.append(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# n.append(val) #LINE# #TAB# return tmp"
#LINE# #TAB# if not np.all(np.isfinite(X)): #LINE# #TAB# #TAB# return False #LINE# #TAB# T = X.shape[0] #LINE# #TAB# n_samples = int(np.sum(X ** 2)) #LINE# #TAB# std = np.sqrt(np.sum((X - T) ** 2) / n_samples) #LINE# #TAB# gamma = gamma * np.log(np.sum(std ** 2)) #LINE# #TAB# return gamma > 0
#LINE# #TAB# cls.command_sub ='save-tiles' #LINE# #TAB# result = cls.execute(cls._construct_command(options)) #LINE# #TAB# return result
"#LINE# #TAB# iff_depth_m = m - iff_depth_adjust(m, t) #LINE# #TAB# return iff_depth_m"
"#LINE# #TAB# mask = np.logical_and(im >= min_value, im <= max_value) #LINE# #TAB# return im[mask]"
"#LINE# #TAB# z0 = _moving_z_ad_rotate_fator_frm_rtio_encoded_com(z, NA) #LINE# #TAB# z1 = _moving_z_ad_rotate_fator_frm_rtio_encoded_com(z2, NA) #LINE# #TAB# return z0, z1"
"#LINE# #TAB# event_plugins = set() #LINE# #TAB# for vocab in vocab_path.split(os.pathsep): #LINE# #TAB# #TAB# for file_name in sorted(os.listdir(vocab)): #LINE# #TAB# #TAB# #TAB# file_path = os.path.join(vocab, file_name) #LINE# #TAB# #TAB# #TAB# if file_path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# event_plugins.add(file_path[:-3]) #LINE# #TAB# return event_plugins"
"#LINE# #TAB# ccles = errors_circle(model, allowedreacs=allowedreacs, reacbounds=reacsbounds) #LINE# #TAB# for rxn in ccles: #LINE# #TAB# #TAB# if rxn.material in model.reactions: #LINE# #TAB# #TAB# #TAB# yield 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ccles[rxn.material] += 1"
#LINE# #TAB# nested_sections = [] #LINE# #TAB# for path in list_of_paths: #LINE# #TAB# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# path_section = os.path.splitext(path)[0] #LINE# #TAB# #TAB# nested_sections.append(path_section) #LINE# #TAB# return nested_sections
#LINE# #TAB# date = date.replace(microsecond=0) #LINE# #TAB# date = date.replace(microsecond=0) #LINE# #TAB# dst = date.astimezone(tzinfo) #LINE# #TAB# return dst.isoformat('%Y-%m-%d %H:%M:%S.%f')!= dst.isoformat('%Y-%m-%d %H:%M:%S' #LINE# #TAB# #TAB# )!= dst.isoformat('%Y-%m-%d %H:%M:%S'): #LINE# #TAB# #TAB# return None
"#LINE# #TAB# new_participants = [] #LINE# #TAB# for i in range(n_adult): #LINE# #TAB# #TAB# for j in range(n_child): #LINE# #TAB# #TAB# #TAB# new_participants.append(participant_xml_b_top(participants, i)) #LINE# #TAB# return new_participants"
"#LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if isfile(path): #LINE# #TAB# #TAB# #TAB# for ini_file in glob.glob(os.path.join(path, 'P.getParameters')): #LINE# #TAB# #TAB# #TAB# #TAB# parasm_source = ini_file.replace(ini_file, '') #LINE# #TAB# #TAB# #TAB# #TAB# add_parasm_source(parasm_source) #LINE# #TAB# return"
"#LINE# #TAB# x = tf.drop_shape(x, -1) #LINE# #TAB# return x"
"#LINE# #TAB# if not s: #LINE# #TAB# #TAB# return s #LINE# #TAB# s = '""' + s + '""' #LINE# #TAB# return s"
#LINE# #TAB# if type(date) is tuple: #LINE# #TAB# #TAB# return '%d-%d' % date #LINE# #TAB# return date
"#LINE# #TAB# jump = offset % 4 #LINE# #TAB# if jump: #LINE# #TAB# #TAB# offset += 4 - jump #LINE# #TAB# (default, low, high), offset = _unpack(_struct_iii, bc, offset) #LINE# #TAB# joffs = list() #LINE# #TAB# for _index in range(high - low + 1): #LINE# #TAB# #TAB# j, offset = _unpack(_struct_i, bc, offset) #LINE# #TAB# #TAB# joffs.append(j) #LINE# #TAB# return (default, low, high, joffs), offset"
"#LINE# #TAB# res = asse_equal_start_with_none_re.match(logical_line #LINE# #TAB# #TAB# ) or asse_equal_end_with_none_re.match(logical_line) #LINE# #TAB# if res: #LINE# #TAB# #TAB# yield 0, 'N318: assertEqual(A, None) or assertEqual(None, A) sentences not allowed'"
#LINE# #TAB# if text: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return text.decode('utf-8') #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return text
#LINE# #TAB# states = [] #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# if big_endian: #LINE# #TAB# #TAB# #TAB# states.append(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# states.append(0) #LINE# #TAB# return states
"#LINE# #TAB# mm_model = getattr(g,'mm_model', None) #LINE# #TAB# if not mm_model: #LINE# #TAB# #TAB# mm_model = _get_model_from_request() #LINE# #TAB# #TAB# if not mm_model: #LINE# #TAB# #TAB# #TAB# mm_model = _get_model_from_request() #LINE# #TAB# return mm_model"
#LINE# #TAB# Optional[Exception]=None) ->Any: #LINE# #TAB# val = request.GET.get(name) #LINE# #TAB# if val is None: #LINE# #TAB# #TAB# val = error_if_missing #LINE# #TAB# if not val: #LINE# #TAB# #TAB# raise web.HTTPParameterNotFound(name) #LINE# #TAB# return val
#LINE# #TAB# name = None #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.kind == TokenKind.NAME and token.value == 'net': #LINE# #TAB# #TAB# #TAB# name = token.value #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return name
#LINE# #TAB# args_dict = {} #LINE# #TAB# if arg: #LINE# #TAB# #TAB# parts = arg.split('.') #LINE# #TAB# #TAB# for part in parts: #LINE# #TAB# #TAB# #TAB# if len(part) == 1: #LINE# #TAB# #TAB# #TAB# #TAB# key = part[0] #LINE# #TAB# #TAB# #TAB# elif len(part) == 2: #LINE# #TAB# #TAB# #TAB# #TAB# val = part[1] #LINE# #TAB# #TAB# #TAB# #TAB# key = val.strip() #LINE# #TAB# #TAB# #TAB# #TAB# val = val.strip() #LINE# #TAB# #TAB# #TAB# #TAB# args_dict[key] = val #LINE# #TAB# return args_dict
#LINE# #TAB# count = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# email = get_emais_message() #LINE# #TAB# #TAB# if email: #LINE# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# #TAB# #TAB# click.echo('Deferred email found.') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return count
#LINE# #TAB# data_path = os.getenv('PATH') #LINE# #TAB# if data_path is not None: #LINE# #TAB# #TAB# return data_path #LINE# #TAB# if platform.system() == 'Windows': #LINE# #TAB# #TAB# return _erach_windows() #LINE# #TAB# elif platform.system() == 'Linux': #LINE# #TAB# #TAB# return _erach_linux() #LINE# #TAB# elif platform.system() == 'Darwin': #LINE# #TAB# #TAB# return _erach_Darwin() #LINE# #TAB# return _erach_other()
"#LINE# #TAB# if nodelist is None: #LINE# #TAB# #TAB# nodelist = [G] #LINE# #TAB# M = nx.to_numpy_array(G.nodes(data=True), nodelist=nodelist) #LINE# #TAB# flowMatrix = nx.to_numpy_matrix(M) #LINE# #TAB# return flowMatrix"
#LINE# #TAB# output = image + mask * np.ones(num_peaks) #LINE# #TAB# peak_index = np.argmax(output) #LINE# #TAB# output[peak_index] = 0 #LINE# #TAB# return output
"#LINE# #TAB# dialog = cls.dialog_factory() #LINE# #TAB# templates = dialog.find_templates() #LINE# #TAB# if len(templates) > 1: #LINE# #TAB# #TAB# dialog.set_title('Multiple templates found') #LINE# #TAB# #TAB# return templates[0], dialog #LINE# #TAB# else: #LINE# #TAB# #TAB# return None, None"
"#LINE# #TAB# if w.size == h.size: #LINE# #TAB# #TAB# return w #LINE# #TAB# from scipy.interpolate import interp1d #LINE# #TAB# w = interp1d(w, h, target=target) #LINE# #TAB# h = interp1d(h, w, target=target) #LINE# #TAB# return w[0]"
"#LINE# #TAB# if len(value) == 0: #LINE# #TAB# #TAB# return value #LINE# #TAB# out = b'' #LINE# #TAB# if value[0] == '""' and value[-1] == '""': #LINE# #TAB# #TAB# out += value[1:-1].encode('ascii') #LINE# #TAB# elif value[0] == ""'"" and value[-1] == ""'': #LINE# #TAB# #TAB# out += value[1:-1].encode('ascii') #LINE# #TAB# elif value[0] == ""'"" and value[-1] == ""'': #LINE# #TAB# #TAB# out += value[1:-1].encode('ascii') #LINE# #TAB# return out"
"#LINE# #TAB# for k, v in doc.items(): #LINE# #TAB# #TAB# if type(v) is str: #LINE# #TAB# #TAB# #TAB# doc[k] = v.strip() #LINE# #TAB# return doc"
"#LINE# #TAB# url = u'' #LINE# #TAB# blacklist_hostnames = blacklist_hostnames or lib.SDL_getBlacklistHostnames(url) #LINE# #TAB# rc = lib.SDL_tacing_hostnames(url, blacklist_hostnames) #LINE# #TAB# return rc"
#LINE# #TAB# rc = lib.sdl_sempost() #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return rc
"#LINE# #TAB# if values.get('data'): #LINE# #TAB# #TAB# not_cont = JobBinary(context, values) #LINE# #TAB# #TAB# not_cont.load_data_field() #LINE# #TAB# return not_cont"
#LINE# #TAB# for _ in range(20): #LINE# #TAB# #TAB# yield q
"#LINE# #TAB# new_edition = {} #LINE# #TAB# for key, value in editions.items(): #LINE# #TAB# #TAB# if type(value) is dict: #LINE# #TAB# #TAB# #TAB# new_edition[key] = remove_pretty_json(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_edition[key] = value #LINE# #TAB# return new_edition"
"#LINE# #TAB# message = protobuf.Message() #LINE# #TAB# message.add_field(struct.pack('!H', GOSSIP_NOTIFY_CODE)) #LINE# #TAB# message.add_field(struct.pack('!H', GOSSIP_CHANGE_DATA)) #LINE# #TAB# message.add_field(struct.pack('!H', GOSSIP_CHANGE_CODE)) #LINE# #TAB# message.content = struct.pack('!H', GOSSIP_NOTIFY_CONTENT) #LINE# #TAB# code = struct.unpack('!H', message.content)[0] #LINE# #TAB# data = message.content[1:] #LINE# #TAB# return {'code': code, 'data': data}"
"#LINE# #TAB# closer = _desc_close(l, r, sep, expr, allow_missing_close) #LINE# #TAB# return ExprId(l, r, sep, expr), closer"
"#LINE# #TAB# if not os.path.exists(WORK_DIRECTORY): #LINE# #TAB# #TAB# os.mkdir(WORK_DIRECTORY) #LINE# #TAB# filepath = os.path.join(WORK_DIRECTORY, filename) #LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# response = requests.get(URL, stream=True) #LINE# #TAB# #TAB# response.raise_for_status() #LINE# #TAB# #TAB# with open(filepath, 'wb') as f: #LINE# #TAB# #TAB# #TAB# f.write(response.content) #LINE# #TAB# return filepath"
"#LINE# #TAB# catch_errors.check_for_period_error(data, period) #LINE# #TAB# wma = wma_linregress(data, period) #LINE# #TAB# return wma"
#LINE# #TAB# params = [] #LINE# #TAB# for i in range(len(paramMat)): #LINE# #TAB# #TAB# params.append(paramMat[i]) #LINE# #TAB# return params
"#LINE# #TAB# if isinstance(entry, sparse.csr_matrix): #LINE# #TAB# #TAB# for i in range(entry.shape[1]): #LINE# #TAB# #TAB# #TAB# if entry.data[i] < 0: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(entry, np.ndarray): #LINE# #TAB# #TAB# return delete_wildcard(entry) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# ustr = tools.bytes2str(ustr) #LINE# #TAB# if not re.match('^[\\w-]+$', ustr): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# as_unit(ustr) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if len(phi.atoms) == 0: #LINE# #TAB# #TAB# return phi #LINE# #TAB# elif len(phi.atoms) > 1: #LINE# #TAB# #TAB# return lassify_cpu_imports_match_action(phi.atoms[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return phi
"#LINE# #TAB# for offset in range(0, limit): #LINE# #TAB# #TAB# r = q.offset(offset).limit(limit).all() #LINE# #TAB# #TAB# for row in r: #LINE# #TAB# #TAB# #TAB# yield row"
"#LINE# #TAB# nx_obj = {} #LINE# #TAB# for line in dict_str.splitlines(): #LINE# #TAB# #TAB# if str_ok: #LINE# #TAB# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# key, value = line.strip().split('=', 1) #LINE# #TAB# #TAB# #TAB# nx_obj[key] = value #LINE# #TAB# return nx_obj"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return td.signal_decimal() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return td
"#LINE# #TAB# choices = [] #LINE# #TAB# for image in images: #LINE# #TAB# #TAB# choices.append((image.name, image)) #LINE# #TAB# return choices"
#LINE# #TAB# lst = [l for l in lst if l] #LINE# #TAB# if not lst: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif len(lst) == 1: #LINE# #TAB# #TAB# return lst[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# tmp = lst[0] #LINE# #TAB# #TAB# for item in lst[1:]: #LINE# #TAB# #TAB# #TAB# if item not in tmp: #LINE# #TAB# #TAB# #TAB# #TAB# tmp = item #LINE# #TAB# #TAB# return tmp
"#LINE# #TAB# preorder_db = namdb_open( cur, preorder_hash) #LINE# #TAB# if preorder_db is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# sql = ""DELETE FROM preorder WHERE preorder_hash =?;"" #LINE# #TAB# args = (preorder_db,) #LINE# #TAB# cur.execute(sql, args) #LINE# #TAB# result = cur.fetchone() #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# focus_widget = widgets.current_focus_widget() #LINE# #TAB# proxy_index = widgets.current_focus_index() #LINE# #TAB# while focus_widget is not None and proxy_index < len(widgets #LINE# #TAB# #TAB#.current_focus_widget()): #LINE# #TAB# #TAB# declaration = widgets.current_focus_widget() #LINE# #TAB# #TAB# if declaration is not None: #LINE# #TAB# #TAB# #TAB# del declaration_index #LINE# #TAB# #TAB# #TAB# active_proxy = None #LINE# #TAB# #TAB# elif proxy_index == len(widgets): #LINE# #TAB# #TAB# #TAB# active_proxy = None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return declaration
"#LINE# #TAB# assert edges >= 0 #LINE# #TAB# if edges == 0: #LINE# #TAB# #TAB# return [], [] #LINE# #TAB# forward = [] #LINE# #TAB# reverse = [] #LINE# #TAB# for i, j in edges: #LINE# #TAB# #TAB# if j!= i: #LINE# #TAB# #TAB# #TAB# forward.append(j) #LINE# #TAB# #TAB# #TAB# reverse.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# forward.append(i) #LINE# #TAB# #TAB# #TAB# reverse.append(j) #LINE# #TAB# if reverse: #LINE# #TAB# #TAB# return forward, reverse #LINE# #TAB# else: #LINE# #TAB# #TAB# return forward, reverse"
"#LINE# #TAB# rc = lib.sdl_ineria(mass, radius, height, transform) #LINE# #TAB# return rc"
#LINE# #TAB# filtered_repos = [] #LINE# #TAB# for repo in repos: #LINE# #TAB# #TAB# if repo in ignore_repos: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# filtered_repos.append(repo) #LINE# #TAB# return filtered_repos
"#LINE# #TAB# if value > 0: #LINE# #TAB# #TAB# timestamp = time.mktime(value) #LINE# #TAB# #TAB# date = time.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%fZ')[:-1] #LINE# #TAB# #TAB# return date.replace(':', '-').replace(':', '-').replace('.', '-') #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# delta = normalize_timedelta(delta) #LINE# #TAB# return f'{delta.days} day{delta.hour:0.2f} hour {delta.minute:0.2f} minute{delta.second:0.2f}'
#LINE# #TAB# ead = PyPDF2(filename=filename) #LINE# #TAB# ead.generate() #LINE# #TAB# try: #LINE# #TAB# #TAB# os.remove(filename) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ead
"#LINE# #TAB# if isinstance(args, str): #LINE# #TAB# #TAB# return [onvert_entity_encode_xml(a, argname) for a in args] #LINE# #TAB# elif isinstance(args, list): #LINE# #TAB# #TAB# return [onvert_entity_encode_xml(a, argname) for a in args] #LINE# #TAB# else: #LINE# #TAB# #TAB# return args"
#LINE# #TAB# cli_profile = dict() #LINE# #TAB# cli_profile['authorization'] = auth #LINE# #TAB# return cli_profile
"#LINE# #TAB# if mime_type is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# extension = mime_type.split('.')[-1] #LINE# #TAB# if extension in ['jpe', 'jpeg']: #LINE# #TAB# #TAB# random.seed() #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# name = get_chbi_class(chebi_id, offline) #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return DEFAULT_CHBI_CLASS #LINE# #TAB# else: #LINE# #TAB# #TAB# if name[-4:] == '_readclass': #LINE# #TAB# #TAB# #TAB# name = name[:-4] #LINE# #TAB# #TAB# return name"
"#LINE# #TAB# logger.debug('Generating current return for actor URN %s', actor_urn) #LINE# #TAB# if not cls._actor_cache.has_key(actor_urn): #LINE# #TAB# #TAB# result = cls.generate_current_return(actor_urn) #LINE# #TAB# #TAB# if result is None: #LINE# #TAB# #TAB# #TAB# raise ActorRefNotFound(actor_urn) #LINE# #TAB# #TAB# cls._actor_cache[actor_urn] = result #LINE# #TAB# #TAB# return result #LINE# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return gt_title_cache[bookmark] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return {}
"#LINE# #TAB# new_paths = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if isinstance(path, list): #LINE# #TAB# #TAB# #TAB# for el in path: #LINE# #TAB# #TAB# #TAB# #TAB# if el not in new_paths: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# new_paths.append(el) #LINE# #TAB# #TAB# #TAB# elif isinstance(el, str): #LINE# #TAB# #TAB# #TAB# #TAB# new_paths.append(el) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_paths.append(path) #LINE# #TAB# return new_paths"
"#LINE# #TAB# result = copy.deepcopy(obj) #LINE# #TAB# if isinstance(result, np.ndarray): #LINE# #TAB# #TAB# if result.ndim == 0: #LINE# #TAB# #TAB# #TAB# return result.T #LINE# #TAB# #TAB# return result #LINE# #TAB# return result"
#LINE# #TAB# if 'flattens' in material: #LINE# #TAB# #TAB# return [material['flattens']] #LINE# #TAB# return []
"#LINE# #TAB# with fs.lock(): #LINE# #TAB# #TAB# if settings_path: #LINE# #TAB# #TAB# #TAB# with open(settings_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# settings = yaml.safe_load(f) #LINE# #TAB# #TAB# config = {} #LINE# #TAB# #TAB# for key in settings: #LINE# #TAB# #TAB# #TAB# if key.startswith('project_'): #LINE# #TAB# #TAB# #TAB# #TAB# project = key[9:] #LINE# #TAB# #TAB# #TAB# #TAB# config[project] = settings #LINE# #TAB# #TAB# return config"
#LINE# #TAB# #TAB# assert len(struct.cell) == 1 #LINE# #TAB# #TAB# assert struct.cell[0] == 0
"#LINE# #TAB# assert issparse(A) #LINE# #TAB# if A.dim() not in [2, 3]: #LINE# #TAB# #TAB# normalize_a = A.tocsc() #LINE# #TAB# else: #LINE# #TAB# #TAB# normalize_a = tf.divide(A, k=3) #LINE# #TAB# for _ in range(A.shape[0]): #LINE# #TAB# #TAB# for _ in range(A.shape[1]): #LINE# #TAB# #TAB# #TAB# normalize_a.data[(_), :] = tf.sqrt(norm(A.data[(_), :, :])) #LINE# #TAB# return normalize_a"
#LINE# #TAB# global _try_i #LINE# #TAB# if idx < 0: #LINE# #TAB# #TAB# raise ValueError('Invalid index passed to setup_try_i') #LINE# #TAB# _try_i = idx
#LINE# #TAB# max_outputs = RPR.GetMaxMIDIOutputs() #LINE# #TAB# return max_outputs
#LINE# #TAB# output_array = '' #LINE# #TAB# for line in input_array: #LINE# #TAB# #TAB# if line!= '': #LINE# #TAB# #TAB# #TAB# if output_array[-1]!= '\n': #LINE# #TAB# #TAB# #TAB# #TAB# output_array += '\n' #LINE# #TAB# #TAB# #TAB# output_array += line #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output_array += line #LINE# #TAB# return output_array
"#LINE# #TAB# resolved = [] #LINE# #TAB# for version in versions: #LINE# #TAB# #TAB# field = resolve_field_pair(version, reverse) #LINE# #TAB# #TAB# if field is None: #LINE# #TAB# #TAB# #TAB# resolved.append(version) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# resolved.append(field) #LINE# #TAB# return resolved"
"#LINE# #TAB# if not _is_at_east(x, y): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if not signature.isCompatible(withSignature): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# var_enum = 0 #LINE# #TAB# for key, value in environ.items(): #LINE# #TAB# #TAB# if key.startswith('CONTENT_LENGTH'): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# var_enum |= int(value) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# if var_enum == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return var_enum"
"#LINE# #TAB# if type(ex) == BaseX: #LINE# #TAB# #TAB# for select in tables: #LINE# #TAB# #TAB# #TAB# unpack_subqueris(ex, select, table_ctor) #LINE# #TAB# elif type(ex) == list: #LINE# #TAB# #TAB# for select in ex: #LINE# #TAB# #TAB# #TAB# unpack_subqueris(select, table_ctor) #LINE# #TAB# elif type(ex) == dict: #LINE# #TAB# #TAB# for key, val in ex.items(): #LINE# #TAB# #TAB# #TAB# ex[key] = unpack_subqueris(val, table_ctor) #LINE# #TAB# return ex"
"#LINE# #TAB# input_shape = input_data.shape #LINE# #TAB# out_shape = output_data.shape #LINE# #TAB# attr_dict = dict(input_shape=input_shape, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# converter_type=converter_type, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# channels=channels) #LINE# #TAB# return attr_dict"
#LINE# #TAB# url = urlparse.urlparse(scheme + '://' + host) #LINE# #TAB# if not url.scheme: #LINE# #TAB# #TAB# url.scheme = scheme #LINE# #TAB# return url.netloc + url.path
"#LINE# #TAB# summary = class_console_get_summary(category, forum) #LINE# #TAB# if summary is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if user.is_anonymous: #LINE# #TAB# #TAB# return False #LINE# #TAB# names = summary.get('forumTopicNames', []) #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# file_path = class_console_get_filename(category, name) #LINE# #TAB# #TAB# file_path.append('{}.{}'.format(user.username, name)) #LINE# #TAB# if file_path is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# for file_path in file_path: #LINE# #TAB# #TAB# if file_path.exists(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# bids = {} #LINE# #TAB# if opts.bid: #LINE# #TAB# #TAB# bids['bid'] = opts.bid #LINE# #TAB# elif opts.name: #LINE# #TAB# #TAB# bids['name'] = opts.name #LINE# #TAB# return bids
"#LINE# #TAB# yaml_file = os.path.join(os.path.dirname(__file__),'version.yaml') #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = 'v' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source = yaml_file #LINE# #TAB# return cfg"
"#LINE# if len(attempt) == 0 or not longopt_list: #LINE# #TAB# return ""No option specified"" #LINE# option_names = [v.split('=')[0] for v in longopt_list] #LINE# distances = [(_DamerauLevenshtein(attempt, option[0:len(attempt)]), option) #LINE# #TAB# #TAB# #TAB# for option in option_names] #LINE# distances.sort(key=lambda t: t[0]) #LINE# least_errors, _ = distances[0] #LINE# if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len(attempt): #LINE# #TAB# return ""More than %d errors"" % least_errors #LINE# elif least_errors == _SUGGESTION_WARNING_RATE_THRESHOLD * len(attempt): #LINE# #TAB# return ""More than %d errors"" % least_errors #LINE# else: #LINE# #TAB# return ""Equal"""
#LINE# #TAB# claimset_data = [] #LINE# #TAB# for item in claimset_data: #LINE# #TAB# #TAB# if type(item) is not int: #LINE# #TAB# #TAB# #TAB# claimset_data.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# claimset_data.append(item) #LINE# #TAB# if len(claimset_data) == 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if cigar_tuple[0] == 0 or cigar_tuple[1] <= 10: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0
#LINE# #TAB# global noebook_ode #LINE# #TAB# noebook_ode = True
"#LINE# #TAB# import os #LINE# #TAB# from pathlib import Path #LINE# #TAB# path = Path(prefix) / 'detectors' / 'cass_names_read_link.py' #LINE# #TAB# f = open(path, 'r') #LINE# #TAB# for line in f: #LINE# #TAB# #TAB# if line.startswith('#') or line == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# sline = line[0:1].replace(' ', '_') #LINE# #TAB# #TAB# rsline = '""' + sline + '""' #LINE# #TAB# f.close() #LINE# #TAB# if f.exists(): #LINE# #TAB# #TAB# return f.name #LINE# #TAB# else: #LINE# #TAB# #TAB# return f.name"
"#LINE# #TAB# module_path = os.path.abspath(os.path.dirname(__file__)) #LINE# #TAB# profile_lib = None #LINE# #TAB# for path in [os.path.dirname(os.path.dirname(module_path)), os.path.join( #LINE# #TAB# #TAB# path, 'profile.py'), os.path.join(path, 'profile.py'), os.path.join( #LINE# #TAB# #TAB# path, 'config.py')]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = import_module(path) #LINE# #TAB# #TAB# #TAB# if hasattr(module, 'profile'): #LINE# #TAB# #TAB# #TAB# #TAB# profile_lib = getattr(module, 'profile') #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return profile_lib"
"#LINE# #TAB# builder = FluidDBBuilder(app, sandbox=sandbox) #LINE# #TAB# return builder"
"#LINE# #TAB# r = decimal_point_twos(x, y, center) #LINE# #TAB# t = decimal_point_twos(x, y, center) #LINE# #TAB# return r, t"
"#LINE# #TAB# xs = [] #LINE# #TAB# ys = [] #LINE# #TAB# if len(spec) == 1: #LINE# #TAB# #TAB# xs = spec[0] #LINE# #TAB# for i in range(1, spec.shape[1]): #LINE# #TAB# #TAB# c = np.sqrt(spec[i] ** 2 + spec[i + 1] ** 2) #LINE# #TAB# #TAB# if c < 0: #LINE# #TAB# #TAB# #TAB# xs.append(c) #LINE# #TAB# #TAB# #TAB# ys.append(spec[i]) #LINE# #TAB# df = pd.DataFrame(xs, columns=['x', 'y']) #LINE# #TAB# df.columns = ['x', 'y', 'z'] #LINE# #TAB# return df"
#LINE# #TAB# if json is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if json_fields is None: #LINE# #TAB# #TAB# json_fields = ['value'] #LINE# #TAB# tbytes = bytearray() #LINE# #TAB# for field in json_fields: #LINE# #TAB# #TAB# if field in node: #LINE# #TAB# #TAB# #TAB# tbytes.append(str(node[field])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tbytes.append(str(node[field])) #LINE# #TAB# return tbytes
#LINE# #TAB# result = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# result = toDecode.decode('utf-8') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return result
"#LINE# #TAB# num_params = operator.param_names.length #LINE# #TAB# parmas = [] #LINE# #TAB# for param in operator.parameters: #LINE# #TAB# #TAB# if param.name in ('gt', 'lt'): #LINE# #TAB# #TAB# #TAB# parmas.append(Parameter(name=param.name, value=0.0)) #LINE# #TAB# #TAB# elif param.name in ('rtr', 'tr'): #LINE# #TAB# #TAB# #TAB# parmas.append(Parameter(name=param.name, value=0.0)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# parmas.append(Parameter(name=param.name, value=0.0)) #LINE# #TAB# return parmas"
"#LINE# #TAB# mochad_controller = hass.data[DOMAIN] #LINE# #TAB# devs = config.get(CONF_DEVICES) #LINE# #TAB# add_entities([MochadLightForm(hass, mochad_controller.ctrl, dev) for dev in #LINE# #TAB# #TAB# devs]) #LINE# #TAB# return True"
"#LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# file_name = file_name.replace('\r\n', '\n') #LINE# #TAB# if encode: #LINE# #TAB# #TAB# file_name = file_name.encode(encoding) #LINE# #TAB# remote = subprocess.Popen(file_name, shell=True, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# stdout=subprocess.PIPE) #LINE# #TAB# else: #LINE# #TAB# #TAB# remote = subprocess.Popen(file_name, shell=True, stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# return remote"
"#LINE# #TAB# if lut is None: #LINE# #TAB# #TAB# lut = {} #LINE# #TAB# for i, j in triangle_list: #LINE# #TAB# #TAB# for k, v in lut.items(): #LINE# #TAB# #TAB# #TAB# if k == j: #LINE# #TAB# #TAB# #TAB# #TAB# yield i, v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield i, j, v"
"#LINE# #TAB# for name in dir(cls): #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if hasattr(cls, '__init__'): #LINE# #TAB# #TAB# if not hasattr(cls, '__name__'): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# spo = unpacked_spo.copy() #LINE# #TAB# if 'algo' in spo: #LINE# #TAB# #TAB# algo = spo['algo'] #LINE# #TAB# #TAB# if algo == 'zero': #LINE# #TAB# #TAB# #TAB# spo['address'] = None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# spo['address'] = None #LINE# #TAB# return spo
#LINE# #TAB# obj = repr(actor) #LINE# #TAB# obj.SetTransform(transformation) #LINE# #TAB# return obj
"#LINE# #TAB# try: #LINE# #TAB# #TAB# group_id = df[cluster_id].unique().tolist()[0] #LINE# #TAB# except: #LINE# #TAB# #TAB# raise Exception('Invalid cluster_id') #LINE# #TAB# group = df.groupby(group_id, as_index=True) #LINE# #TAB# g = pd.DataFrame() #LINE# #TAB# g['date'] = df[date_col].astype(str) #LINE# #TAB# g['cluster_id'] = group_id #LINE# #TAB# g['mor_cluster_date'] = df[group_id].astype(str) #LINE# #TAB# return g"
"#LINE# #TAB# import os #LINE# #TAB# global commnd_message #LINE# #TAB# commnd_message = True #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_call(['namedex', '-h'], stdout=subprocess.STDOUT) #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# commnd_message = False #LINE# #TAB# if e.returncode!= 0: #LINE# #TAB# #TAB# commnd_message = False #LINE# #TAB# return commnd_message"
#LINE# #TAB# L = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# lineno = frame.f_back.f_lineno #LINE# #TAB# #TAB# #TAB# if lineno == L: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# L.append(lineno) #LINE# #TAB# finally: #LINE# #TAB# #TAB# del frame
"#LINE# #TAB# result = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# result = str(random.randint(0, 999999999)) #LINE# #TAB# return result"
"#LINE# #TAB# if not isinstance(node, dict): #LINE# #TAB# #TAB# return node #LINE# #TAB# result = [node] #LINE# #TAB# for k, v in node.items(): #LINE# #TAB# #TAB# if headers: #LINE# #TAB# #TAB# #TAB# if k in headers: #LINE# #TAB# #TAB# #TAB# #TAB# del result[k] #LINE# #TAB# #TAB# #TAB# result[k].append(v) #LINE# #TAB# #TAB# elif isinstance(v, str): #LINE# #TAB# #TAB# #TAB# result[k] = v.split('\t') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(k) #LINE# #TAB# return result"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return char.encode('utf-8') #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# return b''
"#LINE# #TAB# if isinstance(gtype, cls): #LINE# #TAB# #TAB# return gtype.of_fun_nd() #LINE# #TAB# elif isinstance(gtype, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cls.of_fun_nd(gtype) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# outer_indices = [] #LINE# #TAB# for tensor in inputs: #LINE# #TAB# #TAB# inner_indices = [] #LINE# #TAB# #TAB# for index, _ in enumerate(g.get_all_tensors(tensor)): #LINE# #TAB# #TAB# #TAB# if s == index: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# outer_indices.append(i1_cut_i2_wo_output[tensor][index]) #LINE# #TAB# #TAB# outer_indices.append(i1_union_i2[tensor]) #LINE# #TAB# outer_indices = np.array(outer_indices) #LINE# #TAB# return outer_indices"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, #LINE# #TAB# #TAB# #TAB# socket.SOCK_STREAM) #LINE# #TAB# #TAB# return value[0] + value[1] #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# logging.error('Could not resolve hostname ""{}"" with port {}'. #LINE# #TAB# #TAB# #TAB# format(contact_point, port)) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if hasattr(cls,'veboselogs_version'): #LINE# #TAB# #TAB# return cls.veboselogs_version #LINE# #TAB# else: #LINE# #TAB# #TAB# return '0.0.0'"
"#LINE# #TAB# categories = set() #LINE# #TAB# for key, value in net.items(): #LINE# #TAB# #TAB# if 'device' not in value.keys(): #LINE# #TAB# #TAB# #TAB# categories.add(key) #LINE# #TAB# return categories"
#LINE# #TAB# eail = et_model_read_eail_raw(email) #LINE# #TAB# if not eail: #LINE# #TAB# #TAB# return None #LINE# #TAB# return eail[0]
#LINE# #TAB# if not nc_filename: #LINE# #TAB# #TAB# return None #LINE# #TAB# file_name = os.path.basename(nc_filename) #LINE# #TAB# if file_name.endswith('.gz'): #LINE# #TAB# #TAB# file_name = file_name[:-len('.gz')] #LINE# #TAB# with nc.open_file(file_name) as nc_file: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# content = nc_file.read() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# nc_dict = json.loads(content) #LINE# #TAB# #TAB# return nc_dict #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if is_accessible == False: #LINE# #TAB# #TAB# raise ValueError('is_accessible must be a boolean') #LINE# #TAB# if stop is None: #LINE# #TAB# #TAB# stop = start #LINE# #TAB# if step is None: #LINE# #TAB# #TAB# step = 1 #LINE# #TAB# w = [] #LINE# #TAB# for i in range(start, stop): #LINE# #TAB# #TAB# w.append(ipv6_window(is_accessible, size, i, start)) #LINE# #TAB# return w"
"#LINE# #TAB# praams_data = {} #LINE# #TAB# params = urlparse(url).query #LINE# #TAB# for k, v in params.items(): #LINE# #TAB# #TAB# if k == 'azure': #LINE# #TAB# #TAB# #TAB# praams_data[k] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# if 'azure' not in key: #LINE# #TAB# #TAB# #TAB# praams_data[key] = v #LINE# #TAB# return praams_data"
"#LINE# #TAB# result = {""id"": d} #LINE# #TAB# if 'children' not in d: #LINE# #TAB# #TAB# return result #LINE# #TAB# if type(d['children']) == list: #LINE# #TAB# #TAB# result['children'] = [replace_generate_i(child) for child in d['children']] #LINE# #TAB# else: #LINE# #TAB# #TAB# result['children'] = [replace_generate_i(child) for child in d['children']] #LINE# #TAB# return result"
#LINE# #TAB# global _HOSTS #LINE# #TAB# return _HOSTS[name]
#LINE# #TAB# for path in dist.get_paths(): #LINE# #TAB# #TAB# if not path.endswith('record-without-pyc'): #LINE# #TAB# #TAB# #TAB# yield path
"#LINE# #TAB# return {'exchange': exchange, 'queue': queue_name, 'routing': routing if #LINE# #TAB# #TAB# routing else queue_name}"
"#LINE# #TAB# parts = value.split('=') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return #LINE# #TAB# cred_type = parts[0] #LINE# #TAB# if cred_type not in CREDITIAL_TYPES: #LINE# #TAB# #TAB# raise ValueError('Unknown credential type: {}'.format(cred_type)) #LINE# #TAB# try: #LINE# #TAB# #TAB# parts[1] = int(parts[1]) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValueError('Invalid key-value pair: {}'.format(parts)) #LINE# #TAB# if parts[1] is None: #LINE# #TAB# #TAB# raise ValueError('Invalid key-value pair: {}'.format(parts[1])) #LINE# #TAB# return cred_type, parts[2]"
#LINE# #TAB# di = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# q = num_n // 2 #LINE# #TAB# #TAB# if q == 1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# di += 1 #LINE# #TAB# #TAB# num_n -= q #LINE# #TAB# return di
#LINE# #TAB# player_config = PlayerConfig(name=team.name) #LINE# #TAB# for part in player_config_path.parts[:-1]: #LINE# #TAB# #TAB# if part.isdigit(): #LINE# #TAB# #TAB# #TAB# player_config.add_player(team) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# player_config.add_player(part) #LINE# #TAB# return player_config
"#LINE# #TAB# bot_configs = set() #LINE# #TAB# for root, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# if filename.endswith('_diretory_delete_counts_onfig'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# bot_config = load_bot_config(os.path.join(root, filename)) #LINE# #TAB# #TAB# #TAB# if isinstance(bot_config, BOTConfigBundle): #LINE# #TAB# #TAB# #TAB# #TAB# bot_configs.add(bot_config) #LINE# #TAB# return bot_configs"
#LINE# #TAB# if credentials.type_indicator not in cls._loader_credentials: #LINE# #TAB# #TAB# cls._loader_credentials[credentials.type_indicator] = credentials
"#LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# for key, value in l: #LINE# #TAB# #TAB# #TAB# obj = obj.build_text(key, value) #LINE# #TAB# #TAB# return obj"
"#LINE# #TAB# buf = c.data #LINE# #TAB# while len(buf) > 0: #LINE# #TAB# #TAB# if isinstance(buf[-1], bytes): #LINE# #TAB# #TAB# #TAB# buf = buf[:-1] #LINE# #TAB# #TAB# buf += alg.decompress(buf) #LINE# #TAB# buf = b''.join(buf) #LINE# #TAB# return ffi.buffer(buf, len(c.data))[:]"
"#LINE# #TAB# if unit!= 'pt': #LINE# #TAB# #TAB# return False #LINE# #TAB# for i in range(len(contour_levels)): #LINE# #TAB# #TAB# if contourf_idx[i] == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for j in range(len(contour_levels[i])): #LINE# #TAB# #TAB# #TAB# if contour_levels[i][j] == 0: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# contourf_dict = get_contourf_dict(contour_levels[i][j]) #LINE# #TAB# #TAB# #TAB# has_v200_params = {'stroke_width': stroke_width, 'fcolor': fcolor, #LINE# #TAB# #TAB# #TAB# #TAB# 'fill_opacity': fill_opacity} #LINE# #TAB# #TAB# #TAB# yield has_v200_params"
"#LINE# #TAB# l = [0.0] * len(points) #LINE# #TAB# for row in range(1, len(points) - 1): #LINE# #TAB# #TAB# for col in range(1, len(points[row])): #LINE# #TAB# #TAB# #TAB# l[row][col] = points[row][col].distance(points[col]) #LINE# #TAB# lon = [0.0] * len(points) #LINE# #TAB# for row in range(1, len(points) - 1): #LINE# #TAB# #TAB# for col in range(1, len(points)): #LINE# #TAB# #TAB# #TAB# l[col] = points[row][col].distance(points[col]) #LINE# #TAB# l[0] = np.nanmax(l) #LINE# #TAB# return l"
"#LINE# #TAB# #TAB# win = cls.application_clean_win_cache.get(data) #LINE# #TAB# #TAB# if win is None: #LINE# #TAB# #TAB# #TAB# raise GIError(""No application clean win data"") #LINE# #TAB# #TAB# return win"
"#LINE# #TAB# req_hooks = [hooks.BuilRequestHook(controller, transactional)] #LINE# #TAB# if transactional: #LINE# #TAB# #TAB# req_hooks.append(hooks.BuilTransactionHook()) #LINE# #TAB# return req_hooks"
"#LINE# #TAB# lg = geno[0] #LINE# #TAB# n = len(lg) #LINE# #TAB# physical = np.zeros((n, n + 1)) #LINE# #TAB# physical[0, 0] = 1 #LINE# #TAB# physical[-1, 1] = 1 #LINE# #TAB# for i in range(1, n): #LINE# #TAB# #TAB# lg[i] = 0.0 #LINE# #TAB# #TAB# for j in range(i + 1, n): #LINE# #TAB# #TAB# #TAB# physical[i][j] += lg[j][i] * (geno[1][j] - geno[2][j]) #LINE# #TAB# return physical"
"#LINE# #TAB# N, M = mat.shape #LINE# #TAB# Mtemp = np.zeros(N) #LINE# #TAB# for n in range(M): #LINE# #TAB# #TAB# Mtemp[n] = cholesky(mat[n, :]) #LINE# #TAB# return Mtemp"
"#LINE# #TAB# out = {} #LINE# #TAB# for i, row in enumerate(cat_table): #LINE# #TAB# #TAB# if i in cuts: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# row_data = {} #LINE# #TAB# #TAB# row_keys = list(row.keys()) #LINE# #TAB# #TAB# for key in row_keys: #LINE# #TAB# #TAB# #TAB# col_keys = list(row.keys()) #LINE# #TAB# #TAB# #TAB# cut_value = cuts.get(key, None) #LINE# #TAB# #TAB# #TAB# if row_data[col_keys[0]] == cut_value: #LINE# #TAB# #TAB# #TAB# #TAB# out[key] = row_data #LINE# #TAB# return out"
"#LINE# #TAB# response = HttpResponse(content_type='text/csv') #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=weights.csv' #LINE# #TAB# with open(os.path.join(settings.MEDIA_ROOT, 'weights.csv'), 'w') as csvfile: #LINE# #TAB# #TAB# reader = csv.reader(csvfile, delimiter=',') #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# response.write(row) #LINE# #TAB# return response"
#LINE# #TAB# al_params = find_al_param_nothig(player) #LINE# #TAB# if al_params is not None: #LINE# #TAB# #TAB# return al_params #LINE# #TAB# for al_pair in al_params: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield al_pair #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# catalog = cls() #LINE# #TAB# for language in cls.languages(): #LINE# #TAB# #TAB# for topic in cls.topics(): #LINE# #TAB# #TAB# #TAB# catalog.add_language(language, topic) #LINE# #TAB# #TAB# #TAB# catalog.add_topic(topic, language) #LINE# #TAB# for language in cls.languages(): #LINE# #TAB# #TAB# catalog.add_language(language.language, language) #LINE# #TAB# #TAB# for topic in cls.topics(): #LINE# #TAB# #TAB# #TAB# catalog.add_topic(topic, language) #LINE# #TAB# #TAB# for topic in cls.topics(): #LINE# #TAB# #TAB# #TAB# catalog.add_topic(topic, language) #LINE# #TAB# #TAB# yield catalog"
#LINE# #TAB# current_weekday_insert_edit_tim = settings.WEEKDAYS_INSERT_EDIT_TIME #LINE# #TAB# if (not has_start_stop or current_weekday_insert_edit_tim =='s'): #LINE# #TAB# #TAB# current_weekday_insert_edit_tim = WEEKDAYS_INSERT_EDIT_TIME_DEFAULT #LINE# #TAB# return current_weekday_insert_edit_tim
"#LINE# #TAB# user = getattr(context['request'], 'user', None) #LINE# #TAB# if not article: #LINE# #TAB# #TAB# return [query_term(context, user, limit=limit)] #LINE# #TAB# if isinstance(article, Article): #LINE# #TAB# #TAB# return [query_term(context, article, limit=limit) for query_term in #LINE# #TAB# #TAB# #TAB# article.categories.all()] #LINE# #TAB# return []"
#LINE# #TAB# s = 0.0 #LINE# #TAB# n = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# s += 1 #LINE# #TAB# #TAB# pos = json.loads(pos) #LINE# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# if pos['z'] > 0: #LINE# #TAB# #TAB# #TAB# if pos['y'] > 0: #LINE# #TAB# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# elif pos['x'] > 0: #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# elif pos['y'] > 0: #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# return n
"#LINE# #TAB# global _nurbsRenderer #LINE# #TAB# if _nurbsRenderer is None: #LINE# #TAB# #TAB# _nurbsRenderer = lu_system_new() #LINE# #TAB# baseFunction.argtypes = [ctypes.c_void_p, ctypes.c_void_p] #LINE# #TAB# baseFunction.restype = c_void_p #LINE# #TAB# _nurbsRenderer.argtypes = [ctypes.c_void_p, ctypes.c_void_p] #LINE# #TAB# return _nurbsRenderer"
"#LINE# #TAB# obj = Rule(name, table.name, inobj.pop('description', None), inobj.pop( #LINE# #TAB# #TAB# 'owner', None), inobj.pop('privileges', []), inobj.pop( #LINE# #TAB# #TAB# 'rules', []), inobj.pop('source_rules', []), inobj.pop( #LINE# #TAB# #TAB# 'dest_rules', []), inobj.pop('source_rules_to', []), inobj.pop( #LINE# #TAB# #TAB# 'dest_rules_from', []), inobj.pop('source_rules_to', []), inobj.pop( #LINE# #TAB# #TAB# 'dest_rules_from', [])) #LINE# #TAB# obj.fix_privileges() #LINE# #TAB# obj.set_oldname(inobj) #LINE# #TAB# return obj"
"#LINE# #TAB# for dx_node in dx_nodes: #LINE# #TAB# #TAB# if isinstance(dx_node, DiffxNode): #LINE# #TAB# #TAB# #TAB# yield dx_node #LINE# #TAB# #TAB# elif isinstance(dx_node, list): #LINE# #TAB# #TAB# #TAB# for x in dx_node: #LINE# #TAB# #TAB# #TAB# #TAB# translate_permissions_resources(dx_nodes=x, dx_node=dx_node) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield dx_node"
#LINE# #TAB# a = np.array(a) #LINE# #TAB# other = np.array(other) #LINE# #TAB# overlaps = np.abs(particle - other) < a #LINE# #TAB# count = overlaps.sum() #LINE# #TAB# if side == 'right': #LINE# #TAB# #TAB# overlap = overlap / len(a) #LINE# #TAB# elif side == 'left': #LINE# #TAB# #TAB# overlap = overlap / len(particle) #LINE# #TAB# else: #LINE# #TAB# #TAB# overlap = overlap / len(particle) #LINE# #TAB# if normalize: #LINE# #TAB# #TAB# overlap = normalize_overlap(overlap) #LINE# #TAB# return overlap
"#LINE# #TAB# data = refs.update_component(profile, name, sha) #LINE# #TAB# data['head'] = data['head'] #LINE# #TAB# new_sha = data['new_sha'] #LINE# #TAB# return new_sha"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return ''.join([i.capitalize() for i in name.split('_')]) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return name
"#LINE# #TAB# if token is not None: #LINE# #TAB# #TAB# credentials = jwt.encode(token, verify=False) #LINE# #TAB# else: #LINE# #TAB# #TAB# credentials = None #LINE# #TAB# return {'Authorization': 'Bearer {}'.format(credentials.get( #LINE# #TAB# #TAB# 'access_token', ''))}"
"#LINE# #TAB# element_name_absolue = [] #LINE# #TAB# parts = name.split('.') #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# element_name_absolue.append(part) #LINE# #TAB# if len(model_name_absolue) > 1: #LINE# #TAB# #TAB# parts.pop() #LINE# #TAB# name_absolue = '.'.join(parts) #LINE# #TAB# return model_name_absolue, name_absolue"
#LINE# #TAB# res = [] #LINE# #TAB# for t in x: #LINE# #TAB# #TAB# if t == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if t[0].isupper() and len(t) > 1 and t[1:].islower(): #LINE# #TAB# #TAB# #TAB# res.append(TK_MAJ) #LINE# #TAB# #TAB# res.append(t.lower()) #LINE# #TAB# return res
"#LINE# #TAB# num_train_images = num_images #LINE# #TAB# ake = tf.data.DataArray(shape=(num_train_images, num_train_images), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# shape=(num_train_images, num_train_images)) #LINE# #TAB# return ake"
"#LINE# #TAB# with h5py.File(file_path, 'rb') as f: #LINE# #TAB# #TAB# clf = pickle.load(f) #LINE# #TAB# _LOGGER.info('Loaded classifier from %s', file_path) #LINE# #TAB# return clf"
"#LINE# #TAB# #TAB# knx = {} #LINE# #TAB# #TAB# for k, v in value.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# #TAB# knx[k] = cls._pase_knx(v) #LINE# #TAB# #TAB# #TAB# elif isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# knx[k] = cls._pase_knx(v) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# knx[k] = v #LINE# #TAB# #TAB# return knx"
"#LINE# #TAB# return [IdentityRefWithVote(tupl[0].email, tupl[1].id, team_instance) for #LINE# #TAB# #TAB# tupl in reviewers for tupl in verify_identity_reviews(tupl[1], #LINE# #TAB# #TAB# team_instance)]"
#LINE# #TAB# if not resource_id: #LINE# #TAB# #TAB# raise ValueError('No such resource: {0}'.format(resource_id)) #LINE# #TAB# structure = _get_structure(resource_id) #LINE# #TAB# if structure is None: #LINE# #TAB# #TAB# raise ValueError('No structure associated with resource: {0}'.format( #LINE# #TAB# #TAB# #TAB# resource_id)) #LINE# #TAB# return structure
#LINE# #TAB# if t[0] == 'HOOK': #LINE# #TAB# #TAB# t[0] = t[3] #LINE# #TAB# elif t[0] == 'ATOM': #LINE# #TAB# #TAB# t[0] = t[1] + t[2] #LINE# #TAB# else: #LINE# #TAB# #TAB# assert False
#LINE# #TAB# geod = None #LINE# #TAB# try: #LINE# #TAB# #TAB# geod = cor_norm_rot[0] #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# if geod < -1: #LINE# #TAB# #TAB# geod = 1 #LINE# #TAB# elif geod > 1: #LINE# #TAB# #TAB# geod = 2 #LINE# #TAB# assert geod > 0 #LINE# #TAB# return geod
#LINE# #TAB# url = 'key=' + secret_key #LINE# #TAB# return url
"#LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# output = _run_ceck(args) #LINE# #TAB# #TAB# repos = output.split('\n') #LINE# #TAB# #TAB# for repo in repos: #LINE# #TAB# #TAB# #TAB# i = repo.split('-')[0] #LINE# #TAB# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# if os.path.exists(os.path.join(repo, i)): #LINE# #TAB# #TAB# #TAB# #TAB# lib.engine_ceck_kill(args) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(dir) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(dir): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return dir
"#LINE# #TAB# customer = Customer(name=name) #LINE# #TAB# customer.add_contact_method(email, phone) #LINE# #TAB# return customer"
#LINE# #TAB# padding = 0 #LINE# #TAB# n = len(node) #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# if n % 2 == 0: #LINE# #TAB# #TAB# #TAB# padding += 1 #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# return padding
#LINE# #TAB# desktop_entry = create_desktop_entry() #LINE# #TAB# desktop_entry.delete() #LINE# #TAB# return desktop_entry
#LINE# #TAB# global log #LINE# #TAB# log = logger
"#LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(data, six.string_types): #LINE# #TAB# #TAB# return data #LINE# #TAB# if isinstance(data, dict): #LINE# #TAB# #TAB# my_dict = {} #LINE# #TAB# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# #TAB# if k in my_dict: #LINE# #TAB# #TAB# #TAB# #TAB# my_dict[k] = cls.best_effort_datetime(v) #LINE# #TAB# #TAB# #TAB# for k, v in my_dict.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if k == 'datetime': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# my_dict[k] = v #LINE# #TAB# #TAB# return None #LINE# #TAB# return data"
#LINE# #TAB# archs = [] #LINE# #TAB# if up: #LINE# #TAB# #TAB# for arch in arch_info: #LINE# #TAB# #TAB# #TAB# if arch.match(flags): #LINE# #TAB# #TAB# #TAB# #TAB# archs.append(arch) #LINE# #TAB# else: #LINE# #TAB# #TAB# archs = [arch] #LINE# #TAB# return archs
"#LINE# #TAB# arg_spec = argument_spec.copy() #LINE# #TAB# for name, value in argument_spec._asdict().items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# arg_spec[name] = prefix_handler_instance(value) #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# arg_spec[name] = prefix_handler_instance(value) #LINE# #TAB# return arg_spec"
"#LINE# #TAB# samples = utils.to_single_data(samples) #LINE# #TAB# try: #LINE# #TAB# #TAB# bbox = samples[0][""bbox""] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# bbox = [] #LINE# #TAB# return bbox"
"#LINE# #TAB# if os.name == 'posix': #LINE# #TAB# #TAB# filename = os.path.join('VcsPlugins', 'icons', 'preferences-subversion.svg') #LINE# #TAB# else: #LINE# #TAB# #TAB# filename = os.path.join('VcsPlugins', 'icons','subversion.svg') #LINE# #TAB# return {'zzz_subversionPage': filename}"
#LINE# #TAB# new_seq = '' #LINE# #TAB# for c in seq: #LINE# #TAB# #TAB# if c in '{}': #LINE# #TAB# #TAB# #TAB# new_seq +='' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_seq += c #LINE# #TAB# return new_seq
"#LINE# #TAB# try: #LINE# #TAB# #TAB# next(iterator) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return #LINE# #TAB# for item in iterator: #LINE# #TAB# #TAB# if item!= step: #LINE# #TAB# #TAB# #TAB# yield from walk_th_root(ctx, item, step) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield item"
#LINE# #TAB# df = pd.DataFrame(data) #LINE# #TAB# columns = [item[0] for item in df.columns] #LINE# #TAB# df.columns = columns #LINE# #TAB# return df
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t', header=None, usecols=[0, 1, 2], #LINE# #TAB# #TAB# names=['chrom', 'pos', 'genotype']) #LINE# #TAB# if 'chrom' not in df.columns: #LINE# #TAB# #TAB# table = 'chrom' #LINE# #TAB# #TAB# df['chrom'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['pos'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['genotype'] = df['chrom'].astype(str) #LINE# #TAB# return df"
"#LINE# #TAB# print(ctx.obj['__file__']) #LINE# #TAB# with open('src/robotide/editor/tags.py', 'r') as f: #LINE# #TAB# #TAB# f.read() #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# if line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# yield line.strip() #LINE# #TAB# print(ctx.obj['__file__']) #LINE# #TAB# with open('src/robotide/editor/tags.py', 'w') as f: #LINE# #TAB# #TAB# f.write('\n') #LINE# #TAB# with open('src/robotide/editor/tags.py', 'r') as f: #LINE# #TAB# #TAB# f.write('\n') #LINE# #TAB# yield"
"#LINE# #TAB# values = struct.unpack('!I', raw_hex[12:26]) #LINE# #TAB# n = len(values) #LINE# #TAB# b = bytes([0] * n) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# b[i] = struct.pack('!I', *values[i:i + 2]) #LINE# #TAB# return b"
"#LINE# #TAB# context = inspect.getcontext(obj) #LINE# #TAB# fn_filename = os.path.basename(obj.__file__) #LINE# #TAB# context += ':' #LINE# #TAB# if hasattr(context, 'write'): #LINE# #TAB# #TAB# context += '\n' #LINE# #TAB# if hasattr(context,'read'): #LINE# #TAB# #TAB# context += '\n' #LINE# #TAB# for key in ('fn_template', 'fn_filename'): #LINE# #TAB# #TAB# if context.read(key): #LINE# #TAB# #TAB# #TAB# fn_filename += context.read(key) + '\n' #LINE# #TAB# return fn_filename"
"#LINE# #TAB# for suffix in ['complex', 'complex-complex']: #LINE# #TAB# #TAB# name = name.replace(suffix, '') #LINE# #TAB# return name"
"#LINE# #TAB# if not condition: #LINE# #TAB# #TAB# return True #LINE# #TAB# result = {} #LINE# #TAB# for keypath, value in condition.items(): #LINE# #TAB# #TAB# result[keypath] = value #LINE# #TAB# return result"
"#LINE# #TAB# if len(dict_a) > 0 and len(dict_b) > 0: #LINE# #TAB# #TAB# for k, v in dict_b.items(): #LINE# #TAB# #TAB# #TAB# if k in dict_a: #LINE# #TAB# #TAB# #TAB# #TAB# dict_a[k] = v #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dict_a[k] = v #LINE# #TAB# else: #LINE# #TAB# #TAB# for k, v in dict_a.items(): #LINE# #TAB# #TAB# #TAB# if k in dict_b: #LINE# #TAB# #TAB# #TAB# #TAB# dict_b[k] = v #LINE# #TAB# return dict_a"
"#LINE# #TAB# tpes = [] #LINE# #TAB# for s in str_input: #LINE# #TAB# #TAB# match = elmeent_tpes_re.match(s) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# time_code, title = match.groups() #LINE# #TAB# #TAB# #TAB# titles, url = match.groups() #LINE# #TAB# #TAB# #TAB# tpes.append((time_code, title, url)) #LINE# #TAB# return tpes"
#LINE# #TAB# r = requests.get(url) #LINE# #TAB# data = r.json() #LINE# #TAB# if data['ok']: #LINE# #TAB# #TAB# return data['workid'] #LINE# #TAB# return None
"#LINE# #TAB# if s is None: #LINE# #TAB# #TAB# return _generate_scaling_single_feature(MI_FS, k, MI_FS, #LINE# #TAB# #TAB# #TAB# are_data_binned) #LINE# #TAB# if n_jobs == 1: #LINE# #TAB# #TAB# return _generate_scaling_multi_feature(MI_FS, k, F, are_data_binned) #LINE# #TAB# return _generate_scaling_multi_feature(MI_FS, k, s, are_data_binned) + 1"
"#LINE# #TAB# pos = logical_line.find('def ') #LINE# #TAB# if pos > -1: #LINE# #TAB# #TAB# if logical_line[pos - 2].startswith('def '): #LINE# #TAB# #TAB# #TAB# yield pos, 'S360: Do not use %s instead of %s' % ( #LINE# #TAB# #TAB# #TAB# #TAB# 'def ', logical_line[pos + 1:]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield pos, 'S360: Do not use %s instead of %s' % ( #LINE# #TAB# #TAB# #TAB# #TAB# 'S360: Do not use %s instead of %s' % (logical_line[pos], #LINE# #TAB# #TAB# #TAB# #TAB# 'def ', logical_line[pos + 1:]))"
#LINE# #TAB# invalid_degradations = [] #LINE# #TAB# for degradation_args in degradations_args: #LINE# #TAB# #TAB# degradation_args = _validate_string(degradation_args) #LINE# #TAB# #TAB# if degradation_args: #LINE# #TAB# #TAB# #TAB# invalid_degradations.append(degradation_args) #LINE# #TAB# return invalid_degradations
"#LINE# #TAB# summary = [] #LINE# #TAB# ref_finder = HTMLReferenceFinder(xml) #LINE# #TAB# for elm, uri_attr in ref_finder: #LINE# #TAB# #TAB# ref = Reference(elm, uri_attr) #LINE# #TAB# #TAB# summary.append(ref) #LINE# #TAB# return summary"
"#LINE# #TAB# golden_ratio = 0.2 #LINE# #TAB# width, height = math.ceil(golden_ratio * figwidth) #LINE# #TAB# return width, height"
#LINE# #TAB# from python_toolbox import nifty_collections #LINE# #TAB# iterable = list(iterable) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# a = next(iterable) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if filter_function(a): #LINE# #TAB# #TAB# #TAB# #TAB# yield a #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break
"#LINE# #TAB# for m in dictionary: #LINE# #TAB# #TAB# dictionary[m.replace('{', '{{').replace('}', '}}')] = re.sub( #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\-]+)}', #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\-]+)}', #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\-]+)', #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\-]+)', #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\-]+)', #LINE# #TAB# #TAB# #TAB# '{([a-zA-Z0-9_\\1}\\2')', #LINE# #TAB# #TAB# #TAB# dictionary[m]) #LINE# #TAB# return dictionary"
#LINE# #TAB# dik_images = [] #LINE# #TAB# location = normalize_location(location) #LINE# #TAB# for image in pbclient.list_images(location): #LINE# #TAB# #TAB# if image['name'] == image_name: #LINE# #TAB# #TAB# #TAB# dik_images.append(image) #LINE# #TAB# return dik_images
"#LINE# #TAB# rot = np.array([[0, 0, 0], [0, np.sin(2 * np.pi / 180.0), -np.cos(2 * #LINE# #TAB# #TAB# np.pi / 180.0), 0], [0, np.sin(2 * np.pi / 180.0), np.cos(2 * np.pi / #LINE# #TAB# #TAB# 180.0)]]) #LINE# #TAB# return rot"
#LINE# #TAB# if arrays is None: #LINE# #TAB# #TAB# arrays = [] #LINE# #TAB# for ss in ss: #LINE# #TAB# #TAB# yield from ss.map_cod(arrays) #LINE# #TAB# return
"#LINE# #TAB# user = getattr(context['request'], 'user', None) #LINE# #TAB# if user: #LINE# #TAB# #TAB# return user.get('authtoken', None) #LINE# #TAB# return None"
"#LINE# #TAB# auth = keystoneclient() #LINE# #TAB# auth.set_ath(cloud_name, 'keystone') #LINE# #TAB# return auth"
"#LINE# #TAB# if ctx.obj is None: #LINE# #TAB# #TAB# ctx.obj = {} #LINE# #TAB# result = param.split_str(s) #LINE# #TAB# if result.get(""multiple""): #LINE# #TAB# #TAB# result[""multiple""] = True #LINE# #TAB# #TAB# return result #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# sorted_encoders = sorted(encoders, key=lambda enc: enc.score(trainX, #LINE# #TAB# #TAB# percentile=percentile)) #LINE# #TAB# return [encoders[i] for i in sorted_encoders]"
"#LINE# #TAB# toknum, tokval = tok #LINE# #TAB# if toknum == 3: #LINE# #TAB# #TAB# return toknum, tokval #LINE# #TAB# if toknum == 2: #LINE# #TAB# #TAB# return toknum, tokval #LINE# #TAB# return toknum, tokval"
"#LINE# #TAB# data = yaml.safe_load(open(filename)) #LINE# #TAB# if not data: #LINE# #TAB# #TAB# return #LINE# #TAB# reg_score(data) #LINE# #TAB# if not reg_warning: #LINE# #TAB# #TAB# return #LINE# #TAB# if not reg_error: #LINE# #TAB# #TAB# return #LINE# #TAB# if reg_warning <= data[reg_score]: #LINE# #TAB# #TAB# return #LINE# #TAB# if reg_error <= data[reg_warning]: #LINE# #TAB# #TAB# return #LINE# #TAB# return { #LINE# #TAB# #TAB# reg_error: data[reg_error], #LINE# #TAB# #TAB# reg_score: data[reg_score], #LINE# #TAB# }"
"#LINE# #TAB# #TAB# if filename is None: #LINE# #TAB# #TAB# #TAB# filename = url.split('?')[1] #LINE# #TAB# #TAB# cache_file = os.path.join(cache_dir, filename) #LINE# #TAB# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# #TAB# with open(cache_file, 'rb') as f: #LINE# #TAB# #TAB# #TAB# #TAB# image = cls.read_path(url) #LINE# #TAB# #TAB# #TAB# #TAB# if image: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield image"
"#LINE# #TAB# host, port = hosts_and_ports #LINE# #TAB# root_action = 'root' #LINE# #TAB# if len(hosts_and_ports) == 2: #LINE# #TAB# #TAB# host, port = hosts_and_ports #LINE# #TAB# #TAB# root_action = '%s:%d' % (host, port) #LINE# #TAB# elif len(hosts_and_ports) == 1: #LINE# #TAB# #TAB# host, port = hosts_and_ports #LINE# #TAB# #TAB# root_action = '%s:%d' % (host, port) #LINE# #TAB# elif len(hosts_and_ports) == 2: #LINE# #TAB# #TAB# root_action = '%s:%d' % (host, port) #LINE# #TAB# return root_action"
"#LINE# #TAB# assert type(in_file_list) == list #LINE# #TAB# required_in_file = '.vrt' #LINE# #TAB# missing_in_file = '.vrt' if not in_file_list else in_file_list[0] #LINE# #TAB# with open(required_in_file, 'r') as in_file: #LINE# #TAB# #TAB# for in_file in in_file_list: #LINE# #TAB# #TAB# #TAB# with open(in_file, 'r') as in_file: #LINE# #TAB# #TAB# #TAB# #TAB# required_in_file.write(in_file.read()) #LINE# #TAB# with open(out_file, 'w') as out_file: #LINE# #TAB# #TAB# json.dump([required_in_file, out_file]) #LINE# #TAB# return out_file"
"#LINE# #TAB# scenarios = [] #LINE# #TAB# for line in scencmd.splitlines(): #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if re.match(r'\s+', line): #LINE# #TAB# #TAB# #TAB# scenarios.append(parse_line(line)) #LINE# #TAB# return scenarios"
#LINE# #TAB# global _backend #LINE# #TAB# if not _backend: #LINE# #TAB# #TAB# _backend = find_backends() #LINE# #TAB# return _backend
#LINE# #TAB# mean = np.zeros(len(counts)) #LINE# #TAB# count = counts.sum() #LINE# #TAB# for i in range(len(observable)): #LINE# #TAB# #TAB# var = observable[i] #LINE# #TAB# #TAB# mean[i] += count[i] * var #LINE# #TAB# return mean
"#LINE# #TAB# if url.startswith(""#""): #LINE# #TAB# #TAB# return False #LINE# #TAB# if url.find(""://"") > 0 or url.startswith(""//""): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# error = False #LINE# #TAB# count = 0 #LINE# #TAB# for value in sync: #LINE# #TAB# #TAB# if value > 150: #LINE# #TAB# #TAB# #TAB# error = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# count += 1 #LINE# #TAB# if count == 0: #LINE# #TAB# #TAB# error = True #LINE# #TAB# if error: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if re.match( #LINE# #TAB# #TAB# '^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$' #LINE# #TAB# #TAB#, ip) is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# if argument is None: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# return argument
"#LINE# #TAB# if not serialized: #LINE# #TAB# #TAB# return None #LINE# #TAB# cert = None #LINE# #TAB# for line in serialized.split(b'\n'): #LINE# #TAB# #TAB# if line.startswith(b'-----BEGIN PGP'): #LINE# #TAB# #TAB# #TAB# cert = b'\n'.join(line[3:]) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith(b'-----END PGP'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cert = crypto.load_certificate(crypto.FILETYPE_PEM, line) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if cert: #LINE# #TAB# #TAB# #TAB# cert = crypto.load_certificate(cert) #LINE# #TAB# return cert"
#LINE# #TAB# output = [] #LINE# #TAB# for i in range(y.shape[0]): #LINE# #TAB# #TAB# if y[i] == -1: #LINE# #TAB# #TAB# #TAB# output.append(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output.append(0) #LINE# #TAB# return output
"#LINE# #TAB# onl_path = '/var/run/docker.io/docker' #LINE# #TAB# if not os.path.exists(onl_path): #LINE# #TAB# #TAB# os.makedirs(onl_path) #LINE# #TAB# for volumes_ in volumes: #LINE# #TAB# #TAB# container = container_from_path(name, volumes_) #LINE# #TAB# #TAB# if container not in os.listdir(onl_path): #LINE# #TAB# #TAB# #TAB# os.mkdir(onl_path) #LINE# #TAB# return onl_path"
"#LINE# #TAB# if hasattr(value, 'decode') and not isinstance(value, str): #LINE# #TAB# #TAB# return value.encode('utf-8') #LINE# #TAB# return value"
#LINE# #TAB# tempo = create_tempo() #LINE# #TAB# tempo.add(artist +'' + title) #LINE# #TAB# tempo.add(artist +'' + title) #LINE# #TAB# tempo.add(title) #LINE# #TAB# return tempo
#LINE# #TAB# if key.endswith('or'): #LINE# #TAB# #TAB# field = 'or' #LINE# #TAB# elif key.endswith('and'): #LINE# #TAB# #TAB# field = 'and' #LINE# #TAB# elif key.endswith('or'): #LINE# #TAB# #TAB# field = 'or' #LINE# #TAB# return field
"#LINE# #TAB# if not isinstance(templates, (list, tuple)): #LINE# #TAB# #TAB# templates = [templates] #LINE# #TAB# domain = [] #LINE# #TAB# for template in templates: #LINE# #TAB# #TAB# if os.path.exists(template): #LINE# #TAB# #TAB# #TAB# domain.append('%s/%s' % (settings.DOMAIN, template)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# domain.append(template) #LINE# #TAB# if domain: #LINE# #TAB# #TAB# return [('%s/%s' % (settings.DOMAIN, template)) for template in #LINE# #TAB# #TAB# #TAB# domain] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
#LINE# #TAB# for rxn in model.reactions: #LINE# #TAB# #TAB# if not rxn.summary: #LINE# #TAB# #TAB# #TAB# yield rxn
#LINE# #TAB# devices = get_bitox02_devices() #LINE# #TAB# devices.extend(get_bitox02_btc_devices()) #LINE# #TAB# return devices
#LINE# #TAB# global SEP #LINE# #TAB# if type: #LINE# #TAB# #TAB# return SEP[type]
#LINE# #TAB# result = [] #LINE# #TAB# f = _feaured_report() #LINE# #TAB# for _ in range(count): #LINE# #TAB# #TAB# f = _feaured_report(f) #LINE# #TAB# #TAB# result.append(f) #LINE# #TAB# return result
#LINE# #TAB# if layer.keywords['exposure_class'] is None: #LINE# #TAB# #TAB# layer.keywords['exposure_class'] = '' #LINE# #TAB# return layer
#LINE# #TAB# if not is_number(a) or p <= 1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# if a == 1: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# b = a % p #LINE# #TAB# while b!= 0 and b!= 1: #LINE# #TAB# #TAB# b = a // p #LINE# #TAB# while b!= 0: #LINE# #TAB# #TAB# b = a // p #LINE# #TAB# #TAB# if b!= 0 and b!= 1: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return -1
"#LINE# #TAB# cm = np.asarray(cm, dtype=np.float64) #LINE# #TAB# if cm.shape[0]!= cm.shape[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# for i in range(cm.shape[0]): #LINE# #TAB# #TAB# rand = np.random.random() #LINE# #TAB# #TAB# cm[i, i] = 1.0 - (cm[i, i] ** 2).sum() #LINE# #TAB# #TAB# if np.random.random() < 0.5: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# rid = rid or action['rid'] #LINE# #TAB# unit = unit or action['unit'] #LINE# #TAB# try: #LINE# #TAB# #TAB# return action['status'] == 'completed' #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# result = [] #LINE# #TAB# for name, value in list(flag_dict.items()): #LINE# #TAB# #TAB# if isinstance(value, tuple): #LINE# #TAB# #TAB# #TAB# result.append((name, value)) #LINE# #TAB# return result"
"#LINE# #TAB# if rand: #LINE# #TAB# #TAB# stat_id = _get_stat_id(stat, set_) #LINE# #TAB# else: #LINE# #TAB# #TAB# stat_id = stat #LINE# #TAB# stat_dict = {} #LINE# #TAB# stat_dict['stat_id'] = stat_id #LINE# #TAB# stat_dict['values'] = [val] #LINE# #TAB# return stat_dict"
#LINE# #TAB# valid = False #LINE# #TAB# pnr = pnr.as_canonical() #LINE# #TAB# if not pnr.isdigit(): #LINE# #TAB# #TAB# valid = True #LINE# #TAB# if not pnr.isdigit(): #LINE# #TAB# #TAB# raise ValueError('incorrect personnummer') #LINE# #TAB# if not pnr.isdigit(): #LINE# #TAB# #TAB# valid = True #LINE# #TAB# if not valid: #LINE# #TAB# #TAB# raise ValueError('incorrect personnummer') #LINE# #TAB# return valid
"#LINE# #TAB# tools = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# for fp in os.listdir(plugin_dir): #LINE# #TAB# #TAB# #TAB# fn = os.path.join(plugin_dir, fp) #LINE# #TAB# #TAB# #TAB# tool_version = read_tool_version(fn) #LINE# #TAB# #TAB# #TAB# if tool_version is None: #LINE# #TAB# #TAB# #TAB# #TAB# tools.append(fn) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# tools.append(tool_version) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return tools #LINE# #TAB# return tools"
"#LINE# #TAB# disp = np.zeros(values.shape[0]) #LINE# #TAB# for i, v in enumerate(edges): #LINE# #TAB# #TAB# orig_distr = False #LINE# #TAB# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# #TAB# v = v[(np.newaxis), :] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# v = v[(np.newaxis), :] #LINE# #TAB# #TAB# disp = distr(v) #LINE# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# disp = distr.mean(axis=1) #LINE# #TAB# return disp"
#LINE# #TAB# _dict = {} #LINE# #TAB# _list = val.split #LINE# #TAB# for item in _list: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# key = item.lower() #LINE# #TAB# #TAB# #TAB# val = _dict[key] #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# _dict[key] = '_' + item.title() #LINE# #TAB# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return _dict
"#LINE# #TAB# assert isinstance(phonenumbers.Phonenumbers, list) #LINE# #TAB# assert isinstance(freq_offset, int) #LINE# #TAB# freq = seq[loc_ind] #LINE# #TAB# signal = _si_sigle_compute_signal(loc_ind[loc_ind], freq, phantom) #LINE# #TAB# return signal"
#LINE# #TAB# words = tuple(words) #LINE# #TAB# plugin_params = {} #LINE# #TAB# if minimize_indices: #LINE# #TAB# #TAB# plugin_params['minimize_indices'] = minimize_indices #LINE# #TAB# for word in words: #LINE# #TAB# #TAB# word = word.lower() #LINE# #TAB# #TAB# plugin_params[word] = hsh(word) #LINE# #TAB# return plugin_params
"#LINE# #TAB# new_retries = {} #LINE# #TAB# for retry_config in retries: #LINE# #TAB# #TAB# if isinstance(retry_config, RetryConfig): #LINE# #TAB# #TAB# #TAB# new_retries[retry_config.name] = retry_config #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_retries[retry_config.name] = retry_config #LINE# #TAB# return new_retries"
"#LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# contents = f.read() #LINE# #TAB# if contents == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# from json import loads #LINE# #TAB# #TAB# return loads(contents) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# import registry #LINE# #TAB# shell_name = registry.get_value('csidl.shell_name', csidl_name) #LINE# #TAB# summary_path = os.path.join(registry.get_value('csidl.summary_dir', shell_name), #LINE# #TAB# #TAB# csidl_name) #LINE# #TAB# if not os.path.exists(summary_path): #LINE# #TAB# #TAB# os.makedirs(summary_path) #LINE# #TAB# return summary_path"
#LINE# #TAB# os.environ['PATH'] = path #LINE# #TAB# return True
"#LINE# #TAB# reader = vcfutils.VCFReader(infile) #LINE# #TAB# try: #LINE# #TAB# #TAB# header = next(reader) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return #LINE# #TAB# reader = next(reader) #LINE# #TAB# comment = """" #LINE# #TAB# for v in reader: #LINE# #TAB# #TAB# if not comment: #LINE# #TAB# #TAB# #TAB# if v.id == sample: #LINE# #TAB# #TAB# #TAB# #TAB# comment = v.description #LINE# #TAB# #TAB# elif v.id!= sample: #LINE# #TAB# #TAB# #TAB# if comment!= """": #LINE# #TAB# #TAB# #TAB# #TAB# yield reader, v #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# yield reader, v #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return"
"#LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print('Activating %i dependencies.' % len(extension_names)) #LINE# #TAB# active_extensions = [] #LINE# #TAB# for extension_name in extension_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# active_extensions.append(activate_extension(extension_name)) #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# #TAB# print('error activating %i extension: %s' % ( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# extension_name, e)) #LINE# #TAB# active_extensions = list(active_extensions) #LINE# #TAB# shuffle(active_extensions) #LINE# #TAB# return active_extensions"
"#LINE# #TAB# if not limit: #LINE# #TAB# #TAB# return None #LINE# #TAB# grap = Graph() #LINE# #TAB# for item in qs: #LINE# #TAB# #TAB# grap.add(snapshot_cast_grap_item(item, limit=limit)) #LINE# #TAB# return grap"
#LINE# #TAB# axes = [[] for _ in range(num_samples)] #LINE# #TAB# for x in range(num_samples): #LINE# #TAB# #TAB# for y in range(num_samples): #LINE# #TAB# #TAB# #TAB# axes[x].append(widths[y]) #LINE# #TAB# return axes
#LINE# #TAB# used_token = initial_guess #LINE# #TAB# while used_token in graph.nodes_iter(): #LINE# #TAB# #TAB# token = _format.format(used_token) #LINE# #TAB# #TAB# if not graph.has_node(token): #LINE# #TAB# #TAB# #TAB# used_token = _format.format(token) #LINE# #TAB# return used_token
#LINE# #TAB# attr = data.attrs #LINE# #TAB# if axis == 'F1': #LINE# #TAB# #TAB# return attr['top'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return attr
"#LINE# #TAB# eviron = cls() #LINE# #TAB# eviron.initialize(environ['PYTHONPATH_INFO']) #LINE# #TAB# if environ['QUERY_STRING']: #LINE# #TAB# #TAB# eviron.query_string = environ['QUERY_STRING'] #LINE# #TAB# else: #LINE# #TAB# #TAB# eviron.query_string = environ['SCRIPT_NAME'] #LINE# #TAB# for key, value in environ.items(): #LINE# #TAB# #TAB# if key == 'PYTHONPATH_INFO': #LINE# #TAB# #TAB# #TAB# eviron.load_module(key) #LINE# #TAB# #TAB# elif key == 'PYTHONPATH_INFO': #LINE# #TAB# #TAB# #TAB# eviron.load_module(key) #LINE# #TAB# eviron.environ = environ #LINE# #TAB# return eviron"
"#LINE# #TAB# json_formatter = JSONFormatter() #LINE# #TAB# if overrides is None: #LINE# #TAB# #TAB# overrides = {} #LINE# #TAB# for col_type in col_types: #LINE# #TAB# #TAB# json_format = json.dumps(dict(type=col_type), sort_keys=True, indent=4, #LINE# #TAB# #TAB# #TAB# separators=(',', ': ')) #LINE# #TAB# #TAB# json_format.update(nan_display) #LINE# #TAB# #TAB# json_format.update(overrides) #LINE# #TAB# data = json.dumps(json_format) #LINE# #TAB# return data"
"#LINE# #TAB# for i, output in enumerate(outputs): #LINE# #TAB# #TAB# if output['stream'] == 'failure': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del d['__xml_attr__'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if k.startswith('__'): #LINE# #TAB# #TAB# #TAB# if autospace: #LINE# #TAB# #TAB# #TAB# #TAB# del d[k] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# d[k] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break"
#LINE# #TAB# dialog = scrolled_message_dialog.ScrolledMessageDialog(parent) #LINE# #TAB# dialog.setText(text) #LINE# #TAB# dialog.exec_() #LINE# #TAB# return dialog
"#LINE# #TAB# if hasattr(root, 'children'): #LINE# #TAB# #TAB# for c in root.children: #LINE# #TAB# #TAB# #TAB# trim_table_cl(c) #LINE# #TAB# return root"
"#LINE# #TAB# fid_search_code = fidi_search_code_in_city(city_name, token) #LINE# #TAB# if fid_search_code is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# results = [] #LINE# #TAB# for code in fid_search_code: #LINE# #TAB# #TAB# for result in results: #LINE# #TAB# #TAB# #TAB# if result['uid'] == code['uid']: #LINE# #TAB# #TAB# #TAB# #TAB# results.append(result) #LINE# #TAB# return results"
"#LINE# #TAB# assert isinstance(color1, str) #LINE# #TAB# assert isinstance(color2, str) #LINE# #TAB# if N == 1: #LINE# #TAB# #TAB# return [color1] #LINE# #TAB# HSV_arr = [] #LINE# #TAB# if hsv: #LINE# #TAB# #TAB# HSV_arr.append(np.linspace(0, 1, N)) #LINE# #TAB# new_color = [] #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# new_color.append(np.interp(hsv_arr[i], color1[i], color2[i])) #LINE# #TAB# new_color = np.array(new_color) #LINE# #TAB# return new_color"
#LINE# #TAB# global _GITHUB_REPOSITORY #LINE# #TAB# _GITHUB_REPOSITORY = repository #LINE# #TAB# if should_list: #LINE# #TAB# #TAB# repository = list(repository) #LINE# #TAB# _GITHUB_REPOSITORY.sort() #LINE# #TAB# return _GITHUB_REPOSITORY
"#LINE# #TAB# if six.PY3: #LINE# #TAB# #TAB# if isinstance(string, bytes): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return string.decode('hex') #LINE# #TAB# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# return binascii.unhexlify(string) #LINE# #TAB# return string"
"#LINE# #TAB# if create_copy: #LINE# #TAB# #TAB# lnew = left.copy() #LINE# #TAB# #TAB# rnew = right.copy() #LINE# #TAB# else: #LINE# #TAB# #TAB# rnew = left.copy() #LINE# #TAB# for key, value in right.items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# rnew[key] = maybe_array_mrege(lvalue, value, create_copy=False) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# rnew[key] = value #LINE# #TAB# return lnew"
#LINE# #TAB# if s.group_by_clause: #LINE# #TAB# #TAB# return Selectable(s.group_by_clause[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return s
"#LINE# #TAB# if is_string(value): #LINE# #TAB# #TAB# return value #LINE# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# return [it_encode_response(item) for item in value] #LINE# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# return {it_encode_response(key): it_encode_response(val) for key, #LINE# #TAB# #TAB# #TAB# val in value.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# assert data.ndim == 2 #LINE# #TAB# assert data.shape[1] == 2 #LINE# #TAB# cm = np.zeros((data.shape[0], 2), dtype=np.float64) #LINE# #TAB# cm[:, (0)] = -data[:, (0)] / data[:, (1)] #LINE# #TAB# cm[:, (1)] = np.sqrt(data[:, (1)] / data[:, (0)]) #LINE# #TAB# return cm"
#LINE# #TAB# try: #LINE# #TAB# #TAB# del cls._omponents[component] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
#LINE# #TAB# kwargs = {} #LINE# #TAB# for item in header['extras']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# json.loads(item) #LINE# #TAB# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# kwargs[item] = json.loads(item) #LINE# #TAB# return kwargs
"#LINE# #TAB# rows = set() #LINE# #TAB# for doc_type in doc_types: #LINE# #TAB# #TAB# rows.update(get_generator_status(request, doc_type)) #LINE# #TAB# return rows"
#LINE# #TAB# d = {} #LINE# #TAB# libsvm = load_libsvm(filename) #LINE# #TAB# for dataset in libsvm: #LINE# #TAB# #TAB# idx = dataset['idx'] #LINE# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# if idx == 0: #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# elif idx < len(libsvm): #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# #TAB# d[dataset['name']][opt_type] = idx #LINE# #TAB# return d
"#LINE# #TAB# parts = version_string.split('-') #LINE# #TAB# major, minor, revision, prerelease = parts #LINE# #TAB# return major, minor, revision, prerelease"
#LINE# #TAB# _logger = logging.getLogger(name) #LINE# #TAB# return _logger
"#LINE# #TAB# home = os.path.expanduser(""~"") #LINE# #TAB# source_exiss_param_dir = os.path.join(home, "".sourceexiss"") #LINE# #TAB# if not os.path.exists(source_exiss_param_dir): #LINE# #TAB# #TAB# os.makedirs(source_exiss_param_dir) #LINE# #TAB# if not os.path.exists(source_exiss_param_dir): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.mkdir(source_exiss_param_dir) #LINE# #TAB# #TAB# except OSError as e: #LINE# #TAB# #TAB# #TAB# if e.errno!= errno.EEXIST: #LINE# #TAB# #TAB# #TAB# #TAB# raise e"
#LINE# #TAB# record = json_format.MessageToDict(message_proto) #LINE# #TAB# record_dict = json_format.MessageToDict(record) #LINE# #TAB# return record_dict
"#LINE# #TAB# if isinstance(agg, tuple): #LINE# #TAB# #TAB# yield tuple(agg) #LINE# #TAB# elif isinstance(agg, list): #LINE# #TAB# #TAB# for x in agg: #LINE# #TAB# #TAB# #TAB# for y in rename_stats(x): #LINE# #TAB# #TAB# #TAB# #TAB# yield y #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield agg.pop(0) #LINE# #TAB# elif isinstance(agg, dict): #LINE# #TAB# #TAB# for key, val in agg.items(): #LINE# #TAB# #TAB# #TAB# yield key #LINE# #TAB# #TAB# #TAB# for x in rename_stats(val): #LINE# #TAB# #TAB# #TAB# #TAB# yield x #LINE# #TAB# else: #LINE# #TAB# #TAB# yield agg"
"#LINE# #TAB# cmd = shlex.split(cmd) #LINE# #TAB# try: #LINE# #TAB# #TAB# out = check_output(cmd, stderr=STDOUT) #LINE# #TAB# except CalledProcessError as e: #LINE# #TAB# #TAB# err = e.output #LINE# #TAB# #TAB# log_subprocess_error(err) #LINE# #TAB# #TAB# raise CalledProcessError(err) #LINE# #TAB# out = out.decode('utf-8') #LINE# #TAB# return out"
"#LINE# #TAB# outcomes = state.outcoes #LINE# #TAB# if not outcomes: #LINE# #TAB# #TAB# return None #LINE# #TAB# new_outcome = random.choice(outcoes) #LINE# #TAB# state.outcoes.append(new_outcome) #LINE# #TAB# for u, v in state.outcoes.items(): #LINE# #TAB# #TAB# if u in updates: #LINE# #TAB# #TAB# #TAB# state.outcoes[u].append(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# state.outcoes[v].append(u) #LINE# #TAB# return new_outcome"
#LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# context = jinja_context.copy() #LINE# #TAB# context['md5sum'] = md5.hexdigest() #LINE# #TAB# context['filename'] = new_filename #LINE# #TAB# context['page'] = md5.open(get_page_from_string(new_filename)) #LINE# #TAB# context['post'] = md5.open(get_post_from_string(new_filename)) #LINE# #TAB# return context
#LINE# #TAB# if not lookfor: #LINE# #TAB# #TAB# return logging.CRITICAL #LINE# #TAB# elif lookfor[0] == 4: #LINE# #TAB# #TAB# return logging.INFO #LINE# #TAB# elif lookfor[0] == 'info': #LINE# #TAB# #TAB# return logging.WARNING #LINE# #TAB# elif lookfor[0] == 'warning': #LINE# #TAB# #TAB# return logging.ERROR #LINE# #TAB# elif lookfor[0] == 'error': #LINE# #TAB# #TAB# return logging.ERROR #LINE# #TAB# else: #LINE# #TAB# #TAB# return logging.CRITICAL
"#LINE# #TAB# h = '%02x%02x%02x' % (r, g, b) #LINE# #TAB# return h"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return np.sum(np.dot(prediction, solution)[:, (0)]) / np.sum(np.dot( #LINE# #TAB# #TAB# #TAB# prediction, solution)[:, (0)]) #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# return 0"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(os.environ['VOLUME_N_CHIRP']) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return constants.DEFAULT_VOLUME_N_CHIRP
"#LINE# #TAB# auth_params = {'client_id': OAUTH2_CLIENT_ID, 'client_secret': #LINE# #TAB# #TAB# OAUTH2_CLIENT_SECRET, 'grant_type':'refresh_token', #LINE# #TAB# #TAB#'refresh_token': refresh_token} #LINE# #TAB# result = _uth_wth_gather(session, auth_params) #LINE# #TAB# if result['success']: #LINE# #TAB# #TAB# return result['access_token'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# if s.isdigit(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# return True #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if infotype in ('idletime','refcount'): #LINE# #TAB# #TAB# return {'idletime': int(response['results'][infotype]['idletime']), #LINE# #TAB# #TAB# #TAB#'refcount': int(response['results'][infotype]['refcount'])} #LINE# #TAB# return {'idletime': int(response['results'][infotype]['idletime']), #LINE# #TAB# #TAB#'refcount': int(response['results'][infotype]['refcount'])}"
"#LINE# #TAB# identifier = dictionary['identifier'] #LINE# #TAB# creator_id = dictionary['creator_id'] #LINE# #TAB# name = dictionary['name'] #LINE# #TAB# version = dictionary['version'] #LINE# #TAB# is_release_version = dictionary['is_release_version'] #LINE# #TAB# description = dictionary['description'] #LINE# #TAB# required_external_domains = dictionary['required_external_domains'] #LINE# #TAB# docker_image_name = dictionary['docker_image_name'] #LINE# #TAB# docker_image_version = dictionary['docker_image_version'] #LINE# #TAB# Program = Program(identifier, creator_id, name, version, is_release_version, #LINE# #TAB# #TAB# description, required_external_domains, docker_image_name, #LINE# #TAB# #TAB# description) #LINE# #TAB# return Program"
"#LINE# #TAB# exists = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# line = open(filename, 'r').readline() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# print('ERROR: Could not open file:'+ filename) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line == '': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if line[0] == '#': #LINE# #TAB# #TAB# #TAB# exists = line[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return exists"
#LINE# #TAB# style = {} #LINE# #TAB# style['alpha'] = '3' #LINE# #TAB# if fill: #LINE# #TAB# #TAB# style['facecolor'] = 'white' #LINE# #TAB# else: #LINE# #TAB# #TAB# style['facecolor'] = 'white' #LINE# #TAB# style['edgecolor'] = 'white' #LINE# #TAB# style['facecolor'] = 'white' #LINE# #TAB# style['edgewidth'] = '1px' #LINE# #TAB# style['edgewidth'] = '1px' #LINE# #TAB# return style
"#LINE# #TAB# if et_settings_file is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif os.path.isfile(et_settings_file): #LINE# #TAB# #TAB# with open(et_settings_file, 'r') as f: #LINE# #TAB# #TAB# #TAB# data = f.read() #LINE# #TAB# #TAB# #TAB# return json.loads(data) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# plt.close(path) #LINE# #TAB# return
"#LINE# #TAB# if not isinstance(data, bytes): #LINE# #TAB# #TAB# data = data.encode('utf-8') #LINE# #TAB# length = len(data) #LINE# #TAB# shared = bytearray(length) #LINE# #TAB# for i in range(0, length): #LINE# #TAB# #TAB# shared.extend(data[i:i + length]) #LINE# #TAB# return shared"
#LINE# #TAB# res = _cookies.get(name) #LINE# #TAB# if res is None: #LINE# #TAB# #TAB# raise ValueError('cookie does not exist') #LINE# #TAB# parsed_value = parse_header(res) #LINE# #TAB# if value is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# dimensions = parsed_value.split('|') #LINE# #TAB# #TAB# #TAB# max_dimensions = int(dimensions[0]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise ValueError('cookie value is invalid') #LINE# #TAB# elif len(dimensions) > 1: #LINE# #TAB# #TAB# raise ValueError('cookie value has more than one dimension: %s' % #LINE# #TAB# #TAB# #TAB# dimensions[0]) #LINE# #TAB# return value
"#LINE# #TAB# if not os.path.exists(dest): #LINE# #TAB# #TAB# with open(dest, 'rb') as f: #LINE# #TAB# #TAB# #TAB# yaml.safe_dump(f.read(), f, default_flow_style=False) #LINE# #TAB# return"
"#LINE# #TAB# mX = np.asarray(mX, dtype=float) #LINE# #TAB# mY = np.asarray(mY, dtype=float) #LINE# #TAB# mX += 0.5 #LINE# #TAB# mY += 0.5 #LINE# #TAB# return mX, mY"
#LINE# #TAB# if atom.symbol == 'X': #LINE# #TAB# #TAB# return atom #LINE# #TAB# if atom.symbol == 'Y': #LINE# #TAB# #TAB# return atom.pos #LINE# #TAB# if atom.symbol == 'Z': #LINE# #TAB# #TAB# return atom.negated #LINE# #TAB# return atom
#LINE# #TAB# response = request.response #LINE# #TAB# clipboard.clear() #LINE# #TAB# request.response = None #LINE# #TAB# return response
#LINE# #TAB# global __head #LINE# #TAB# __head = True #LINE# #TAB# if passwd is None: #LINE# #TAB# #TAB# __head = False #LINE# #TAB# else: #LINE# #TAB# #TAB# __head = passwd
#LINE# #TAB# try: #LINE# #TAB# #TAB# decode_mac(address) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# N, T, k = a.shape #LINE# #TAB# l = np.zeros((k, T)) #LINE# #TAB# for i in range(k): #LINE# #TAB# #TAB# l[i] = target_reshape(a[i], x) #LINE# #TAB# return l"
#LINE# #TAB# router = ctx.obj['router'] #LINE# #TAB# if router is not None: #LINE# #TAB# #TAB# ctx.obj['router'].reload() #LINE# #TAB# #TAB# ctx.obj['router'] = None
"#LINE# #TAB# parsed_bookmark = json.loads(bookmark) #LINE# #TAB# return {'type': parsed_bookmark['type'], 'name': parsed_bookmark['name'], #LINE# #TAB# #TAB#'slug': parsed_bookmark['slug'], 'content': parsed_bookmark['content'], #LINE# #TAB# #TAB# 'location': parsed_bookmark['location'], 'tags': [ #LINE# #TAB# #TAB# parsed_bookmark['tags'] if parsed_bookmark['tags'] else None, 'content-type': #LINE# #TAB# #TAB# parsed_bookmark['content_type'], 'content-location': parsed_bookmark[ #LINE# #TAB# #TAB# 'content_location']}"
"#LINE# #TAB# files = [] #LINE# #TAB# infiles = [] #LINE# #TAB# outfiles = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# tmp = [] #LINE# #TAB# #TAB# with open(f, 'r') as fp: #LINE# #TAB# #TAB# #TAB# for line in fp: #LINE# #TAB# #TAB# #TAB# #TAB# if'start' in line: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# tmp.append(line.rstrip('\n')) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# tmp.append(line.rstrip('\n')) #LINE# #TAB# #TAB# if outfiles: #LINE# #TAB# #TAB# #TAB# outfiles.append(tmp) #LINE# #TAB# return outfiles"
#LINE# #TAB# type_indicator = resolver_helper.type_indicator #LINE# #TAB# if type_indicator not in cls._resolver_helpers: #LINE# #TAB# #TAB# raise KeyError('Resolver helper object not set for type indicator: {0:s}.' #LINE# #TAB# #TAB# #TAB#.format(type_indicator)) #LINE# #TAB# resolver_helper._type_indicator = type_indicator
"#LINE# #TAB# n = int(len(R1) * percent) #LINE# #TAB# if n > 0: #LINE# #TAB# #TAB# R1.sort() #LINE# #TAB# #TAB# R2.sort() #LINE# #TAB# idx = np.arange(n) #LINE# #TAB# if os.path.splitext(R1[idx])[-1] in ['.fastq', '.fastq.gz']: #LINE# #TAB# #TAB# R1 = R1[idx] #LINE# #TAB# if os.path.splitext(R2[idx])[-1] in ['.fastq', '.fastq.gz']: #LINE# #TAB# #TAB# R2 = R2[idx] #LINE# #TAB# return R1, R2"
#LINE# #TAB# tenant = get_tenant(tenant_id) #LINE# #TAB# sigma_installed = False #LINE# #TAB# for service in tenant['services']: #LINE# #TAB# #TAB# if service['type'] == 'network': #LINE# #TAB# #TAB# #TAB# for port in service['ports']: #LINE# #TAB# #TAB# #TAB# #TAB# if port['id']!= tenant['id']: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# sigma_installed = True #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return sigma_installed
"#LINE# #TAB# ret = {} #LINE# #TAB# cwd = os.getcwd() #LINE# #TAB# os.chdir(cwd) #LINE# #TAB# cmd = ""xdotool pluginge -num {0} -o {1}"".format(num, cwd) #LINE# #TAB# out, err = subprocess.check_output(cmd, stderr=subprocess.STDOUT) #LINE# #TAB# ret.update({ #LINE# #TAB# #TAB# ""retcode"": 0, #LINE# #TAB# #TAB# ""stdout"": out, #LINE# #TAB# #TAB# ""stderr"": err, #LINE# #TAB# }) #LINE# #TAB# ret.update({ #LINE# #TAB# #TAB# ""retcode"": 0, #LINE# #TAB# #TAB# ""stdout"": out, #LINE# #TAB# #TAB# ""stderr"": err, #LINE# #TAB# }) #LINE# #TAB# return ret"
#LINE# #TAB# if score_c == 5: #LINE# #TAB# #TAB# return 1.0 #LINE# #TAB# elif score_c == 6: #LINE# #TAB# #TAB# return 0.25 #LINE# #TAB# elif score_c == 7: #LINE# #TAB# #TAB# return 2.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 3.0
"#LINE# #TAB# old_formatters = [ #LINE# #TAB# #TAB# lambda x: x.replace('{', '{{').replace('}', '}}'), #LINE# #TAB# #TAB# lambda x: x.replace('{', '{{').replace('}', '}}'), #LINE# #TAB# ] #LINE# #TAB# for c in old_formatters: #LINE# #TAB# #TAB# if hasattr(c,'render'): #LINE# #TAB# #TAB# #TAB# f = c.render(f) #LINE# #TAB# return f"
#LINE# #TAB# std = np.sqrt((y1 - y) ** 2 + (z1 - z) ** 2) #LINE# #TAB# for i in range(len(pt_id)): #LINE# #TAB# #TAB# if pt_1_id == pt_id[i]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif pt_1_id == pt_2_id[i]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False
#LINE# #TAB# for window in _window_list: #LINE# #TAB# #TAB# if title in window.title: #LINE# #TAB# #TAB# #TAB# if not exact: #LINE# #TAB# #TAB# #TAB# #TAB# return window #LINE# #TAB# #TAB# return None #LINE# #TAB# return window
#LINE# #TAB# exts = [] #LINE# #TAB# for ext in remote.get_extensions(verbose=False): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# exts.append(ext.name()) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if len(exts) > 0: #LINE# #TAB# #TAB# return [ext for ext in exts if ext.name()] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# file_path = config.get(main_section, 'log.file') #LINE# #TAB# file_path = os.path.expandvars(file_path) #LINE# #TAB# if file_path.startswith('~') and not os.path.isabs(file_path): #LINE# #TAB# #TAB# file_path = os.path.join(os.getcwd(), file_path) #LINE# #TAB# file_path = file_path.replace('~', os.path.expanduser('~')) #LINE# #TAB# return file_path"
#LINE# #TAB# node.parent = node #LINE# #TAB# return node
#LINE# #TAB# ev_0 = bk.BKTensor(0) #LINE# #TAB# ev_1 = bk.BKTensor(0) #LINE# #TAB# ev_2 = bk.BKTensor(0) #LINE# #TAB# ev_3 = bk.BKTensor(0) #LINE# #TAB# ev_4 = bk.BKTensor(0) #LINE# #TAB# ev_5 = bk.BKTensor(0) #LINE# #TAB# ev_6 = bk.BKTensor(0) #LINE# #TAB# ev_7 = bk.BKTensor(0) #LINE# #TAB# return ev_0 * ev_0 + ev_1 * ev_2
"#LINE# #TAB# a_row, a_col = a[0], a[1] #LINE# #TAB# b_row, b_col = b[0], b[1] #LINE# #TAB# dist = np.abs(a_row - a_col) + np.abs(b_row - b_col) #LINE# #TAB# return dist < 1e-10"
"#LINE# #TAB# body = request_body #LINE# #TAB# if body is None: #LINE# #TAB# #TAB# body = '' #LINE# #TAB# elif isinstance(body, dict): #LINE# #TAB# #TAB# body = json.dumps(body) #LINE# #TAB# #TAB# body = request_body.get('table', {}) #LINE# #TAB# if isinstance(body, list): #LINE# #TAB# #TAB# for i in range(len(body)): #LINE# #TAB# #TAB# #TAB# body[i] = set_table_bod(body[i]) #LINE# #TAB# return body"
"#LINE# #TAB# encoding = sys.getdefaultencoding() #LINE# #TAB# n = len(input) #LINE# #TAB# output = input #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# j = output.find('\x00') #LINE# #TAB# #TAB# if j < n - 1: #LINE# #TAB# #TAB# #TAB# output = output.replace('\x00', '').replace('\x01', '') #LINE# #TAB# #TAB# #TAB# output = output.replace('\x02', '') #LINE# #TAB# #TAB# #TAB# if j == n - 1: #LINE# #TAB# #TAB# #TAB# #TAB# output = output.replace('\x00', '') #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# output = output.replace('\\x00', '') #LINE# #TAB# return output"
#LINE# #TAB# try: #LINE# #TAB# #TAB# ha_monitor_module(module) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# h = r if isinstance(r, list) else [r, g, b] #LINE# #TAB# best = {} #LINE# #TAB# for index, item in enumerate(hex_table): #LINE# #TAB# #TAB# d = __distance(item, h) #LINE# #TAB# #TAB# if not best or d <= best['distance']: #LINE# #TAB# #TAB# #TAB# best = {'distance': d, 'index': index} #LINE# #TAB# return best"
"#LINE# #TAB# if 'DOCKER_HOST' in os.environ: #LINE# #TAB# #TAB# host = os.environ['DOCKER_HOST'] #LINE# #TAB# #TAB# if ':' in host: #LINE# #TAB# #TAB# #TAB# host = host.split(':')[0] #LINE# #TAB# #TAB# org = host.split('.')[0] #LINE# #TAB# #TAB# org2 = host.split(':')[1] #LINE# #TAB# #TAB# return org, org2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'unknown', 'unknown'"
#LINE# #TAB# res = [] #LINE# #TAB# for i in range(maglen): #LINE# #TAB# #TAB# res.append(mags[i * lag:(i + 1) * lag + magmed]) #LINE# #TAB# return res
#LINE# #TAB# if symbol: #LINE# #TAB# #TAB# return Xlib.XMODE_SYMBOL_TO_MODIFIER[symbol] #LINE# #TAB# else: #LINE# #TAB# #TAB# flags = Xlib.XMODE_SYMBOL_TO_MODIFIER[symbol] #LINE# #TAB# #TAB# return flags
"#LINE# #TAB# plot_type = 'plot' #LINE# #TAB# ax = plt.figure() #LINE# #TAB# ax = ax.append(iq_array) #LINE# #TAB# h = np.array([1.0, 0.0, 1.0]) #LINE# #TAB# for i in range(len(ax)): #LINE# #TAB# #TAB# for j in range(len(ax)): #LINE# #TAB# #TAB# #TAB# ax[i, j] = hilbert_transform(ax[i, j]) #LINE# #TAB# #TAB# ax[i, 0] = ax[i, 0] #LINE# #TAB# plt.ylabel('q_cstring_plot') #LINE# #TAB# return ax"
#LINE# #TAB# request_func._handles_header_report = True #LINE# #TAB# return request_func
"#LINE# #TAB# new_value = this.store.change('depnds_drive', value) #LINE# #TAB# this.store.change('depnds_drive', new_value) #LINE# #TAB# return new_value"
#LINE# #TAB# if reference_fasta_map_param is None: #LINE# #TAB# #TAB# return {} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {assembly_key: reference_fasta_map_param[assembly_key] for #LINE# #TAB# #TAB# #TAB# assembly_key in reference_fasta_map_param}
"#LINE# #TAB# if ctypes is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# vrsion_strig_ctypes = ctypes.c_char_p #LINE# #TAB# try: #LINE# #TAB# #TAB# vrsion_strig_ctypes = _ctypes.c_char_p(vrsion_strig_ctypes) #LINE# #TAB# except: #LINE# #TAB# #TAB# vrsion_strig_ctypes = None #LINE# #TAB# if not isinstance(vrsion_strig_ctypes, (ctypes.c_char_p, 0)): #LINE# #TAB# #TAB# vrsion_strig_ctypes = _ctypes.c_char_p(vrsion_strig_ctypes) #LINE# #TAB# fix_vrsion_strig_ctypes(vrsion_strig_ctypes) #LINE# #TAB# return vrsion_strig_ctypes"
"#LINE# #TAB# source = None #LINE# #TAB# try: #LINE# #TAB# #TAB# source = open(script_information['path']) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None #LINE# #TAB# module = compile(source, script_information['type'], source.filename, #LINE# #TAB# #TAB# 'exec') #LINE# #TAB# scope = getattr(module,'scope', None) #LINE# #TAB# if not scope: #LINE# #TAB# #TAB# return None #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# llog('Created block class: {0}'.format(class_name)) #LINE# #TAB# return scope"
"#LINE# #TAB# prms = getattr(task, 'perms', []) #LINE# #TAB# if prms: #LINE# #TAB# #TAB# for perm in prms: #LINE# #TAB# #TAB# #TAB# if user.has_perm(perm): #LINE# #TAB# #TAB# #TAB# #TAB# task.perms.append(perm) #LINE# #TAB# else: #LINE# #TAB# #TAB# task.perms = []"
#LINE# #TAB# pkg_grah = nx.DiGraph() #LINE# #TAB# for node_id in node_ids: #LINE# #TAB# #TAB# pkg_grah.add_node(node_id) #LINE# #TAB# pkg_grah.clear() #LINE# #TAB# return pkg_grah
#LINE# #TAB# if len(seq)!= 2: #LINE# #TAB# #TAB# raise ValueError('Input sequence must be two-dimensional.') #LINE# #TAB# return seq[0:2]
#LINE# #TAB# if name in klass.public_methods: #LINE# #TAB# #TAB# return klass.public_methods[name] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# internal_assert(len(tokens) == 3, ""invalid assignment function definition tokens"", tokens) #LINE# #TAB# (args, varargs, varkw, defaults) = tokens #LINE# #TAB# return {""args"": args, ""varkw"": varkw, ""defaults"": defaults}"
"#LINE# #TAB# if method in ('get', 'delete'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(params, dict): #LINE# #TAB# #TAB# method, #LINE# #TAB# #TAB# params = list(params.items()) #LINE# #TAB# if method in ('post', 'put'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return data['class']['vaule'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# if encoding is None: #LINE# #TAB# #TAB# #TAB# encoding = data['encoding'] #LINE# #TAB# #TAB# return data['class']['value'] #LINE# #TAB# return ''
"#LINE# #TAB# df = pd.DataFrame() #LINE# #TAB# np.random.seed(seed) #LINE# #TAB# for i in range(numDims): #LINE# #TAB# #TAB# x = np.random.randint(0, numActiveInputBits) #LINE# #TAB# #TAB# df[i] = np.dot(x, np.random.rand(numDims)) #LINE# #TAB# return df"
#LINE# #TAB# db_path = cls.get_redis_db_path() #LINE# #TAB# if not os.path.isdir(db_path): #LINE# #TAB# #TAB# return None #LINE# #TAB# return db_path
"#LINE# #TAB# with open(location, 'r') as csvfile: #LINE# #TAB# #TAB# return [yaml.safe_load(row) for row in csv.DictReader(csvfile)]"
"#LINE# #TAB# val = _hid_service_get_int_property(dev_ref, key) #LINE# #TAB# return val"
"#LINE# #TAB# unitaries = np.array(unitaries) #LINE# #TAB# for i, x in enumerate(unitaries): #LINE# #TAB# #TAB# if unitaries[i] == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if unitaries[i] > unitaries[i + 1][0] and unitaries[i + 1][1 #LINE# #TAB# #TAB# #TAB# ] > unitaries[i + 2][0]: #LINE# #TAB# #TAB# #TAB# unitaries[i + 1][1] = -unitaries[i + 1][1] #LINE# #TAB# commutres = [] #LINE# #TAB# for x, y in enumerate(unitaries): #LINE# #TAB# #TAB# computres.append(np.cumsum(x * x + y * y)) #LINE# #TAB# return commutres"
"#LINE# #TAB# client = ControllerClient() #LINE# #TAB# result = client.get_chute(name) #LINE# #TAB# if len(result) > 0: #LINE# #TAB# #TAB# click.echo('{count} {name} detected'.format(count=result[0].count, #LINE# #TAB# #TAB# #TAB# name=name)) #LINE# #TAB# elif len(result) > 0: #LINE# #TAB# #TAB# click.echo('{count} {name} detected'.format(count=result[0].count, #LINE# #TAB# #TAB# #TAB# name=name)) #LINE# #TAB# return result"
"#LINE# #TAB# global _has_chif #LINE# #TAB# if _has_chif is None: #LINE# #TAB# #TAB# _has_chif = True #LINE# #TAB# for run in _get_runs(run_name): #LINE# #TAB# #TAB# tmp = task in _get_tasks(run) #LINE# #TAB# #TAB# if isinstance(tmp, backend.Task): #LINE# #TAB# #TAB# #TAB# _has_chif = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return _has_chif"
#LINE# #TAB# model_one.update_t_facors(S12) #LINE# #TAB# model_two.update_t_facors(S12) #LINE# #TAB# model_one.update_t_facors(S12) #LINE# #TAB# model_two.update_t_facors(S12) #LINE# #TAB# return S12
#LINE# #TAB# if old_value is None: #LINE# #TAB# #TAB# return new_value is None #LINE# #TAB# elif new_value is None: #LINE# #TAB# #TAB# return old_value == new_value #LINE# #TAB# if np.isscalar(old_value) and np.isscalar(new_value): #LINE# #TAB# #TAB# return float(new_value) - float(old_value) #LINE# #TAB# return old_value == new_value
#LINE# #TAB# if wildcard_pattern.match(name1) is not None and wildcard_pattern.match(name2): #LINE# #TAB# #TAB# loss = False #LINE# #TAB# else: #LINE# #TAB# #TAB# loss = True #LINE# #TAB# return loss
#LINE# #TAB# validate(config) #LINE# #TAB# _validate_params(config) #LINE# #TAB# return config
#LINE# #TAB# terms = {} #LINE# #TAB# for table in database.tables: #LINE# #TAB# #TAB# terms[table.name] = {column.name: {'term': '{0}'.format(column.name)} for #LINE# #TAB# #TAB# #TAB# column in table.columns} #LINE# #TAB# return terms
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for node in tree.iter(): #LINE# #TAB# #TAB# #TAB# node.tag = '{}{}'.format(URI, node.tag) #LINE# #TAB# #TAB# yield node #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass"
#LINE# #TAB# decription = 'Plugin' #LINE# #TAB# try: #LINE# #TAB# #TAB# decription = cls.DESCRIPTION #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return decription
#LINE# #TAB# bytecode_mod = imp.load_source(str(bytecode_path)) #LINE# #TAB# try: #LINE# #TAB# #TAB# return bytecode_mod.glathida_module() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# daa = pd.DataFrame() #LINE# #TAB# daa['full_daa'] = {'HID': {'area_id': 0, 'Household_id': 0}, #LINE# #TAB# #TAB# 'HID_Household_ID': {'area_id': 1, 'Household_ID': None}, #LINE# #TAB# #TAB# 'HID_Household_ID': {'area_id': 2, 'Household_ID': None}} #LINE# #TAB# return daa"
#LINE# #TAB# if not i % 58: #LINE# #TAB# #TAB# return i #LINE# #TAB# return i
"#LINE# #TAB# if config is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(config, 'r') as f: #LINE# #TAB# #TAB# config = json.load(f) #LINE# #TAB# return config"
"#LINE# #TAB# if isinstance(maybe_dttm, datetime.datetime): #LINE# #TAB# #TAB# maybe_dttm = _format_datetime(maybe_dttm) #LINE# #TAB# return maybe_dttm"
#LINE# #TAB# new_lst = [] #LINE# #TAB# for el in lst: #LINE# #TAB# #TAB# if el!= index: #LINE# #TAB# #TAB# #TAB# new_lst.append(el) #LINE# #TAB# return new_lst
#LINE# #TAB# data = api.get_uer_info(user_access_token) #LINE# #TAB# return data['uer']['email']
"#LINE# #TAB# for key, value in obj.items(): #LINE# #TAB# #TAB# if not key.startswith('_') and type(value) is dict: #LINE# #TAB# #TAB# #TAB# obj[key] = replace_attributes(value) #LINE# #TAB# return obj"
"#LINE# #TAB# col = dataframe[colname] #LINE# #TAB# col_numerics = col.loc[col.apply(lambda x: isinstance(x, (int, float)))] #LINE# #TAB# dataframe.loc[col.notnull() & col.apply(lambda x: isinstance(x, #LINE# #TAB# #TAB# (int, float)))] = col_numerics.mode().replace('?', '') #LINE# #TAB# return dataframe"
"#LINE# #TAB# bstring = [] #LINE# #TAB# while len(bstring) > length: #LINE# #TAB# #TAB# bstring.append(rnd.randint(0, 255)) #LINE# #TAB# return bstring"
"#LINE# #TAB# windows = [] #LINE# #TAB# for t in time: #LINE# #TAB# #TAB# if method == 'between': #LINE# #TAB# #TAB# #TAB# windows.append(compute_window(t)) #LINE# #TAB# #TAB# elif method == 'date': #LINE# #TAB# #TAB# #TAB# windows.append(compute_date(t, col)) #LINE# #TAB# return windows"
#LINE# #TAB# u = urlparse(url) #LINE# #TAB# if u.username!= username: #LINE# #TAB# #TAB# raise RuntimeError('Invalid username') #LINE# #TAB# key = hashlib.sha256(u.password).hexdigest() #LINE# #TAB# key = base64.urlsafe_b64encode(key.encode('utf-8')) #LINE# #TAB# return key
#LINE# #TAB# try: #LINE# #TAB# #TAB# return os.environ['GE_EMAIL_PORT'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return '127.0.0.1'
"#LINE# #TAB# result = Shard tail(num_rows) #LINE# #TAB# for _ in range(num_rows): #LINE# #TAB# #TAB# result.append(sbody._hard_references_core(num_rows, sbody)) #LINE# #TAB# return result"
"#LINE# #TAB# result = {} #LINE# #TAB# for k, v in vertex_json.items(): #LINE# #TAB# #TAB# if type(v) is dict: #LINE# #TAB# #TAB# #TAB# escape_surface(v) #LINE# #TAB# #TAB# #TAB# result[k] = escape_surface(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[k] = v #LINE# #TAB# return result"
"#LINE# #TAB# for x in range(w - room_size): #LINE# #TAB# #TAB# for y in range(h - room_size): #LINE# #TAB# #TAB# #TAB# yield { #LINE# #TAB# #TAB# #TAB# #TAB# 'x': x + delete_chance, #LINE# #TAB# #TAB# #TAB# #TAB# 'y': y + delete_chance, #LINE# #TAB# #TAB# #TAB# }"
"#LINE# #TAB# min_n_obs = min(min_n_obs, len(group_sizes)) #LINE# #TAB# tile_overridden = np.ones(group_sizes, dtype=np.int32) * min_n_obs #LINE# #TAB# for group_size in group_sizes: #LINE# #TAB# #TAB# num_groups = len(group_sizes) #LINE# #TAB# #TAB# tile_overridden[num_groups] = group_sizes[0] #LINE# #TAB# return tile_overridden"
#LINE# #TAB# commands = [] #LINE# #TAB# root_dir = os.path.normpath(root_dir) #LINE# #TAB# while True: #LINE# #TAB# #TAB# cmd = 'cd {}'.format(root_dir) #LINE# #TAB# #TAB# if os.path.isdir(cmd): #LINE# #TAB# #TAB# #TAB# if empty_only: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# commands.append(cmd) #LINE# #TAB# #TAB# #TAB# root_dir = os.path.dirname(root_dir) #LINE# #TAB# #TAB# if not os.path.exists(root_dir): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return commands
#LINE# #TAB# indices = np.arange(model.parameters.data.shape[0]) #LINE# #TAB# np.random.shuffle(indices) #LINE# #TAB# model.parameters.data = indices #LINE# #TAB# return model
"#LINE# #TAB# x = ax.get_xlim() #LINE# #TAB# y = ax.get_ylim() #LINE# #TAB# return np.all(np.abs(x.min() - x.max()) < 1e-08, np.abs(y.min() - y.max())) and \ #LINE# #TAB# #TAB# not np.all(np.abs(y.min()) < 1e-08, np.abs(y.max() - y.min())) \ #LINE# #TAB# #TAB# and not np.all(np.abs(y.max() - y.min())) \ #LINE# #TAB# #TAB# and not np.all(np.abs(y.max() - y.min()) > 1e-08): #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# with gzip.open(filename, 'rb') as f: #LINE# #TAB# #TAB# worddict = pickle.load(f) #LINE# #TAB# return worddict"
"#LINE# #TAB# json_dict = {} #LINE# #TAB# for key, val in ordered_pairs: #LINE# #TAB# #TAB# if key in json_dict: #LINE# #TAB# #TAB# #TAB# raise ValueError('Duplicate key: %r' % (key,)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# json_dict[key] = val #LINE# #TAB# return json_dict"
"#LINE# #TAB# root = MainWindow() #LINE# #TAB# root.resize((SCREEN_WIDTH, SCREEN_HEIGHT)) #LINE# #TAB# root.show() #LINE# #TAB# screen_size = 75 #LINE# #TAB# for i in range(screen_size): #LINE# #TAB# #TAB# root.resize(i, screen_size[i]) #LINE# #TAB# crate_topic(root) #LINE# #TAB# return root"
#LINE# #TAB# #TAB# popsi = np.shape(ftrue) #LINE# #TAB# #TAB# fval = ftrue #LINE# #TAB# #TAB# have = np.zeros(popsi) #LINE# #TAB# #TAB# for i in range(popsi): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# fval = ftrue[i] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# have[i] = beta * (fval - ftrue) ** -alpha #LINE# #TAB# #TAB# return have
#LINE# #TAB# machine_ino = '' #LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# host_ino = socket.gethostname() #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if host_ino == '': #LINE# #TAB# #TAB# machine_ino = 'unknown' #LINE# #TAB# return machine_ino
#LINE# #TAB# dbref = {} #LINE# #TAB# for line in sacct_stream: #LINE# #TAB# #TAB# split_line = line.split() #LINE# #TAB# #TAB# if len(split_line) < 2: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# dbref[split_line[0]] = split_line[1] #LINE# #TAB# return dbref
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# f = formatter.randomInt #LINE# #TAB# #TAB# if f: #LINE# #TAB# #TAB# #TAB# value = f(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# f = lambda x: formatter.randomInt(x) #LINE# #TAB# return value
"#LINE# #TAB# for pos, refl, iBeg, iFin in profList: #LINE# #TAB# #TAB# yc[iBeg:iFin] += refl[11 + im] * refl[9 + im] * refl[10 + im] #LINE# #TAB# return yc"
#LINE# #TAB# map_data = {} #LINE# #TAB# if os.path.exists('pypi/map.txt'): #LINE# #TAB# #TAB# with open('pypi/map.txt') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# #TAB# if line!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# map_data[line] = get_url_from_string(line) #LINE# #TAB# return map_data
"#LINE# #TAB# position = [] #LINE# #TAB# for index, tag in enumerate(doc): #LINE# #TAB# #TAB# if index == 0: #LINE# #TAB# #TAB# #TAB# yield tag #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# position.append(tag) #LINE# #TAB# for index, tag in enumerate(doc): #LINE# #TAB# #TAB# if index == 0: #LINE# #TAB# #TAB# #TAB# yield tag #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield tag"
"#LINE# #TAB# obj = load_template(request, template_name) #LINE# #TAB# if obj: #LINE# #TAB# #TAB# obj = obj.render(request) #LINE# #TAB# if obj.get('ext'): #LINE# #TAB# #TAB# obj = obj.replace('.htm', '.html') #LINE# #TAB# return obj"
"#LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# return s.decode('utf-8') #LINE# #TAB# return s"
#LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# set_options(html_eelment_warning_inputs.color=color) #LINE# #TAB# return DEFAULT_HTML_EELMENT_WARNING_COLOR
"#LINE# #TAB# return [ #LINE# #TAB# #TAB# re.sub('(^|;)\\s*(\\d+)(\\.\\d+)$', '\\1', re.sub('^)\\s*(\\d+)\\2', '\\3', #LINE# #TAB# #TAB# re.sub('^)\\s*(\\d+)\\3', '\\1', re.sub('^\\s*(\\d+)\\2', '\\3', #LINE# #TAB# #TAB# re.sub('^\\s*(\\d+)\\4', '\\1', re.sub('^\\s*(\\d+)\\4', '\\1', re. #LINE# #TAB# #TAB# sub('^\\s*(\\d+)\\2', '\\3', re.sub('^\\s*(\\d+)\\3', '\\1', re. #LINE# #TAB# #TAB# sub('^\\s*(\\d+)\\2', '\\4', re.sub('^\\s*(\\d+)\\4', '\\1', re.sub( #LINE# #TAB# #TAB# )))) #LINE# #TAB# ]"
"#LINE# #TAB# if not protcol: #LINE# #TAB# #TAB# protcol = mzidtsvdata.HEADER_MASTER_PROT #LINE# #TAB# top_3_psms = generate_top_psms(psms, protcol) #LINE# #TAB# for protein in proteins: #LINE# #TAB# #TAB# prot_acc = protein[prottabledata.HEADER_PROTEIN] #LINE# #TAB# #TAB# prec_area = calculate_protein_precursor_quant(top_3_psms, prot_acc) #LINE# #TAB# #TAB# outprotein = {k: v for k, v in protein.items()} #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][prottabledata.HEADER_AREA #LINE# #TAB# #TAB# #TAB# ][None]] = str(prec_area) #LINE# #TAB# #TAB# yield outprotein"
"#LINE# #TAB# pattern = re.compile('\\S+') #LINE# #TAB# output = pattern.sub('', text) #LINE# #TAB# if dedupe: #LINE# #TAB# #TAB# return output.split(' ')[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return output"
#LINE# #TAB# it = iter(iterable) #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# yield next(it) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return
#LINE# #TAB# gevent = event.object #LINE# #TAB# if gevent.properties.recording: #LINE# #TAB# #TAB# info = gevent.properties.recording #LINE# #TAB# #TAB# if info: #LINE# #TAB# #TAB# #TAB# if info.scroll_offset < info.document.get_cursor_position().y: #LINE# #TAB# #TAB# #TAB# #TAB# info.scroll_offset = info.document.get_cursor_position().x #LINE# #TAB# #TAB# #TAB# if info.document.get_cursor_position().y <= info.document.get_cursor_height(): #LINE# #TAB# #TAB# #TAB# #TAB# info.scroll_offset += 1
"#LINE# #TAB# valid_url = False #LINE# #TAB# parsed = urlparse(url) #LINE# #TAB# if parsed.scheme!= 'https': #LINE# #TAB# #TAB# valid_url = True #LINE# #TAB# repository_name = parsed.netloc #LINE# #TAB# instances = [] #LINE# #TAB# while valid_url is not None: #LINE# #TAB# #TAB# data = parse_url(parsed.path) #LINE# #TAB# #TAB# username, password = data.netloc.split(':') #LINE# #TAB# #TAB# instance_id = data.netloc + ':' + username #LINE# #TAB# #TAB# instances.append(AccountInstance(instance_id, username, password)) #LINE# #TAB# #TAB# valid_url = True #LINE# #TAB# return instances"
"#LINE# #TAB# calc_dtype = type(a_dtype).__name__ + '_interp' #LINE# #TAB# res_dtype = type(b_dtype).__name__ + '_res' #LINE# #TAB# if a_dtype.names is not None: #LINE# #TAB# #TAB# calc_dtype = a_dtype.names #LINE# #TAB# if res_dtype.names is not None: #LINE# #TAB# #TAB# res_dtype = b_dtype.names #LINE# #TAB# return calc_dtype, res_dtype"
"#LINE# #TAB# result = [content] #LINE# #TAB# if isinstance(content, unicode): #LINE# #TAB# #TAB# result = content.encode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = cgi.escape(content) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return result"
#LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if'message_position' in cell: #LINE# #TAB# #TAB# #TAB# cell['message_position'] = interpolate_message_positions(cell['message_position']) #LINE# #TAB# #TAB# yield cell
#LINE# #TAB# registries = pd.DataFrame() #LINE# #TAB# for name in df.columns: #LINE# #TAB# #TAB# chrom = df[name].chrom #LINE# #TAB# #TAB# if not chrom in registries.index: #LINE# #TAB# #TAB# #TAB# registries[chrom] = [] #LINE# #TAB# #TAB# registries[chrom].append(name) #LINE# #TAB# df = df[registries.index] #LINE# #TAB# return df
"#LINE# #TAB#mediatype = request.POST.get('medi', None) #LINE# #TAB# if mediatype: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cache_key = request.GET['cache_key'] #LINE# #TAB# #TAB# #TAB# media = cache.get(cache_key) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# media = Media() #LINE# #TAB# #TAB# #TAB# if media: #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield media #LINE# #TAB# #TAB# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# #TAB# cache.delete(cache_key) #LINE# #TAB# else: #LINE# #TAB# #"
"#LINE# #TAB# out = [] #LINE# #TAB# for analysis in batch.get(""analysis""): #LINE# #TAB# #TAB# for region in analysis.get(""regions"", []): #LINE# #TAB# #TAB# #TAB# if region.get(""name"")!= ""Region"": #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if region.get(""type"")!= ""Function"": #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# out.append(region) #LINE# #TAB# return out"
#LINE# #TAB# client = cls.get_client() #LINE# #TAB# return client
#LINE# #TAB# assert seed >= 0 #LINE# #TAB# return seed - 1
"#LINE# #TAB# x, y, z = P #LINE# #TAB# return x, y"
#LINE# #TAB# new_state = copy.deepcopy(state) #LINE# #TAB# new_state.pickup = pickup #LINE# #TAB# new_state.reso = pickup.reso[0] + pickup.reso[1] #LINE# #TAB# return new_state
"#LINE# #TAB# conn = boto.connect_s3() #LINE# #TAB# key = conn.get_key(bucket) #LINE# #TAB# return bucket, key"
"#LINE# #TAB# x += marker.width #LINE# #TAB# y += marker.height #LINE# #TAB# return x, y"
"#LINE# #TAB# path = root.get_path(target, pred) #LINE# #TAB# n = len(path) #LINE# #TAB# filename = '' #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# edge = path[i] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if edge == target: #LINE# #TAB# #TAB# #TAB# n = i + 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# filename += edge #LINE# #TAB# return filename"
#LINE# #TAB# microphne = 10.0 ** (8.0 / 9.0 * (sensitivity + 273.15)) #LINE# #TAB# transfer_factor = (sensitivity + 7.0 / 13.0) * 1000.0 #LINE# #TAB# return transfer_factor
#LINE# #TAB# facets = [] #LINE# #TAB# for term in facets_terms: #LINE# #TAB# #TAB# if term.is_diag_component: #LINE# #TAB# #TAB# #TAB# facets.append(term) #LINE# #TAB# return facets
"#LINE# #TAB# if configuration.get('download_uri'): #LINE# #TAB# #TAB# uri = configuration['download_uri'] #LINE# #TAB# #TAB# conf = ET.SubElement(uri,'version_creation') #LINE# #TAB# #TAB# conf.text = '%s (download)s' % uri #LINE# #TAB# #TAB# application.session.add(conf) #LINE# #TAB# elif 'download_uri' in configuration: #LINE# #TAB# #TAB# uri = configuration['download_uri'] #LINE# #TAB# #TAB# session.add(uri) #LINE# #TAB# #TAB# application.session.commit() #LINE# #TAB# return uri"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return _history_stack[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return _history_stack[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return _history_stack[-1]
"#LINE# #TAB# if hasattr(module, '__doc__'): #LINE# #TAB# #TAB# doc = module.__doc__.splitlines()[-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# doc = '' #LINE# #TAB# return doc"
#LINE# #TAB# if type(secret) is not bytes: #LINE# #TAB# #TAB# secret = secret.encode('utf-8') #LINE# #TAB#ky_secret = md5(secret).hexdigest() #LINE# #TAB# returnky_secret
#LINE# #TAB# context = {} #LINE# #TAB# metric = Metric.objects.get(id=request.matchdict['metric_id']) #LINE# #TAB# if metric.secret_public: #LINE# #TAB# #TAB# context['public'] = True #LINE# #TAB# #TAB# context['quotes'] = [] #LINE# #TAB# return context
#LINE# #TAB# if G.is_directed(): #LINE# #TAB# #TAB# _LOGGER.warning('Graph has not isolated nodes') #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if line_or_func in (int, float): #LINE# #TAB# #TAB# return lines.index(line_or_func) + 1 #LINE# #TAB# elif line_or_func.isdigit(): #LINE# #TAB# #TAB# return lines.index(line_or_func) - 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# string_data = {} #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# string_data[key] = encode_array(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# string_data[key] = str(value) #LINE# #TAB# return string_data"
"#LINE# #TAB# md5sum = '' #LINE# #TAB# with open(src_file,'md5') as f: #LINE# #TAB# #TAB# md5sum = f.read() #LINE# #TAB# return md5sum"
#LINE# #TAB# global _ey_unc #LINE# #TAB# _ey_unc = key_func
"#LINE# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# if load_order: #LINE# #TAB# #TAB# #TAB# var_order = json.load(f) #LINE# #TAB# #TAB# bdd.add(var_order) #LINE# #TAB# return bdd"
"#LINE# #TAB# path = os.path.expanduser(path) #LINE# #TAB# lang1, sep, lang2 = os.path.splitext(path) #LINE# #TAB# if lang1 == '': #LINE# #TAB# #TAB# lang1 = 'python' #LINE# #TAB# if lang2 == '': #LINE# #TAB# #TAB# lang2 = 'python' #LINE# #TAB# return lang1, lang2"
#LINE# #TAB# matching_ids = [] #LINE# #TAB# for id_item in ids: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# matching_ids.append(int(id_item)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# matching_ids.append(id_item) #LINE# #TAB# return matching_ids
"#LINE# #TAB# DataFrame) ->pd.DataFrame: #LINE# #TAB# for interaction in interactions: #LINE# #TAB# #TAB# if interaction.iloc[0] not in complexes.iloc[1]: #LINE# #TAB# #TAB# #TAB# logging.info( #LINE# #TAB# #TAB# #TAB# #TAB#'remove interaction {} from zones - not in dns...'.format #LINE# #TAB# #TAB# #TAB# #TAB# (interaction.iloc[0]), #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return interactions"
"#LINE# #TAB# t = tf.convert_to_tensor(v) #LINE# #TAB# index = tf.expand_dims(t.get_shape().as_list(), axis=0) #LINE# #TAB# index_shape = tf.expand_dims(t.get_shape().as_list(), axis=0) #LINE# #TAB# total_shape = tf.convert_to_tensor(t.get_shape().as_list(), axis=0) #LINE# #TAB# total_size = tf.convert_to_tensor(t.get_size().as_list(), axis=0) #LINE# #TAB# return t.shape, t.shape[index_shape], t.shape[index_size], total_size"
#LINE# #TAB# with mp.Pool(inputs) as p: #LINE# #TAB# #TAB# return p.result()[0]
#LINE# #TAB# import sys #LINE# #TAB# try: #LINE# #TAB# #TAB# return [snpsts[i] for i in pkg_resources.iter_entry_points( #LINE# #TAB# #TAB# #TAB# 'chattie.plugins.snpsts')] #LINE# #TAB# except pkg_resources.DistributionNotFound: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# script_delete = nexus_client.scripts.delete(name) #LINE# #TAB# _LOGGER.info('Script deleted: %s', name) #LINE# #TAB# return script_delete"
#LINE# #TAB# #TAB# logging_rules = [] #LINE# #TAB# #TAB# if opts.quiet: #LINE# #TAB# #TAB# #TAB# logging_rules.append(logging.WARNING) #LINE# #TAB# #TAB# logging_rules.append(logging.INFO) #LINE# #TAB# #TAB# if opts.verbose: #LINE# #TAB# #TAB# #TAB# logging_rules.append(logging.DEBUG) #LINE# #TAB# #TAB# return logging_rules
#LINE# #TAB# chain_fields = {} #LINE# #TAB# if options is None: #LINE# #TAB# #TAB# options = {} #LINE# #TAB# for option in options: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# chain_fields[option] = os.environ[option] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if environment is None: #LINE# #TAB# #TAB# for option in COMPOSE_COMPATIBILITY_OPTIONS: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# chain_fields[option] = os.environ[option] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return chain_fields
#LINE# #TAB# data = iter(data) #LINE# #TAB# while size: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# next(data) #LINE# #TAB# #TAB# #TAB# size -= 1 #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# yield data[-size:]
"#LINE# #TAB# if not connection: #LINE# #TAB# #TAB# return #LINE# #TAB# targets = {} #LINE# #TAB# for key, value in connection.get_extra_info().items(): #LINE# #TAB# #TAB# if not key.startswith(""rethink://""): #LINE# #TAB# #TAB# #TAB# raise RethinkException(""Key %s is not a valid rethink connection key"" % key) #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# values = [v for v in value.values() if v is not None] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# values = [v for v in value.values() if isinstance(v, list)] #LINE# #TAB# #TAB# targets[key] = values #LINE# #TAB# return targets"
"#LINE# #TAB# return {'et_task_simx': {'path': os.environ['PATH'],'server_port': #LINE# #TAB# #TAB# None,'server_username': os.environ['SERVER_USERNAME'],'server_password': #LINE# #TAB# #TAB# os.environ['SERVER_PASSWORD'],'server_hostname': os.environ[ #LINE# #TAB# #TAB# 'SERVER_HOST'],'server_port': int(os.environ['SERVER_PORT']), #LINE# #TAB# #TAB#'server_name': os.environ['SERVER_NAME']}"
"#LINE# #TAB# o_result = o_evaluate_tree(content_path, content) #LINE# #TAB# if o_result == 'True': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if not isinstance(pathname, str): #LINE# #TAB# #TAB# return pathname #LINE# #TAB# pathname = pathname.replace('\\', '\\\\') #LINE# #TAB# pathname = pathname.replace('&', '\\&') #LINE# #TAB# pathname = pathname.replace('<', '\\<') #LINE# #TAB# pathname = pathname.replace('>', '\\>') #LINE# #TAB# return pathname"
#LINE# #TAB# response.status_int = 200 #LINE# #TAB# if response.status_int == 429: #LINE# #TAB# #TAB# response.headers['Cache-Control'] ='max-age=0' #LINE# #TAB# #TAB# response.headers['X-Requested-With'] = 'XMLHttpRequest' #LINE# #TAB# return response
"#LINE# #TAB# if q is not None: #LINE# #TAB# #TAB# qdict = {} #LINE# #TAB# #TAB# for k, v in q.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, astropy.units.Quantity): #LINE# #TAB# #TAB# #TAB# #TAB# qdict[k] = quote(v) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# qdict[k] = v #LINE# #TAB# #TAB# return qdict"
"#LINE# #TAB# if params is None: #LINE# #TAB# #TAB# return url #LINE# #TAB# p = {} #LINE# #TAB# for key in params: #LINE# #TAB# #TAB# if isinstance(params[key], list): #LINE# #TAB# #TAB# #TAB# p[key] = serialize_unsigned_encode_parasm(url, params[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# p[key] = params[key] #LINE# #TAB# return url, p"
"#LINE# #TAB# lon0 = np.mean(polygon[:, (0)]) #LINE# #TAB# lat0 = np.mean(polygon[:, (1)]) #LINE# #TAB# lon1 = np.mean(polygon[:, (1)]) #LINE# #TAB# a = 0 #LINE# #TAB# for i in range(0, len(polygon)): #LINE# #TAB# #TAB# b = colors_gradient(polygon[i], polygon[i], lon0, lat0) #LINE# #TAB# #TAB# a += b #LINE# #TAB# return a"
"#LINE# #TAB# from six import StringIO #LINE# #TAB# with open(file, 'r') as f: #LINE# #TAB# #TAB# config = yaml.load(f) #LINE# #TAB# return config"
#LINE# #TAB# dbs = [] #LINE# #TAB# node = section #LINE# #TAB# while node.parent: #LINE# #TAB# #TAB# dbs.append(node) #LINE# #TAB# #TAB# node = node.parent #LINE# #TAB# return dbs
"#LINE# #TAB# html = ET.tostring(elem, encoding='utf-8') #LINE# #TAB# if html.tag == 'html': #LINE# #TAB# #TAB# html = html.replace('&quot;', '""').replace('&apos;', ""'"") #LINE# #TAB# #TAB# return html #LINE# #TAB# if pretty_print: #LINE# #TAB# #TAB# html = html.replace('&amp;', '&') #LINE# #TAB# #TAB# return html #LINE# #TAB# return html"
"#LINE# #TAB# url = '/'.join([table_name, objid]) #LINE# #TAB# return url"
"#LINE# #TAB# avg_x = np.average(points[:, (0)], axis=0) #LINE# #TAB# avg_y = np.average(points[:, (1)], axis=0) #LINE# #TAB# return avg_x / avg_y"
"#LINE# #TAB# if friendly: #LINE# #TAB# #TAB# return 'User role restriction' #LINE# #TAB# role = role_restrictions.get(current_user.name, current_user.email) #LINE# #TAB# return current_user.name"
"#LINE# #TAB# lines = string.count('\n', 0, line_offset) #LINE# #TAB# if lines > 0: #LINE# #TAB# #TAB# first_line = string.rfind('\n', line_offset, index) #LINE# #TAB# #TAB# if first_line == -1: #LINE# #TAB# #TAB# #TAB# return line_offset, index #LINE# #TAB# #TAB# second_line = string.rfind('\n', line_offset, index) #LINE# #TAB# #TAB# if second_line == -1: #LINE# #TAB# #TAB# #TAB# return line_offset, index #LINE# #TAB# #TAB# column = second_line + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# column = index - first_line + 1 #LINE# #TAB# return line_offset, column"
"#LINE# #TAB# new_message = None #LINE# #TAB# for message in messages: #LINE# #TAB# #TAB# new_message = cls.walk_format_record(message) #LINE# #TAB# #TAB# if new_message is None: #LINE# #TAB# #TAB# #TAB# new_message = message #LINE# #TAB# #TAB# elif isinstance(message, str): #LINE# #TAB# #TAB# #TAB# new_message = cls.from_string(message) #LINE# #TAB# #TAB# yield new_message"
#LINE# #TAB# links = [] #LINE# #TAB# for image in skil.images.images: #LINE# #TAB# #TAB# if image.type == resource_type: #LINE# #TAB# #TAB# #TAB# links.append(image) #LINE# #TAB# return links
"#LINE# #TAB# _, circle_cov, _ = impact_function #LINE# #TAB# circle_cov_dir = os.path.dirname(circle_cov) #LINE# #TAB# path_to_flow_pe = os.path.join(circle_cov_dir, 'flow_pe/{}'.format(circle_cov_dir)) #LINE# #TAB# return [circle_cov_to_flow_pe[path_to_flow_pe[0]], circle_cov_to_flow_pe[path_to_flow_pe[1]], #LINE# #TAB# #TAB# circle_cov_to_flow_pe[0]"
"#LINE# #TAB# if is_list(obj): #LINE# #TAB# #TAB# return concatenate(obj) #LINE# #TAB# elif isinstance(obj, unicode): #LINE# #TAB# #TAB# return obj.encode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# if enabled is True: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# space = get_keyspace(word) #LINE# #TAB# try: #LINE# #TAB# #TAB# return space[0].capitalize() #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return 'n/a'
#LINE# #TAB# if PY3: #LINE# #TAB# #TAB# return obj #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# except: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# coords = [] #LINE# #TAB# for group in config: #LINE# #TAB# #TAB# for x in group: #LINE# #TAB# #TAB# #TAB# if x not in coords: #LINE# #TAB# #TAB# #TAB# #TAB# coords.append(x) #LINE# #TAB# #TAB# #TAB# elif isinstance(config[x], dict): #LINE# #TAB# #TAB# #TAB# #TAB# for k, v in config[x].items(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if not isinstance(v, list): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# coords[-1].append(k) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# coords[-1].append(v) #LINE# #TAB# return coords"
#LINE# #TAB# from cfg.CONF import settings #LINE# #TAB# table_lower = table.lower() #LINE# #TAB# if table_lower == 'ipv4': #LINE# #TAB# #TAB# return socket.AF_INET #LINE# #TAB# elif table_lower == 'ipv6': #LINE# #TAB# #TAB# return socket.AF_INET6 #LINE# #TAB# else: #LINE# #TAB# #TAB# return socket.AF_INET
"#LINE# #TAB# codig = '' #LINE# #TAB# while n: #LINE# #TAB# #TAB# n, codig = divmod(n, 5) #LINE# #TAB# #TAB# if not codig: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return codig"
"#LINE# #TAB# if isinstance(obj, bool): #LINE# #TAB# #TAB# return 'true' #LINE# #TAB# return 'false'"
"#LINE# #TAB# for k, v in nested_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# next_widget(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return v"
#LINE# #TAB# pare_b = [] #LINE# #TAB# for field in description.split(' '): #LINE# #TAB# #TAB# pare_b.append(field) #LINE# #TAB# return pare_b
#LINE# #TAB# client = Client() #LINE# #TAB# global singleton_instance #LINE# #TAB# if singleton_instance is None: #LINE# #TAB# #TAB# singleton_instance = client.Session() #LINE# #TAB# return singleton_instance
#LINE# #TAB# typ = idl_type(i) #LINE# #TAB# if typ == IDL_STRING: #LINE# #TAB# #TAB# return i << 8 #LINE# #TAB# elif typ == IDL_NUMBER: #LINE# #TAB# #TAB# return i << 16 #LINE# #TAB# elif typ == IDL_FLOAT: #LINE# #TAB# #TAB# return i << 8 #LINE# #TAB# elif typ == IDL_BOOLEAN: #LINE# #TAB# #TAB# return i & 255 #LINE# #TAB# elif typ == IDL_BOOLEAN: #LINE# #TAB# #TAB# return i & 255 #LINE# #TAB# else: #LINE# #TAB# #TAB# return i
#LINE# #TAB# cls.task_id = task_id #LINE# #TAB# cls.save_tas_user(task_id) #LINE# #TAB# if create: #LINE# #TAB# #TAB# cls.create_tas_user(task_id) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# word_dn = dn + '/' + filtr + '/' + attr #LINE# #TAB# if not conn.search(word_dn): #LINE# #TAB# #TAB# return None #LINE# #TAB# return word_dn
#LINE# #TAB# re_result = False #LINE# #TAB# for result in state['results']: #LINE# #TAB# #TAB# if tap_stream_id == result['tap_stream_id']: #LINE# #TAB# #TAB# #TAB# if re_result: #LINE# #TAB# #TAB# #TAB# #TAB# re_result = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if not re_result: #LINE# #TAB# #TAB# return None #LINE# #TAB# return re_result
"#LINE# #TAB# root = ElementTree.fromstring(raw) #LINE# #TAB# qualified_name = root.tag.rsplit('.', 1)[0] #LINE# #TAB# attrib_dict = {} #LINE# #TAB# for child in root: #LINE# #TAB# #TAB# attrib_dict[child.tag] = child.text.strip() #LINE# #TAB# return qualified_name, attrib_dict"
#LINE# #TAB# try: #LINE# #TAB# #TAB# int(val) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if edges is None: #LINE# #TAB# #TAB# edges = graph.edges() #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# graph.edge_dict[edge]['highlight'] = graph.edge_dict[edge]['highlight'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# graph.edge_dict[edge] = {'highlight': ''}
#LINE# #TAB# filename = os.path.abspath(filename) #LINE# #TAB# while os.path.isfile(filename): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.unlink(filename) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# else: #LINE# #TAB# #TAB# return filename
#LINE# #TAB# if dir == 0: #LINE# #TAB# #TAB# return Color.BLUE #LINE# #TAB# elif dir == 1: #LINE# #TAB# #TAB# return Color.GREEN #LINE# #TAB# else: #LINE# #TAB# #TAB# return Color.BLUE
"#LINE# #TAB# if pexpect is None: #LINE# #TAB# #TAB# raise ImportError(""pexpect unavailable, use paramiko"") #LINE# #TAB# cmd = ['latexsimplify', '-i', server] #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# cmd.append('--keyfile=' + keyfile) #LINE# #TAB# p = Popen(cmd, stdout=PIPE, stderr=PIPE) #LINE# #TAB# out, err = p.communicate() #LINE# #TAB# if p.returncode!= 0: #LINE# #TAB# #TAB# raise RuntimeError(err) #LINE# #TAB# out = out.decode('utf-8').strip() #LINE# #TAB# keydata = p.communicate()[0].strip() #LINE# #TAB# with open(keydata, 'r') as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# return keydata"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return 1 - np.sqrt(np.mean(np.log(numbers))) #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# return np.nan
"#LINE# #TAB# for symbol in get_equivalent_v1_symbols(span): #LINE# #TAB# #TAB# yield symbol, 1"
"#LINE# #TAB# ret = int(integer_string) #LINE# #TAB# if ret < 0 or ret == 0 and strict: #LINE# #TAB# #TAB# raise ValueError() #LINE# #TAB# if cutoff: #LINE# #TAB# #TAB# return min(ret, cutoff) #LINE# #TAB# return ret"
"#LINE# #TAB# ret = [] #LINE# #TAB# for m in p.monomers: #LINE# #TAB# #TAB# resiues = len(m.get_residues()) #LINE# #TAB# #TAB# for i, r in enumerate(resiues): #LINE# #TAB# #TAB# #TAB# if not r.is_tur: #LINE# #TAB# #TAB# #TAB# #TAB# ret.append([m, i]) #LINE# #TAB# return ret"
"#LINE# #TAB# d = dict(x=x, y=y) #LINE# #TAB# w = d.get('w') #LINE# #TAB# h = d.get('h') #LINE# #TAB# if w: #LINE# #TAB# #TAB# w2 = int(w.get('w')) #LINE# #TAB# #TAB# h3 = int(h.get('h')) #LINE# #TAB# #TAB# if h3: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# w2 = int(h.get('v')) #LINE# #TAB# #TAB# #TAB# h3 = int(h.get('v')) #LINE# #TAB# return w, h, w2, h3"
#LINE# #TAB# strainT = [strainT[1] for strainT in genesO] #LINE# #TAB# lG = len(geneT) #LINE# #TAB# strainT = [strainT[0] for geneT in genesT] #LINE# #TAB# for g in genesO: #LINE# #TAB# #TAB# if g in strainT: #LINE# #TAB# #TAB# #TAB# strainT[g] = strainT[g] / lG #LINE# #TAB# return strainT
"#LINE# #TAB# try: #LINE# #TAB# #TAB# name = obj.name #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# name = str(obj) #LINE# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# name = name[1:] #LINE# #TAB# if name in _atom_parameter: #LINE# #TAB# #TAB# value = _atom_parameter[name] #LINE# #TAB# else: #LINE# #TAB# #TAB# value = None #LINE# #TAB# return name, value"
#LINE# #TAB# if not doc: #LINE# #TAB# #TAB# return [] #LINE# #TAB# entities = [] #LINE# #TAB# for entity in doc: #LINE# #TAB# #TAB# if len(entity['children']) == 1: #LINE# #TAB# #TAB# #TAB# entity['children'] = [entity['children'][0]] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# entities.append(entity) #LINE# #TAB# if len(entities) > 0: #LINE# #TAB# #TAB# return merge_entities(*entities) #LINE# #TAB# else: #LINE# #TAB# #TAB# return entities
"#LINE# #TAB# _lDay = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', #LINE# #TAB# #TAB# 'Saturday', 'Sunday'] #LINE# #TAB# if day is None: #LINE# #TAB# #TAB# day = _lDay[1] #LINE# #TAB# _lDay.insert(day, 4) #LINE# #TAB# _lDay.insert(day, 5) #LINE# #TAB# return _lDay"
#LINE# #TAB# maps = {} #LINE# #TAB# for key in config: #LINE# #TAB# #TAB# if'map_url' in config[key]: #LINE# #TAB# #TAB# #TAB# maps[key] = config[key]['map_url'] #LINE# #TAB# return maps
#LINE# #TAB# key = crypto.decode_address(address) #LINE# #TAB# private_key = crypto.decode_private_key(key) #LINE# #TAB# return private_key
#LINE# #TAB# if 'index' in params: #LINE# #TAB# #TAB# return params['index'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return uuid.uuid4().hex
"#LINE# #TAB# msg_sparse = np.random.normal(0, 1, size=(3, 3)) #LINE# #TAB# msg_sparse /= np.linalg.norm(msg_sparse) #LINE# #TAB# return msg_sparse"
"#LINE# #TAB# lp_names = [] #LINE# #TAB# p_names = [] #LINE# #TAB# for child in fnode.body: #LINE# #TAB# #TAB# if isinstance(child, astroid.Name): #LINE# #TAB# #TAB# #TAB# lp_names.append(child.id) #LINE# #TAB# #TAB# elif isinstance(child, astroid.Attribute): #LINE# #TAB# #TAB# #TAB# lp_names.extend(gt_paam_group(child)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lp_names.append(child.id) #LINE# #TAB# if len(lp_names) > 1: #LINE# #TAB# #TAB# lp_names = lp_names[1:] #LINE# #TAB# return lp_names"
"#LINE# #TAB# d_space = [] #LINE# #TAB# for line in fin_txt.split('\n'): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line == """": #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if len(line) < 3 or line[1] == "" "": #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# d_space.append(line.replace("" "", """")) #LINE# #TAB# return d_space"
#LINE# #TAB# try: #LINE# #TAB# #TAB# db_session = request.session #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# auth_strength = db_session.query(Identity).one() #LINE# #TAB# except NoResultFound: #LINE# #TAB# #TAB# return None #LINE# #TAB# if auth_strength == None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return auth_strength[0]
#LINE# #TAB# dev_template = InputTemplate(dev_type=dev_type) #LINE# #TAB# dev_obj = dev_template.create_input(dev_name) #LINE# #TAB# return dev_obj
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for k, v in obj.items(): #LINE# #TAB# #TAB# #TAB# obj[k] = TitleCase(v) #LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# obj[item] = TitleCase(item) #LINE# #TAB# return obj"
"#LINE# #TAB# _, ext = os.path.splitext(path) #LINE# #TAB# if ext == '.pkl': #LINE# #TAB# #TAB# return 'pkl' #LINE# #TAB# if ext == '.pickle': #LINE# #TAB# #TAB# return 'pickle' #LINE# #TAB# return 'file'"
"#LINE# #TAB# items = request.get_json(device, 'items', None) #LINE# #TAB# if items is not None: #LINE# #TAB# #TAB# for key in items: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# items[key] = items[key].split('=') #LINE# #TAB# #TAB# #TAB# #TAB# del items[key] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
#LINE# #TAB# day_of_week = (weekday - 1) * 7 + 1 #LINE# #TAB# if is_leap_year(year): #LINE# #TAB# #TAB# day_of_week = day_of_week - 1 #LINE# #TAB# elif is_leap_week(year): #LINE# #TAB# #TAB# day_of_week = day_of_week - 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# day_of_week = None #LINE# #TAB# return day_of_week
#LINE# #TAB# for rr in BLACKLIST_URLS: #LINE# #TAB# #TAB# if url.startswith(rr): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if val is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(val, bool): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif hasattr(val, '__iter__'): #LINE# #TAB# #TAB# return any(still_mounted_binary(v) for v in val) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if len(results) > 1: #LINE# #TAB# #TAB# return [view_row(r) for r in results] #LINE# #TAB# return [view_row(r) for r in results]
#LINE# #TAB# if np.isscalar(data) or len(data)!= 1: #LINE# #TAB# #TAB# return data #LINE# #TAB# key = list(data.keys())[0] #LINE# #TAB# if len(data[key]) == 1 and key in dataset.vdims: #LINE# #TAB# #TAB# return data[key][0]
#LINE# #TAB# if ndim is None: #LINE# #TAB# #TAB# ndim = x.ndim #LINE# #TAB# assert len(x.shape) == 2 #LINE# #TAB# for i in range(ndim - 1): #LINE# #TAB# #TAB# if x.shape[i]!= 2: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# split_diectory_i = [] #LINE# #TAB# cur_path = path #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cur_path = os.path.dirname(path) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if not cur_path: #LINE# #TAB# #TAB# #TAB# split_diectory_i.append(cur_path) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.mkdir(cur_path) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
"#LINE# #TAB# if remove_brackets_content: #LINE# #TAB# #TAB# trans = pangloss.remove_content_in_brackets(trans, ""[]"") #LINE# #TAB# trans = fr_nlp("" "".join(trans.split()[:])) #LINE# #TAB# trans = "" "".join([token.lower_ for token in trans if not token.is_punct]) #LINE# #TAB# return trans"
#LINE# #TAB# a = name.count('.') #LINE# #TAB# if a: #LINE# #TAB# #TAB# ext = name.split('.')[-1] #LINE# #TAB# #TAB# if ext in FEATURE_SUFFIXES: #LINE# #TAB# #TAB# #TAB# return FEATURE_SUFFIXES[ext] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# rst = '\n\n' #LINE# #TAB# for field in entry.keys(): #LINE# #TAB# #TAB# rst += '#TAB# - *%s: %s\n' % (field, entry[field]) #LINE# #TAB# #TAB# rst += '\n' #LINE# #TAB# for field in entry.keys(): #LINE# #TAB# #TAB# rst += '#TAB# - *%s: %s\n' % (field, entry[field]) #LINE# #TAB# markdown = '\n\n' #LINE# #TAB# for field in entry.keys(): #LINE# #TAB# #TAB# rst += '#TAB# - *%s: %s\n' % (field, entry[field]) #LINE# #TAB# markdown += '\n\n' #LINE# #TAB# return rst"
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# else: #LINE# #TAB# #TAB# return value
#LINE# #TAB# power_state = __nova.compute.power_state(pvm_state) #LINE# #TAB# if power_state == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return power_state
#LINE# #TAB# slope = y2 - y1 * x2 - x1 #LINE# #TAB# return slope
#LINE# #TAB# try: #LINE# #TAB# #TAB# next(data) #LINE# #TAB# #TAB# return True #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# d = datetime.date(iso_year, 1, 4) #LINE# #TAB# units = d.units #LINE# #TAB# return units"
#LINE# #TAB# t.value = t.value[1:].upper() #LINE# #TAB# return t
"#LINE# #TAB# sa = mac2str(source) #LINE# #TAB# sa += b'\x00' * (4 - len(sa)) #LINE# #TAB# mac = mac2str(dest) #LINE# #TAB# mic = MIC(mic_key, sa, sa + b'\x00' * (4 - len(sa)) + b'\x00' * (4 - len(sa)) + data) #LINE# #TAB# return mic.decrypt(data)[:4]"
"#LINE# #TAB# spectral_radius = np.sum(np.power(a, 2), axis=0) #LINE# #TAB# return spectral_radius"
"#LINE# #TAB# meta8chan = {} #LINE# #TAB# for op, blob_name, version in ssa: #LINE# #TAB# #TAB# if version is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# meta8chan[op] = blob_name, version #LINE# #TAB# return meta8chan"
"#LINE# #TAB# t = [x1, y1] #LINE# #TAB# while t[0] < n * (x2 - x1): #LINE# #TAB# #TAB# t.append((x2, y2)) #LINE# #TAB# #TAB# x += n * t[0] #LINE# #TAB# while t[1] < n * (x2 - x1): #LINE# #TAB# #TAB# x += n #LINE# #TAB# #TAB# y += n * t[1] #LINE# #TAB# return x, y"
#LINE# #TAB# if pObj.__class__.__name__ == 'ClusterNode': #LINE# #TAB# #TAB# assert pObj.cluster_node._id == pObj.h_node._id #LINE# #TAB# elif pObj.__class__.__name__ == 'Hypothesis_Node': #LINE# #TAB# #TAB# assert pObj.h_node._id == pObj.h_node._id #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError
"#LINE# #TAB# txt = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# txt = f.read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return txt"
"#LINE# #TAB# orig_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# orig_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# with contextlib.closing(orig_socket) as sock: #LINE# #TAB# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# #TAB# yield ctx.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# orig_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# sock.close() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return is_valid_thread_id(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# logger.info('Aborted.') #LINE# #TAB# #TAB# raise
#LINE# #TAB# try: #LINE# #TAB# #TAB# for part in value.split(): #LINE# #TAB# #TAB# #TAB# yield int(part) #LINE# #TAB# #TAB# #TAB# yield float(part) #LINE# #TAB# except: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield value #LINE# #TAB# #TAB# #TAB# yield value #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass
#LINE# #TAB# n = len(node.children) #LINE# #TAB# if n == 1: #LINE# #TAB# #TAB# n2 = [node[0]] #LINE# #TAB# elif n == 2: #LINE# #TAB# #TAB# n1 = [node[0]] #LINE# #TAB# else: #LINE# #TAB# #TAB# n1 = [node[0]] #LINE# #TAB# return n1
"#LINE# #TAB# _path = Path(AbstractSample._full_path('map_inc5000.csv')) #LINE# #TAB# df = pd.read_csv(_path, encoding='latin1') #LINE# #TAB# return df.iloc[:size]"
#LINE# #TAB# if source not in source_data: #LINE# #TAB# #TAB# raise KeyError('Unknown source {}'.format(source)) #LINE# #TAB# return source_data[source]['ge_settings']
"#LINE# #TAB# if not isinstance(net, PetriNet): #LINE# #TAB# #TAB# raise TypeError('Net must be of type PetriNet') #LINE# #TAB# if not isinstance(place, Place): #LINE# #TAB# #TAB# raise TypeError('Place must be of type PetriNet') #LINE# #TAB# net = copy.deepcopy(net) #LINE# #TAB# del net.places[place.id] #LINE# #TAB# return net"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = cdf.name #LINE# #TAB# temp = deepcopy(cdf) #LINE# #TAB# s = Suite(name=name) #LINE# #TAB# for ind, val in cdf.iterrows(): #LINE# #TAB# #TAB# temp.append(val) #LINE# #TAB# for ind, val in cdf.iterrows(): #LINE# #TAB# #TAB# temp.append(split_suite_build_log(val, ind)) #LINE# #TAB# return temp"
"#LINE# #TAB# f = open(filename, 'rb') #LINE# #TAB# f.seek(2048) #LINE# #TAB# posbyte = 0 #LINE# #TAB# allsentences = '' #LINE# #TAB# for _ in list(range(32)): #LINE# #TAB# #TAB# tt = f.read(32) #LINE# #TAB# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# #TAB# if s1!= '': #LINE# #TAB# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# #TAB# posbyte += 32 #LINE# #TAB# #TAB# s2 = tt.strip('\x00') #LINE# #TAB# #TAB# if s2!= '': #LINE# #TAB# #TAB# #TAB# allsentences += s2 + '\n' #LINE# #TAB# #TAB# posbyte += 32 #LINE# #TAB# f.close() #LINE# #TAB# return allsentences"
#LINE# #TAB# with open(json_fn) as f: #LINE# #TAB# #TAB# parsed_dict = json.load(f) #LINE# #TAB# return parsed_dict
"#LINE# #TAB# if num_bins is None: #LINE# #TAB# #TAB# num_bins=len(G.nodes()) #LINE# #TAB# bin_labels = range(num_bins) #LINE# #TAB# attr_values = pd.Series([data[attr] for u, v, key, data in G.nodes(data=True)]) #LINE# #TAB# cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels) #LINE# #TAB# colors = get_ode_polygon_colors(num_bins, cmap, start, stop) #LINE# #TAB# node_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats] #LINE# #TAB# return node_colors"
"#LINE# #TAB# if schema.get(""type"") == ""array"": #LINE# #TAB# #TAB# return ""array["" #LINE# #TAB# elif schema.get(""type"") == ""array2"": #LINE# #TAB# #TAB# return ""array["" #LINE# #TAB# elif schema.get(""type"") == ""array3"": #LINE# #TAB# #TAB# return ""array["" #LINE# #TAB# #TAB# elif schema.get(""type"") == ""object"": #LINE# #TAB# #TAB# #TAB# return ""object["" #LINE# #TAB# #TAB# #TAB# elif schema.get(""type"") == ""array"": #LINE# #TAB# #TAB# #TAB# #TAB# return ""array["" #LINE# #TAB# #TAB# #TAB# #TAB# elif schema.get(""type"") == ""array4""]: #LINE# #TAB# #TAB# #TAB# #TAB# return ""array["" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return """""
#LINE# #TAB# try: #LINE# #TAB# #TAB# return array.get_volume(module.params['mtu']) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# for part in itertools.count(1): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# part = sender.split('|')[1] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if part == 'name': #LINE# #TAB# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# #TAB# 'name': sender, #LINE# #TAB# #TAB# #TAB# #TAB# 'part': part #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# #TAB# 'name': sender, #LINE# #TAB# #TAB# #TAB# #TAB# 'part': part #LINE# #TAB# #TAB# #TAB# }"
#LINE# #TAB# cache.post_fifo(Event(signal=signals.cache_file_access_waiting)) #LINE# #TAB# return return_status.HANDLED
"#LINE# #TAB# b0 = 0.0215 #LINE# #TAB# b1 = 0.2122 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.00084 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['In'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# elements = browser.execute_script( #LINE# #TAB# #TAB#'return ($ || jQuery)(arguments[0]).get();', selector) #LINE# #TAB# html = elements[0] #LINE# #TAB# output = browser.execute_script( #LINE# #TAB# #TAB#'return ($ || jQuery)(arguments[0]).get();', selector) #LINE# #TAB# return output"
#LINE# #TAB# ar = np.asanyarray(ar) #LINE# #TAB# ar.sort() #LINE# #TAB# if sort: #LINE# #TAB# #TAB# ar = ar[ar.argsort()] #LINE# #TAB# return ar
"#LINE# #TAB# if not isinstance(xmlid, basestring): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# xmlid = unicode(xmlid) #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if noupdate: #LINE# #TAB# #TAB# cr.update(xmlid) #LINE# #TAB# return cr"
"#LINE# #TAB# results = [] #LINE# #TAB# if isinstance(string, list): #LINE# #TAB# #TAB# for e in string: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# results.append(datetime.strptime(e, '%Y-%m-%dT%H:%M:%S.%f' % #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# (width - len(e))) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# results.append(e) #LINE# #TAB# return results"
"#LINE# #TAB# start = sample.min() #LINE# #TAB# end = sample.max() + start #LINE# #TAB# return sample - start, end"
#LINE# #TAB# rewrite_key = impact_function.function_name.split('/')[0] #LINE# #TAB# found = False #LINE# #TAB# while not found: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# impact_function.rewrite() #LINE# #TAB# #TAB# #TAB# found = True #LINE# #TAB# #TAB# except NotImplementedError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if rewrite_key =='stop': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if os.access(filepath, os.W_OK): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# px, py = cv2.contourArea(contour) #LINE# #TAB# px_max = px[0] * px[1] + py[1] * py[0] #LINE# #TAB# py_max = py[0] * py[1] + py[1] * py[1] #LINE# #TAB# return px_max, py_max"
#LINE# #TAB# new_data = [] #LINE# #TAB# for data_dict in data_dicts: #LINE# #TAB# #TAB# for item in data_dict.values(): #LINE# #TAB# #TAB# #TAB# formatted_item = format_value_for_spreadsheet(item) #LINE# #TAB# #TAB# #TAB# if formatted_item: #LINE# #TAB# #TAB# #TAB# #TAB# new_data.append(formatted_item) #LINE# #TAB# return new_data
"#LINE# #TAB# global data #LINE# #TAB# val = data[key] #LINE# #TAB# if val is None: #LINE# #TAB# #TAB# return #LINE# #TAB# if isinstance(val, dict): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# val = json.loads(val) #LINE# #TAB# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# if not isinstance(val, list): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# val = json.loads(val) #LINE# #TAB# #TAB# except json.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# data[key] = val #LINE# #TAB# return val"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# response = requests.get(url, headers={'User-Agent': #LINE# #TAB# #TAB# #TAB# AKE_USER_AGENT}) #LINE# #TAB# #TAB# return response.text #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# raise AKE_USER_AGENT #LINE# #TAB# except requests.exceptions.HTTPError as e: #LINE# #TAB# #TAB# raise AKE_USER_AGENT #LINE# #TAB# except requests.exceptions.Timeout: #LINE# #TAB# #TAB# raise AKE_USER_AGENT #LINE# #TAB# except requests.exceptions.HTTPError as e: #LINE# #TAB# #TAB# raise AKE_USER_AGENT #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# raise AKE_USER_AGENT"
"#LINE# #TAB# p = X_history.shape[1] #LINE# #TAB# X_train = sparse.dia_matrix((X_test, 0), shape=(n_components, p)) #LINE# #TAB# X_val = sparse.dia_matrix((X_test, 0), shape=(n_components, p)) #LINE# #TAB# for i in range(n_components): #LINE# #TAB# #TAB# t = np.random.randn(p) #LINE# #TAB# #TAB# X_train.data[i, :] = sst.svd(X_train.data, t) #LINE# #TAB# #TAB# X_history.data[i, :] = t #LINE# #TAB# res = sparse.dia_matrix((X_train, X_val), shape=(n_components, p)) #LINE# #TAB# res = res.transpose() #LINE# #TAB# return res"
#LINE# #TAB# idx_order = np.argsort(geno_samples)[::-1] #LINE# #TAB# return x_samples[idx_order]
"#LINE# #TAB# if not os.path.isdir(dirpath): #LINE# #TAB# #TAB# raise SuppressError('missing {}'.format(dirpath)) #LINE# #TAB# files = [] #LINE# #TAB# for root, subdirs, files in os.walk(dirpath): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# filepath = os.path.join(root, file) #LINE# #TAB# #TAB# #TAB# remove_files(filepath) #LINE# #TAB# #TAB# #TAB# files.append(filepath) #LINE# #TAB# return files"
"#LINE# #TAB# socks_proxy = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# socks_proxy.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# socks_proxy.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) #LINE# #TAB# socks_proxy.setblocking(0) #LINE# #TAB# socks_proxy.bind(proxy) #LINE# #TAB# socks_proxy.settimeout(timeout) #LINE# #TAB# return socks_proxy"
"#LINE# #TAB# if object_dict.get('action') == 'append': #LINE# #TAB# #TAB# return True #LINE# #TAB# if type(object_dict) in [bool, int, float]: #LINE# #TAB# #TAB# return True #LINE# #TAB# if type(object_dict) in [dict, List]: #LINE# #TAB# #TAB# return True #LINE# #TAB# if type(object_dict) == dict: #LINE# #TAB# #TAB# for key, value in object_dict.items(): #LINE# #TAB# #TAB# #TAB# if type(value) in [bool, int]: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# from rafcon.gui import Gui #LINE# #TAB# if kernel is None: #LINE# #TAB# #TAB# kernel = Gui.find_kernel(gui) #LINE# #TAB# if kernel is not None: #LINE# #TAB# #TAB# return True if kernel.should_gui(gui) else False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# for i in range(len(lv)): #LINE# #TAB# #TAB# if lv[i][0]!= lt1[i]: #LINE# #TAB# #TAB# #TAB# lv[i][0] = lt1[i] #LINE# #TAB# #TAB# if lv[i][1]!= lt2[i]: #LINE# #TAB# #TAB# #TAB# lv[i][1] = lt2[i] #LINE# #TAB# #TAB# for i in range(len(lv)): #LINE# #TAB# #TAB# #TAB# lv[i][0] = lv[i][0] + lt1[i] #LINE# #TAB# #TAB# #TAB# lv[i][1] = lt2[i] #LINE# #TAB# return lv
"#LINE# #TAB# obs = xw.sum(axis=1) #LINE# #TAB# Nw = weights.shape[0] #LINE# #TAB# s = np.zeros((Nw, Nw), dtype=float) #LINE# #TAB# for ii in range(Nw): #LINE# #TAB# #TAB# for jj in range(Nw): #LINE# #TAB# #TAB# #TAB# iif = ii[jj] #LINE# #TAB# #TAB# #TAB# s[iif, jj] = 1.0 / weights[iif] * (xw[iii] - xw[jj]) #LINE# #TAB# return s"
#LINE# #TAB# if len(ops) == 1: #LINE# #TAB# #TAB# return ops[0] #LINE# #TAB# if len(ops) > 1: #LINE# #TAB# #TAB# new_ops = PauliTerm([_mx_normal_op(ops[0])] + ops[1:]) #LINE# #TAB# #TAB# return new_ops #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# new_dct = dict() #LINE# #TAB# for key, value in d.items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# new_dct[key] = list(value) #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# new_dct[key] = dct_to_identifier(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_dct[key] = value #LINE# #TAB# return new_dct"
"#LINE# #TAB# return get_reference_hsh(coin_symbol=coin_symbol, api_key=api_key)[ #LINE# #TAB# #TAB#'reference_hsh']"
#LINE# #TAB# header = fits.getheader(image_file) #LINE# #TAB# block_hash = hashlib.sha256(header).hexdigest() #LINE# #TAB# return block_hash
"#LINE# #TAB# if isinstance(file_path_or_generator, six.string_types): #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# line = file_path_or_generator #LINE# #TAB# #TAB# #TAB# decoded = decode_line(line) #LINE# #TAB# #TAB# #TAB# if decoded: #LINE# #TAB# #TAB# #TAB# #TAB# yield decoded"
"#LINE# #TAB# labels = [] #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# for label in email.labels: #LINE# #TAB# #TAB# #TAB# if label =='subject': #LINE# #TAB# #TAB# #TAB# #TAB# labels.append(email.subject) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# if label == 'body': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# labels.append(email.body) #LINE# #TAB# labels.sort() #LINE# #TAB# namespace = [] #LINE# #TAB# for i in range(len(labels)): #LINE# #TAB# #TAB# for j in range(len(labels[i])): #LINE# #TAB# #TAB# #TAB# namespace.append('{0} {1}'.format(labels[i], labels[j])) #LINE# #TAB# return namespace"
"#LINE# #TAB# if dd.get_coverage_interval(data) == 'eV': #LINE# #TAB# #TAB# return data.copy() #LINE# #TAB# new_data = OrderedDict() #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# if isinstance(value, float): #LINE# #TAB# #TAB# #TAB# new_value = '%.2f' % value #LINE# #TAB# #TAB# elif isinstance(value, int): #LINE# #TAB# #TAB# #TAB# new_value = '%.2f' % value #LINE# #TAB# #TAB# new_data[key] = new_value #LINE# #TAB# data['bed_expr'] = pd.Series(new_data, index=data.index) #LINE# #TAB# return data"
#LINE# #TAB# df = df[config['engine'].contains(config['contaminant_tag']).any( #LINE# #TAB# #TAB# )] #LINE# #TAB# return df
"#LINE# #TAB# if overwrite: #LINE# #TAB# #TAB# for instrument in barcodes: #LINE# #TAB# #TAB# #TAB# instrument.filename = os.path.join(output_dir, instrument.filename) #LINE# #TAB# if os.path.isdir(output_dir): #LINE# #TAB# #TAB# shutil.rmtree(output_dir) #LINE# #TAB# else: #LINE# #TAB# #TAB# for instrument in barcodes: #LINE# #TAB# #TAB# #TAB# os.rmdir(output_dir) #LINE# #TAB# return"
"#LINE# #TAB# sz_str, sh_str = cncode.write_ode() #LINE# #TAB# sz_bytes = str2bytes(sz_str) #LINE# #TAB# sh_bytes = str2bytes(sh_str) #LINE# #TAB# return sz_bytes, sh_bytes"
#LINE# #TAB# dists_used_configs = [] #LINE# #TAB# for item1 in input_list: #LINE# #TAB# #TAB# for item2 in input_list: #LINE# #TAB# #TAB# #TAB# if item1 in item2 and item2!= item1: #LINE# #TAB# #TAB# #TAB# #TAB# dists_used_configs.append(item1) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return dists_used_configs
"#LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# ax = plt.gca() #LINE# #TAB# net_exposures = [net_exposures[i] for i in net_exposures] #LINE# #TAB# for i in net_exposures: #LINE# #TAB# #TAB# net_exposures[i].set_visible(False) #LINE# #TAB# ax.set_xticks(np.array([0.0, 0.0, 1.0])) #LINE# #TAB# ax.set_yticks(np.array([0.0, 0.0, 1.0])) #LINE# #TAB# return ax"
#LINE# #TAB# if num_bytes < 0: #LINE# #TAB# #TAB# raise ValueError('pinnd can not be negative') #LINE# #TAB# buf = (c_char_p * num_bytes)() #LINE# #TAB# _buf = bytes(buf) #LINE# #TAB# while True: #LINE# #TAB# #TAB# if _buf: #LINE# #TAB# #TAB# #TAB# yield _buf #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
#LINE# #TAB# result = [] #LINE# #TAB# with open('/etc/rfc-index.txt') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# split_line = line.split('\t') #LINE# #TAB# #TAB# #TAB# if len(split_line) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(split_line[0]) #LINE# #TAB# return result
"#LINE# #TAB# qfont = QFont() #LINE# #TAB# for key, value in font.properties().items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# qfont.setAttribute(key, value) #LINE# #TAB# return qfont"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# data = receiver.recv() #LINE# #TAB# #TAB# #TAB# #TAB# yield data #LINE# #TAB# #TAB# except (EOFError, KeyboardInterrupt): #LINE# #TAB# #TAB# #TAB# return"
#LINE# #TAB# thread = {} #LINE# #TAB# for person in fake.persons(): #LINE# #TAB# #TAB# person['id'] = str(person.id) #LINE# #TAB# #TAB# thread['first_name'] = str(person.first_name) #LINE# #TAB# #TAB# thread['last_name'] = str(person.last_name) #LINE# #TAB# #TAB# thread['method'] = 'generate' #LINE# #TAB# #TAB# thread['relations'] = [] #LINE# #TAB# #TAB# for relation in person.relations: #LINE# #TAB# #TAB# #TAB# thread['relations'].append(relation) #LINE# #TAB# #TAB# thread['first_name'] = str(person.first_name) #LINE# #TAB# #TAB# thread['last_name'] = str(person.last_name) #LINE# #TAB# return thread
#LINE# #TAB# module_path = os.path.dirname(__file__) #LINE# #TAB# files_path = os.listdir(module_path) #LINE# #TAB# return files_path
"#LINE# #TAB# if isinstance(n, six.string_types): #LINE# #TAB# #TAB# n = n.split('.') #LINE# #TAB# n = tuple(n) #LINE# #TAB# if len(n) == 1: #LINE# #TAB# #TAB# col_names = n[0] #LINE# #TAB# #TAB# return col_names #LINE# #TAB# elif len(n) > 2: #LINE# #TAB# #TAB# col_names = [c[0] for c in n[1:]] #LINE# #TAB# #TAB# return col_names #LINE# #TAB# else: #LINE# #TAB# #TAB# return n"
#LINE# #TAB# n1 = abs(f1) #LINE# #TAB# n2 = abs(f2) #LINE# #TAB# if n1 > n2: #LINE# #TAB# #TAB# return f1 + n2 #LINE# #TAB# elif n1 < n2: #LINE# #TAB# #TAB# return f1 + n1 #LINE# #TAB# return f2
#LINE# #TAB# if item is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del item #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# return None #LINE# #TAB# return item
"#LINE# #TAB# client = app.client #LINE# #TAB# return [('/', client.view_functions), ('/', client.view_functions), ( #LINE# #TAB# #TAB# '/{0}/{1}/*'.format(app.config['PORT'], client.endpoint)), ( #LINE# #TAB# #TAB# '/{0}/{1}/*'.format(app.config['PORT'], client.endpoint)), ( #LINE# #TAB# #TAB# '/{0}/{1}/*'.format(app.config['PORT'], client.endpoint)), ( #LINE# #TAB# #TAB# '/{0}/{1}/*'.format(app.config['PORT'], client.endpoint)), ( #LINE# #TAB# #TAB# '/{0}/{1}/*'.format(app.config['PORT'], client.endpoint)), ( #LINE# #TAB# #TAB# '/{0}/*/*'.format(app.config['PORT']), client. #LINE# #TAB# #TAB# endpoint)]"
#LINE# #TAB# char_list = list(s) #LINE# #TAB# char_list.sort() #LINE# #TAB# i = 0 #LINE# #TAB# n = len(char_list) #LINE# #TAB# result = [] #LINE# #TAB# while i < n: #LINE# #TAB# #TAB# code1 = ord(char_list[i]) #LINE# #TAB# #TAB# code2 = code1 + 1 #LINE# #TAB# #TAB# i = i + 1 #LINE# #TAB# #TAB# while i < n and code2 >= ord(char_list[i]): #LINE# #TAB# #TAB# #TAB# code2 = code2 + 1 #LINE# #TAB# #TAB# #TAB# i = i + 1 #LINE# #TAB# #TAB# result.append(code1) #LINE# #TAB# #TAB# result.append(code2) #LINE# #TAB# return result
"#LINE# #TAB# if isinstance(cls, models.ManyToManyRel): #LINE# #TAB# #TAB# return {'type': cls, 'type_prefix': type_prefix + '_many_to_many', #LINE# #TAB# #TAB# #TAB# 'type_value': cls} #LINE# #TAB# elif isinstance(cls, models.Date): #LINE# #TAB# #TAB# return {'type': cls, 'type_prefix': type_prefix + '_date'} #LINE# #TAB# elif isinstance(cls, models.TextField): #LINE# #TAB# #TAB# return {'type': cls, 'type_prefix': type_prefix + '_single_value', #LINE# #TAB# #TAB# #TAB# 'type_value': cls} #LINE# #TAB# else: #LINE# #TAB# #TAB# return cls"
"#LINE# #TAB# arr = np.asarray(arr) #LINE# #TAB# if diff is not None: #LINE# #TAB# #TAB# diff = np.asarray(diff) #LINE# #TAB# #TAB# if arr.ndim == 2: #LINE# #TAB# #TAB# #TAB# blob_proto = diff #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# blob_proto = np.zeros((diff.shape[0], arr.shape[1])) #LINE# #TAB# #TAB# blob_proto.data = diff #LINE# #TAB# else: #LINE# #TAB# #TAB# return blob_proto"
#LINE# #TAB# i = -1 #LINE# #TAB# while i < len(nodes): #LINE# #TAB# #TAB# if nodes[i].tag == 'nav_extender': #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# elif nodes[i].tag =='span': #LINE# #TAB# #TAB# #TAB# i -= 1 #LINE# #TAB# return i
"#LINE# #TAB# if incremental_state is not None: #LINE# #TAB# #TAB# full_locale_key = _get_full_locale_key(module, key) #LINE# #TAB# #TAB# return incremental_state[full_locale_key] #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# config_file = os.environ.get('DJANGO_DATA_YAML', None) #LINE# #TAB# if config_file is None: #LINE# #TAB# #TAB# raise ValueError('No DJANGO_DATA_YAML environment variable specified') #LINE# #TAB# return json.loads(config_file.strip())['data']"
"#LINE# #TAB# if isinstance(uri, Namespace): #LINE# #TAB# #TAB# return uri.uri #LINE# #TAB# return uri"
"#LINE# #TAB# username = user if user is not None else 'root' #LINE# #TAB# password = password if password is not None else 'root' #LINE# #TAB# return username, password"
"#LINE# #TAB# layers = [] #LINE# #TAB# while data: #LINE# #TAB# #TAB# name, data = parse_chain_def(data) #LINE# #TAB# #TAB# if name is None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# layers.append(Layer(name, data)) #LINE# #TAB# return layers"
#LINE# #TAB# rdict = {} #LINE# #TAB# for l in t: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# rdict[l] += t[l] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# rdict[l] = t[l] #LINE# #TAB# return rdict
#LINE# #TAB# if flags & os.O_RDONLY: #LINE# #TAB# #TAB# return '-R' #LINE# #TAB# elif flags & os.O_RDWR: #LINE# #TAB# #TAB# return '-W' #LINE# #TAB# elif flags & os.O_CREAT: #LINE# #TAB# #TAB# return '-F' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
"#LINE# #TAB# penn_tree = clean_text_tree(penn_tree) #LINE# #TAB# for node in penn_tree.getroot().getchildren(): #LINE# #TAB# #TAB# if isinstance(node, etree._Element): #LINE# #TAB# #TAB# #TAB# node = node.getparent() #LINE# #TAB# #TAB# clean_dist_mapping(node, penn_tree) #LINE# #TAB# return penn_tree"
#LINE# #TAB# index2 = index.astype('int64') / 10 ** 9 #LINE# #TAB# return index2
#LINE# #TAB# if ch not in _registered_services: #LINE# #TAB# #TAB# _registered_services[ch] = {} #LINE# #TAB# _registered_services[ch][service_name] = config
#LINE# #TAB# state = gt_modules_state() #LINE# #TAB# return state[0]
"#LINE# #TAB# if file is None: #LINE# #TAB# #TAB# file = default_config() #LINE# #TAB# d = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# data_file = open(file, 'rt') #LINE# #TAB# #TAB# data = json.loads(data_file.read()) #LINE# #TAB# #TAB# data_file.close() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# finally: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f = open(file, 'rt') #LINE# #TAB# #TAB# #TAB# data = json.loads(f.read()) #LINE# #TAB# #TAB# #TAB# f.close() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return data"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# proc = subprocess.Popen(['youtbe-dl'], stdout=subprocess.PIPE) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return False #LINE# #TAB# proc.communicate() #LINE# #TAB# return proc.returncode == 0"
"#LINE# #TAB# tokens = [] #LINE# #TAB# n_off_diag = int((n_features ** 2 - n_features) / 2) #LINE# #TAB# for i in range(n_off_diag): #LINE# #TAB# #TAB# tokens.append(prng.randint(0, 2 * n_features)) #LINE# #TAB# for j in range(n_features): #LINE# #TAB# #TAB# tokens.append(prng.randint(0, 2 * n_features)) #LINE# #TAB# return tokens"
"#LINE# #TAB# datamean = data.mean(axis=2).imag #LINE# #TAB# datameanmin, datameanmax = rtlib.sigma_clip(datamean.flatten()) #LINE# #TAB# good = n.where((datamean > datameanmin) & (datamean < datameanmax)) #LINE# #TAB# if len(good) > 0: #LINE# #TAB# #TAB# datamean = datamean[good].flatten() #LINE# #TAB# #TAB# logger.debug('Clipped to %d%% of data (%.3f to %.3f). Noise = %.3f.' % #LINE# #TAB# #TAB# #TAB# (datamean.size, datameanmin, datameanmax, good)) #LINE# #TAB# return datamean"
#LINE# #TAB# out = [] #LINE# #TAB# for rea in fvaMinmax: #LINE# #TAB# #TAB# if rea not in out: #LINE# #TAB# #TAB# #TAB# out.append(rea) #LINE# #TAB# return out
"#LINE# #TAB# value = value.replace(',', '') #LINE# #TAB# for v in value.split(' '): #LINE# #TAB# #TAB# v = v.replace(',', '') #LINE# #TAB# #TAB# if v == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for k, v in v.split('/'): #LINE# #TAB# #TAB# #TAB# if k == 'p': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# v = int(v) #LINE# #TAB# #TAB# #TAB# if v == 0 or v == 1: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return v"
#LINE# #TAB# is_tdt = block_is_tdt(path) #LINE# #TAB# return is_tdt
"#LINE# #TAB# if isinstance(n, float): #LINE# #TAB# #TAB# if n == 1.0: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# #TAB# elif n == -1.0: #LINE# #TAB# #TAB# #TAB# return n #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Could not convert {} to int'.format(n)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return n"
"#LINE# #TAB# annotations = IAnnotations(context) #LINE# #TAB# dataset = None #LINE# #TAB# if slug: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# dataset = annotations[slug] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if limit: #LINE# #TAB# #TAB# i = range(0, limit) #LINE# #TAB# #TAB# if title: #LINE# #TAB# #TAB# #TAB# return format_html('{} - {}', slug, i) #LINE# #TAB# #TAB# if text: #LINE# #TAB# #TAB# #TAB# return format_html('{} {}', slug, text) #LINE# #TAB# return dataset"
"#LINE# #TAB# assert_type_or_raise(array, dict, parameter_name='array') #LINE# #TAB# data = Result.validate_array(array) #LINE# #TAB# data['type'] = u(array.get('type')) #LINE# #TAB# data['id'] = u(array.get('id')) #LINE# #TAB# data['name'] = u(array.get('name')) #LINE# #TAB# data['contents'] = u(array.get('contents')) #LINE# #TAB# return data"
"#LINE# #TAB# endyear = startyear #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# endyear += 1 #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return endyear #LINE# #TAB# #TAB# if endyear < startyear: #LINE# #TAB# #TAB# #TAB# return endyear, 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return endyear, 0"
#LINE# #TAB# image = Image.open(io.BytesIO(image_bytes)) #LINE# #TAB# image.load() #LINE# #TAB# return image
#LINE# #TAB# et = g['widget_et'] #LINE# #TAB# g['widget_et'] = et #LINE# #TAB# return et
#LINE# #TAB# global registered_defult_ist #LINE# #TAB# if not registered_defult_ist: #LINE# #TAB# #TAB# registered_defult_ist = [] #LINE# #TAB# return registered_defult_ist
#LINE# #TAB# for child in root.iter('./looup'): #LINE# #TAB# #TAB# if child.tag == 'title': #LINE# #TAB# #TAB# #TAB# yield child
"#LINE# #TAB# #TAB# if not host: #LINE# #TAB# #TAB# #TAB# host = 'localhost' #LINE# #TAB# #TAB# if not port: #LINE# #TAB# #TAB# #TAB# port = 443 #LINE# #TAB# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# s.bind((host, port)) #LINE# #TAB# #TAB# return s"
#LINE# #TAB# result = MapConverter.to_nullable_mp(value) #LINE# #TAB# return result if result!= None else {}
"#LINE# #TAB# logging.info('Ijon content: %s', content) #LINE# #TAB# response = requests.get(BASE_URL) #LINE# #TAB# if response.status_code == 403: #LINE# #TAB# #TAB# return response.json()['message'] #LINE# #TAB# return content"
"#LINE# #TAB# N = X.shape[1] #LINE# #TAB# D = np.zeros((num_folds, N)) #LINE# #TAB# for i in range(num_folds): #LINE# #TAB# #TAB# C = np.zeros(N) #LINE# #TAB# #TAB# for j in range(num_folds): #LINE# #TAB# #TAB# #TAB# D[i, j] = GraphLassoCV(X[(i), :, :]) #LINE# #TAB# #TAB# D[i, j] = C.dot(D) #LINE# #TAB# return D"
"#LINE# #TAB# rimg = np.zeros((ncomp, 3), dtype=img.dtype) #LINE# #TAB# for i in range(ncomp): #LINE# #TAB# #TAB# rimg[:, :, (i)] = pca_solve(img) #LINE# #TAB# if norm_type == 'perc': #LINE# #TAB# #TAB# rimg = rimg / np.sum(rimg, axis=0) #LINE# #TAB# return rimg"
"#LINE# #TAB# if user.is_authenticated: #LINE# #TAB# #TAB# ctype = ContentType.objects.get_for_model(model) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# entity = Entity.objects.get(user=user, content_type=ctype, #LINE# #TAB# #TAB# #TAB# #TAB# model_id=model.id) #LINE# #TAB# #TAB# except ObjectDoesNotExist: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if entity.is_superuser: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# obj = None #LINE# #TAB# if 'objtype' in params and params['objtype'] == 'image': #LINE# #TAB# #TAB# obj = et_mg_image(toyz_settings, tid, params) #LINE# #TAB# elif 'objtype' in params and params['objtype'] == 'fits': #LINE# #TAB# #TAB# obj = et_mg_fits(toyz_settings, tid, params) #LINE# #TAB# return obj"
#LINE# #TAB# C = 1 / (-xm / (1 - a) - xm / a + math.exp(a) * xm / a) #LINE# #TAB# return C
"#LINE# #TAB# for win in find_featd_windows(artist, albumartist): #LINE# #TAB# #TAB# if win: #LINE# #TAB# #TAB# #TAB# yield win"
"#LINE# #TAB# bars = [] #LINE# #TAB# for k, v in dic.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# bar = cls.ceate_clean_dct(v) #LINE# #TAB# #TAB# #TAB# bars.append(bar) #LINE# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# for x in v: #LINE# #TAB# #TAB# #TAB# bars.append(cls.ceate_clean_dct(x)) #LINE# #TAB# #TAB# return bars #LINE# #TAB# return bars"
"#LINE# #TAB# epilog = parser.epilog #LINE# #TAB# if not epilog: #LINE# #TAB# #TAB# return #LINE# #TAB# for line in epilog.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line or line[0] == '#': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith('# '): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif '=' in line: #LINE# #TAB# #TAB# #TAB# key, val = line.split('=', 1) #LINE# #TAB# #TAB# #TAB# val = val.strip() #LINE# #TAB# #TAB# #TAB# epilog[key] = val #LINE# #TAB# return epilog"
#LINE# #TAB# neuron.h.load_file('morphology.hoc') #LINE# #TAB# neuron.h.load_file('biophysics.hoc') #LINE# #TAB# neuron.h.load_file('template.hoc') #LINE# #TAB# print('Loading cell cADpyr232_L5_STPC_d16b0be14e') #LINE# #TAB# cell.cADpyr232_L5_STPC_d16b0be14e(1 if add_synapses else 0) #LINE# #TAB# return neuron.h.cADpyr232_L5_STPC_d16b0be14e
"#LINE# #TAB# result = remve_t0_validate_schema(times_needed, _db) #LINE# #TAB# result = result - times_needed #LINE# #TAB# return result"
#LINE# #TAB# if name.startswith(prefix): #LINE# #TAB# #TAB# return name #LINE# #TAB# if any(name.endswith(suffix) for suffix in LANGS): #LINE# #TAB# #TAB# return name #LINE# #TAB# return prefix + '-' + name
"#LINE# #TAB# if dst.parent is None: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(dst, exist_ok=True) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# if not hasattr(cls, '_upper'): #LINE# #TAB# cls._upper = timestamp #LINE# #TAB# return #LINE# #TAB# if timestamp > cls._upper: #LINE# #TAB# cls._upper = timestamp"
"#LINE# #TAB# with open(file_name, 'rb') as f: #LINE# #TAB# #TAB# image = Image.open(f) #LINE# #TAB# #TAB# texture = image.convert('RGB') #LINE# #TAB# #TAB# resolver.resolve(file_name, texture) #LINE# #TAB# return image"
"#LINE# #TAB# typing.Any) ->tf.Tensor: #LINE# #TAB# vocab_size = attention_input.shape[0] #LINE# #TAB# attention_output = tf.zeros_like(attention_input) #LINE# #TAB# for idx in range(vocab_size): #LINE# #TAB# #TAB# text_a = attention_input[:, (idx), :] #LINE# #TAB# #TAB# text_b = attention_input[:, (idx), :] #LINE# #TAB# #TAB# text_c = tf.cast(text_a, tf.float32) + tf.cast(text_b, tf.float32) #LINE# #TAB# #TAB# attention_output[idx, :] = text_c #LINE# #TAB# return attention_output"
"#LINE# #TAB# r = session.get(url) #LINE# #TAB# if r.status_code!= 200: #LINE# #TAB# #TAB# logger.warning('Error getting %s: %s', url, r.status_code) #LINE# #TAB# return r.headers['location']"
#LINE# #TAB# #TAB# rules = [] #LINE# #TAB# #TAB# for rule in cls.RULES: #LINE# #TAB# #TAB# #TAB# if rule.match(line): #LINE# #TAB# #TAB# #TAB# #TAB# rules.append(rule) #LINE# #TAB# #TAB# return rules
"#LINE# #TAB# if max_prime == 2: #LINE# #TAB# #TAB# return [1, 0] #LINE# #TAB# elif max_prime == 3: #LINE# #TAB# #TAB# return [1, 2, 3] #LINE# #TAB# elif max_prime == 4: #LINE# #TAB# #TAB# return [0, 2, 4, 5] #LINE# #TAB# elif max_prime == 5: #LINE# #TAB# #TAB# return [0, 2, 5, 6] #LINE# #TAB# elif max_prime == 7: #LINE# #TAB# #TAB# return [0, 2, 3, 6, 7] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [0, 0, 1, 3, 6, 6, 7]"
"#LINE# #TAB# obj = cls._fnd_mask_objects.get(name, None) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# myokit_unit = cls._fnd_mask_objects.get(name, None) #LINE# #TAB# #TAB# if myokit_unit is None: #LINE# #TAB# #TAB# #TAB# raise CellMLError('Unknown units name ""' + str(name) + '"".') #LINE# #TAB# #TAB# obj = cls(name, myokit_unit, predefined=True) #LINE# #TAB# return obj"
"#LINE# #TAB# matched = [] #LINE# #TAB# non_matching = [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if cond(item): #LINE# #TAB# #TAB# #TAB# matched.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# non_matching.append(item) #LINE# #TAB# return matched, non_matching"
"#LINE# #TAB# return {'task_id': req.task_id, 'task_full_name': req.full_name, #LINE# #TAB# #TAB# 'task_stage': req.task_stage,'state': req.state, 'user': req. #LINE# #TAB# #TAB# user, 'updated_at': req.updated_at, 'created_at': req.created_at, #LINE# #TAB# #TAB# 'updated_at': req.updated_at}"
#LINE# #TAB# s = 0 #LINE# #TAB# if pt1[0] < pt2[0] and pt2[0] < pt3[0]: #LINE# #TAB# #TAB# s += 3 #LINE# #TAB# if pt1[1] < pt3[1] and pt3[1] < pt1[1]: #LINE# #TAB# #TAB# s += 4 #LINE# #TAB# return s
"#LINE# #TAB# global _import_worker_thread #LINE# #TAB# if _import_worker_thread is None: #LINE# #TAB# #TAB# with _import_worker_thread: #LINE# #TAB# #TAB# #TAB# _import_worker_thread = threading.Thread(None, callback) #LINE# #TAB# #TAB# return _import_worker_thread"
#LINE# #TAB# if warningstuple[2] is ProxyWarning: #LINE# #TAB# #TAB# return _remove_proxy(warningstuple) #LINE# #TAB# else: #LINE# #TAB# #TAB# return warningstuple
"#LINE# #TAB# packet = p.Packet(MsgType.Imu) #LINE# #TAB# packet.add_subpacket(p.SetListOfBytes(ImuMsgCode.SetFamily, #LINE# #TAB# #TAB# 0, AckCode.OK)) #LINE# #TAB# return packet"
"#LINE# #TAB# if kind == ""linear"": #LINE# #TAB# #TAB# cti = 6378137 #LINE# #TAB# else: #LINE# #TAB# #TAB# cti = 0 #LINE# #TAB# h = np.asarray(h) #LINE# #TAB# lon = np.asarray(lon) #LINE# #TAB# lat = np.asarray(lat) #LINE# #TAB# mask = cti < (lon + lat) * dx #LINE# #TAB# if plot: #LINE# #TAB# #TAB# plot_no_plot(mask) #LINE# #TAB# #TAB# return mask #LINE# #TAB# else: #LINE# #TAB# #TAB# return mask"
"#LINE# #TAB# instanc_names = [] #LINE# #TAB# for task_instance in task_instances: #LINE# #TAB# #TAB# if isinstance(task_instance, astroid.Module): #LINE# #TAB# #TAB# #TAB# inst_name = task_instance.name #LINE# #TAB# #TAB# #TAB# if inst_name not in instanc_names: #LINE# #TAB# #TAB# #TAB# #TAB# instanc_names.append(inst_name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# inst_names.append(task_instance.name) #LINE# #TAB# return instanc_names"
"#LINE# #TAB# ax = plt.axes() #LINE# #TAB# service_region = service.upper() #LINE# #TAB# for region in service.get_available_regions(): #LINE# #TAB# #TAB# if service_region in region: #LINE# #TAB# #TAB# #TAB# ax.add_axis(region, 'x', axis=ax.x) #LINE# #TAB# #TAB# #TAB# ax.add_axis(region, 'y', axis=ax.y) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ax.add_axis(region, 'x', axis=ax.x) #LINE# #TAB# return ax"
#LINE# #TAB# if host: #LINE# #TAB# #TAB# group.host = host #LINE# #TAB# group.scheduled_at = None #LINE# #TAB# return group
"#LINE# #TAB# theta = 0.06 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
#LINE# #TAB# if op in opc_cache: #LINE# #TAB# #TAB# return opc_cache[op] #LINE# #TAB# size = 0 #LINE# #TAB# while size < len(opargs): #LINE# #TAB# #TAB# size += opargs[size] #LINE# #TAB# #TAB# size += opargs[size] #LINE# #TAB# return size
"#LINE# #TAB# L = [] #LINE# #TAB# if test is None: #LINE# #TAB# #TAB# for i in iterator: #LINE# #TAB# #TAB# #TAB# L.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for i in iterator: #LINE# #TAB# #TAB# #TAB# #TAB# L.append(logger_constant(i, test)) #LINE# #TAB# return L"
#LINE# #TAB# if flip: #LINE# #TAB# #TAB# images = [pygame.image.flip_image(image) for image in images] #LINE# #TAB# if rotate: #LINE# #TAB# #TAB# images = [pygame.image.rotate_image(image) for image in images] #LINE# #TAB# return images
"#LINE# #TAB# songToSplit = spit(songToSplit) #LINE# #TAB# songToSplitStart = min(songToSplit, start1) #LINE# #TAB# songToSplitStart = max(songToSplit, start2) #LINE# #TAB# return songToSplitStart, songToSplitStart"
"#LINE# #TAB# if n_gram is None: #LINE# #TAB# #TAB# n_gram = get_ngram_features() #LINE# #TAB# X, y = load_imdb_data(n_gram) #LINE# #TAB# del X, y #LINE# #TAB# return X, y"
#LINE# #TAB# if cls._sign_sort is None: #LINE# #TAB# #TAB# from. dial import Speed dial #LINE# #TAB# #TAB# cls._sign_sort = Speed dial #LINE# #TAB# return cls._sign_sort
#LINE# #TAB# #TAB# op_id = None #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# relation_id = cursor.fetchone()[0] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if relation_id!= '0': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# op_id = relation_id #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return op_id
#LINE# #TAB# #TAB# axis_indices = [] #LINE# #TAB# #TAB# for var in vars_list: #LINE# #TAB# #TAB# #TAB# axis_indices.append(fluent.scope.get_axis(var).index) #LINE# #TAB# #TAB# return axis_indices
"#LINE# #TAB# config_path = os.path.join(config_dir, CONFIG_FILE_NAME) #LINE# #TAB# if os.path.exists(config_path): #LINE# #TAB# #TAB# return config_path #LINE# #TAB# elif detect_location: #LINE# #TAB# #TAB# ensue_config_base(config_dir) #LINE# #TAB# else: #LINE# #TAB# #TAB# ensue_config_base = create_default_file() #LINE# #TAB# return ensue_config_base"
"#LINE# #TAB# return { #LINE# #TAB# #TAB#'method':'su', #LINE# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# 'username': spec.become_user(), #LINE# #TAB# #TAB# #TAB# 'password': spec.become_pass(), #LINE# #TAB# #TAB# #TAB# 'python_path': spec.python_path(), #LINE# #TAB# #TAB# #TAB#'su_path': spec.become_exe(), #LINE# #TAB# #TAB# #TAB# 'connect_timeout': spec.timeout(), #LINE# #TAB# #TAB# #TAB#'remote_name': get_remote_name(spec), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"#LINE# #TAB# read_bytes = f.read(4) #LINE# #TAB# val = struct.unpack('>I', read_bytes)[0] #LINE# #TAB# return val"
"#LINE# #TAB# ses, auto_close = ensure_session(engine_or_session) #LINE# #TAB# obj = ses.query(cls).get(_id) #LINE# #TAB# if auto_close: #LINE# #TAB# #TAB# obj = ses.close() #LINE# #TAB# return obj"
"#LINE# #TAB# numba = [] #LINE# #TAB# with io.open(path, ""r"", encoding=""utf-8"") as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# if line and not line.startswith(""#""): #LINE# #TAB# #TAB# #TAB# #TAB# ngram = line.split()[1] #LINE# #TAB# #TAB# #TAB# #TAB# ngram = ngram.replace("","", """") #LINE# #TAB# #TAB# #TAB# #TAB# numba.append(ngram) #LINE# #TAB# return numba"
#LINE# #TAB# if 'tasks' not in details: #LINE# #TAB# #TAB# return True #LINE# #TAB# for task in details['tasks']: #LINE# #TAB# #TAB# if 'taskType' in task and task['taskType'] == 'expand': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# materials = [] #LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# if filename.endswith('.md'): #LINE# #TAB# #TAB# #TAB# m = load_elemental_material(os.path.join(path, filename)) #LINE# #TAB# #TAB# #TAB# materials.append(m) #LINE# #TAB# return materials"
#LINE# #TAB# from gt_factor_workflow import GTFactorWorkflow #LINE# #TAB# return GTFactorWorkflow
#LINE# #TAB# assert doc_type in page.DOC_TYPES #LINE# #TAB# return 'class:document-type:' + doc_type
"#LINE# #TAB# tris = np.array(vertices, dtype=np.float64) #LINE# #TAB# tris[:, (1)] = tris[:, (0)] #LINE# #TAB# tris[:, (2)] = tris[:, (1)] #LINE# #TAB# faces = np.array(faces, dtype=np.float64) #LINE# #TAB# normals = np.cross(faces[:, (0)], faces[:, (1)]) #LINE# #TAB# return normals"
#LINE# #TAB# w = np.sqrt(qx[0] ** 2 + qy[0] ** 2 + qy[1] ** 2 + qz[1] ** 2) #LINE# #TAB# return w
"#LINE# #TAB# result = [] #LINE# #TAB# for i in range(0, size): #LINE# #TAB# #TAB# new_chunk = [] #LINE# #TAB# #TAB# while i < chunk_size: #LINE# #TAB# #TAB# #TAB# new_chunk.append(i) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# if i == size: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return result"
"#LINE# #TAB# deps = set() #LINE# #TAB# with open('Pipfile.lock', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# for dependency in parse_requirements(line): #LINE# #TAB# #TAB# #TAB# #TAB# if dependency.required: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# deps.add(dependency) #LINE# #TAB# return deps"
#LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if l[i] and resolve_row(l[i]) is not None: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if path is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# found = [path] #LINE# #TAB# name = name.lower() #LINE# #TAB# for child in path.iterdir(): #LINE# #TAB# #TAB# if child.name.lower() == name.lower(): #LINE# #TAB# #TAB# #TAB# found = child #LINE# #TAB# if not found: #LINE# #TAB# #TAB# return None #LINE# #TAB# for child in found: #LINE# #TAB# #TAB# if child.is_dir(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if serch_yaml_in_id(child, name): #LINE# #TAB# #TAB# #TAB# return child #LINE# #TAB# return None"
"#LINE# #TAB# lines = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise ski_error.SkiCstringError( #LINE# #TAB# #TAB# #TAB# #TAB# 'invalid spacegroup: %s, setting: %i' % ( #LINE# #TAB# #TAB# #TAB# #TAB# spacegroup, setting)) #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# lines.append(line) #LINE# #TAB# return lines"
"#LINE# #TAB# if isinstance(obj, six.string_types): #LINE# #TAB# #TAB# return False #LINE# #TAB# for key in obj: #LINE# #TAB# #TAB# if not key.startswith('_'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if predicate[0]: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif predicate[1]: #LINE# #TAB# #TAB# return isinstance(value, dict) and value[path[0]] in predicate[1] #LINE# #TAB# elif predicate[2]: #LINE# #TAB# #TAB# return isinstance(value, list) and value[0] in predicate[2] #LINE# #TAB# return False"
"#LINE# #TAB# cls.logger.debug('find_activiy_tye() called') #LINE# #TAB# result = cls.search([('domain', '=', domain), ('name', '=', name), ( #LINE# #TAB# #TAB#'version', '=', version)]) #LINE# #TAB# if result is not None: #LINE# #TAB# #TAB# return result.group(1) #LINE# #TAB# cls.logger.debug('find_activiy_tye() called') #LINE# #TAB# return None"
#LINE# #TAB# lines = [] #LINE# #TAB# for line in open(cmds_file): #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for which in which_list: #LINE# #TAB# #TAB# #TAB# line = line.split(' ')[0] #LINE# #TAB# #TAB# #TAB# if which == 'date_positive_mds_in_insert': #LINE# #TAB# #TAB# #TAB# #TAB# lines.append(line) #LINE# #TAB# return lines
#LINE# #TAB# if mem is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return (time / mem) ** 0.5
"#LINE# #TAB# size = _numbers_create_sring_lengh(handle, key) #LINE# #TAB# return size"
#LINE# #TAB# if mana_div < 10: #LINE# #TAB# #TAB# return '' #LINE# #TAB# else: #LINE# #TAB# #TAB# return mana_div
"#LINE# #TAB# s = asarray_ndim(s, 1) #LINE# #TAB# if s.shape[0] == 0: #LINE# #TAB# #TAB# return s #LINE# #TAB# n = asarray_ndim(s, 1) #LINE# #TAB# out = zeros(n) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# out[i] = s[i] * f_weighted(s[i], n) #LINE# #TAB# return out"
"#LINE# #TAB# return {'action':'store_tags', 'env': {'PIP_CONFIG_DIR': config_dir or #LINE# #TAB# #TAB# get_default_config_dir()}}"
"#LINE# #TAB# ps_script = ('Get-VM -VMName ""{}"" | Select VMName,Name | ConvertTo-Json' #LINE# #TAB# #TAB#.format(vm_name)) #LINE# #TAB# rs = run_ps(ps_script) #LINE# #TAB# return rs"
"#LINE# #TAB# CleavingContextList = [] #LINE# #TAB# for c in broker.broker: #LINE# #TAB# #TAB# if isinstance(c, CleavingContext): #LINE# #TAB# #TAB# #TAB# cleaving_context = c #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# cleaving_context = cls.load_from_cache(c) #LINE# #TAB# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# #TAB# raise Exception( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'Failed to load CleavingContext in broker %s: %s' % ( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# c, str(e))) #LINE# #TAB# #TAB# CleavingContextList.append((cleaving_context, now())) #LINE# #TAB# return CleavingContextList"
#LINE# #TAB# if not os.path.exists(file_path): #LINE# #TAB# #TAB# raise ValueError('Cannot find {}'.format(file_path)) #LINE# #TAB# if not os.path.getsize(file_path): #LINE# #TAB# #TAB# raise ValueError('Empty file') #LINE# #TAB# return True
"#LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# soup = _get_soup_from_url(url) #LINE# #TAB# icon_links = soup.find_all('link', attrs={'rel': 'icon'}) #LINE# #TAB# if icon_links: #LINE# #TAB# #TAB# icon_links = [icon_links[0]] #LINE# #TAB# law = {} #LINE# #TAB# for link in icon_links: #LINE# #TAB# #TAB# title = link.get('href') #LINE# #TAB# #TAB# if title: #LINE# #TAB# #TAB# #TAB# law[title] = link.get('href') #LINE# #TAB# return law"
"#LINE# #TAB# if not isinstance(number, str): #LINE# #TAB# #TAB# return False #LINE# #TAB# if number[0]!= '0': #LINE# #TAB# #TAB# return False #LINE# #TAB# parts = number.split('-') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not parts[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# username = click.prompt('Please enter your One Codex (email)') #LINE# #TAB# password = click.prompt('Please enter your password', hide_input=True) #LINE# #TAB# if api_key is not None: #LINE# #TAB# #TAB# return rewrite_async(server, username, password, api_key) #LINE# #TAB# return None"
#LINE# #TAB# if value in node.tags: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# crc = 0 #LINE# #TAB# for b in bytearray(data): #LINE# #TAB# #TAB# crc = (crc << 1) ^ _CRC_MAP[b] #LINE# #TAB# return crc
"#LINE# #TAB# fun = click.option('--zone', type=click.STRING)(fun) #LINE# #TAB# fun = click.option('--title', type=click.STRING)(fun) #LINE# #TAB# fun = click.option('--url', type=click.STRING)(fun) #LINE# #TAB# fun = click.option('--zone-name', type=click.STRING)(fun) #LINE# #TAB# fun = click.option('--category', type=click.option('--category-name')) #LINE# #TAB# fun = click.option('--category-regex', type=click.option('--category-regex', #LINE# #TAB# #TAB# type=click.option('--category-regex', type=click.option('--category-name')) #LINE# #TAB# return fun"
"#LINE# #TAB# if start_time is None: #LINE# #TAB# #TAB# start_time = time.time() #LINE# #TAB# df = pd.DataFrame(data=source, index=source.index, columns= #LINE# #TAB# #TAB# source.columns) #LINE# #TAB# start_time = pd.to_datetime(start_time, format='%Y-%m-%dT%H:%M:%S') #LINE# #TAB# df = df.join(start_time) #LINE# #TAB# df.index = pd.to_datetime(df.index, format='%Y-%m-%dT%H:%M:%S') #LINE# #TAB# return df"
"#LINE# #TAB# master_class = spec.pop('class') #LINE# #TAB# if isinstance(spec, dict): #LINE# #TAB# #TAB# normalize_spec(org_client, spec) #LINE# #TAB# elif isinstance(spec, list): #LINE# #TAB# #TAB# normalize_spec(org_client, spec) #LINE# #TAB# elif isinstance(spec, dict): #LINE# #TAB# #TAB# for org in spec: #LINE# #TAB# #TAB# #TAB# normalize_master_class(org_client, org) #LINE# #TAB# return org_client, spec"
"#LINE# #TAB# m = Manifold() #LINE# #TAB# d = json.load(open(fn, 'r')) #LINE# #TAB# if isinstance(d['args'], list): #LINE# #TAB# #TAB# for item in d['args']: #LINE# #TAB# #TAB# #TAB# m[item['name']] = item['value'] #LINE# #TAB# elif isinstance(d['args'], dict): #LINE# #TAB# #TAB# for item in d['args']: #LINE# #TAB# #TAB# #TAB# m[item['name']] = item['value'] #LINE# #TAB# return m"
"#LINE# #TAB# address = generate_rmote_yaml_int_address(backupID, blockNum) #LINE# #TAB# if backupID == 0: #LINE# #TAB# #TAB# return address #LINE# #TAB# else: #LINE# #TAB# #TAB# return address"
"#LINE# #TAB# resp = make_response(dumps({'response': data}), code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
#LINE# #TAB# output = '' #LINE# #TAB# idx = 0 #LINE# #TAB# while idx < len(buff): #LINE# #TAB# #TAB# line = buff[idx:idx + 2] #LINE# #TAB# #TAB# if line.startswith(prefix): #LINE# #TAB# #TAB# #TAB# start = idx #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output += line #LINE# #TAB# #TAB# #TAB# idx += 1 #LINE# #TAB# return output
"#LINE# #TAB# if type(val) is str: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# val = val.lower() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if val in ('',): #LINE# #TAB# #TAB# val = '' #LINE# #TAB# return val"
#LINE# #TAB# log_stream = logging.Stream() #LINE# #TAB# try: #LINE# #TAB# #TAB# yield log_stream #LINE# #TAB# finally: #LINE# #TAB# #TAB# log_stream.flush() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# log_stream.close() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# fnd_dependencies = set() #LINE# #TAB# for fnd in maya.cmds.ls(l=True, type='file'): #LINE# #TAB# #TAB# if maya.cmds.referenceQuery(fnd, isNodeReferenced=True): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# fnd_dependencies.update(get_fnd_dependencies(fnd)) #LINE# #TAB# return fnd_dependencies"
"#LINE# #TAB# found_format = None #LINE# #TAB# try: #LINE# #TAB# #TAB# open(file_name, 'r').close() #LINE# #TAB# #TAB# found_format = open(file_name, 'r').read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return file_name, found_format"
"#LINE# #TAB# x = np.linspace(-1, 1, 3) #LINE# #TAB# y = np.linspace(-1, 1, 3) #LINE# #TAB# df = pd.DataFrame(x, y) #LINE# #TAB# df.columns = ['x', 'y'] #LINE# #TAB# return df"
"#LINE# #TAB# sorted_keys = [key for key, value in response.items()] #LINE# #TAB# sorted_keys.sort() #LINE# #TAB# return {key: value for key, value in sorted_keys}"
#LINE# #TAB# delta = {} #LINE# #TAB# if scope == 'aws4_request': #LINE# #TAB# #TAB# delta['timeZone'] = cano_req['region'] #LINE# #TAB# #TAB# delta['timeZone'] = cano_req['region'] #LINE# #TAB# if 'timeZone' in req: #LINE# #TAB# #TAB# delta['timeZone'] = cano_req['timeZone'] #LINE# #TAB# if delta: #LINE# #TAB# #TAB# return ( #LINE# #TAB# #TAB# #TAB# f'{datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return f'{datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}'
#LINE# #TAB# #TAB# max_pos = cio.tell() #LINE# #TAB# #TAB# while cio.tell() < max_pos: #LINE# #TAB# #TAB# #TAB# if cio.read(1): #LINE# #TAB# #TAB# #TAB# #TAB# max_pos = cio.tell() #LINE# #TAB# #TAB# #TAB# cio.seek(max_pos) #LINE# #TAB# #TAB# return max_pos
"#LINE# #TAB# import numpy as np #LINE# #TAB# bb = bb.astype('float32') #LINE# #TAB# rows = bb.shape[0] // factor #LINE# #TAB# cols = bb.shape[1] // factor #LINE# #TAB# shrin = np.zeros((rows, cols)) #LINE# #TAB# for i in range(rows): #LINE# #TAB# #TAB# for j in range(cols): #LINE# #TAB# #TAB# #TAB# shrin[i][j] = np.mod(shrin[i][j], factor) #LINE# #TAB# return shrin"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return globals[chr_str] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if urlconf: #LINE# #TAB# #TAB# resolver = urlconf.resolve(path) #LINE# #TAB# #TAB# if not resolver: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if isinstance(resolver, Resolver): #LINE# #TAB# #TAB# return resolver.resolve(path) is not None #LINE# #TAB# return True"
"#LINE# #TAB# if 'IPython' not in sys.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'IPython' not in sys.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# import IPython #LINE# #TAB# return getattr(ipython, 'kernel', None) is not None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# t = [s] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# t = [s] #LINE# #TAB# if len(t) == 1: #LINE# #TAB# #TAB# t = t[0] #LINE# #TAB# elif len(t) == 2: #LINE# #TAB# #TAB# t = t[1] #LINE# #TAB# return t
"#LINE# #TAB# LOG.debug('remove_google_details() called') #LINE# #TAB# session = bc.get_writer_session() #LINE# #TAB# binding = session.query(nexus_models_v2.NexusNVEBinding).filter_by(vni #LINE# #TAB# #TAB# =vni, switch_ip=switch_ip, device_id=device_id).one() #LINE# #TAB# if binding: #LINE# #TAB# #TAB# session.delete(binding) #LINE# #TAB# #TAB# session.flush() #LINE# #TAB# #TAB# return binding"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return arrow.get(value).datetime #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
#LINE# #TAB# shuffle(lst) #LINE# #TAB# return lst[::-1]
"#LINE# #TAB# return [{'id': rep.id, 'name': rep.name} for rep in Repository.query. #LINE# #TAB# #TAB# order_by(None)]"
"#LINE# #TAB# operative_conig = None #LINE# #TAB# for root, dirs, files in os.walk(restore_dir): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.endswith('.latst.conig'): #LINE# #TAB# #TAB# #TAB# #TAB# operative_conig = os.path.basename(f) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if not operative_conig: #LINE# #TAB# #TAB# raise RuntimeError('No saved operative_config at {}'.format(restore_dir) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return operative_conig"
#LINE# #TAB# mapping = dict() #LINE# #TAB# for activity in trace: #LINE# #TAB# #TAB# if activity not in mapping: #LINE# #TAB# #TAB# #TAB# mapping[activity] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mapping[activity] += 1 #LINE# #TAB# return mapping
"#LINE# #TAB# dn = '%s.%s.%s' % (version[0], version[2], version[3]) #LINE# #TAB# return dn"
#LINE# #TAB# version = get_version() #LINE# #TAB# if version.major == 0: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return api.show() #LINE# #TAB# return 0
"#LINE# #TAB# roles_file_root = os.path.dirname(pkg_libraries.__file__) #LINE# #TAB# return [ #LINE# #TAB# #TAB# fname.replace("".py"", """") #LINE# #TAB# #TAB# for fname in os.listdir(roles_file_root) #LINE# #TAB# #TAB# if fname.endswith("".pyc"") #LINE# #TAB# ]"
"#LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# rgb_dict = yaml.safe_load(f.read()) #LINE# #TAB# return rgb_dict"
"#LINE# #TAB# for prop_name, prop_type in model.relationships.items(): #LINE# #TAB# #TAB# if prop_type =='relationship': #LINE# #TAB# #TAB# #TAB# path = model.relationships[prop_name]['path'] #LINE# #TAB# #TAB# #TAB# props[prop_name] = Path(path) #LINE# #TAB# #TAB# elif prop_type == 'activity': #LINE# #TAB# #TAB# #TAB# props.pop(prop_name) #LINE# #TAB# #TAB# elif prop_type == 'date': #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# del props[prop_name] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# map_dir = os.path.join(os.path.dirname(__file__),'maps') #LINE# #TAB# map_files = [os.path.join(map_dir, f) for f in os.listdir(map_dir) if #LINE# #TAB# #TAB# os.path.isfile(os.path.join(map_dir, f)) and not f.startswith('.')] #LINE# #TAB# return map_files"
"#LINE# #TAB# return {'name': wallet_name, 'config': {'engine': 'postgres', 'host': #LINE# #TAB# #TAB# 'localhost', 'port': 443, 'dbname': 'localhost', 'user': {'name': #LINE# #TAB# #TAB# 'localhost', 'password': '127.0.0.1', 'port': 59}})[wallet_name]"
#LINE# #TAB# try: #LINE# #TAB# #TAB# tuple(s) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
#LINE# #TAB# flux_cmd ='samtools flux -f'+ flux_path #LINE# #TAB# if no_errors: #LINE# #TAB# #TAB# return flux_cmd #LINE# #TAB# return flux_cmd
#LINE# #TAB# try: #LINE# #TAB# #TAB# return request.registry.ServerGroup.list() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# result = {} #LINE# #TAB# if force: #LINE# #TAB# #TAB# with open(file_name, 'r') as fp: #LINE# #TAB# #TAB# #TAB# data = json.load(fp) #LINE# #TAB# else: #LINE# #TAB# #TAB# with open(file_name, 'r') as fp: #LINE# #TAB# #TAB# #TAB# data = json.load(fp) #LINE# #TAB# parent_dir = os.path.dirname(file_name) #LINE# #TAB# for key in result: #LINE# #TAB# #TAB# result[key] = parent_dir + '/' + key #LINE# #TAB# return result"
"#LINE# #TAB# if not path: #LINE# #TAB# #TAB# path = [] #LINE# #TAB# for key in b: #LINE# #TAB# #TAB# if key in a: #LINE# #TAB# #TAB# #TAB# if isinstance(a[key], dict) and key in a: #LINE# #TAB# #TAB# #TAB# #TAB# r = a[key] #LINE# #TAB# #TAB# #TAB# #TAB# a[key] = r #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# a[key] = b[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# a[key] = b[key] #LINE# #TAB# return a"
#LINE# #TAB# for item in delta: #LINE# #TAB# #TAB# item['resource_type'] = item['resource_type'] #LINE# #TAB# #TAB# if item['resource'] is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield item
"#LINE# #TAB# filter_fn = filter_fn or _default_filter #LINE# #TAB# matches = {} #LINE# #TAB# for k, v in obs.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# filter_fn(k, v) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# match = _unit_matcher(filter_fn, owner, unit_type, tag) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# matches[k] = match #LINE# #TAB# return matches"
"#LINE# #TAB# if isinstance(pe, _bp('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('ContainerEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('ContainerEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('ContainerEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('ContainerEntity')): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# shelf = TableShelf(table) #LINE# #TAB# randomize_columns(shelf) #LINE# #TAB# return shelf
"#LINE# #TAB# checksum = file.stat().st_size #LINE# #TAB# if checksum!= file.stat().st_mtime: #LINE# #TAB# #TAB# with open(file, 'rb') as f: #LINE# #TAB# #TAB# #TAB# with open(file.path, 'rb') as f: #LINE# #TAB# #TAB# #TAB# #TAB# file_data = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# return sasert_unpack_file(file_data) #LINE# #TAB# return None"
"#LINE# #TAB# text = re.sub('---', smart_dash_generator, text) #LINE# #TAB# text = re.sub('---', smart_em_dash_generator, text) #LINE# #TAB# return text"
#LINE# #TAB# #TAB# if response is None: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if 'expiration' not in response: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(s, list): #LINE# #TAB# #TAB# return [merge_data(v) for v in s] #LINE# #TAB# return s"
#LINE# #TAB# index = 0 #LINE# #TAB# while urls: #LINE# #TAB# #TAB# match = regex.search(urls[index]) #LINE# #TAB# #TAB# if not match: #LINE# #TAB# #TAB# #TAB# urls[index] = [] #LINE# #TAB# #TAB# #TAB# index += 1 #LINE# #TAB# return urls
#LINE# #TAB# condition1 = bs_terms[0] >= 0 and bs_terms[0] <= bp_terms[0] and ( #LINE# #TAB# #TAB# bs_terms[1] <= 0 or bs_terms[1] >= bp_terms[1]): #LINE# #TAB# #TAB# condition2 = bs_terms[1] <= 0 and bp_terms[1] >= 0 #LINE# #TAB# #TAB# condition3 = bs_terms[2] <= 0 and bp_terms[3] >= 0 #LINE# #TAB# #TAB# if condition1 and condition2 and condition3: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# for key, value in cls.liv.items(): #LINE# #TAB# #TAB# setattr(cls, '%s_liv' % key, value) #LINE# #TAB# return cls"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('dense', dtype='d', direction=function.IN) #LINE# #TAB# function.result_type = 'i' #LINE# #TAB# return function"
"#LINE# #TAB# if ignore is None: #LINE# #TAB# #TAB# ignore = [] #LINE# #TAB# root = os.path.dirname(path) #LINE# #TAB# path_list = os.listdir(root) #LINE# #TAB# for name in path_list: #LINE# #TAB# #TAB# full_path = os.path.join(root, name) #LINE# #TAB# #TAB# if os.path.isdir(full_path): #LINE# #TAB# #TAB# #TAB# create_ast_normalized_numpy(full_path, ignore) #LINE# #TAB# #TAB# elif os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# create_ast_normalized_numpy(full_path, ignore) #LINE# #TAB# return ignore"
#LINE# #TAB# cleaned_input_toml = input_toml.strip() #LINE# #TAB# return cleaned_input_toml
"#LINE# #TAB# name, top = index #LINE# #TAB# while top is not None: #LINE# #TAB# #TAB# if isinstance(top, (list, tuple)): #LINE# #TAB# #TAB# #TAB# name = '.'.join(top) #LINE# #TAB# #TAB# elif isinstance(top, Node): #LINE# #TAB# #TAB# #TAB# name = top.node.name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise TypeError('invalid index: %s' % (top,)) #LINE# #TAB# #TAB# top = top.parent #LINE# #TAB# return name"
#LINE# #TAB# mag_spectrum = np.sqrt(image ** 2 + image ** 3) #LINE# #TAB# return mag_spectrum
"#LINE# #TAB# is_history = False #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# return is_history #LINE# #TAB# elif os.path.isfile(path): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fh = open(path, 'r') #LINE# #TAB# #TAB# #TAB# is_history = fh.readline().strip() == 'history' #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if is_history: #LINE# #TAB# #TAB# #TAB# #TAB# is_history = True #LINE# #TAB# else: #LINE# #TAB# #TAB# is_history = True #LINE# #TAB# fh.close() #LINE# #TAB# return is_history"
#LINE# #TAB# message = exp.get_message() #LINE# #TAB# if message is None: #LINE# #TAB# #TAB# message = exp #LINE# #TAB# message = message.strip() #LINE# #TAB# return message
#LINE# #TAB# user = network.get_user_by_login(login_or_id) #LINE# #TAB# if user is None: #LINE# #TAB# #TAB# raise ValueError('User %s does not exist' % login_or_id) #LINE# #TAB# if user.is_locked(): #LINE# #TAB# #TAB# return user #LINE# #TAB# with db.session.begin_nested(): #LINE# #TAB# #TAB# user_locked = db.session.query(User).filter(User.login == login_or_id).one() #LINE# #TAB# #TAB# user_locked.delete() #LINE# #TAB# #TAB# db.session.commit() #LINE# #TAB# return user_locked
"#LINE# #TAB# derivative_document = {} #LINE# #TAB# temporal_derivatives = [ #LINE# #TAB# #TAB# (0), #LINE# #TAB# #TAB# (1), #LINE# #TAB# #TAB# (2), #LINE# #TAB# #TAB# (3), #LINE# #TAB# #TAB# (4), #LINE# #TAB# #TAB# (5), #LINE# #TAB# #TAB# (6, 8), #LINE# #TAB# #TAB# (7, 9), #LINE# #TAB# #TAB# (10), #LINE# #TAB# #TAB# (11, 13), #LINE# #TAB# #TAB# (12), #LINE# #TAB# #TAB# (13), #LINE# #TAB# #TAB# (14), #LINE# #TAB# #TAB# (15), #LINE# #TAB# #TAB# (16), #LINE# #TAB# #TAB# (19, 20), #LINE# #TAB# #TAB# ] #LINE# #TAB# return derivative_document"
"#LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# line = in_handle.readline().strip() #LINE# #TAB# #TAB# n_header = line.split()[0] #LINE# #TAB# #TAB# x_min = int(n_header[0]) #LINE# #TAB# #TAB# x_max = int(n_header[1]) #LINE# #TAB# #TAB# y_min = int(n_header[2]) #LINE# #TAB# #TAB# y_max = int(n_header[3]) #LINE# #TAB# #TAB# w = float(n_header[4]) / float(n_header[5]) #LINE# #TAB# #TAB# h = float(n_header[6]) / float(n_header[7]) #LINE# #TAB# #TAB# return w, x_min, y_min, h"
#LINE# #TAB# if type(_yaml_bool_value) == bool: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# nd_request = nd.Request(req=req, collector_addr=collector_addr, prefix=prefix) #LINE# #TAB# for k, v in req.metadata.items(): #LINE# #TAB# #TAB# v.end = True #LINE# #TAB# #TAB# nd_request.metadata[k] = v #LINE# #TAB# return nd_request"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return zk.get_min_isr(topic) #LINE# #TAB# except kazoo.client.NoNodeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# min_val = np.inf #LINE# #TAB# max_val = np.inf #LINE# #TAB# has_nan = ~np.isfinite(values) #LINE# #TAB# has_inf = ~np.isfinite(values) #LINE# #TAB# if has_nan or has_inf: #LINE# #TAB# #TAB# min_val = min(values) #LINE# #TAB# if has_nan: #LINE# #TAB# #TAB# max_val = np.inf #LINE# #TAB# elif has_inf: #LINE# #TAB# #TAB# max_val = max(values) #LINE# #TAB# return min_val, max_val"
