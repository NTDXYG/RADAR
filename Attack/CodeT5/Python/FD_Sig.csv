def is_printable(c): #LINE# #TAB# if c == '\x00': #LINE# #TAB# #TAB# return True #LINE# #TAB# if c in string.printable: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"def get_bounds(p_state, idx_image=-1, idx_chain=-1): #LINE# #TAB# bb_min = float(ctypes.c_float)() #LINE# #TAB# bb_max = float(ctypes.c_float)() #LINE# #TAB# _Get_Bounds(ctypes.c_void_p(p_state), bb_min, bb_max, ctypes.c_int( #LINE# #TAB# #TAB# idx_image), ctypes.c_int(idx_chain)) #LINE# #TAB# return bb_min, bb_max"
"def get_command_and_argv(argv): #LINE# #TAB# command_name = argv[0] #LINE# #TAB# if command_name == settings.command: #LINE# #TAB# #TAB# argv = argv[1:] #LINE# #TAB# elif command_name == settings.command: #LINE# #TAB# #TAB# argv.remove(command_name) #LINE# #TAB# return command_name, argv"
"def get_next_slip(raw): #LINE# #TAB# raw_str = str(raw) #LINE# #TAB# pad_len = 2 #LINE# #TAB# if raw_str[-pad_len:] == b'\x00\x00\x00': #LINE# #TAB# #TAB# pad_len = 4 #LINE# #TAB# #TAB# return raw_str[:pad_len], raw_str[-pad_len:] #LINE# #TAB# if raw_str[-pad_len:] == b'\x00\x00\x00': #LINE# #TAB# #TAB# return raw_str[-pad_len:] #LINE# #TAB# return None, None"
"def sc_encode(gain, peak): #LINE# #TAB# if gain > 0: #LINE# #TAB# #TAB# gain = int(gain) #LINE# #TAB# if peak > 0: #LINE# #TAB# #TAB# peak = int(peak) #LINE# #TAB# else: #LINE# #TAB# #TAB# peak = '' #LINE# #TAB# return _encode_int(gain), _encode_int(peak) + ','"
"def retryable_http_error( e ): #LINE# #TAB# if isinstance(e, urllib.error.HTTPError) and e.code in ('503', '408', '500' #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(e, BadStatusLine): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
def get_mime_type(page): #LINE# #TAB# mime_type = None #LINE# #TAB# meta = page.meta #LINE# #TAB# if meta: #LINE# #TAB# #TAB# mime_type = meta['content-type'] #LINE# #TAB# if mime_type is None: #LINE# #TAB# #TAB# if meta['charset']: #LINE# #TAB# #TAB# #TAB# mime_type = 'text/plain' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mime_type = meta['charset'] #LINE# #TAB# return mime_type
"def floating_number(value, minimum=None, maximum=None, cut=False, pad=False): #LINE# #TAB# try: #LINE# #TAB# #TAB# ret = float(value) #LINE# #TAB# #TAB# if cut: #LINE# #TAB# #TAB# #TAB# ret = int(value) #LINE# #TAB# #TAB# if maximum: #LINE# #TAB# #TAB# #TAB# ret = max(float(value), minimum) #LINE# #TAB# #TAB# if cut: #LINE# #TAB# #TAB# #TAB# ret = min(float(ret), maximum) #LINE# #TAB# #TAB# if pad: #LINE# #TAB# #TAB# #TAB# return int(ret) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return ret #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# if not cut: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return ret"
"def get_dependants(cls, dist): #LINE# #TAB# #TAB# if dist.lower() in cls.DEPENDENCIES: #LINE# #TAB# #TAB# #TAB# for r in cls.get_dependants(dist): #LINE# #TAB# #TAB# #TAB# #TAB# yield r"
"def center_fwhm_bymoments(signal): #LINE# #TAB# moments = np.linalg.moments(signal) #LINE# #TAB# max_width = np.max(moments[0]) #LINE# #TAB# center = signal[0:2] + moments[2] * max_width #LINE# #TAB# return center, full_width"
def get_out_object(ctx): #LINE# #TAB# out_object = ctx.out_object #LINE# #TAB# return out_object
"def get_trust_client(trust_id): #LINE# #TAB# auth_user, auth_pass = get_user_auth(trust_id) #LINE# #TAB# trust_client = keystoneclient.Client(auth_user=auth_user, auth_pass=auth_pass) #LINE# #TAB# _trust_client = trust_client.trusts.get(trust_id) #LINE# #TAB# if _trust_client: #LINE# #TAB# #TAB# return _trust_client #LINE# #TAB# else: #LINE# #TAB# #TAB# return trust_client"
"def multi_split(txt, delims): #LINE# #TAB# rsplit = [] #LINE# #TAB# for delim in delims: #LINE# #TAB# #TAB# if delim == '\\': #LINE# #TAB# #TAB# #TAB# rsplit.append(txt) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# rsplit.append(txt[delim:]) #LINE# #TAB# return rsplit"
"def get_bool(prompt): #LINE# #TAB# try: #LINE# #TAB# #TAB# bool_str = str_2_bool(raw_input(prompt)) #LINE# #TAB# except PipelineError: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""'{0}' did not match a boolean expression (true/false, yes/no, t/f, y/n)"" #LINE# #TAB# #TAB# #TAB#.format(bool_str)) #LINE# #TAB# #TAB# return get_bool(prompt) #LINE# #TAB# return bool_str"
"def extract_lzh(archive, compression, cmd, verbosity, interactive, outdir): #LINE# #TAB# opts = 'x' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# opts += ""w=%s"" % outdir #LINE# #TAB# return [cmd, opts, archive]"
def get_user_information(): #LINE# #TAB# import inspect #LINE# #TAB# frame = inspect.currentframe() #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = frame.f_back.f_locals #LINE# #TAB# #TAB# #TAB# user_name = data[0] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# except: #LINE# #TAB# #TAB# user_name = None #LINE# #TAB# if user_name is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return user_name
"def find_minimum_spanning_forest_as_subgraphs(graph): #LINE# #TAB# forest = find_minimum_spanning_forest(graph) #LINE# #TAB# list_of_subgraphs = [get_subgraph_from_edge_list(graph, edge_list) for edge_list in forest] #LINE# #TAB# return list_of_subgraphs"
"def list_instances(name_only=True): #LINE# #TAB# command = ['compute', 'instances'] #LINE# #TAB# if name_only: #LINE# #TAB# #TAB# command.append('name') #LINE# #TAB# else: #LINE# #TAB# #TAB# command.append('name') #LINE# #TAB# res = __salt__['cmd.run_all'](command, python_shell=False) #LINE# #TAB# instances = [] #LINE# #TAB# if res['retcode']!= 0: #LINE# #TAB# #TAB# raise CommandExecutionError('{0}: {1}'.format(res['stdout'], res['stderr'])) #LINE# #TAB# elif res['stdout']: #LINE# #TAB# #TAB# raise CommandExecutionError('{0}: {1}'.format(res['stdout'], res['stderr'])) #LINE# #TAB# return instances"
"def check_list_type(objects, allowed_type, name, allow_none=True): #LINE# #TAB# if allow_none: #LINE# #TAB# #TAB# allowed_type = None #LINE# #TAB# assert isinstance(objects, list) #LINE# #TAB# for obj in objects: #LINE# #TAB# #TAB# if obj is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not isinstance(obj, allowed_type): #LINE# #TAB# #TAB# #TAB# raise TypeError(f'{name} must be a {type(allowed_type)}') #LINE# #TAB# return objects"
"def convert_stress_to_mass(q, width, length, gravity): #LINE# #TAB# d = length / np.sqrt(q ** 2 + width ** 2 + gravity ** 2) #LINE# #TAB# if d < 0.001: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# elif d < 0.01: #LINE# #TAB# #TAB# return 1.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 2.0 / d"
def parse_py_config(): #LINE# #TAB# from django.conf import settings #LINE# #TAB# apps = [] #LINE# #TAB# if 'dever_config' in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# for app in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# apps.append(import_module(app)) #LINE# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# apps.append(app) #LINE# #TAB# return apps
"def get_json_content(file_path): #LINE# #TAB# with open(file_path, 'r') as data_file: #LINE# #TAB# #TAB# data = json.load(data_file) #LINE# #TAB# #TAB# return data"
"def lookup_needs_distinct(opts, lookup_path): #LINE# #TAB# field_name = lookup_path.split('__', 1)[0] #LINE# #TAB# field = opts.get_field(field_name) #LINE# #TAB# if hasattr(field,'rel') and isinstance(field.rel, models.ManyToManyRel #LINE# #TAB# #TAB# ) or isinstance(field, models.related.RelatedObject #LINE# #TAB# #TAB# ) and not field.field.unique: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def unpatch_app(app): #LINE# #TAB# patched_apps = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# for method_name in app.__dict__: #LINE# #TAB# #TAB# #TAB# if method_name.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# wrapped = getattr(app, method_name, None) #LINE# #TAB# #TAB# #TAB# if wrapped is not None: #LINE# #TAB# #TAB# #TAB# #TAB# patched_apps.append(wrapped) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return patched_apps"
"def get_day_counts(d, country): #LINE# #TAB# d = d.copy() #LINE# #TAB# d.update(country=country) #LINE# #TAB# days = 1 #LINE# #TAB# spread = 1 #LINE# #TAB# for i in range(1, len(d) - 1): #LINE# #TAB# #TAB# if d[i + 1] == country: #LINE# #TAB# #TAB# #TAB# spread += d[i + 1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if spread > 500: #LINE# #TAB# #TAB# #TAB# days += 1 #LINE# #TAB# return days, spread"
"def clean_fields(allowed_fields: dict, fields: FieldsParam) ->Iterable[str]: #LINE# #TAB# if fields == ALL: #LINE# #TAB# #TAB# fields = allowed_fields.keys() #LINE# #TAB# else: #LINE# #TAB# #TAB# fields = tuple(fields) #LINE# #TAB# #TAB# unknown_fields = set(fields) - allowed_fields.keys() #LINE# #TAB# #TAB# if unknown_fields: #LINE# #TAB# #TAB# #TAB# raise ValueError('Unknown fields: {}'.format(unknown_fields)) #LINE# #TAB# return fields"
def should_update_package(version): #LINE# #TAB# if version is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# parts = version.split('.') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# if int(part)!= int(part): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"def parse_header_row(row: List[Any], file_name=None) ->HeaderRecord: #LINE# #TAB# header_record = HeaderRecord() #LINE# #TAB# header_record.name = row[0] #LINE# #TAB# header_record.num_elements = row[1] #LINE# #TAB# header_record.value = row[2] #LINE# #TAB# header_record.sub_elements = row[3] #LINE# #TAB# return header_record"
"def _get_row_tag(row, tag): #LINE# #TAB# #TAB# columns = [] #LINE# #TAB# #TAB# if tag in row: #LINE# #TAB# #TAB# #TAB# columns.append(row[tag]) #LINE# #TAB# #TAB# return columns"
def get_rotationtable(): #LINE# #TAB# global McQuillanRotationTable #LINE# #TAB# if McQuillanRotationTable is None: #LINE# #TAB# #TAB# McQuillanRotationTable = {} #LINE# #TAB# return McQuillanRotationTable
"def lag_observations(observations, lag, stride=1): #LINE# #TAB# n = len(observations) #LINE# #TAB# if lag < stride: #LINE# #TAB# #TAB# raise Exception('Lag should be larger than stride') #LINE# #TAB# index = int(n - lag * stride) #LINE# #TAB# new_observations = np.empty(shape=(n,)) #LINE# #TAB# for i in range(n - lag): #LINE# #TAB# #TAB# new_observations[i:] = observations[i + lag:i + stride] #LINE# #TAB# return new_observations"
"def get_pv_args(name, session=None, call=None): #LINE# #TAB# if call == 'action': #LINE# #TAB# #TAB# raise SaltCloudSystemExit( #LINE# #TAB# #TAB# #TAB# 'The get_pv_args function must be called with -f or --function.' #LINE# #TAB# #TAB# ) #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = get_session() #LINE# #TAB# pv_args = session.pv_args(name) #LINE# #TAB# return pv_args"
"def get_api_key(username, password): #LINE# #TAB# user_db = init_db() #LINE# #TAB# user = user_db.find_one({'username': username}) #LINE# #TAB# user.set_password(password) #LINE# #TAB# api_key = user.api_key #LINE# #TAB# logging.debug('Authenticated user {} with password {}'.format(username, #LINE# #TAB# #TAB# password)) #LINE# #TAB# return api_key"
"def check_time_only(time, signal, verb): #LINE# #TAB# if len(time)!= len(signal): #LINE# #TAB# #TAB# raise ValueError('Time and signal must have the same length.') #LINE# #TAB# if verb: #LINE# #TAB# #TAB# if time.ndim!= 1: #LINE# #TAB# #TAB# #TAB# raise ValueError('Time and signal must have 1 dimension.') #LINE# #TAB# if signal.ndim!= 1: #LINE# #TAB# #TAB# raise ValueError('Signal must have 1 dimension.') #LINE# #TAB# if not time.ndim == 2: #LINE# #TAB# #TAB# raise ValueError('Time only has 2 dimensions.') #LINE# #TAB# if not signal.ndim == 2: #LINE# #TAB# #TAB# raise ValueError('Signal only has 2 dimensions.') #LINE# #TAB# return True"
"def get_etree_root(doc, encoding=None): #LINE# #TAB# if etree is None: #LINE# #TAB# #TAB# etree = _get_etree(doc, encoding=encoding) #LINE# #TAB# #TAB# if etree is None: #LINE# #TAB# #TAB# #TAB# raise etree.XMLSyntaxError('Cannot parse document.') #LINE# #TAB# #TAB# root = etree.Element(doc) #LINE# #TAB# else: #LINE# #TAB# #TAB# root = etree.Element(doc) #LINE# #TAB# return root"
"def window_sum(data, window_len): #LINE# #TAB# sum_data = [] #LINE# #TAB# for row in data: #LINE# #TAB# #TAB# sum_data.append(row) #LINE# #TAB# #TAB# if len(sum_data) >= window_len: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return sum_data"
"def calculate_overlap(min1, max1, min2, max2): #LINE# #TAB# total1 = max1 - min1 #LINE# #TAB# total2 = min2 - min2 #LINE# #TAB# overlap = float(total1 / total2) #LINE# #TAB# if overlap > 0.0: #LINE# #TAB# #TAB# return overlap / 100.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0.0"
"def need_regeneration(directory, cache_file_path, cache_mtime): #LINE# #TAB# return os.path.isdir(os.path.join(directory, cache_file_path)) and os.path.getmtime( #LINE# #TAB# #TAB# os.path.join(directory, cache_file_path)) > cache_mtime"
def get_operator_name(name=None): #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = operator.__name__ #LINE# #TAB# for operator in operator.__all__: #LINE# #TAB# #TAB# if name in operator.__name__: #LINE# #TAB# #TAB# #TAB# return operator.__name__ #LINE# #TAB# else: #LINE# #TAB# #TAB# return'safe'
"def generate_gpt_partitions(partition_dicts): #LINE# #TAB# return [Partition(key='name', value=partition_dict['p']) for #LINE# #TAB# #TAB# partition_dict in partition_dicts]"
"def get_no_nan(array, idx_col): #LINE# #TAB# no_nan_idx = 0 #LINE# #TAB# for val in array[idx_col]: #LINE# #TAB# #TAB# if np.isnan(val): #LINE# #TAB# #TAB# #TAB# no_nan_idx += 1 #LINE# #TAB# return no_nan_idx"
def check_existance(paths): #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"def restart_cluster(name): #LINE# #TAB# ret = {'name': name, #LINE# #TAB# #TAB# 'changes': {}, #LINE# #TAB# #TAB#'result': None, #LINE# #TAB# #TAB# 'comment': ''} #LINE# #TAB# if __opts__['test']: #LINE# #TAB# #TAB# ret['comment'] = 'Restarting cluster {}'.format(name) #LINE# #TAB# #TAB# return ret #LINE# #TAB# __salt__['traffic_manager.restart_cluster']() #LINE# #TAB# ret['result'] = True #LINE# #TAB# ret['comment'] = 'Restart cluster {}'.format(name) #LINE# #TAB# return ret"
"def get_handler(handler_name): #LINE# #TAB# module_name, handler_name = handler_name.rsplit('.', 1) #LINE# #TAB# module = import_module(module_name) #LINE# #TAB# handler = getattr(module, handler_name) #LINE# #TAB# return handler"
def get_tools(): #LINE# #TAB# py_tools = {} #LINE# #TAB# for tool in TOOLS: #LINE# #TAB# #TAB# if tool.exists(): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# py_tools[tool.name] = tool #LINE# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# if tool.name not in py_tools: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# py_tools[tool.name] = tool #LINE# #TAB# return py_tools
"def get_store_for_field(field): #LINE# #TAB# store = None #LINE# #TAB# name = field.getName() #LINE# #TAB# widget = field.widget #LINE# #TAB# if hasattr(widget,'store'): #LINE# #TAB# #TAB# store = widget.store #LINE# #TAB# if name: #LINE# #TAB# #TAB# store = store.name #LINE# #TAB# return store"
def get_current_waypoints(boatd=None): #LINE# #TAB# if boatd is None: #LINE# #TAB# #TAB# boatd = Boatd() #LINE# #TAB# content = boatd.get('/waypoints') #LINE# #TAB# return [Point(*coords) for coords in content.get('waypoints')]
"def get_default_path(): #LINE# #TAB# path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# path = os.path.join(path, 'OutputData') #LINE# #TAB# return path"
"def get_fragment_lengths(fastafile): #LINE# #TAB# frag_lengths = {} #LINE# #TAB# for seq in SeqIO.parse(fastafile, ""fasta""): #LINE# #TAB# #TAB# id = seq.id #LINE# #TAB# #TAB# if id not in frag_lengths: #LINE# #TAB# #TAB# #TAB# frag_lengths[id] = 0 #LINE# #TAB# #TAB# frag_lengths[id] += len(seq.get_fragment_lengths(id)) #LINE# #TAB# return frag_lengths"
def get_device_count(): #LINE# #TAB# c = ctypes.c_int(0) #LINE# #TAB# r = lib.TCOD_get_device_count(ctypes.byref(c)) #LINE# #TAB# assert r == 0 #LINE# #TAB# return c.value
"def psi_mgoh_oh_so4_hmw84(T, P): #LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"def dict_to_nvlist(dict): #LINE# #TAB# result = [] #LINE# #TAB# for item in list(dict.keys()): #LINE# #TAB# #TAB# result.append(SDOPackage.NameValue(item, omniORB.any.to_any(dict[ #LINE# #TAB# #TAB# #TAB# item]))) #LINE# #TAB# return result"
"def is_python_file(filename): #LINE# #TAB# if filename.endswith('.py'): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# with open_with_encoding(filename, limit_byte_check= #LINE# #TAB# #TAB# #TAB# MAX_PYTHON_FILE_DETECTION_BYTES) as f: #LINE# #TAB# #TAB# #TAB# text = f.read(MAX_PYTHON_FILE_DETECTION_BYTES) #LINE# #TAB# #TAB# #TAB# if not text: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# first_line = text.splitlines()[0] #LINE# #TAB# except (IOError, IndexError): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not PYTHON_SHEBANG_REGEX.match(first_line): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def authors_non_byline(soup, detail=""full""): #LINE# #TAB# contrib_type = ""author non-byline"" #LINE# #TAB# contributors_ = contributors(soup, detail) #LINE# #TAB# non_byline_authors = [author for author in contributors_ if author.get( #LINE# #TAB# #TAB# 'type', None) == contrib_type] #LINE# #TAB# position = 1 #LINE# #TAB# for author in non_byline_authors: #LINE# #TAB# #TAB# author['position'] = position #LINE# #TAB# #TAB# position = position + 1 #LINE# #TAB# return non_byline_authors"
"def browserify_file(entry_point, output_file, babelify=False, export_as=None): #LINE# #TAB# from.modules import browserify #LINE# #TAB# if not babelify: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'dependencies_fn': browserify.browserify_deps_entry_point, #LINE# #TAB# #TAB# #TAB# 'compiler_fn': browserify.browserify_compile_entry_point, #LINE# #TAB# #TAB# #TAB# 'input': entry_point, #LINE# #TAB# #TAB# #TAB# 'output': output_file, #LINE# #TAB# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# #TAB# 'babelify': babelify, #LINE# #TAB# #TAB# #TAB# }, #LINE# #TAB# #TAB# } #LINE# #TAB# return output_file"
def all_possible_combinations(barcode): #LINE# #TAB# combinations = [] #LINE# #TAB# for c in all_possible_combinations(barcode): #LINE# #TAB# #TAB# if c!='': #LINE# #TAB# #TAB# #TAB# combinations.append(tuple(c)) #LINE# #TAB# for c in all_possible_combinations(barcode[:-1]): #LINE# #TAB# #TAB# if c!='': #LINE# #TAB# #TAB# #TAB# combinations.append(tuple(c)) #LINE# #TAB# return combinations
"def get_markov_matrix(m): #LINE# #TAB# rowL = [] #LINE# #TAB# colL = [] #LINE# #TAB# for row in m: #LINE# #TAB# #TAB# temp = [float(i) for i in row] #LINE# #TAB# #TAB# for j in range(len(temp)): #LINE# #TAB# #TAB# #TAB# if temp[j]!= 0: #LINE# #TAB# #TAB# #TAB# #TAB# colL.append(temp[j]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# rowL.append(float(temp[j])) #LINE# #TAB# matrix = MarkovMatrix(len(rowL), len(colL)) #LINE# #TAB# for row in m: #LINE# #TAB# #TAB# matrix.add_row(row) #LINE# #TAB# return matrix"
"def search_sa_partyname(cls, name, clause): #LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.partyname, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"def align_with_mafft(file_to_align): #LINE# #TAB# clustered_fn = file_to_align #LINE# #TAB# cmd ='mafft -c {0} -o {1}'.format(clustered_fn, file_to_align) #LINE# #TAB# process = subprocess.Popen(cmd, stdout=subprocess.PIPE) #LINE# #TAB# stdout, stderr = process.communicate() #LINE# #TAB# if process.returncode!= 0: #LINE# #TAB# #TAB# logger.error(stderr) #LINE# #TAB# #TAB# return None #LINE# #TAB# return stdout"
"def encode_v2(value): #LINE# #TAB# encoded = '' #LINE# #TAB# for idx, char in enumerate(value): #LINE# #TAB# #TAB# if char == '::': #LINE# #TAB# #TAB# #TAB# encoded += char #LINE# #TAB# #TAB# elif char == '\\': #LINE# #TAB# #TAB# #TAB# encoded += '$' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# encoded += char #LINE# #TAB# return encoded"
"def get_tot_topics_moderate(forum): #LINE# #TAB# required_topics = forum.get('required_topics', []) #LINE# #TAB# tot_topics = 0 #LINE# #TAB# if required_topics: #LINE# #TAB# #TAB# for topic_num in required_topics: #LINE# #TAB# #TAB# #TAB# topics = forum.get_topic_moderate_topics(topic_num) #LINE# #TAB# #TAB# #TAB# tot_topics += len(topics) #LINE# #TAB# return tot_topics"
def is_square_matrix(array): #LINE# #TAB# if array.ndim!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# shape = array.shape #LINE# #TAB# if shape[0]!= shape[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(shape) == 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"def encode_pydantic_model(v: BaseModel) ->Any: #LINE# #TAB# return {'__kind__': kind_inst, 'class': fqname_for(v.__class__), 'args': #LINE# #TAB# #TAB# encode([str(v)])}"
"def json_api_document(): #LINE# #TAB# return {'body': [{'type': 'object', 'properties': {'name': {'type':'string'}, #LINE# #TAB# #TAB#'required': True}, {'type': 'array'}, {'type': 'object', 'properties': { #LINE# #TAB# #TAB# 'name': {'type':'string'},'required': True}, {'type': 'object', #LINE# #TAB# #TAB# 'properties': {'name': 'type','required': True}, {'name': #LINE# #TAB# #TAB# 'value','required': True}, {'type': 'array'}, {'name': #LINE# #TAB# #TAB# 'value','required': True}, {'type': 'array'}, {'type': 'object', #LINE# #TAB# #TAB# 'properties': {'name': 'type','required': True}, {'type': 'array'}}}]}"
"def get_current_branch(repo): #LINE# #TAB# try: #LINE# #TAB# #TAB# return next(filter(lambda x: x!='master', repo.heads)) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return'master'"
"def json_format_graph(graph_object, graph_identifier): #LINE# #TAB# ordered_dict = {} #LINE# #TAB# for node in graph_object: #LINE# #TAB# #TAB# ordered_dict[node.id] = dict() #LINE# #TAB# #TAB# ordered_dict[node.id] = dict() #LINE# #TAB# #TAB# if graph_identifier in node: #LINE# #TAB# #TAB# #TAB# ordered_dict[node[graph_identifier]] = node[graph_identifier] #LINE# #TAB# for edge in graph_object.edges: #LINE# #TAB# #TAB# json_format_edge(edge, graph_identifier) #LINE# #TAB# return ordered_dict"
"def socket_accept(descriptor): #LINE# #TAB# client, address = descriptor #LINE# #TAB# try: #LINE# #TAB# #TAB# client.accept(address) #LINE# #TAB# #TAB# return client, address #LINE# #TAB# except socket.error as ex: #LINE# #TAB# #TAB# if ex.errno in (errno.ECONNREFUSED, errno.EINVAL): #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise"
def ros_transform(dual_quat): #LINE# #TAB# transform = geometry_msgs.msg.Transform() #LINE# #TAB# transform.set_dual_quat(dual_quat) #LINE# #TAB# return transform
"def merge_blocks(blocks): #LINE# #TAB# out = np.zeros(blocks[0].shape, dtype=blocks[0].dtype) #LINE# #TAB# for i in range(blocks[1]): #LINE# #TAB# #TAB# out[i] = blocks[i] #LINE# #TAB# return out"
def zart_flatten(a: Iterable) ->List: #LINE# #TAB# b = list() #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# e = a.pop(0) #LINE# #TAB# #TAB# #TAB# #TAB# b.append(e) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return b
"def create_attributes(klass, attributes, previous_object=None): #LINE# #TAB# result = {} #LINE# #TAB# if previous_object is not None: #LINE# #TAB# #TAB# result = {k: v for k, v in previous_object.to_json().items() if k!= #LINE# #TAB# #TAB# #TAB#'sys'} #LINE# #TAB# result.update(attributes) #LINE# #TAB# return result"
"def output_json_pretty(request, data, code, headers=None): #LINE# #TAB# settings = request.registry.settings #LINE# #TAB# if settings.get('debug', False): #LINE# #TAB# #TAB# settings['indent'] = 4 #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = make_response(dumped, code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
"def reindex_df(subdomain_df): #LINE# #TAB# subdomain_idx = subdomain_df.index.values #LINE# #TAB# subdomain_idx = subdomain_idx[subdomain_idx == 0] #LINE# #TAB# subdomain_df.loc[subdomain_idx, 'index'] = subdomain_df.index.astype('category') #LINE# #TAB# return subdomain_df"
"def b32_normalize(key: str) ->str: #LINE# #TAB# if len(key)!= 32: #LINE# #TAB# #TAB# raise ValueError('invalid key length %d' % len(key)) #LINE# #TAB# normalized = base64.urlsafe_b64encode(key).decode('utf-8') #LINE# #TAB# padding = b32_pad(normalized) #LINE# #TAB# if padding!= b32_pad(normalized): #LINE# #TAB# #TAB# raise ValueError('padding %d!= %d' % (len(normalized), padding)) #LINE# #TAB# return normalized"
"def psi_k_hco3_so4_hmw84(T, P): #LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"def get_node(conn, name): #LINE# #TAB# for node in conn.list_nodes()['nodes']: #LINE# #TAB# #TAB# if node['name'] == name: #LINE# #TAB# #TAB# #TAB# return node #LINE# #TAB# return None"
"def prep_collection_image(imgcol, collection, date, validate=False): #LINE# #TAB# if imgcol.format == 'JPEG': #LINE# #TAB# #TAB# imgcol = imgcol.convert_to_jpeg() #LINE# #TAB# if date is None: #LINE# #TAB# #TAB# date = datetime.date.today() #LINE# #TAB# image = save_collection_image(imgcol, collection, date) #LINE# #TAB# if validate: #LINE# #TAB# #TAB# image = validate_image(image) #LINE# #TAB# return image"
def line_search_subject_is_valid(line): #LINE# #TAB# l = line.split() #LINE# #TAB# if len(l) > 1: #LINE# #TAB# #TAB# if l[-1].strip() =='search': #LINE# #TAB# #TAB# #TAB# return True
"def carry_over_color(lines): #LINE# #TAB# has_color = False #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith(""^""): #LINE# #TAB# #TAB# #TAB# has_color = True #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif has_color: #LINE# #TAB# #TAB# #TAB# end = line.find(""^"") #LINE# #TAB# #TAB# #TAB# if line.endswith(""*""): #LINE# #TAB# #TAB# #TAB# #TAB# end = line.find(""*"") #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# end = line[:end] #LINE# #TAB# #TAB# yield line + ""\x1b[0m"" #LINE# #TAB# #TAB# has_color = False"
"def relative_datetime(relPeriod='today', tz=None): #LINE# #TAB# if tz is None: #LINE# #TAB# #TAB# tz = pytz.timezone('UTC') #LINE# #TAB# dttm = datetime.timedelta(days=1) #LINE# #TAB# if relPeriod == 'today': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() #LINE# #TAB# elif relPeriod =='monthly': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() - timedelta(days=1) #LINE# #TAB# elif relPeriod == 'yesterday': #LINE# #TAB# #TAB# dttm = datetime.datetime.now() - timedelta(days=1) #LINE# #TAB# d = tz.localize(dttm) #LINE# #TAB# return d"
"def load_model(path): #LINE# #TAB# with h5py.File(path, 'r') as f: #LINE# #TAB# #TAB# model = pickle.load(f) #LINE# #TAB# return model"
"def want_color_output(): #LINE# #TAB# return os.environ.get('COCOTB_ANSI_OUTPUT', '1') == '1' or os.environ.get( #LINE# #TAB# #TAB# 'COCOTB_ANSI_OUTPUT', '0') == '1'"
def is_float(s): #LINE# #TAB# try: #LINE# #TAB# #TAB# float(s) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"def convert_color_rgb(c: Union[str, tuple]) ->Tuple[float, float, float]: #LINE# #TAB# if isinstance(c, str): #LINE# #TAB# #TAB# c = [c] #LINE# #TAB# c = tuple(map(convert_color, c)) #LINE# #TAB# if len(c)!= 6: #LINE# #TAB# #TAB# raise ValueError('input #{0} is not a valid color tuple'.format(c)) #LINE# #TAB# alpha = 1 #LINE# #TAB# r, g, b = c[:2], c[2:] #LINE# #TAB# return r / 255.0, g / 255.0, b / 255.0"
"def threshold_predict(X, w, theta): #LINE# #TAB# y = np.zeros(X.shape) #LINE# #TAB# for i in range(1, len(X)): #LINE# #TAB# #TAB# y[i] = threshold_predict_1d(X[i], w, theta) #LINE# #TAB# return y"
"def fix_orig_vcf_refs(data): #LINE# #TAB# if dd.get_batch(data) == ""ensemble"": #LINE# #TAB# #TAB# data[""config""][""algorithm""][""orig_vcf""] = dd.get_sample_name(data) #LINE# #TAB# #TAB# data[""config""][""algorithm""][""orig_vcf""] = dd.get_sample_name(data) #LINE# #TAB# return data"
"def calc_graph(analysis_fields, data_frame): #LINE# #TAB# check_graph = {} #LINE# #TAB# for key in analysis_fields: #LINE# #TAB# #TAB# check_graph[key] = calculate_heart_balance_graph(analysis_fields[ #LINE# #TAB# #TAB# #TAB# key], data_frame[key]) #LINE# #TAB# return check_graph"
"def handle_radius(url, key, radius): #LINE# #TAB# if key == 'radius': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# radius = float(radius) #LINE# #TAB# #TAB# #TAB# url = url.replace(' ', '-') #LINE# #TAB# #TAB# #TAB# return radius #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return url"
"def without_interwiki_page_generator(total=None, site=None): #LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page in site.no_interwiki_pages(total=total): #LINE# #TAB# #TAB# yield page"
"def localsys_get_login(): #LINE# #TAB# try: #LINE# #TAB# #TAB# username = getpass.getuser() #LINE# #TAB# #TAB# password = getpass.getpass('Enter password: ') #LINE# #TAB# #TAB# return localsys_get_password(username, password) #LINE# #TAB# except localsys.CalledProcessError: #LINE# #TAB# #TAB# return False"
"def create_target_space_from_image(image): #LINE# #TAB# gray = cv2.imread(image, cv2.IMREAD_GRAYSCALE).astype(np.uint8) #LINE# #TAB# white = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY) #LINE# #TAB# target_space = white - white.astype(np.int) #LINE# #TAB# return target_space"
"def choose_start_node(e): #LINE# #TAB# if not isinstance(e, Expression): #LINE# #TAB# #TAB# return e #LINE# #TAB# if e.is_atom(): #LINE# #TAB# #TAB# return e.node #LINE# #TAB# head = e.head #LINE# #TAB# j = e.j #LINE# #TAB# while j and j!= head: #LINE# #TAB# #TAB# head = j #LINE# #TAB# e = e.next #LINE# #TAB# while not isinstance(e, Expression): #LINE# #TAB# #TAB# if e.is_atom(): #LINE# #TAB# #TAB# #TAB# j = e.parent #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return e #LINE# #TAB# end = head #LINE# #TAB# while not end.is_atom(): #LINE# #TAB# #TAB# end = end.parent #LINE# #TAB# return head, e"
"def set_valid_props(artist, kwargs): #LINE# #TAB# artist.set(**{k: kwargs[k] for k in kwargs if hasattr(artist,'set_' + k)}) #LINE# #TAB# return artist"
"def unique_everseen(iterable, filterfalse_=itertools.filterfalse): #LINE# #TAB# seen = set() #LINE# #TAB# seen_add = seen.add #LINE# #TAB# for element in iterable: #LINE# #TAB# #TAB# if element not in seen: #LINE# #TAB# #TAB# #TAB# seen_add(element) #LINE# #TAB# #TAB# #TAB# yield element"
"def subst_libs(env, libs): #LINE# #TAB# substd = dict(env) #LINE# #TAB# for lib in libs: #LINE# #TAB# #TAB# ops = [] #LINE# #TAB# #TAB# for k, v in libs.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# #TAB# ops.append(v) #LINE# #TAB# #TAB# #TAB# elif isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# substd[k] = substd[v].split(':') #LINE# #TAB# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# substd[k] = v #LINE# #TAB# return substd"
def ods_bool_value(value): #LINE# #TAB# if value is True: #LINE# #TAB# #TAB# return '1' #LINE# #TAB# elif value is False: #LINE# #TAB# #TAB# return '0' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '0'
def make_authserv_id(as_id): #LINE# #TAB# if as_id == 'HOSTNAME': #LINE# #TAB# #TAB# as_id = socket.gethostname() #LINE# #TAB# return as_id
"def expand_path(path, opts=None): #LINE# #TAB# if opts is None: #LINE# #TAB# #TAB# opts = {} #LINE# #TAB# root, ext = os.path.splitext(path) #LINE# #TAB# if ext == '.py': #LINE# #TAB# #TAB# return _expand_path(path, opts=opts) #LINE# #TAB# if ext == '.pyc': #LINE# #TAB# #TAB# return _expand_path(path, opts=opts) #LINE# #TAB# return path"
"def set_max_buffer_size(limit): #LINE# #TAB# global _max_buffer_size #LINE# #TAB# LOGGER.debug('Setting maximum buffer size to %i', limit) #LINE# #TAB# _max_buffer_size = limit"
def check_forked(orgrepo): #LINE# #TAB# api = get_api() #LINE# #TAB# path = f'/repos/{orgrepo}/forked' #LINE# #TAB# response = api.get(path) #LINE# #TAB# if response.status_code == 200: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def to_rgba_255(colorname, alpha=255): #LINE# #TAB# col = get_color(colorname) #LINE# #TAB# r = col[3] #LINE# #TAB# g = col[1] #LINE# #TAB# b = col[2] #LINE# #TAB# a = (alpha - r) / 255.0 #LINE# #TAB# return [int(r * 255.0) / 255.0, int(g * 255.0), int(b * 255.0), int(a * #LINE# #TAB# #TAB# 255.0), int(b * 255.0)]"
"def parse_host_string(host_string, default_user=None, default_port=None): #LINE# #TAB# user = default_user #LINE# #TAB# port = default_port #LINE# #TAB# if '@' in host_string: #LINE# #TAB# #TAB# user, host = host_string.split('@') #LINE# #TAB# if ':' in host: #LINE# #TAB# #TAB# host, port = host.rsplit(':', 1) #LINE# #TAB# return host, port"
"def get_comments(f): #LINE# #TAB# c = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# c = open(f, 'r') #LINE# #TAB# #TAB# for l in c: #LINE# #TAB# #TAB# #TAB# if '#' in l: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# elif '#' in l: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return c"
"def float_version(f): #LINE# #TAB# if f is None: #LINE# #TAB# #TAB# return 0, 0 #LINE# #TAB# if '.' in f: #LINE# #TAB# #TAB# return f, 0 #LINE# #TAB# return float(f), 0"
"def pixel_2d_check(value): #LINE# #TAB# if not isinstance(value, list): #LINE# #TAB# #TAB# raise TypeError('value must be a list') #LINE# #TAB# if len(value)!= 2: #LINE# #TAB# #TAB# raise ValueError('value must be a 2D pixel list') #LINE# #TAB# for i in value: #LINE# #TAB# #TAB# if not 0 <= i < len(value[0]): #LINE# #TAB# #TAB# #TAB# raise ValueError('value must be a positive integer') #LINE# #TAB# if not 0 <= value[1] <= 0: #LINE# #TAB# #TAB# raise ValueError('value must be a positive integer') #LINE# #TAB# if not 0 <= value[1] <= 1: #LINE# #TAB# #TAB# raise ValueError('value must be a positive integer') #LINE# #TAB# return value"
"def date_string_to_date(p_date): #LINE# #TAB# result = None #LINE# #TAB# if p_date: #LINE# #TAB# #TAB# parsed_date = re.match(r'(\d{4})-(\d{2})-(\d{2})', p_date) #LINE# #TAB# #TAB# if parsed_date: #LINE# #TAB# #TAB# #TAB# result = date( #LINE# #TAB# #TAB# #TAB# #TAB# int(parsed_date.group(1)), #LINE# #TAB# #TAB# #TAB# #TAB# int(parsed_date.group(2)), #LINE# #TAB# #TAB# #TAB# #TAB# int(parsed_date.group(3)) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# return result"
"def file_properties_dictionary(sql_raw_list): #LINE# #TAB# if sql_raw_list[5] == 0: #LINE# #TAB# #TAB# is_deployed = False #LINE# #TAB# else: #LINE# #TAB# #TAB# is_deployed = True #LINE# #TAB# properties_dictionary = {'id': sql_raw_list[0],'model_version_id': #LINE# #TAB# #TAB# sql_raw_list[1], 'absolute_path': sql_raw_list[2], #LINE# #TAB# #TAB# 'file_commit_state': sql_raw_list[3], 'last_modified_time': #LINE# #TAB# #TAB# sql_raw_list[4], 'is_deployed': is_deployed} #LINE# #TAB# return properties_dictionary"
"def area_under_curve(poly, bounds, algorithm): #LINE# #TAB# if not bounds: #LINE# #TAB# #TAB# raise ValueError('bounds is required') #LINE# #TAB# for b in poly: #LINE# #TAB# #TAB# if b is None: #LINE# #TAB# #TAB# #TAB# raise ValueError('poly is required') #LINE# #TAB# #TAB# area = 0 #LINE# #TAB# #TAB# for beg, end in zip(bounds[::-1], bounds[::-1]): #LINE# #TAB# #TAB# #TAB# e = area_under_curve(b, beg, end, algorithm) #LINE# #TAB# #TAB# #TAB# if e < 0: #LINE# #TAB# #TAB# #TAB# #TAB# return area #LINE# #TAB# #TAB# #TAB# area += e #LINE# #TAB# return area"
def should_start_runner_thread(): #LINE# #TAB# runner_thread = threading.currentThread().ident #LINE# #TAB# if runner_thread.isDaemon(): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def get_expressions(text): #LINE# #TAB# for expr in re.finditer(EXPRESSION_REGEX, text): #LINE# #TAB# #TAB# value = expr.group() #LINE# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# yield value"
"def jupyter_bundlerextension_paths(): #LINE# #TAB# return [{'name': 'notebook_zip_download', 'label': #LINE# #TAB# #TAB# 'IPython Notebook bundle (.zip)','module_name': #LINE# #TAB# #TAB# 'notebook.bundler.zip_bundler', 'group': 'download'}]"
"def project_oblique(B, C): #LINE# #TAB# P = project_oblique_on_line(B, C) #LINE# #TAB# rB = np.empty((B.shape[0], B.shape[1]), dtype=bool) #LINE# #TAB# rC = project_oblique_on_line(C, B) #LINE# #TAB# p = np.empty((C.shape[0], C.shape[1], C.shape[1])) #LINE# #TAB# p[0] = 1 #LINE# #TAB# p[1] = 1 #LINE# #TAB# for row in range(B.shape[0]): #LINE# #TAB# #TAB# rB[row] = project_oblique_on_line(B[row], C[row]) #LINE# #TAB# #TAB# p[2] = project_oblique_on_line(C[row], B[row]) #LINE# #TAB# return rB, p, rC"
"def calculate_nbf(cls, atom_map, center_data) ->int: #LINE# #TAB# nbf = 0 #LINE# #TAB# for atom in atom_map: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# nbf += cls._get_nbf(atom, center_data) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return nbf"
"def flat_model(tree): #LINE# #TAB# names = [] #LINE# #TAB# for columns in viewvalues(tree): #LINE# #TAB# #TAB# for col in columns: #LINE# #TAB# #TAB# #TAB# if isinstance(col, dict): #LINE# #TAB# #TAB# #TAB# #TAB# col_name = list(col)[0] #LINE# #TAB# #TAB# #TAB# #TAB# names += [col_name + '__' + c for c in flat_model(col)] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# names.append(col) #LINE# #TAB# return names"
"def header_from_line(line): #LINE# #TAB# fields = line.rstrip('\r\n').split(',') #LINE# #TAB# names = fields[:2] #LINE# #TAB# indices = [int(i) for i in range(len(names))] #LINE# #TAB# header = {'names': names, 'indices': indices} #LINE# #TAB# return header"
def fixup_theme_comment_selectors(rules): #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# if rule['type'] == 'comment': #LINE# #TAB# #TAB# #TAB# rule['xsl:comment'] = 'xml:id:%s' % rule['value']
"def find_files(dl_paths, publisher, url_dict): #LINE# #TAB# files = [] #LINE# #TAB# for dl_path in dl_paths: #LINE# #TAB# #TAB# for root, _, files in os.walk(dl_path): #LINE# #TAB# #TAB# #TAB# for filename in url_dict[root]: #LINE# #TAB# #TAB# #TAB# #TAB# if filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# files.append(os.path.join(root, filename)) #LINE# #TAB# return files"
"def data_from_sheet(df, station_name, as_df=True): #LINE# #TAB# sheet = pd.read_excel(df) #LINE# #TAB# stations = [s for s in sheet. stations if s.name == station_name] #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return stations #LINE# #TAB# df_list = [] #LINE# #TAB# for s in stations: #LINE# #TAB# #TAB# df_list.append(df[s.id]) #LINE# #TAB# df = pd.concat(df_list, axis=1) #LINE# #TAB# df = df.reset_index(drop=True) #LINE# #TAB# return df"
"def get_args(op, name): #LINE# #TAB# if isinstance(op, ops.Selection): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# result = op.selections #LINE# #TAB# #TAB# return [col for col in result if col._name == name] #LINE# #TAB# elif isinstance(op, ops.Aggregation): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# col #LINE# #TAB# #TAB# #TAB# for col in itertools.chain(op.by, op.metrics) #LINE# #TAB# #TAB# #TAB# if col._name == name #LINE# #TAB# #TAB# ] #LINE# #TAB# else: #LINE# #TAB# #TAB# return op.args"
"def check_import(module_names): #LINE# #TAB# try: #LINE# #TAB# #TAB# return importlib.import_module(module_names) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# last_error = sys.exc_info()[1] #LINE# #TAB# #TAB# last_path = sys.path #LINE# #TAB# #TAB# modules = [] #LINE# #TAB# #TAB# for module in module_names: #LINE# #TAB# #TAB# #TAB# if not module: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# modules.append(module) #LINE# #TAB# #TAB# for name, is_pkg in pkgutil.iter_modules(modules): #LINE# #TAB# #TAB# #TAB# if is_pkg: #LINE# #TAB# #TAB# #TAB# #TAB# last_path = os.path.abspath(os.path.join(last_path, name)) #LINE# #TAB# #TAB# return last_path"
def write_multi(parameters): #LINE# #TAB# multi = [] #LINE# #TAB# n = len(parameters) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# scalar = parameters[i][0] #LINE# #TAB# #TAB# value = parameters[i][1] #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if scalar!= 1: #LINE# #TAB# #TAB# #TAB# multi.append(write_scalar(scalar)) #LINE# #TAB# #TAB# #TAB# multi.append(value)) #LINE# #TAB# return multi
"def detect_available_configs(): #LINE# #TAB# for path in _CONFIG_PATHS: #LINE# #TAB# #TAB# for root, dirs, files in os.walk(path): #LINE# #TAB# #TAB# #TAB# dirs[:] = [d for d in dirs if not os.path.isdir(os.path.join(root, d))] #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# config = loads(f.read()) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield config"
"def read_index_tuple(fd): #LINE# #TAB# while True: #LINE# #TAB# #TAB# n, t, x = read_index_bytes(fd) #LINE# #TAB# #TAB# if not n: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return n, t, x"
def get_currency(value): #LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(str(value)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# value = None #LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(str(value)) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# value = None #LINE# #TAB# try: #LINE# #TAB# #TAB# value = Decimal(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# if value == None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return value / 100.0 #LINE# #TAB# if value == None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return value
"def get_backend(name: str = None): #LINE# #TAB# backend = backends.get(name, default_backend) #LINE# #TAB# if backend is None: #LINE# #TAB# #TAB# name = os.path.basename(os.path.realpath(__file__)) #LINE# # # return backend"
"def prepare_group_by_item(item): #LINE# #TAB# item = item.replace('dimensions#', '') #LINE# #TAB# item = item.replace('meta#', '') #LINE# #TAB# if item == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# if item =='meta#': #LINE# #TAB# #TAB# return'meta[:,]' #LINE# #TAB# if item == 'value_meta': #LINE# #TAB# #TAB# return 'value_meta[:,]' #LINE# #TAB# return item"
def get_schedule(_sched): #LINE# #TAB# cts = [] #LINE# #TAB# for run_id in _sched: #LINE# #TAB# #TAB# cts.append(_sched[run_id]) #LINE# #TAB# #TAB# if run_id not in cts: #LINE# #TAB# #TAB# #TAB# cts.append(_sched[run_id]) #LINE# #TAB# return cts
"def decode_int(rlp, start): #LINE# #TAB# length = decode_uint(rlp, start) #LINE# #TAB# value = rlp[start:start + length] #LINE# #TAB# return value, length"
"def get_host_ip(hostname, fallback=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# return socket.gethostbyname(hostname) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return fallback"
"def read_configfile(file_path: str, force: bool=True) ->ConfigParser: #LINE# #TAB# cfg = ConfigParser() #LINE# #TAB# if force or not os.path.exists(file_path): #LINE# #TAB# #TAB# configparser.read(file_path) #LINE# #TAB# #TAB# return cfg #LINE# #TAB# cfg.read(file_path) #LINE# #TAB# return cfg"
"def neg_grad_biomass_per_flux(flux_vec, biomass_index): #LINE# #TAB# neg_grad = np.zeros_like(flux_vec) #LINE# #TAB# for i in range(flux_vec.shape[1]): #LINE# #TAB# #TAB# flux_vec[biomass_index, i] -= flux_vec[biomass_index, 0] #LINE# #TAB# #TAB# neg_grad[i] = np.exp(-flux_vec[biomass_index, 1] / flux_vec[ #LINE# #TAB# #TAB# #TAB# biomass_index, 2]) #LINE# #TAB# return neg_grad"
"def decorrelate_colors(image, color_correlation_svd_sqrt): #LINE# #TAB# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #LINE# #TAB# decorrelate_colors = cv2.cvtColor(image, color_correlation_svd_sqrt, #LINE# #TAB# #TAB# cv2.COLOR_BGR2RGB) #LINE# #TAB# image = cv2.cvtColor(image, decorrelate_colors, cv2.COLOR_BGR2RGB) #LINE# #TAB# return image"
"def contains_array(store, path=None): #LINE# #TAB# path = normalize_storage_path(path) #LINE# #TAB# prefix = _path_to_prefix(path) #LINE# #TAB# for key in store: #LINE# #TAB# #TAB# if prefix == path: #LINE# #TAB# #TAB# #TAB# return key in store #LINE# #TAB# #TAB# if isinstance(store[key], np.ndarray): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def as_shared_dtype(scalars_or_arrays): #LINE# #TAB# arrays = [asarray(x) for x in scalars_or_arrays] #LINE# #TAB# out_type = dtypes.result_type(*arrays) #LINE# #TAB# return [x.astype(out_type, copy=False) for x in arrays]"
"def get_heartbeat(request): #LINE# #TAB# try: #LINE# #TAB# #TAB# value = getattr(request, 'heartbeat', None) #LINE# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 'Unknown' #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return 'Unknown'"
"def normalize_string(string, length): #LINE# #TAB# normalized_string = u"""" #LINE# #TAB# for line in [string]: #LINE# #TAB# #TAB# if len(line) > length: #LINE# #TAB# #TAB# #TAB# normalized_string += line #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# normalized_string += ""\n"" #LINE# #TAB# return normalized_string"
def nl_socket_alloc(cb=None): #LINE# #TAB# sock = nl_socket() #LINE# #TAB# if cb: #LINE# #TAB# #TAB# nl_socket_set_cb(cb) #LINE# #TAB# _LOGGER.debug('Allocated new socket') #LINE# #TAB# return sock
"def not_found(environ, start_response): #LINE# #TAB# start_response('404 NOT FOUND', [('Content-Type', 'text/plain')]) #LINE# #TAB# return ['Not Found']"
"def read_metadata(filename): #LINE# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# if'metadata' in f.keys(): #LINE# #TAB# #TAB# #TAB# return f['metadata'] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return {}"
"def common_random_base(cls, traces, weight_vec=None, dims=2, seed=None): #LINE# #TAB# tot_actions = len(traces) #LINE# #TAB# if weight_vec is not None: #LINE# #TAB# #TAB# weights = weight_vec #LINE# #TAB# else: #LINE# #TAB# #TAB# weights = np.ones(tot_actions) #LINE# #TAB# num_nodes = len(traces) #LINE# #TAB# random_base = np.zeros(num_nodes) #LINE# #TAB# for i in range(dims): #LINE# #TAB# #TAB# num_actions += 1 #LINE# #TAB# #TAB# random_base[i] = traces[i] #LINE# #TAB# np.random.shuffle(random_base) #LINE# #TAB# return random_base"
"def set_uids(pifs, uids=None): #LINE# #TAB# if uids is None: #LINE# #TAB# #TAB# uids = {} #LINE# #TAB# for pif in pifs: #LINE# #TAB# #TAB# if 'uids' in pif: #LINE# #TAB# #TAB# #TAB# uids[pif['uid']] = pif['uids'] #LINE# #TAB# return uids"
"def create_absolute_values_structure(layer, fields): #LINE# #TAB# absolute_values = {} #LINE# #TAB# absolute_values['layer_id'] = layer['id'] #LINE# #TAB# absolute_values['key'] = layer['key'] #LINE# #TAB# absolute_values['description'] = layer.get('description', '') #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# if field in absolute_values: #LINE# #TAB# #TAB# #TAB# absolute_values[field] = absolute_values[field] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# absolute_values[field] = '' #LINE# #TAB# if absolute_values: #LINE# #TAB# #TAB# for field in absolute_values: #LINE# #TAB# #TAB# #TAB# if field in fields: #LINE# #TAB# #TAB# #TAB# #TAB# absolute_values[field] = fields[field] #LINE# #TAB# return absolute_values"
"def execute_command(cmd: str) ->str: #LINE# #TAB# process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, #LINE# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# output, stderr = process.communicate() #LINE# #TAB# return output"
"def greater_than(data, loperand, roperand, is_not=False, extras=None): #LINE# #TAB# value = get_field_value(data, loperand) #LINE# #TAB# gte = False #LINE# #TAB# if value: #LINE# #TAB# #TAB# if extras: #LINE# #TAB# #TAB# #TAB# value = apply_extras(value, extras) #LINE# #TAB# #TAB# if type(value) == type(roperand) and value >= roperand: #LINE# #TAB# #TAB# #TAB# gte = True #LINE# #TAB# if is_not: #LINE# #TAB# #TAB# gte = not gte #LINE# #TAB# return gte"
"def filter_string(value): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# value = '' #LINE# #TAB# value = value.replace('\n','') #LINE# #TAB# value = value.replace('\r','') #LINE# #TAB# value = value.replace('\t','') #LINE# #TAB# value = value.replace('|','') #LINE# #TAB# value = ''.join(c for c in value if 31 < ord(c) < 127) #LINE# #TAB# return value"
"def align_buf(buf, sample_width): #LINE# #TAB# if len(buf) < sample_width: #LINE# #TAB# #TAB# sw_len = sample_width - len(buf) #LINE# #TAB# #TAB# return buf + (sw_len % sample_width) * '0' #LINE# #TAB# return buf"
"def is_encodable(typ, arg): #LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type = typ.type #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if is_list_like(arg): #LINE# #TAB# #TAB# for item in arg.split(','): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# if abi_type.is_struct(item): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# except (AttributeError, TypeError): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type.is_encodable(arg) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def to_dict(obj: object) ->dict: #LINE# #TAB# result = {} #LINE# #TAB# for attr in dir(obj): #LINE# #TAB# #TAB# value = getattr(obj, attr) #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# value = [to_dict(item) for item in value] #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# value = {attr: to_dict(value) for attr, value in value.items()} #LINE# #TAB# #TAB# elif isinstance(value, datetime): #LINE# #TAB# #TAB# #TAB# value = value.isoformat() #LINE# #TAB# #TAB# result[attr] = value #LINE# #TAB# return result"
"def rio_uri(band: BandInfo) ->str: #LINE# #TAB# uri = band.uri #LINE# #TAB# if isinstance(band.layer, (Dataset, Dataset)): #LINE# #TAB# #TAB# uri = str(band.layer) #LINE# #TAB# if isinstance(band.layer, (NetCDF4, NetCDF5)): #LINE# #TAB# #TAB# uri = '{}:'.format(uri) #LINE# #TAB# return uri"
"def get_image_date(filename): #LINE# #TAB# date = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# date = f['Date'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# date = datetime.datetime.now().strftime('%Y-%m-%d') #LINE# #TAB# return date"
def deserialize_to_instance(exc_type): #LINE# #TAB# key = _get_module_path(exc_type) #LINE# #TAB# Registry.exceptions[key] = exc_type #LINE# #TAB# return exc_type
"def gerrit_user_to_author(props, username='unknown'): #LINE# #TAB# username = props.get('username', username) #LINE# #TAB# if 'email' in props: #LINE# #TAB# #TAB# username +='<%(email)s>' % props #LINE# #TAB# if 'first_name' in props: #LINE# #TAB# #TAB# username +='<%(first_name)s>' % props #LINE# #TAB# return username"
"def index_exists(engine, table_name, index_name): #LINE# #TAB# client = get_client(engine) #LINE# #TAB# table = client.get_table(table_name) #LINE# #TAB# if index_exists(table, index_name): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"def convert_datagram(chunk): #LINE# #TAB# n = struct.unpack('!H', chunk[0:2])[0] #LINE# #TAB# length = struct.unpack('!H', chunk[2:4])[0] #LINE# #TAB# type = struct.unpack('!H', chunk[4:6])[0] #LINE# #TAB# data = b'' #LINE# #TAB# if length == 0: #LINE# #TAB# #TAB# data = b'' #LINE# #TAB# elif length == 1: #LINE# #TAB# #TAB# data = chunk[0:2] #LINE# #TAB# #TAB# type = struct.unpack('!H', chunk[6:4])[0] #LINE# #TAB# data += b'\x00' * (length - 2) + data #LINE# #TAB# return type, data"
"def check_if_installed(package, installed_packages=None): #LINE# #TAB# if installed_packages is None: #LINE# #TAB# #TAB# installed_packages = [] #LINE# #TAB# for installed_package in installed_packages: #LINE# #TAB# #TAB# if package in installed_package: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def time_to_sec(s): #LINE# #TAB# #TAB# sec = 0 #LINE# #TAB# #TAB# minutes, seconds = divmod(s.hour * 3600, 60) #LINE# #TAB# #TAB# hours, minutes = divmod(minutes, 60) #LINE# #TAB# #TAB# seconds += hours * 60 * 60 #LINE# #TAB# #TAB# if minutes!= 0: #LINE# #TAB# #TAB# #TAB# raise ValueError('Invalid time period') #LINE# #TAB# #TAB# return seconds"
"def create_opengl_object(gl_gen_function, n=1): #LINE# #TAB# handle = gl.GLuint(1) #LINE# #TAB# gl_gen_function(n, byref(handle)) #LINE# #TAB# if n > 1: #LINE# #TAB# #TAB# return [handle.value + el for el in range(n)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return handle.value"
def is_containing_bracket(sentence): #LINE# #TAB# depth = 0 #LINE# #TAB# for ch in sentence: #LINE# #TAB# #TAB# if ch.isspace(): #LINE# #TAB# #TAB# #TAB# depth += 1 #LINE# #TAB# #TAB# elif ch in '[({': #LINE# #TAB# #TAB# #TAB# if depth > 0 and _is_begin_of_string(ch.group(0)): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif ch in '[{': #LINE# #TAB# #TAB# #TAB# if _is_begin_of_string(ch.group(0)): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"def parse_resources(html): #LINE# #TAB# xpath = '//*[@data-type=""resources""]//xhtml:li/xhtml:a' #LINE# #TAB# for resource in html.xpath(xpath, namespaces=HTML_DOCUMENT_NAMESPACES): #LINE# #TAB# #TAB# yield {'id': resource.get('href'), 'filename': resource.text.strip()}"
"def get_k_bounds(data, spacing, packed=True): #LINE# #TAB# spacing = np.array(spacing) #LINE# #TAB# if packed: #LINE# #TAB# #TAB# k_min, k_max = data.shape #LINE# #TAB# #TAB# k_min /= spacing[0] #LINE# #TAB# #TAB# k_max /= spacing[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# k_min = 0 if data.shape[0] == 1 else data.shape[0] - 1 #LINE# #TAB# #TAB# k_max = 0 if data.shape[1] == 1 else data.shape[1] - 1 #LINE# #TAB# k_bounds = [min(k_min), min(k_max), max(k_min), max(k_max)] #LINE# #TAB# return k_bounds"
def get_device(): #LINE# #TAB# global device #LINE# #TAB# if device is None: #LINE# #TAB# #TAB# torch.cuda.init() #LINE# #TAB# #TAB# device = torch.device('cpu') #LINE# #TAB# return device
"def flush_api_stage_cache(restApiId, stageName, region=None, key=None, keyid=None, profile=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# stage = conn.get_stage(restApiId=restApiId, stageName=stageName) #LINE# #TAB# #TAB# pvCache = conn.get_stage_cache(stage) #LINE# #TAB# #TAB# for stage in pvCache: #LINE# #TAB# #TAB# #TAB# del stage.cache #LINE# #TAB# #TAB# #TAB# conn.delete_stage_cache(stage) #LINE# #TAB# #TAB# return {'flushed': True} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'flushed': False, 'error': __utils__['boto3.get_error'](e)}"
"def shannon_entropy(time_series): #LINE# #TAB# if len(time_series) == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# n_samples = len(time_series) #LINE# #TAB# entropy = 0 #LINE# #TAB# for i in range(n_samples): #LINE# #TAB# #TAB# val = 0 #LINE# #TAB# #TAB# for j in range(i + 1, n_samples): #LINE# #TAB# #TAB# #TAB# if time_series[j]!= 0: #LINE# #TAB# #TAB# #TAB# #TAB# entropy += math.log(val) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return -entropy"
def flatten_subclass_tree(cls): #LINE# #TAB# return cls.__subclasses__() + [c for c in cls.__subclasses__() for c in #LINE# #TAB# #TAB# flatten_subclass_tree(c)]
"def gelman_rubin(models): #LINE# #TAB# gelman_rubin = np.zeros(len(models)) #LINE# #TAB# for index, model in enumerate(models): #LINE# #TAB# #TAB# gelman_rubin[index] = gelman_rubin(model) #LINE# #TAB# return gelman_rubin"
"def ang_veltodcmdot(R, Omega): #LINE# #TAB# try: #LINE# #TAB# #TAB# return 1.0 * Omega * R #LINE# #TAB# except NotImplementedError: #LINE# #TAB# #TAB# return Omega"
"def get_best_sep(string, sep): #LINE# #TAB# sep_list = string.split(sep) #LINE# #TAB# if len(sep_list) == 1: #LINE# #TAB# #TAB# return string, '' #LINE# #TAB# sep = sep_list[0] #LINE# #TAB# names = [] #LINE# #TAB# for idx in range(len(sep_list)): #LINE# #TAB# #TAB# if len(sep_list[idx]) > len(names): #LINE# #TAB# #TAB# #TAB# names.append(sep) #LINE# #TAB# return names, sep"
def equiv_url_is(url_parts): #LINE# #TAB# eq_url = equiv_url(url_parts) #LINE# #TAB# if eq_url.startswith('http') and not eq_url.endswith('/api'): #LINE# #TAB# #TAB# return eq_url #LINE# #TAB# if eq_url.startswith('https'): #LINE# #TAB# #TAB# return 'https' + eq_url[4:] #LINE# #TAB# return eq_url
def random_soup(face_count=100): #LINE# #TAB# soup = Trimesh(random.choice(faces)) #LINE# #TAB# for i in range(face_count): #LINE# #TAB# #TAB# soup.add_face(random.choice(faces)) #LINE# #TAB# return soup
"def get_movie_size_pix(movie_path): #LINE# #TAB# image_path = generate_thumbnail(movie_path) #LINE# #TAB# im = Image.open(image_path) #LINE# #TAB# size = im.size #LINE# #TAB# im.close() #LINE# #TAB# os.remove(image_path) #LINE# #TAB# return size[0], size[1]"
"def is_safe_url(target): #LINE# #TAB# ref_url = urlparse(flask.request.host_url) #LINE# #TAB# test_url = urlparse(urljoin(flask.request.host_url, target)) #LINE# #TAB# return test_url.scheme in ('http', 'https' #LINE# #TAB# #TAB# ) and ref_url.netloc == test_url.netloc"
def enum_cpus(): #LINE# #TAB# try: #LINE# #TAB# #TAB# from multiprocessing import cpu_count #LINE# #TAB# #TAB# return cpu_count() #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# yield
def get_snowav_path(): #LINE# #TAB# try: #LINE# #TAB# #TAB# f = open('/etc/snowav') #LINE# #TAB# #TAB# f.readline() #LINE# #TAB# #TAB# path = f.readline() #LINE# #TAB# #TAB# f.close() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# if SNOWAV_VERSION_INFO.match(snowav_version): #LINE# #TAB# #TAB# #TAB# return SNOWAV_PATH #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return Path.cwd() #LINE# #TAB# else: #LINE# #TAB# #TAB# return Path.cwd()
def get_countrycode(profile): #LINE# #TAB# country_code = None #LINE# #TAB# for key in profile['addresses']: #LINE# #TAB# #TAB# if 'country_code' in key: #LINE# #TAB# #TAB# #TAB# country_code = key['country_code'] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if country_code is not None: #LINE# #TAB# #TAB# return country_code #LINE# #TAB# return None
"def is_child_node(child, parent): #LINE# #TAB# if not parent.has_children(): #LINE# #TAB# #TAB# return False #LINE# #TAB# for child_node in child.get_nodes(): #LINE# #TAB# #TAB# if parent.has_children(child_node): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def iter_child_nodes(node): #LINE# #TAB# for name, field in iter_fields(node): #LINE# #TAB# #TAB# if isinstance(field, Fortran): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, Fortran): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item"
def small_manuf(vin): #LINE# #TAB# if vin < 500: #LINE# #TAB# #TAB# return False #LINE# #TAB# if vin > 760: #LINE# #TAB# #TAB# return False #LINE# #TAB# if vin < 1000: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def parse_preferences(file, preferences): #LINE# #TAB# for name, value in preferences.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# preference = parse_preference(name, value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if value is True: #LINE# #TAB# #TAB# #TAB# #TAB# preference['has_default'] = True #LINE# #TAB# #TAB# #TAB# elif value is False: #LINE# #TAB# #TAB# #TAB# #TAB# preference['has_default'] = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# preference['default'] = value #LINE# #TAB# return preference"
def remove_html_comments(text): #LINE# #TAB# num_comments = 0 #LINE# #TAB# num_unmatched = 0 #LINE# #TAB# while num_comments < len(text): #LINE# #TAB# #TAB# num_comments += len(text[num_comments]) #LINE# #TAB# #TAB# text = text[:num_unmatched] #LINE# #TAB# #TAB# num_unmatched += 1 #LINE# #TAB# return text
"def format_request(request): #LINE# #TAB# if not request: #LINE# #TAB# #TAB# return None #LINE# #TAB# data = {} #LINE# #TAB# if request.headers: #LINE# #TAB# #TAB# headers = request.headers #LINE# #TAB# #TAB# data['Accept'] = request.headers.get('Accept', '') #LINE# #TAB# #TAB# data['Date'] = parser.parse(request.headers.get('Date', '')) #LINE# #TAB# #TAB# data['Time'] = parser.parse(request.headers.get('Time', '')) #LINE# #TAB# return data"
"def get_issue_metadata(filename, issue_id, talker): #LINE# #TAB# meta_path = get_issue_meta_path(issue_id, talker) #LINE# #TAB# print(meta_path) #LINE# #TAB# metadata = talker.get_issue_metadata(issue_id, filename) #LINE# #TAB# date = datetime.datetime.strptime(metadata['created_at'], #LINE# #TAB# #TAB# '%a %d %b %Y %H:%M:%S %z') #LINE# #TAB# str_date = date.strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# with open(meta_path, 'w') as f: #LINE# #TAB# #TAB# f.write(metadata) #LINE# #TAB# str_date = date.strftime('%Y-%m-%d') #LINE# #TAB# return metadata, date, str_date"
"def format_invariants(invariants: List[icontract._Contract]) -> List[str]: #LINE# #TAB# result = [] #LINE# #TAB# for contract in invariants: #LINE# #TAB# #TAB# result.append(""#TAB# * '{}'"".format(contract.name)) #LINE# #TAB# return result"
"def fix_newlines(lines): #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.endswith('\n\r'): #LINE# #TAB# #TAB# #TAB# line = line.replace('\n\r', '\n') #LINE# #TAB# #TAB# line = line.replace('\r', '\n') #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield line"
"def zero_to_none(value): #LINE# #TAB# if value == 0 or isinstance(value, basestring) and value.strip() == '0': #LINE# #TAB# #TAB# return None #LINE# #TAB# return value"
"def iterate_flattened(d): #LINE# #TAB# if isinstance(d, collections.Mapping): #LINE# #TAB# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# #TAB# for nested_k, nested_v in iterate_flattened(v).items(): #LINE# #TAB# #TAB# #TAB# #TAB# yield nested_k, nested_v #LINE# #TAB# elif isinstance(d, collections.Iterable): #LINE# #TAB# #TAB# for item in d: #LINE# #TAB# #TAB# #TAB# for nested_k, nested_v in iterate_flattened(nested_v): #LINE# #TAB# #TAB# #TAB# #TAB# yield nested_k, nested_v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield d[k]"
"def ber_audio(original_audio_bin, decoded_audio_bin): #LINE# #TAB# original_audio_len = original_audio_bin.shape[0] #LINE# #TAB# decoded_audio_len = decoded_audio_bin.shape[0] #LINE# #TAB# ber_audio = np.zeros((original_audio_len, decoded_audio_len)) #LINE# #TAB# for i in range(original_audio_len): #LINE# #TAB# #TAB# with np.errstate(divide='ignore') as err: #LINE# #TAB# #TAB# #TAB# if np.logical_or(original_audio_bin[i], decoded_audio_bin[i]) == 0: #LINE# #TAB# #TAB# #TAB# #TAB# ber_audio[i] = 0 #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return ber_audio"
"def sort_prefixes(orig, prefixes='@+'): #LINE# #TAB# new = '' #LINE# #TAB# for prefix in prefixes: #LINE# #TAB# #TAB# if prefix in orig: #LINE# #TAB# #TAB# #TAB# new += prefix #LINE# #TAB# return new"
"def name_append_suffix(records, suffix): #LINE# #TAB# logging.info('Applying _name_append_suffix generator:'#LINE# #TAB# #TAB# #TAB# #TAB# 'appending suffix'+ suffix) #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# yield record"
"def force_clean(ws, proj): #LINE# #TAB# for path in get_paths(ws, proj): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shutil.rmtree(str(path)) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
"def find_topics(token_lists, num_topics=10): #LINE# #TAB# token_lists_with_topics = get_token_lists_with_topics(token_lists) #LINE# #TAB# matching_topics = {} #LINE# #TAB# for i, token_list in enumerate(token_lists_with_topics): #LINE# #TAB# #TAB# topics = get_topics(token_list, num_topics=num_topics, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# token_list=token_lists_with_topics) #LINE# #TAB# #TAB# if len(topics) == num_topics: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# matching_topics[i] = topics #LINE# #TAB# return matching_topics"
def get_all_positional_parameter_names(fn): #LINE# arg_spec = _get_cached_arg_spec(fn) #LINE# args = arg_spec.args #LINE# if arg_spec.defaults: #LINE# #TAB# args = args[:-len(arg_spec.defaults)] #LINE# return args
"def get_cuda_device_id(gpu_id, job_name, using_cpu_only): #LINE# #TAB# if using_cpu_only or job_name not in ('serving', 'learner'): #LINE# #TAB# #TAB# return '' #LINE# #TAB# cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None) #LINE# #TAB# if len(cuda_visible_devices.split(',')) > gpu_id: #LINE# #TAB# #TAB# return cuda_visible_devices.split(',')[gpu_id] #LINE# #TAB# return cuda_visible_devices"
def get_path_to_executable(executable): #LINE# #TAB# import distutils.spawn #LINE# #TAB# path = distutils.spawn.find_executable(executable) #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# raise ValueError('{} executable not found in PATH.'.format(executable)) #LINE# #TAB# return path
def get_recording_filetype(file_dir): #LINE# #TAB# if not os.path.isdir(file_dir): #LINE# #TAB# #TAB# raise ValueError('Input directory does not exist: {}'.format(file_dir)) #LINE# #TAB# file_type = input('Enter file type of recording (ex. %s):' % #LINE# #TAB# #TAB# file_dir) #LINE# #TAB# if file_type.startswith('zip'): #LINE# #TAB# #TAB# file_type = 'zip' #LINE# #TAB# if not os.path.isdir(file_dir): #LINE# #TAB# #TAB# raise ValueError('Input directory does not contain a valid directory: {}' #LINE# #TAB# #TAB# #TAB#.format(file_dir)) #LINE# #TAB# return file_type
"def get_unquoted_value(value: str): #LINE# #TAB# if value[0] == value[-1] == '""': #LINE# #TAB# #TAB# return value[1:-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
"def is_gae(): #LINE# #TAB# if hasattr(settings, 'INSTALLED_APPS'): #LINE# #TAB# #TAB# for engine in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# #TAB# if engine.name == 'Google App Engine': #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def write_pxi(filename, definitions): #LINE# #TAB# lines = [] #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# lines.append('# include %s\n' % line) #LINE# #TAB# #TAB# for d in definitions: #LINE# #TAB# #TAB# #TAB# lines.append('# %-14s %s\n' % (d, d)) #LINE# #TAB# #TAB# f.close() #LINE# #TAB# return lines"
"def get_union_communities_from_g(grounding, optimum): #LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode brave --opt-mode=optN --opt-bound=' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# models = clyngor.solve_from_grounded(grounding, options=options) #LINE# #TAB# best_model = models.most_common(1)[0][0] #LINE# #TAB# return best_model"
"def parse_form(text, base_class=Form): #LINE# #TAB# class Form(base_class): #LINE# #TAB# #TAB# pass #LINE# #TAB# text = text.replace('\r\n', '\n') #LINE# #TAB# lines = text.split('\n') #LINE# #TAB# form = None #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if '=' in line: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = line.split('=', 1) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# form = Form(key, value) #LINE# #TAB# return form"
"def iter_window(iterable, size=2, step=1, wrap=False): #LINE# #TAB# it = iter(iterable) #LINE# #TAB# if wrap: #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# yield next(it) #LINE# #TAB# #TAB# if i >= size: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# i += step"
"def start_update_daemon(updater=None, roller_registry=ROLLER_REGISTRY): #LINE# #TAB# update_thread = threading.Thread(target=update_daemon, args=(updater, roller_registry) #LINE# #TAB# #TAB# ) #LINE# #TAB# update_thread.daemon = True #LINE# #TAB# update_thread.start() #LINE# #TAB# return update_thread"
"def profile_poly2o(data, mask): #LINE# #TAB# x = np.empty(data.shape + (mask,), dtype=np.bool) #LINE# #TAB# y = np.empty(data.shape + (mask,), dtype=np.bool) #LINE# #TAB# if mask.ndim > 0: #LINE# #TAB# #TAB# y = np.array(mask, dtype=np.bool) #LINE# #TAB# x = np.array(data[mask], dtype=np.bool) #LINE# #TAB# y = np.array(y, dtype=np.bool) #LINE# #TAB# return x, y"
"def load_config(filepath=None): #LINE# #TAB# if filepath is None: #LINE# #TAB# #TAB# filepath = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# #TAB# '.fydarc') #LINE# #TAB# with open(filepath, 'r') as data: #LINE# #TAB# #TAB# config = yaml.safe_load(data) #LINE# #TAB# return config"
def camel_case_from_underscores(string): #LINE# #TAB# components = string.split('_') #LINE# #TAB# string = '' #LINE# #TAB# for component in components: #LINE# #TAB# #TAB# string += component[0].upper() + component[1:] #LINE# #TAB# return string
"def cards_from_dir(dirname): #LINE# #TAB# cards = [] #LINE# #TAB# for root, dirs, files in os.walk(dirname): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# card = Card.from_filename(os.path.join(root, f)) #LINE# #TAB# #TAB# #TAB# #TAB# cards.append(card) #LINE# #TAB# return cards"
def im_from_blocks(blocks): #LINE# #TAB# im_blocks = [] #LINE# #TAB# for block in blocks: #LINE# #TAB# #TAB# im_blocks.append(im_from_block(block)) #LINE# #TAB# return im_blocks
"def iter_contents(cls, data): #LINE# #TAB# if isinstance(data, Mapping): #LINE# #TAB# #TAB# yield data #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# for item in cls.iter_contents(v): #LINE# #TAB# #TAB# #TAB# #TAB# yield item #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# for item in v: #LINE# #TAB# #TAB# #TAB# #TAB# yield item"
"def select_header_content_type(B, content_types): #LINE# #TAB# if not content_types: #LINE# #TAB# #TAB# return 'application/json' #LINE# #TAB# content_types = [x.lower() for x in content_types] #LINE# #TAB# if 'application/json' in content_types or '*/*' in content_types: #LINE# #TAB# #TAB# return 'application/json' #LINE# #TAB# return content_types[0]"
"def get_all_slots(class_): #LINE# #TAB# all_slots = [] #LINE# #TAB# parent_param_classes = [c for c in classlist(class_)[1:]] #LINE# #TAB# for c in parent_param_classes: #LINE# #TAB# #TAB# if hasattr(c, '__slots__'): #LINE# #TAB# #TAB# #TAB# all_slots += c.__slots__ #LINE# #TAB# return all_slots"
"def get_colors(score: List[T]) ->List[Tuple]: #LINE# #TAB# from matplotlib.colors import get_colors #LINE# #TAB# colors = [] #LINE# #TAB# for item in score: #LINE# #TAB# #TAB# for color in item: #LINE# #TAB# #TAB# #TAB# if isinstance(color, (tuple, list)): #LINE# #TAB# #TAB# #TAB# #TAB# colors.append(get_colors(color)) #LINE# #TAB# return colors"
"def render_path(path, args): #LINE# #TAB# rendered = None #LINE# #TAB# if path: #LINE# #TAB# #TAB# rendered = path(args) #LINE# #TAB# return rendered"
"def read_local_ddf(ddf_id, base_dir='./'): #LINE# #TAB# base_dir = os.path.abspath(base_dir) #LINE# #TAB# file_path = os.path.join(base_dir, ddf_id) #LINE# #TAB# dataset = DDF(file_path) #LINE# #TAB# return dataset"
def set_close_exec(fd): #LINE# #TAB# global FD_CLOEXEC #LINE# #TAB# FD_CLOEXEC = True
"def has_variable(obj, variable): #LINE# #TAB# try: #LINE# #TAB# #TAB# if variable in obj: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False"
def table_fullname(cls): #LINE# #TAB# fullname = cls.table_name #LINE# #TAB# if cls.schema: #LINE# #TAB# #TAB# schema = cls.schema.get() #LINE# #TAB# #TAB# if schema: #LINE# #TAB# #TAB# #TAB# fullname = schema.table_name #LINE# #TAB# return fullname
"def lowercase_range(code1, code2): #LINE# #TAB# code3 = max(code1, ord('a')) #LINE# #TAB# code4 = min(code2, ord('z') + 1) #LINE# #TAB# if code3 < code4: #LINE# #TAB# #TAB# d = ord('A') - ord('a') #LINE# #TAB# #TAB# return code3 + d, code4 + d #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def cast_fields(list_): #LINE# #TAB# list_ = [(x if isinstance(x, tuple) else Field(x)) for x in list_] #LINE# #TAB# return list_"
def parse_multiple_statements(statement): #LINE# #TAB# parsed_statement = statement.split(';') #LINE# #TAB# return [parse_statement(s) for s in parsed_statement]
"def workbench_scenarios(): #LINE# #TAB# return [('HTML5XBlock', '<html5/>\n#TAB# #TAB# #TAB# '), ( #LINE# #TAB# #TAB# 'HTML5XBlock with sanitized content', #LINE# #TAB# #TAB# ), ('HTML5XBlock with JavaScript', #LINE# #TAB# #TAB# )]"
def has_instance(name): #LINE# #TAB# global _instances #LINE# #TAB# return name in _instances
"def get_address_family(cls, address): #LINE# #TAB# res = socket.AF_INET #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.inet_pton(socket.AF_INET6, address) #LINE# #TAB# #TAB# res = socket.AF_INET6 #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# pass #LINE# #TAB# return res"
"def cache_key(method, args=None): #LINE# #TAB# if args is None: #LINE# #TAB# #TAB# args = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# cache_key = hashlib.sha256(method.encode('utf-8')) #LINE# #TAB# #TAB# cache_key.update(args) #LINE# #TAB# #TAB# return cache_key #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# if args is None: #LINE# #TAB# #TAB# #TAB# args = {} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cache_key = hashlib.sha256(args).hexdigest() #LINE# #TAB# #TAB# return cache_key"
"def doc_reader(infile): #LINE# #TAB# if infile.endswith('.docx'): #LINE# #TAB# #TAB# docid = 'word/document.xml' #LINE# #TAB# else: #LINE# #TAB# #TAB# docid = 'content.xml' #LINE# #TAB# try: #LINE# #TAB# #TAB# zfile = zipfile.ZipFile(infile) #LINE# #TAB# except: #LINE# #TAB# #TAB# print(""Sorry, can't open {}."".format(infile)) #LINE# #TAB# #TAB# return #LINE# #TAB# body = ET.fromstring(zfile.read(docid)) #LINE# #TAB# text = '\n'.join([et.text.strip() for et in body.iter() if et.text]) #LINE# #TAB# return text"
"def retrieve_download_url(): #LINE# #TAB# FEH_CONFIG_FILE = 'hosted_json.toml' #LINE# #TAB# if os.path.isfile(FEH_CONFIG_FILE): #LINE# #TAB# #TAB# with open(FEH_CONFIG_FILE, 'r') as data_file: #LINE# #TAB# #TAB# #TAB# data = json.load(data_file) #LINE# #TAB# #TAB# return data['download_url']"
def t_floatlit(t): #LINE# #TAB# t.value = float(t.value) #LINE# #TAB# return t
def to_unicode(s): #LINE# #TAB# if PY2: #LINE# #TAB# #TAB# return s.decode('utf-8') #LINE# #TAB# elif PY3: #LINE# #TAB# #TAB# return s.decode('latin-1') #LINE# #TAB# return s
"def force_int(num, default=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(num) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# return num"
def from_float_to_int(val): #LINE# #TAB# ret = int(float(val)) #LINE# #TAB# if ret > 0: #LINE# #TAB# #TAB# return ret #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"def write_figure_in_svg(ai_figure, ai_filename): #LINE# #TAB# with open(ai_filename, 'wb') as f_figure: #LINE# #TAB# #TAB# for i_ol in ai_figure: #LINE# #TAB# #TAB# #TAB# f_outline = outline_arc_line(i_ol, 'b') #LINE# #TAB# #TAB# #TAB# if type(f_outline) == tuple: #LINE# #TAB# #TAB# #TAB# #TAB# write_svg(f_outline,'svg') #LINE# #TAB# #TAB# #TAB# elif type(f_outline) == list: #LINE# #TAB# #TAB# #TAB# #TAB# write_svg(f_outline,'svg') #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# print('Output file:', ai_filename) #LINE# #TAB# #TAB# #TAB# #TAB# f_figure.close() #LINE# #TAB# return 0"
"def template_generatable(cls, template: str) ->bool: #LINE# #TAB# try: #LINE# #TAB# #TAB# cls.get_required_code_templates(template) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def unlink_all_files(_, dst, filenames): #LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# _, src = os.path.split(filename) #LINE# #TAB# #TAB# if os.path.exists(os.path.join(dst, src)): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# os.unlink(os.path.join(src, filename)) #LINE# #TAB# return"
"def image_circle(r): #LINE# #TAB# w, h = image_size(r) #LINE# #TAB# qt_image = numpy.zeros((w * w, h * h)) #LINE# #TAB# qt_image[:, :, (0)] = r #LINE# #TAB# qt_image[:, :, (1)] = r #LINE# #TAB# return qt_image"
"def get_work_dir() ->Path: #LINE# #TAB# work_dir = Path(os.getenv('BCML_WORKDIR', Path.home())) / 'bcml' #LINE# #TAB# if not work_dir.exists(): #LINE# #TAB# #TAB# work_dir.mkdir(parents=True, exist_ok=True) #LINE# #TAB# return work_dir"
"def flat_unity(length, delta_f, low_freq_cutoff): #LINE# #TAB# if low_freq_cutoff < 0: #LINE# #TAB# #TAB# raise ValueError(""The low_freq_cutoff can't be negative."") #LINE# #TAB# freq_series = FrequencySeries( #LINE# #TAB# #TAB# values=numpy.arange(0, length + 1), #LINE# #TAB# #TAB# index=length, #LINE# #TAB# #TAB# delta_f=delta_f, #LINE# #TAB# #TAB# low_freq_cutoff=low_freq_cutoff) #LINE# #TAB# return freq_series"
def build_rate(fps): #LINE# #TAB# rate = ET.Element('rate') #LINE# #TAB# rate.text = fps #LINE# #TAB# return rate
def group_tag(group_name): #LINE# #TAB# if not group_name: #LINE# #TAB# #TAB# raise ValueError('group_name parameter must be a string') #LINE# #TAB# tag_set = TagSet() #LINE# #TAB# tag_set.name = group_name #LINE# #TAB# tag_set.count = 1 #LINE# #TAB# tag.python_type = 'group' #LINE# #TAB# return tag
"def sanitize_order_by(string): #LINE# #TAB# string = re.sub(r'_(\w)', lambda m: m.group(1).upper(), string) #LINE# #TAB# string = re.sub(r'_(\w)', lambda m: m.group(1).upper(), string) #LINE# #TAB# string = re.sub(r'_(\w)', lambda m: m.group(1).upper(), string) #LINE# #TAB# string = re.sub(r'_(\w)', lambda m: m.group(1).upper(), string) #LINE# #TAB# return string"
"def geometry_type(geometry): #LINE# #TAB# if maya.cmds.objExists(geometry): #LINE# #TAB# #TAB# geo_shape = maya.cmds.listRelatives(geometry, s=True, pa=True) #LINE# #TAB# #TAB# if not geo_shape: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# geometry = geo_shape[0] #LINE# #TAB# #TAB# if 'Polygon' in geometry: #LINE# #TAB# #TAB# #TAB# return 'Polygon' #LINE# #TAB# #TAB# if 'MultiPolygon' in geometry: #LINE# #TAB# #TAB# #TAB# return 'MultiPolygon' #LINE# #TAB# if 'GeometryCollection' in geometry: #LINE# #TAB# #TAB# geometry_type = 'GeometryCollection' #LINE# #TAB# elif 'GeometryCollection' in geometry: #LINE# #TAB# #TAB# geometry_type = 'GeometryCollection' #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def batch_iter(data, batch_size, num_epochs): #LINE# #TAB# num_epochs = int(num_epochs * batch_size) #LINE# #TAB# for batch_id in range(num_epochs): #LINE# #TAB# #TAB# start = 0 #LINE# #TAB# #TAB# end = batch_id + num_epochs * batch_size #LINE# #TAB# #TAB# if start < len(data): #LINE# #TAB# #TAB# #TAB# yield data[start:end] #LINE# #TAB# #TAB# #TAB# start = end #LINE# #TAB# #TAB# if end >= len(data): #LINE# #TAB# #TAB# #TAB# break"
"def split_str(s, n): #LINE# #TAB# res = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# res.append(s[0:n]) #LINE# #TAB# #TAB# #TAB# s = s[n:] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return res"
"def get_possible_pig_config_from(file_name): #LINE# #TAB# config = {'configs': utils.load_hadoop_xml_defaults(file_name, #LINE# #TAB# #TAB#'sahara_plugin_vanilla'), 'args': [], 'params': {}} #LINE# #TAB# return config"
def is_accentuated(letter): #LINE# #TAB# if len(letter)!= 1: #LINE# #TAB# #TAB# raise ValueError('The target string must be one letter.') #LINE# #TAB# if letter[0] == 'A' and letter[-1] == 'Z': #LINE# #TAB# #TAB# return True #LINE# #TAB# elif letter in _accentuated_letter: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def sql_clone(obj, skip={}, session=None): #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = db.session #LINE# #TAB# obj = session.query(obj).filter_by(pk=obj.pk).first() #LINE# #TAB# if skip!= {}: #LINE# #TAB# #TAB# skip.update(skip) #LINE# #TAB# for column in obj.columns: #LINE# #TAB# #TAB# if column not in skip: #LINE# #TAB# #TAB# #TAB# obj.add_column(column) #LINE# #TAB# return obj"
"def extract_incoming_edges(G): #LINE# #TAB# V, E = G #LINE# #TAB# n = len(V) #LINE# #TAB# Eout = lists(n) #LINE# #TAB# for i, e in enumerate(E): #LINE# #TAB# #TAB# Eout[e[0]].append(i) #LINE# #TAB# return Eout"
"def map_dict_to_list(d, width): #LINE# #TAB# result = [] #LINE# #TAB# for start, quantity in d.items(): #LINE# #TAB# #TAB# result.append((start, start + width, quantity)) #LINE# #TAB# return result"
"def show_path(reponame, reposave, path): #LINE# #TAB# return {'repo_name': reponame,'repo_id': reposave, 'name': path.replace( #LINE# #TAB# #TAB# '\\', '/'), 'path': path.replace('\\', '/'), 'issue_template': path. #LINE# #TAB# #TAB# replace('\\', '/'),'source': path.replace('\\', '/')}"
def parse_color_string(value): #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return None #LINE# #TAB# value = value.strip('#') #LINE# #TAB# if len(value) == 6: #LINE# #TAB# #TAB# return parse_color_hex(value[0:2]) #LINE# #TAB# elif len(value) == 8: #LINE# #TAB# #TAB# return parse_color_hex(value[2:4]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
def ticket_properties_of_holder(holder): #LINE# #TAB# if not'milestone_properties' in holder: #LINE# #TAB# #TAB# holder['milestone_properties'] = MilestoneProperties() #LINE# #TAB# return holder['milestone_properties']
"def iter_fields(node): #LINE# #TAB# for field in node._fields: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield field, getattr(node, field) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass"
def true_color_supported() ->bool: #LINE# #TAB# tty_instance = get_tty_instance() #LINE# #TAB# while True: #LINE# #TAB# #TAB# supported = tty_instance.get_true_color() #LINE# #TAB# #TAB# if not supported: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if supported == '24bit': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# tty_instance.close() #LINE# #TAB# return False
"def from_remote_redis(cls) ->(str, str): #LINE# #TAB# _module_name = 'ds_connectors.handlers.aws_s3_handlers' #LINE# #TAB# _handler = 'AwsS3PersistHandler' #LINE# #TAB# return _module_name, _handler"
"def import_pyfile(filepath, mod_name=None): #LINE# #TAB# from imp import load_module, PY_SOURCE #LINE# #TAB# spec = spec_from_file_location(filepath, mod_name=mod_name) #LINE# #TAB# module = import_module(spec) #LINE# #TAB# spec.loader.exec_module(module) #LINE# #TAB# return module"
"def decode_dict_keys_to_str(src): #LINE# #TAB# output = {} #LINE# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# output[key.decode('utf8')] = decode_dict_keys_to_str(value) #LINE# #TAB# #TAB# elif isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# output[key] = value.decode('utf8') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output[key] = value #LINE# #TAB# return output"
"def fill_subparser(subparser): #LINE# #TAB# urls = ([None] * len(ALL_FILES)) #LINE# #TAB# filenames = list(ALL_FILES) #LINE# #TAB# subparser.set_defaults(urls=urls, filenames=filenames) #LINE# #TAB# subparser.add_argument('-P', '--url-prefix', type=str, default=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# help=""URL prefix to prepend to the filenames of "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""non-public files, in order to download them"") #LINE# #TAB# subparser.add_argument('-i', '--url-prefix', type=str, default=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# help=""URL prefix to prepend to the filenames of "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# help=""URL prefix to prepend to the filenames"") #LINE# #TAB#"
def article_id_list(soup): #LINE# #TAB# id_list = [] #LINE# #TAB# tags = raw_parser.article_id(soup) #LINE# #TAB# for tag in tags: #LINE# #TAB# #TAB# id_list.append(article_id(tag)) #LINE# #TAB# return id_list
def get_languages_default(): #LINE# #TAB# languages = LanguageTool._get_languages() #LINE# #TAB# if languages: #LINE# #TAB# #TAB# return languages #LINE# #TAB# else: #LINE# #TAB# #TAB# return settings.LANGUAGES_DEFAULT
"def show_win(key): #LINE# #TAB# try: #LINE# #TAB# #TAB# reg = winreg.OpenKey(key) #LINE# #TAB# except WindowsError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# values = reg.QueryValueEx(reg, 'Value') #LINE# #TAB# #TAB# for val in values: #LINE# #TAB# #TAB# #TAB# print(str(val)) #LINE# #TAB# #TAB# reg.CloseKey(key) #LINE# #TAB# return"
"def token_from_request(request): #LINE# #TAB# token = request.query_params.get('access_token', None) #LINE# #TAB# if not token: #LINE# #TAB# #TAB# return None #LINE# #TAB# return token"
"def replace_objects_with_links(json: dict) ->dict: #LINE# #TAB# for k, v in json.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# json[k] = replace_objects_with_links(v) #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# json[k] = [replace_objects_with_links(link) for link in v] #LINE# #TAB# return json"
def required_header(header): #LINE# #TAB# if header in IGNORE_HEADERS: #LINE# #TAB# #TAB# return False #LINE# #TAB# if header.startswith('HTTP_') or header == 'CONTENT_TYPE': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"def pass_number(inp, err=error.TYPE_MISMATCH): #LINE# #TAB# if not isinstance(inp, numbers.Number): #LINE# #TAB# #TAB# check_value(inp) #LINE# #TAB# #TAB# raise error.BASICError(err) #LINE# #TAB# return inp"
def label_from_cid(cid): #LINE# #TAB# if _is_cid_valid(cid): #LINE# #TAB# #TAB# source = _get_source_by_cid(cid) #LINE# #TAB# elif _is_dev(cid): #LINE# #TAB# #TAB# source = _get_dev_by_cid(cid) #LINE# #TAB# else: #LINE# #TAB# #TAB# return '' #LINE# #TAB# label = source.get('label') #LINE# #TAB# if label is not None: #LINE# #TAB# #TAB# return label #LINE# #TAB# return ''
"def get_bigdata_root(envkey='TEST_BIGDATA'): #LINE# #TAB# bigdata_root = os.path.join(os.path.abspath(os.path.expanduser( #LINE# #TAB# #TAB# os.environ[envkey]), 'bigdata')) #LINE# #TAB# return bigdata_root"
"def resolve_label(cfg: NLPConfig) ->NLPConfig: #LINE# #TAB# label = cfg.sequence_classification.label #LINE# #TAB# if label == 'ner': #LINE# #TAB# #TAB# _LOGGER.debug('Using ner label') #LINE# #TAB# #TAB# return cfg #LINE# #TAB# if label =='seq': #LINE# #TAB# #TAB# _LOGGER.debug('Using seq label') #LINE# #TAB# #TAB# return cfg #LINE# #TAB# if label in _NEER_SEQUENCE_LABELS: #LINE# #TAB# #TAB# _LOGGER.debug('Using seq label') #LINE# #TAB# #TAB# return cfg #LINE# #TAB# return { #LINE# #TAB# #TAB# 'ner': _NEER_SEQUENCE_LABELS[label], #LINE# #TAB# #TAB#'sequence_classification': _NEVER_SEQUENCE_LABELS[label], #LINE# #TAB# }"
"def try_compile(source, name): #LINE# #TAB# try: #LINE# #TAB# #TAB# c = compile(source, name, 'eval') #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# c = compile(source, name, 'exec') #LINE# #TAB# return c"
"def get_num_spellcheck_errors(build_dir): #LINE# #TAB# process = subprocess.Popen(['spellcheck', '--output-dir', build_dir], #LINE# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# for line in process.communicate(): #LINE# #TAB# #TAB# if'spellcheck-error' in line: #LINE# #TAB# #TAB# #TAB# yield line.strip().split()[1]"
"def plaintext_to_html(s): #LINE# #TAB# i = 0 #LINE# #TAB# s = s.replace('&', '&amp;') #LINE# #TAB# s = s.replace('<', '&lt;') #LINE# #TAB# s = s.replace('>', '&gt;') #LINE# #TAB# s = s.replace('""', '&quot;') #LINE# #TAB# if i < len(s): #LINE# #TAB# #TAB# s = s.replace('&', '&amp;') #LINE# #TAB# return s"
"def next_week(yw): #LINE# #TAB# for i, w in enumerate(yw): #LINE# #TAB# #TAB# if w >= 7: #LINE# #TAB# #TAB# #TAB# return i, 1 #LINE# #TAB# return i + 1, 1"
"def find_steam_location(): #LINE# #TAB# if registry is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# key = registry.CreateKey(registry.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Valve\\Steam') #LINE# #TAB# return registry.QueryValueEx(key, 'SteamPath')[0]"
"def _set_textarea(el, value): #LINE# #TAB# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# el.text = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# val = el.text.strip() #LINE# #TAB# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# #TAB# val = '' #LINE# #TAB# #TAB# #TAB# el.text = val"
"def get_variable_id(line, program_ast): #LINE# #TAB# for node in program_ast.body: #LINE# #TAB# #TAB# result = get_variable_id(node, line) #LINE# #TAB# #TAB# if result is not None: #LINE# #TAB# #TAB# #TAB# return result"
"def make_snapshot_from_ops_hash( cls, record_root_hash, prev_consensus_hashes ): #LINE# #TAB# consensus_snapshot = cls.generate_consensus_snapshot_from_ops_hash( record_root_hash ) #LINE# #TAB# for prev_hash in prev_consensus_hashes: #LINE# #TAB# #TAB# consensus_snapshot = cls.generate_consensus_snapshot_from_ops_hash( #LINE# #TAB# #TAB# #TAB# prev_hash ) #LINE# #TAB# #TAB# if consensus_snapshot is not None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return consensus_snapshot"
"def same_signature(a, b): #LINE# #TAB# a_names = set(a.args) #LINE# #TAB# b_names = set(b.args) #LINE# #TAB# if a_names!= b_names: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def open_store(): #LINE# #TAB# with open('{}/store.json'.format(BASE_DIR), 'r') as store_file: #LINE# #TAB# #TAB# data = json.load(store_file) #LINE# #TAB# return data"
def action_button(button: mara_page.response.ActionButton): #LINE# #TAB# context = {} #LINE# #TAB# context['action_button'] = button #LINE# #TAB# return context
"def add_usable_app(name, app): #LINE# #TAB# if name in _usable_apps: #LINE# #TAB# #TAB# return #LINE# #TAB# _usable_apps[name] = app"
"def make_socket(): #LINE# #TAB# mreq = struct.pack('4sl', socket.inet_aton(MCAST_IP), socket.INADDR_ANY) #LINE# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.bind(('', MCAST_PORT)) #LINE# #TAB# sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq) #LINE# #TAB# sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq) #LINE# #TAB# return sock"
"def create_genome_regions(data): #LINE# #TAB# genome_file = tz.get_in([""config"", ""algorithm"", ""genome_contigs""], data) #LINE# #TAB# out = [] #LINE# #TAB# if genome_file: #LINE# #TAB# #TAB# data = tz.update_in(data, [""config"", ""algorithm"", ""genome_contigs""], lambda x: x) #LINE# #TAB# #TAB# regions = set(x.split("","") for x in data) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# regions = set(tz.get_in([""config"", ""algorithm"", ""genome_contigs""], data)) #LINE# #TAB# #TAB# for region in regions: #LINE# #TAB# #TAB# #TAB# region_name = ""%s-%s"" % (genome_file, region) #LINE# #TAB# #TAB# #TAB# out.append((region_name, region)) #LINE# #TAB# return out"
def numpy_has_correlate_flip_bug(): #LINE# #TAB# if HAS_NUMPY: #LINE# #TAB# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def get_repo_hexsha(git_folder): #LINE# #TAB# repo = Repo(str(git_folder)) #LINE# #TAB# if repo.bare: #LINE# #TAB# #TAB# not_git_hexsha = 'notgitrepo' #LINE# #TAB# #TAB# _LOGGER.warning('Not a git repo, SHA1 used will be: %s', not_git_hexsha #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return not_git_hexsha #LINE# #TAB# hexsha = repo.head.commit.hexsha #LINE# #TAB# _LOGGER.info('Found REST API repo SHA1: %s', hexsha) #LINE# #TAB# return hexsha"
def to_int(x): #LINE# #TAB# try: #LINE# #TAB# #TAB# return int(x) #LINE# #TAB# except: #LINE# #TAB# #TAB# return x
"def read_data_from_bytes(fileContent): #LINE# #TAB# ret = [] #LINE# #TAB# totalData = 0 #LINE# #TAB# byteContent = bytearray(fileContent) #LINE# #TAB# for i in range(0, len(byteContent), 2): #LINE# #TAB# #TAB# byteData = bytearray(byteContent[i:i + 2]) #LINE# #TAB# #TAB# sampleRate = int.from_bytes(byteData[i:i + 2], 'big') #LINE# #TAB# #TAB# length = int.from_bytes(byteData[i + 2:i + 4], 'big') #LINE# #TAB# #TAB# ret.append(sampleRate) #LINE# #TAB# #TAB# byteContent = bytearray(byteData[4:]) #LINE# #TAB# return ret"
"def load_points(): #LINE# #TAB# import json #LINE# #TAB# file_name = os.path.splitext(os.path.basename(__file__))[0] #LINE# #TAB# points_file = resource_string(__name__, file_name) #LINE# #TAB# points = json.loads(points_file.read()) #LINE# #TAB# return points"
"def get_deep_class_from_string(class_string, module): #LINE# #TAB# found = get_class_from_string(class_string, module) #LINE# #TAB# if found is not None: #LINE# #TAB# #TAB# return found #LINE# #TAB# parent_module = module #LINE# #TAB# for child_class_string in class_string.split('.'): #LINE# #TAB# #TAB# parent_module = get_deep_class_from_string(parent_class_string, #LINE# #TAB# #TAB# #TAB# module) #LINE# #TAB# #TAB# if parent_module is not None: #LINE# #TAB# #TAB# #TAB# found = get_deep_class_from_string(parent_class_string, #LINE# #TAB# #TAB# #TAB# #TAB# module) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return found"
"def load_img(path, grayscale=False, target_size=None): #LINE# #TAB# img = cv2.imread(path) #LINE# #TAB# if grayscale: #LINE# #TAB# #TAB# img = cv2.convertScaleAbs(img, target_size) #LINE# #TAB# if target_size is not None: #LINE# #TAB# #TAB# img = cv2.resize(img, target_size) #LINE# #TAB# return img"
"def parse_yaml(input_file): #LINE# #TAB# with open(input_file, 'r') as f: #LINE# #TAB# #TAB# config = yaml.safe_load(f) #LINE# #TAB# return config"
def database_locales(request): #LINE# #TAB# locales = {} #LINE# #TAB# for language in database_locales(request): #LINE# #TAB# #TAB# if language.short_name: #LINE# #TAB# #TAB# #TAB# language_code = language.language_code #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# language_code = None #LINE# #TAB# #TAB# if not language_code: #LINE# #TAB# #TAB# #TAB# if language.code not in locales: #LINE# #TAB# #TAB# #TAB# #TAB# locales[language.code] = Language() #LINE# #TAB# #TAB# #TAB# locales[language.code].save() #LINE# #TAB# return locales
"def get_user(uid): #LINE# #TAB# try: #LINE# #TAB# #TAB# acl_url = urljoin(_acl_url(), 'users/{}'.format(uid)) #LINE# #TAB# #TAB# r = http.get(acl_url) #LINE# #TAB# #TAB# return r.json() #LINE# #TAB# except DCOSHTTPException as e: #LINE# #TAB# #TAB# if e.response.status_code!= 400: #LINE# #TAB# #TAB# #TAB# raise"
"def get_case(word, correction): #LINE# #TAB# if word.isupper(): #LINE# #TAB# #TAB# return correction.upper() #LINE# #TAB# if word.islower(): #LINE# #TAB# #TAB# return word #LINE# #TAB# if correction == word: #LINE# #TAB# #TAB# return word #LINE# #TAB# if len(word) == 3 and word[:2] in 'aeiou': #LINE# #TAB# #TAB# return 'A' + word[2:] #LINE# #TAB# if not word.islower(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return CASE_MAPPED[correction] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return correction #LINE# #TAB# return correction"
"def filter_names(names): #LINE# #TAB# names = [n for n in names if n not in EXCLUDE_NAMES] #LINE# #TAB# for pattern in EXCLUDE_PATTERNS: #LINE# #TAB# #TAB# names = [n for n in names if not fnmatch.fnmatch(n, pattern) and #LINE# #TAB# #TAB# #TAB# not n.endswith('.py')] #LINE# #TAB# return names"
"def find_unpickable_doc(dict_of_dict): #LINE# #TAB# unpickable_doc = {} #LINE# #TAB# for k, v in dict_of_dict.items(): #LINE# #TAB# #TAB# if type(v) in [dict, bytearray]: #LINE# #TAB# #TAB# #TAB# pickable_doc[k] = pickle.loads(v) #LINE# #TAB# #TAB# elif type(v) is dict: #LINE# #TAB# #TAB# #TAB# pickable_doc[k] = find_unpickable_doc(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unpickable_doc[k] = v #LINE# #TAB# return unpickable_doc"
def is_radar_sequential(radar): #LINE# #TAB# radar = _radar_to_tuple(radar) #LINE# #TAB# for sweep in radar: #LINE# #TAB# #TAB# if not sweep: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if len(sweep)!= 2: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# first = sweep[0] #LINE# #TAB# #TAB# for second in sweep[1:]: #LINE# #TAB# #TAB# #TAB# if not first: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# if first!= second: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"def double_array(mylist): #LINE# #TAB# c = doubleArray(len(mylist)) #LINE# #TAB# for i, v in enumerate(mylist): #LINE# #TAB# #TAB# c[i] = v #LINE# #TAB# return c"
"def contains_python_files_or_subdirs(folder): #LINE# #TAB# if not os.path.isfile(folder): #LINE# #TAB# #TAB# return False #LINE# #TAB# for path, _, files in os.walk(folder): #LINE# #TAB# #TAB# if files.endswith('.py'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def compose_help_block_message(): #LINE# #TAB# message = ( #LINE# #TAB# #TAB# 'You must supply either a project id (like ""/projectinfo/<project_id>"")' #LINE# #TAB# #TAB# ) #LINE# #TAB# message += ( #LINE# #TAB# #TAB#'or search for one using the form ""/projectinfo/search/<search_term_1>+<search_term_2>""' #LINE# #TAB# #TAB# ) #LINE# #TAB# return message"
def analytic_signal(s): #LINE# #TAB# sig = np.zeros_like(s) #LINE# #TAB# if s.shape[0] <= 2: #LINE# #TAB# #TAB# n = s.shape[1] #LINE# #TAB# #TAB# sig[0] = n * np.cos(s[0]) #LINE# #TAB# #TAB# sig[1] = n * np.sin(s[1]) #LINE# #TAB# else: #LINE# #TAB# #TAB# n = np.arange(s.shape[0]) #LINE# #TAB# #TAB# sig[0] = 2 * np.pi * np.cos(s[0]) #LINE# #TAB# #TAB# sig[1] = 2 * np.sin(s[1]) #LINE# #TAB# return sig
"def create_hash(file): #LINE# #TAB# h = sha256() #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file, ""rb"") as f: #LINE# #TAB# #TAB# #TAB# h.update(f.read()) #LINE# #TAB# #TAB# return h.hexdigest() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None"
"def build_output_table(cls, name='inputTableName', output_name='output'): #LINE# #TAB# #TAB# obj = cls(name) #LINE# #TAB# #TAB# obj.exporter = 'get_output_table_name' #LINE# #TAB# #TAB# obj.output_name = output_name #LINE# #TAB# #TAB# return obj"
"def fasta_iter(fasta_name): #LINE# #TAB# file_handle = open(fasta_name, 'r') #LINE# #TAB# fasta_name = file_handle.readline() #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = file_handle.readline() #LINE# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# line_handle.close() #LINE# #TAB# #TAB# yield line"
"def get_named_graph(identifier, store_id=DEFAULT_STORE, create=True): #LINE# #TAB# if not isinstance(identifier, URIRef): #LINE# #TAB# #TAB# identifier = URIRef(identifier) #LINE# #TAB# store = DjangoStore(store_id) #LINE# #TAB# graph = Graph(store, identifier=identifier) #LINE# #TAB# if graph.open(None, create=create)!= VALID_STORE: #LINE# #TAB# #TAB# raise ValueError(""The store identified by {0} is not a valid store"".format(store_id)) #LINE# #TAB# return graph"
"def graph_diff(g1, g2): #LINE# #TAB# a = set(g1.edges()) #LINE# #TAB# b = set(g2.edges()) #LINE# #TAB# diff = [] #LINE# #TAB# for a_node in a: #LINE# #TAB# #TAB# b.remove(a_node) #LINE# #TAB# for b_node in b: #LINE# #TAB# #TAB# if a_node not in b: #LINE# #TAB# #TAB# #TAB# diff.append((a_node, b_node)) #LINE# #TAB# for a_node in a: #LINE# #TAB# #TAB# b.remove((a_node, b_node)) #LINE# #TAB# for b_node in b: #LINE# #TAB# #TAB# if b_node not in a: #LINE# #TAB# #TAB# #TAB# diff.append((a_node, b_node)) #LINE# #TAB# return diff"
"def simple_merge_handler(previous_props, next_props): #LINE# #TAB# result = {} #LINE# #TAB# if previous_props: #LINE# #TAB# #TAB# for key in list(previous_props.keys()): #LINE# #TAB# #TAB# #TAB# if key in next_props: #LINE# #TAB# #TAB# #TAB# #TAB# result[key] = next_props[key] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result[key] = previous_props[key] #LINE# #TAB# if next_props: #LINE# #TAB# #TAB# for key in list(next_props.keys()): #LINE# #TAB# #TAB# #TAB# if key not in result: #LINE# #TAB# #TAB# #TAB# #TAB# result[key] = next_props[key] #LINE# #TAB# return result"
"def get_mdlfrom_commit_message(message): #LINE# #TAB# lines = message.split('\n', 1) #LINE# #TAB# if len(lines) == 1: #LINE# #TAB# #TAB# return lines[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# mdl = None #LINE# #TAB# #TAB# for line in lines[1:]: #LINE# #TAB# #TAB# #TAB# match = re.match('mdl\\s+(\\d+)', line) #LINE# #TAB# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# #TAB# mdl = match.group(1) #LINE# #TAB# #TAB# return mdl"
def filter_languages(search_text: str) ->iter: #LINE# #TAB# global _SUPPORTED_LANGUAGES #LINE# #TAB# if not search_text: #LINE# #TAB# #TAB# return #LINE# #TAB# for language in _SUPPORTED_LANGUAGES: #LINE# #TAB# #TAB# if search_text in language: #LINE# #TAB# #TAB# #TAB# yield language
"def escape_haproxy_config_string(value): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return value #LINE# #TAB# escaped = False #LINE# #TAB# for char in value: #LINE# #TAB# #TAB# if char == '\n': #LINE# #TAB# #TAB# #TAB# escaped = True #LINE# #TAB# #TAB# elif char == '\\': #LINE# #TAB# #TAB# #TAB# escaped = False #LINE# #TAB# #TAB# elif char == '\\': #LINE# #TAB# #TAB# #TAB# escaped = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# return '""' + escaped + '""'"
"def auto_text_highlight(text): #LINE# #TAB# text = text.lower() #LINE# #TAB# lines = text.split('\n') #LINE# #TAB# selected_lines = [] #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# line_index = lines.index('---') #LINE# #TAB# #TAB# #TAB# part = lines[line_index] #LINE# #TAB# #TAB# #TAB# selected_lines.append(highlight_line(part, line_index)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# text = '\n'.join(selected_lines) #LINE# #TAB# return text"
"def clone_module(m): #LINE# #TAB# try: #LINE# #TAB# #TAB# return m.__class__() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# if isinstance(m, types.ModuleType): #LINE# #TAB# #TAB# #TAB# m.__class__ = clone_module(m) #LINE# #TAB# #TAB# return m"
def import_matplotlib(): #LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib.pyplot as plt #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return plt
"def from_data( name, coors, ngroups, conns, mat_ids, descs, igs = None ): #LINE# #TAB# if igs is None: #LINE# #TAB# #TAB# igs = range(len(conns)) #LINE# #TAB# mesh = Mesh(name) #LINE# #TAB# mesh._set_data(coors=coors, ngroups=ngroups, conns=[conns[ig] for ig in #LINE# #TAB# #TAB# igs], mat_ids=[mat_ids[ig] for ig in igs], descs=[descs[ig] for ig in #LINE# #TAB# #TAB# igs]) #LINE# #TAB# mesh._set_shape_info() #LINE# #TAB# return mesh"
"def get_first_builder_window(builder): #LINE# #TAB# for obj in builder.get_objects(): #LINE# #TAB# #TAB# if isinstance(obj, gtk.Window): #LINE# #TAB# #TAB# #TAB# return obj"
"def add_object(objects, summary): #LINE# #TAB# global _OBJECTS #LINE# #TAB# cur = None #LINE# #TAB# if _OBJECTS is None: #LINE# #TAB# #TAB# _OBJECTS = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# for obj in objects: #LINE# #TAB# #TAB# #TAB# full_name, obj_summary = get_object_info(obj, summary) #LINE# #TAB# #TAB# #TAB# if full_name not in _OBJECTS: #LINE# #TAB# #TAB# #TAB# #TAB# _OBJECTS[full_name] = obj, summary #LINE# #TAB# #TAB# #TAB# #TAB# _OBJECTS[full_name].append(obj_summary) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# _OBJECTS[full_name] = objects, summary"
"def load_data_time_machine(): #LINE# #TAB# path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# data_path = os.path.join(path, 'data', 'time_machine.json') #LINE# #TAB# with open(data_path) as data_file: #LINE# #TAB# #TAB# data = json.load(data_file) #LINE# #TAB# return data"
"def color_transparency(img, bgcolor): #LINE# #TAB# if bgcolor is not None: #LINE# #TAB# #TAB# bg = bg.red #LINE# #TAB# #TAB# img = Image.new('RGB', (bgcolor, bg), img) #LINE# #TAB# else: #LINE# #TAB# #TAB# img = img #LINE# #TAB# return img"
"def get_most_common_field_value_for_group(df, groupby, fld): #LINE# #TAB# df_list = df[groupby].set_index('value') #LINE# #TAB# df_list.sort_values(key=fld, inplace=True) #LINE# #TAB# highest_freq = df_list.shape[0] #LINE# #TAB# for key in fld: #LINE# #TAB# #TAB# df_list.pop(key) #LINE# #TAB# #TAB# if df_list.count(key) > highest_freq: #LINE# #TAB# #TAB# #TAB# return key #LINE# #TAB# return None"
def config_loggers(): #LINE# #TAB# formatter = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(levelname)s - %(message)s') #LINE# #TAB# handler = logging.StreamHandler(sys.stderr) #LINE# #TAB# handler.setFormatter(formatter) #LINE# #TAB# logger = logging.getLogger('refunc') #LINE# #TAB# logger.setLevel(logging.DEBUG) #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# return logger
"def get_sysv_script(name): #LINE# #TAB# import os #LINE# #TAB# sysv_script_dir = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# sysv_script_name = os.path.join(sysv_script_dir, '{}.py'.format(name)) #LINE# #TAB# if os.path.isfile(sysv_script_name): #LINE# #TAB# #TAB# return sysv_script_name #LINE# #TAB# init_script_path = os.path.join(sysv_script_dir, '{}.py'.format(name)) #LINE# #TAB# if os.path.isfile(init_script_path): #LINE# #TAB# #TAB# return init_script_path"
"def default_diff(latest_config, current_config): #LINE# #TAB# pop_no_diff_fields(latest_config, current_config) #LINE# #TAB# diff = DeepDiff( #LINE# #TAB# #TAB# latest_config, #LINE# #TAB# #TAB# current_config, #LINE# #TAB# #TAB# ignore_order=True #LINE# #TAB# ) #LINE# #TAB# return diff"
"def get_unicode_from_response(r): #LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB# 'In requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)' #LINE# #TAB# #TAB#, DeprecationWarning) #LINE# #TAB# tried_encodings = [] #LINE# #TAB# encoding = get_encoding_from_headers(r.headers) #LINE# #TAB# if encoding: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return str(r.content, encoding) #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# tried_encodings.append(encoding) #LINE# #TAB# try: #LINE# #TAB# #TAB# return str(r.content, encoding, errors='replace') #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return r.content"
"def stock_sort_key(item, max_amount): #LINE# #TAB# amount, stock = item #LINE# #TAB# if amount > max_amount: #LINE# #TAB# #TAB# return amount - max_amount, stock #LINE# #TAB# return amount, item"
"def is_bam_valid(bam_file): #LINE# #TAB# import pysam #LINE# #TAB# try: #LINE# #TAB# #TAB# pysam.AlignmentFile(bam_file, 'r') #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"def convert_logic_from_string(name): #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(name, six.string_types): #LINE# #TAB# #TAB# if name.lower() in {'true', 'True'}: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(name, int): #LINE# #TAB# #TAB# return int(name) #LINE# #TAB# else: #LINE# #TAB# #TAB# return name"
"def suggest_obj_has_no(value, frame, groups): #LINE# #TAB# del value, frame #LINE# #TAB# func_name, = groups #LINE# #TAB# functions = get_func_by_name(func_name, frame) #LINE# #TAB# if any([(not hasattr(f, '__call__')) for f in functions]): #LINE# #TAB# #TAB# yield NO_FUNC_MSG"
"def node_to_mfd(node, taglist): #LINE# #TAB# if 'incrementalMFD' in taglist: #LINE# #TAB# #TAB# mfd = node_to_evenly_discretized(node.nodes[taglist.index( #LINE# #TAB# #TAB# #TAB# 'incrementalMFD')]) #LINE# #TAB# elif 'truncGutenbergRichterMFD' in taglist: #LINE# #TAB# #TAB# mfd = node_to_truncated_gr(node.nodes[taglist.index( #LINE# #TAB# #TAB# #TAB# 'truncGutenbergRichterMFD')]) #LINE# #TAB# else: #LINE# #TAB# #TAB# mfd = None #LINE# #TAB# return mfd"
"def digits_only(string): #LINE# #TAB# if not isinstance(string, str): #LINE# #TAB# #TAB# string = str(string) #LINE# #TAB# if not string.startswith('0.'): #LINE# #TAB# #TAB# return set() #LINE# #TAB# result = set() #LINE# #TAB# for char in string: #LINE# #TAB# #TAB# if char.isdigit(): #LINE# #TAB# #TAB# #TAB# result.add(char) #LINE# #TAB# return result"
"def get_templates(fnames, blend=True): #LINE# #TAB# templates = set() #LINE# #TAB# for name in fnames: #LINE# #TAB# #TAB# header = _process_instrument_header(name, blend) #LINE# #TAB# #TAB# if header is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# template = HeaderSet() #LINE# #TAB# #TAB# for name in header: #LINE# #TAB# #TAB# #TAB# template.add(name) #LINE# #TAB# #TAB# if blend and header not in templates: #LINE# #TAB# #TAB# #TAB# templates.add(header) #LINE# #TAB# return templates"
def ecc_map_names(): #LINE# #TAB# names = [] #LINE# #TAB# for name in _map: #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if name not in _ecc_map: #LINE# #TAB# #TAB# #TAB# names.append(name) #LINE# #TAB# return names
"def normalized_mae_score(model_mae, naive_mae): #LINE# #TAB# if model_mae is None or naive_mae is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# model_mae = float(model_mae) #LINE# #TAB# naive_mae = float(naive_mae) #LINE# #TAB# if model_mae!= naive_mae: #LINE# #TAB# #TAB# model_mae /= naive_mae #LINE# #TAB# return model_mae"
def find_files(directory): #LINE# #TAB# pattern = '{directory}/*.xml'.format(directory=directory) #LINE# #TAB# files = glob(pattern) #LINE# #TAB# return files
"def calc_bounds(xy, entity): #LINE# #TAB# width, height = entity.width, entity.height #LINE# #TAB# if width is not None and height is not None: #LINE# #TAB# #TAB# x_min = xy[0] - width #LINE# #TAB# #TAB# y_min = xy[1] - width #LINE# #TAB# #TAB# x_max = xy[2] - height #LINE# #TAB# #TAB# y_max = xy[3] - height #LINE# #TAB# else: #LINE# #TAB# #TAB# x_min = xy[0] - width #LINE# #TAB# #TAB# y_min = xy[1] - width #LINE# #TAB# #TAB# y_max = xy[2] - height #LINE# #TAB# return x_min, y_min, x_max, y_max"
def no_expression_eval(obj): #LINE# #TAB# try: #LINE# #TAB# #TAB# return eval(obj) #LINE# #TAB# except: #LINE# #TAB# #TAB# return obj
def get_email_forwarding(netid): #LINE# #TAB# forwarding = get_netid_model().UwEmailForwarding(netid) #LINE# #TAB# if forwarding: #LINE# #TAB# #TAB# return forwarding #LINE# #TAB# return None
"def columns_from_list(df, series): #LINE# #TAB# columns = [] #LINE# #TAB# for item in series: #LINE# #TAB# #TAB# columns.append(item) #LINE# #TAB# final_columns = df.columns.tolist() #LINE# #TAB# return final_columns"
"def from_string(cls, string): #LINE# #TAB# tree = eTree.parse(string) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# md = cls() #LINE# #TAB# md.document = root #LINE# #TAB# return md"
"def scale_polygon(polygon, scale): #LINE# #TAB# new_polygon = [] #LINE# #TAB# for pt in polygon: #LINE# #TAB# #TAB# new_polygon.append(scale * pt) #LINE# #TAB# return new_polygon"
"def wrap_data(data): #LINE# #TAB# numpy_data = None #LINE# #TAB# if isinstance(data, np.ndarray): #LINE# #TAB# #TAB# numpy_data = wrap_data(data) #LINE# #TAB# elif isinstance(data, dict): #LINE# #TAB# #TAB# numpy_data = {} #LINE# #TAB# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# #TAB# numpy_data[key] = wrap_data(value) #LINE# #TAB# elif isinstance(data, h5py.highlevel.Dataset): #LINE# #TAB# #TAB# numpy_data = wrap_data(data) #LINE# #TAB# else: #LINE# #TAB# #TAB# return data #LINE# #TAB# return numpy_data"
def aaindex_lookup(records): #LINE# #TAB# aaindex_lookup = {} #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# aaindex_lookup[record.id] = AAIndexObject() #LINE# #TAB# #TAB# aaindex_lookup[record.id].append(record) #LINE# #TAB# return aaindex_lookup
"def sanitize_unicode(u): #LINE# #TAB# sanitized = unicodedata.normalize('NFC', u) #LINE# #TAB# for invalid_symbol in VALID_SYMBOLS: #LINE# #TAB# #TAB# sanitized = sanitized.replace(invalid_symbol, '_') #LINE# #TAB# return sanitized"
"def get_sdk_object_dict(object): #LINE# #TAB# result_dict = {'name': object.name,'version': object.version, 'config': #LINE# #TAB# #TAB# _get_sdk_object_config(object)} #LINE# #TAB# return result_dict"
"def shift_and_scale(matrix, shift, scale): #LINE# #TAB# m = len(matrix) #LINE# #TAB# min_val = min(matrix) + shift #LINE# #TAB# max_val = max(matrix) - shift #LINE# #TAB# matrix[min_val:max_val] *= scale #LINE# #TAB# return matrix"
def validate_mime_type(mimetype): #LINE# #TAB# if mimetype is None: #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# _mimetype_dict[mimetype] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if mimetype not in _MIMETYPE_VALIDATORS: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def get_fs_type(mypath): #LINE# #TAB# mypath = normalize_path(mypath) #LINE# #TAB# for type_, _, fs_type in FILESYSTEM_TYPES.items(): #LINE# #TAB# #TAB# if mypath.startswith(type_): #LINE# #TAB# #TAB# #TAB# return fs_type"
"def handle_response (response): #LINE# #TAB# res = json.loads(response.text) #LINE# #TAB# response_code = res.get('status_code') #LINE# #TAB# response_headers = { #LINE# #TAB# #TAB# 'Content-Type': response.get('content-type'), #LINE# #TAB# #TAB# 'X-Requested-With': 'XMLHttpRequest' #LINE# #TAB# } #LINE# #TAB# if response_code in response_headers: #LINE# #TAB# #TAB# response_content = response_headers[response_code] #LINE# #TAB# #TAB# response = {'data': response_content,'status_code': response_code} #LINE# #TAB# return response"
"def load_manifest(data): #LINE# #TAB# output = {} #LINE# #TAB# for line in data.splitlines(): #LINE# #TAB# #TAB# x = line.strip() #LINE# #TAB# #TAB# if not x: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if '=' not in x: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# k, v = x.split('=') #LINE# #TAB# #TAB# output[k] = v #LINE# #TAB# return output"
def warn_and_continue(exn): #LINE# #TAB# try: #LINE# #TAB# #TAB# exn.show() #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False
"def from_name(cls, name: str) ->'Curve': #LINE# #TAB# curve = cls(name) #LINE# #TAB# curve.generate() #LINE# #TAB# return curve"
"def open_stream(stream): #LINE# #TAB# global stream_fd #LINE# #TAB# try: #LINE# #TAB# #TAB# stream_fd = stream.open() #LINE# #TAB# except StreamError as err: #LINE# #TAB# #TAB# if err.errno!= errno.EINVAL: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# try: #LINE# #TAB# #TAB# log.debug('Pre-buffering 8192 bytes') #LINE# #TAB# #TAB# prebuffer = stream_fd.read(8192) #LINE# #TAB# except IOError as err: #LINE# #TAB# #TAB# raise StreamError('Failed to read data from stream: {}'.format(err)) #LINE# #TAB# if not prebuffer: #LINE# #TAB# #TAB# stream_fd.close() #LINE# #TAB# #TAB# raise StreamError('No data returned from stream') #LINE# #TAB# return stream_fd, prebuffer"
"def compute_complementary_atoms(model, predicate): #LINE# #TAB# binding_dict = {} #LINE# #TAB# for atom in model.atoms: #LINE# #TAB# #TAB# binding = compute_binding(atom, predicate) #LINE# #TAB# #TAB# if binding in binding_dict: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# atoms = [] #LINE# #TAB# #TAB# for o in binding_dict[atom]: #LINE# #TAB# #TAB# #TAB# if o not in model.atoms: #LINE# #TAB# #TAB# #TAB# #TAB# atoms.append(o) #LINE# #TAB# #TAB# binding_dict[atom] =''.join(atoms) #LINE# #TAB# #TAB# print(binding_dict) #LINE# #TAB# return binding_dict"
"def distance_kullback(A, B): #LINE# #TAB# alpha = 0 #LINE# #TAB# beta = 0 #LINE# #TAB# for i in range(A.shape[0]): #LINE# #TAB# #TAB# for j in range(A.shape[1]): #LINE# #TAB# #TAB# #TAB# alpha += distance_euc(A[i, j], B[i, j]) #LINE# #TAB# #TAB# #TAB# beta += distance_euc(A[i, j], B[i, j]) #LINE# #TAB# return alpha * beta"
"def iter_copy(structure): #LINE# #TAB# if hasattr(structure, 'iteritems'): #LINE# #TAB# #TAB# return structure.iteritems() #LINE# #TAB# elif isinstance(structure, dict): #LINE# #TAB# #TAB# out = structure.copy() #LINE# #TAB# #TAB# for inner_dict in structure.iteritems(): #LINE# #TAB# #TAB# #TAB# out.update(inner_dict) #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return structure"
"def item_perceel_adapter(obj, request): #LINE# #TAB# return {'id': obj.id,'sectie': obj.sectie, 'capakey': obj.capakey, #LINE# #TAB# #TAB# 'percid': obj.percid}"
"def read_time(path): #LINE# #TAB# if path: #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# return f.read() #LINE# #TAB# return None"
"def duration_to_date_and_time(duration_in_seconds: int) ->str: #LINE# #TAB# n_days = int(duration_in_seconds / 86400) #LINE# #TAB# duration_in_seconds = duration_in_seconds - n_days * 86400 #LINE# #TAB# minutes, seconds = divmod(duration_in_seconds, 60) #LINE# #TAB# hours, minutes = divmod(minutes, 60) #LINE# #TAB# result = f'{n_days}:{minutes:02d}:{seconds:02d}' #LINE# #TAB# if days > 0: #LINE# #TAB# #TAB# result = f'{days} days {result}' #LINE# #TAB# if hours > 0: #LINE# #TAB# #TAB# result = f'{hours:02d}:{minutes:02d}' #LINE# #TAB# if seconds > 0: #LINE# #TAB# #TAB# result = f'{seconds:02d}:{seconds:02d}' #LINE# #TAB# return result"
def get_config(): #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = '' #LINE# #TAB# cfg.parentdir_prefix ='sphinx_ioam_theme-' #LINE# #TAB# cfg.versionfile_source ='sphinx_ioam_theme/_version.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"def get_lon_lat_crs(searchList): #LINE# #TAB# cols = [x.lower() for x in searchList] #LINE# #TAB# lon = searchList[0] #LINE# #TAB# lat = searchList[1] #LINE# #TAB# crs = searchList[2] #LINE# #TAB# for x in cols: #LINE# #TAB# #TAB# if x == 'lon': #LINE# #TAB# #TAB# #TAB# lon = np.inf #LINE# #TAB# #TAB# #TAB# lat = np.inf #LINE# #TAB# #TAB# elif x == 'lat': #LINE# #TAB# #TAB# #TAB# lat = np.inf #LINE# #TAB# #TAB# elif x == 'crs': #LINE# #TAB# #TAB# #TAB# crs = np.inf #LINE# #TAB# return lon, lat, crs"
"def git_tag(tag_name, push=False): #LINE# #TAB# git_tag_cmd = ['git', 'tag', '-a', tag_name] #LINE# #TAB# if push: #LINE# #TAB# #TAB# git_tag_cmd.append('-s') #LINE# #TAB# #TAB# git_run(git_tag_cmd) #LINE# #TAB# output = _run(git_tag_cmd) #LINE# #TAB# if output: #LINE# #TAB# #TAB# if not push: #LINE# #TAB# #TAB# #TAB# git_run(git_tag_cmd) #LINE# #TAB# #TAB# return output #LINE# #TAB# return None"
def create_keymaps(raw_keymaps): #LINE# #TAB# keymaps = Keymap() #LINE# #TAB# for raw_keymap in raw_keymaps: #LINE# #TAB# #TAB# for line in raw_keymap: #LINE# #TAB# #TAB# #TAB# yield keymap #LINE# #TAB# return keymaps
"def get_cmor_fp_meta(fp): #LINE# #TAB# directory_meta = list(DIR_ATTS) #LINE# #TAB# meta = get_cmor_dir_meta(fp, directory_meta) #LINE# #TAB# meta.update(get_cmor_fname_meta(fp)) #LINE# #TAB# return meta"
def should_enable_dhcp(): #LINE# #TAB# return node_cache.introspection_active( #LINE# #TAB# #TAB# ) or CONF.processing.node_not_found_hook is not None
"def parse_module_name(string): #LINE# #TAB# if string[0] == '/': #LINE# #TAB# #TAB# if string.endswith('.py'): #LINE# #TAB# #TAB# #TAB# return string[:-3], string[:-4] #LINE# #TAB# #TAB# elif string.endswith('.pyc'): #LINE# #TAB# #TAB# #TAB# return string[:-5], string[-4:] #LINE# #TAB# try: #LINE# #TAB# #TAB# parts = string.split('/') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return string, None #LINE# #TAB# author = parts[0] #LINE# #TAB# name = parts[1] #LINE# #TAB# return author, name"
"def format_text(content, regexps): #LINE# #TAB# if not regexps: #LINE# #TAB# #TAB# return content #LINE# #TAB# for regex, replace in regexps: #LINE# #TAB# #TAB# content = re.sub(regex, replace, content) #LINE# #TAB# return content"
"def step_remove_rows_with_missing_ids(df, id_cols=None): #LINE# #TAB# if id_cols is None: #LINE# #TAB# #TAB# id_cols = df.columns #LINE# #TAB# invalid_ids = [] #LINE# #TAB# for col in id_cols: #LINE# #TAB# #TAB# if df[col].isnull().any().any(): #LINE# #TAB# #TAB# #TAB# invalid_ids.append(col) #LINE# #TAB# df = df[~invalid_ids] #LINE# #TAB# if len(df) > 0: #LINE# #TAB# #TAB# return df"
def mk_server_cfg(args: ofxget.ArgsType) ->configparser.SectionProxy: #LINE# #TAB# server_cfg = configparser.SectionProxy() #LINE# #TAB# server_cfg.optionxform = lambda option: option #LINE# #TAB# server_cfg.read(args) #LINE# #TAB# return server_cfg
"def read_folder(directory): #LINE# #TAB# temp = [] #LINE# #TAB# if not os.path.isdir(directory): #LINE# #TAB# #TAB# return temp #LINE# #TAB# for file_name in os.listdir(directory): #LINE# #TAB# #TAB# file_path = os.path.join(directory, file_name) #LINE# #TAB# #TAB# with open(file_path) as f: #LINE# #TAB# #TAB# #TAB# temp.append(f.read()) #LINE# #TAB# return temp"
"def create_table_date_partitioning(field, expiration_ms=None): #LINE# #TAB# if expiration_ms is None: #LINE# #TAB# #TAB# expiration_ms = 0.0 #LINE# #TAB# date_part = field.split('-') #LINE# #TAB# part = [] #LINE# #TAB# for i in range(1, len(date_part)): #LINE# #TAB# #TAB# part.append(datetime.datetime.strptime(date_part[i], '%Y-%m-%d %H:%M:%S')) #LINE# #TAB# #TAB# part.append(datetime.datetime.now()) #LINE# #TAB# #TAB# part.append(datetime.datetime.strptime(date_part[i], '%Y-%m-%d %H:%M:%S')) #LINE# #TAB# part.append('') #LINE# #TAB# create_table_date(part) #LINE# #TAB# return part"
"def extract_vtodos(calendars): #LINE# #TAB# vtodos = calendars[0].vtodos #LINE# #TAB# for href, calendar in calendars: #LINE# #TAB# #TAB# vtodos = extract_vtodos(calendar) #LINE# #TAB# #TAB# if vtodos: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return vtodos"
"def get_completions(text, start, end): #LINE# #TAB# begidx = get_begidx(text, start) #LINE# #TAB# endidx = get_endidx(text, end) #LINE# #TAB# completions = [] #LINE# #TAB# while begidx < end: #LINE# #TAB# #TAB# start += 1 #LINE# #TAB# #TAB# completions.append(text[begidx:end]) #LINE# #TAB# #TAB# begidx = begidx + 1 #LINE# #TAB# completions.append('') #LINE# #TAB# return completions"
"def request_key(): #LINE# #TAB# log('Please enter your API key ( from'+ read_webapp_url() +'):') #LINE# #TAB# api_key = click.prompt('API KEY', hide_input=True) #LINE# #TAB# return api_key"
"def to_polar(x, y, theta_units=""radians""): #LINE# #TAB# assert theta_units in ['radians', 'degrees'],\ #LINE# #TAB# #TAB# ""kwarg theta_units must specified in radians or degrees"" #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = to_radians(x) #LINE# #TAB# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# #TAB# theta = to_degrees(theta) #LINE# #TAB# return theta"
"def handle_resource_get(resource, request, parent_resource): #LINE# #TAB# method = request.method.lower() #LINE# #TAB# param = request.GET.split('/', 1)[1] #LINE# #TAB# resource_type = request.GET.split('$', 1)[0] #LINE# #TAB# new_method = '%s.%s' % (resource_type, param) #LINE# #TAB# if new_method == 'get': #LINE# #TAB# #TAB# return handle_get(resource, request, parent_resource) #LINE# #TAB# if method == 'post': #LINE# #TAB# #TAB# return handle_post(resource, request, parent_resource) #LINE# #TAB# return None"
"def import_plot_and_formatter(renderer='agg'): #LINE# #TAB# if renderer is None: #LINE# #TAB# #TAB# renderer = 'agg' #LINE# #TAB# try: #LINE# #TAB# #TAB# import matplotlib #LINE# #TAB# #TAB# renderer_module = importlib.import_module(renderer) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# raise ImportError('renderer {} not found'.format(renderer)) #LINE# #TAB# renderer_module = getattr(matplotlib, renderer_module) #LINE# #TAB# plt.rcParams['figure.figsize'] = '10pt' #LINE# #TAB# formatter = getattr(matplotlib, renderer_module) #LINE# #TAB# return plt, formatter"
"def preprocess_input(x): #LINE# #TAB# x = x[(...), ::-1] #LINE# #TAB# x[..., 0] -= 103.939 #LINE# #TAB# x[..., 1] -= 116.779 #LINE# #TAB# x[..., 2] -= 123.68 #LINE# #TAB# return x"
"def fix_pair_betweennesses(G): #LINE# #TAB# mapping = {} #LINE# #TAB# for e in G: #LINE# #TAB# #TAB# for f in G[e]: #LINE# #TAB# #TAB# #TAB# if f not in mapping: #LINE# #TAB# #TAB# #TAB# #TAB# mapping[f] = e #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# for e in G: #LINE# #TAB# #TAB# if f not in mapping: #LINE# #TAB# #TAB# #TAB# add_pair_betweennesses(mapping[e], e) #LINE# #TAB# for e in G.edges(): #LINE# #TAB# #TAB# if not mapping[e[0]]: #LINE# #TAB# #TAB# #TAB# del mapping[e[0]]) #LINE# #TAB# #TAB# #TAB# add_pair_betweennesses(mapping[e[1]], e[2]) #LINE# #TAB# return"
"def get_sound_file_duration(fn): #LINE# #TAB# with wave.open(fn, 'rb') as f: #LINE# #TAB# #TAB# n_channels = wavfile.getnchannels() #LINE# #TAB# #TAB# if len(n_channels) == 1: #LINE# #TAB# #TAB# #TAB# duration = f.getnframes()[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# duration = float(n_channels[1]) / wavfile.getnframes()[1] #LINE# #TAB# return duration"
"def get_args_and_defaults(args, defaults): #LINE# #TAB# arg_list = list(args) #LINE# #TAB# default_value = None #LINE# #TAB# if defaults: #LINE# #TAB# #TAB# for i, default_value in enumerate(defaults): #LINE# #TAB# #TAB# #TAB# if default_value is not None: #LINE# #TAB# #TAB# #TAB# #TAB# arg_list.append((i, default_value)) #LINE# #TAB# #TAB# #TAB# #TAB# default_value = default_value #LINE# #TAB# return arg_list, default_value"
"def hybrid_extractor(widget, data): #LINE# #TAB# extractor = widget.extractor(data) #LINE# #TAB# if extractor is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# extractor = compound_extractor(widget, data) #LINE# #TAB# #TAB# except NoExtractorError: #LINE# #TAB# #TAB# #TAB# return extractor #LINE# #TAB# if type(extractor) is dict: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# extractor = extractor['children'][0] #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# return extractor #LINE# #TAB# return extractor"
"def _get_repr(obj, pretty=False, indent=1): #LINE# #TAB# #TAB# if pretty: #LINE# #TAB# #TAB# #TAB# result = repr(obj) #LINE# #TAB# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# #TAB# result = json.dumps(obj, sort_keys=True, indent=indent) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result = repr(obj) #LINE# #TAB# #TAB# return result"
"def open_indices(elastic_client, indices): #LINE# #TAB# open_index_failures = [] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# open_index_failures.append(elastic_client.indices.open(index)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# open_index_failures.append(index) #LINE# #TAB# return open_index_failures"
"def create_module(name, code=None): #LINE# #TAB# if name not in sys.modules: #LINE# #TAB# #TAB# sys.modules[name] = imp.new_module(name) #LINE# #TAB# module = sys.modules[name] #LINE# #TAB# if code: #LINE# #TAB# #TAB# print('executing code for %s: %s' % (name, code)) #LINE# #TAB# #TAB# exec(code in module.__dict__) #LINE# #TAB# #TAB# exec('from %s import %s' % (name, '*')) #LINE# #TAB# return module"
def list_to_json(object_list): #LINE# #TAB# data = [] #LINE# #TAB# for obj in object_list: #LINE# #TAB# #TAB# data.append(_model_to_json(obj)) #LINE# #TAB# return data
"def angle_to_theta(angles): #LINE# #TAB# angles = np.asanyarray(angles, dtype=np.float64) #LINE# #TAB# if angles.ndim == 0: #LINE# #TAB# #TAB# theta = np.zeros((2, 3)) #LINE# #TAB# elif angles.ndim == 1: #LINE# #TAB# #TAB# theta = np.arctan2(angles, angles) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('angles should be (radian, 2, 3)!') #LINE# #TAB# return theta"
def set_data_id(params): #LINE# #TAB# params['data_id'] = params['order_id'] + 1 #LINE# #TAB# return params
def get_system_columns(columns): #LINE# #TAB# system_columns = [] #LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# system_columns.append(col) #LINE# #TAB# return system_columns
"def get_lr(lr, epoch, steps, factor): #LINE# #TAB# if epoch >= 0: #LINE# #TAB# #TAB# lr *= factor #LINE# #TAB# else: #LINE# #TAB# #TAB# lr *= factor #LINE# #TAB# return lr"
"def infer_cm(tpm): #LINE# #TAB# cm = np.zeros((tpm.shape[0], tpm.shape[1]), dtype=np.float64) #LINE# #TAB# for i in range(tpm.shape[0]): #LINE# #TAB# #TAB# cm[:, (i)] = tpm[:, (i)] #LINE# #TAB# return cm"
"def get_all_attributes(network): #LINE# #TAB# attributes = {} #LINE# #TAB# for key, value in network.nodes(data=True).items(): #LINE# #TAB# #TAB# if isinstance(value, ComplexMode): #LINE# #TAB# #TAB# #TAB# attributes[key] = value.get_all_attributes() #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for key, value in value.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if key in network: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# attributes[key] = get_all_attributes(network[key]) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# attributes[key] = [get_all_attributes(network[key]) for key in #LINE# #TAB# #TAB# #TAB# #TAB# value] #LINE# #TAB# return attributes"
"def parse_file_dict(file_dict): #LINE# #TAB# d = {} #LINE# #TAB# for f in file_dict.values(): #LINE# #TAB# #TAB# with open(f, 'r') as f: #LINE# #TAB# #TAB# #TAB# line = f.readline().strip() #LINE# #TAB# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# #TAB# d[f] = line #LINE# #TAB# return d"
"def from_bigquery(sql): #LINE# #TAB# try: #LINE# #TAB# #TAB# args = sql.bigquery.args #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# args = sql.columns #LINE# #TAB# metrics = Metrics() #LINE# #TAB# for i in args: #LINE# #TAB# #TAB# if hasattr(i, 'columns'): #LINE# #TAB# #TAB# #TAB# if hasattr(i, 'columns'): #LINE# #TAB# #TAB# #TAB# #TAB# metrics.columns.append(i.columns) #LINE# #TAB# #TAB# #TAB# elif hasattr(i,'metric'): #LINE# #TAB# #TAB# #TAB# #TAB# metrics.columns.append(i.metric) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# metrics.columns.append(i) #LINE# #TAB# return metrics"
"def get_content_type(ct_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# ct = ContentType.objects.get( #LINE# #TAB# #TAB# #TAB# slug=ct_name, #LINE# #TAB# #TAB# #TAB# is_localized=True #LINE# #TAB# #TAB# ) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ct = ContentType.objects.get( #LINE# #TAB# #TAB# #TAB# #TAB# slug=ct_name, #LINE# #TAB# #TAB# #TAB# #TAB# is_localized=False #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return ct"
def itransform_define(transform): #LINE# #TAB# transformation_define = True #LINE# #TAB# if transform == 'tanh': #LINE# #TAB# #TAB# transformation_define = False #LINE# #TAB# elif transform == 'exp': #LINE# #TAB# #TAB# transformation_define = True #LINE# #TAB# elif transform == 'logit': #LINE# #TAB# #TAB# transformation_define = False #LINE# #TAB# elif transform is None: #LINE# #TAB# #TAB# transformation_define = False #LINE# #TAB# return transformation_define
def maybe_evaluate(obj: Any) ->Any: #LINE# #TAB# try: #LINE# #TAB# #TAB# return obj.__maybe_evaluate__() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return obj
"def get_module_sources(module_name: str) ->typing.List[str]: #LINE# #TAB# if module_name is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return [os.path.abspath(os.path.dirname(file_)) for file_ in os.listdir( #LINE# #TAB# #TAB# os.getcwd()) if os.path.isfile(os.path.join(module_name, file_)) and #LINE# #TAB# #TAB# not file_.endswith('.py')]"
"def extension_module_tags(): #LINE# #TAB# return {key: value for key, value in vars(sys).items() if key.startswith #LINE# #TAB# #TAB# ('_EXT_')}"
"def cgi_parameter_exists(form: cgi.FieldStorage, key: str) -> bool: #LINE# #TAB# s = get_cgi_parameter_str(form, key) #LINE# #TAB# return s is not None"
"def recursive_dict(element): #LINE# #TAB# d = {} #LINE# #TAB# for el in element: #LINE# #TAB# #TAB# if isinstance(el, dict): #LINE# #TAB# #TAB# #TAB# for k, v in el.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# d[k] = recursive_dict(v) #LINE# #TAB# #TAB# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# d[k] = [recursive_dict(x) for x in v] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# d[el] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d[el] = recursive_dict(el) #LINE# #TAB# return d"
"def read_ssm_locs(in_file): #LINE# #TAB# out = {} #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# if line.startswith(""#""): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# fields = line.strip().split() #LINE# #TAB# #TAB# #TAB# if len(fields)!= 2: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# chrom, loc = fields[0], fields[1].split("" "") #LINE# #TAB# #TAB# #TAB# out[chrom] = [float(x) for x in loc] #LINE# #TAB# return out"
"def get_machine_id(): #LINE# #TAB# if sys.version_info[0] > 2: #LINE# #TAB# #TAB# return hashlib.sha1(os.urandom(4)).hexdigest() #LINE# #TAB# else: #LINE# #TAB# #TAB# machine_id = hashlib.sha1() #LINE# #TAB# #TAB# with open(machine_id, 'r') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# #TAB# if not line.startswith('id'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# machine_id.append(line) #LINE# #TAB# #TAB# return machine_id[-4:]"
def is_user_context(context): #LINE# #TAB# if not context: #LINE# #TAB# #TAB# return False #LINE# #TAB# if context.is_os_admin: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not context.user_id or not context.project_id: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
def account_number(profile): #LINE# #TAB# config = _get_config_parser(profile) #LINE# #TAB# try: #LINE# #TAB# #TAB# return config['Account']['Number'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return 'unknown'
def getridof_double_peaks(data): #LINE# #TAB# data = data.copy() #LINE# #TAB# peak_index = data.index.copy() #LINE# #TAB# dpeaks = pd.Series(peak_index[1:]) #LINE# #TAB# dpeaks.index = peak_index #LINE# #TAB# a = data.loc[peak_index[1:]] #LINE# #TAB# peak_index = list(range(dpeaks.shape[0])) #LINE# #TAB# a.columns = ['peak_index'] #LINE# #TAB# next(x for x in a.index if x!= 'peak_index' #LINE# #TAB# #TAB# ) #LINE# #TAB# while next(x['peak_index']!= 'peak_index'): #LINE# #TAB# #TAB# x = next(x['peak_index']) #LINE# #TAB# #TAB# peak_index = next(x['peak_index']) #LINE# #TAB# data = data.loc[next(x.index)] #LINE# #TAB# return data
"def gen_function_prototype(operand_list, operand_name_list=None): #LINE# #TAB# if operand_name_list is None: #LINE# #TAB# #TAB# operand_name_list = [] #LINE# #TAB# output_body = [] #LINE# #TAB# for operand in operand_list: #LINE# #TAB# #TAB# if isinstance(operand, Array): #LINE# #TAB# #TAB# #TAB# output_body.extend(gen_function_prototype(operand, operand_name_list)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# output_body.append(operand.to_string()) #LINE# #TAB# return output_body"
"def get_rest_host(): #LINE# #TAB# try: #LINE# #TAB# #TAB# c = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# c.connect(('localhost', 9000)) #LINE# #TAB# #TAB# host = c.getsockname()[0] #LINE# #TAB# #TAB# c.close() #LINE# #TAB# #TAB# return host #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return ''"
"def get_lcdata(filename): #LINE# #TAB# lc_input_data = None #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# lc_input_data = f.read() #LINE# #TAB# return lc_input_data"
"def remove_typedefs(signature: str) ->str: #LINE# #TAB# no_typedef = re.sub('\\(typedef\\)', '', signature) #LINE# #TAB# return no_typedef"
"def collect_staticroot_removal(app, blueprints): #LINE# #TAB# collect_root = app.extensions['collect'].static_root #LINE# #TAB# return [bp for bp in blueprints if ( #LINE# #TAB# #TAB# bp.has_static_folder and bp.static_folder!= collect_root)]"
"def next_datetime_with_utc_hour(table_name, utc_hour): #LINE# #TAB# yesterday = None #LINE# #TAB# try: #LINE# #TAB# #TAB# yesterday = table(table_name) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if utc_hour is None: #LINE# #TAB# #TAB# utc_hour = 0 #LINE# #TAB# left_datetime = datetime.datetime.utcnow() #LINE# #TAB# while yesterday: #LINE# #TAB# #TAB# next_datetime = left_datetime + datetime.timedelta(hours=utc_hour) #LINE# #TAB# #TAB# if left_datetime > right_datetime: #LINE# #TAB# #TAB# #TAB# left_datetime = right_datetime #LINE# #TAB# return left_datetime"
"def copy_field_if_exists(fromDic, toDic, keys): #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in fromDic and fromDic[key]!= toDic[key]: #LINE# #TAB# #TAB# #TAB# del fromDic[key]"
"def parse_arrow(sourcelist: Iterable[str]) ->List[Source]: #LINE# #TAB# return [Source(urlparse.urljoin(sourcelist[0:-1], '%Y-%m-%dT%H:%M:%SZ')) for sourcelist in #LINE# #TAB# #TAB# map(lambda x: Source.parse_url(x), sourcelist) if isinstance(x, str) and #LINE# #TAB# #TAB# x.strip()]"
"def pkg_walk(package, top): #LINE# #TAB# paths = [] #LINE# #TAB# top = os.path.abspath(top) #LINE# #TAB# for dirpath, dirnames, filenames in os.walk(package): #LINE# #TAB# #TAB# paths.append((dirpath, filename)) #LINE# #TAB# #TAB# if os.path.isdir(dirpath): #LINE# #TAB# #TAB# #TAB# for dirpath, dirnames in os.walk(dirpath): #LINE# #TAB# #TAB# #TAB# #TAB# yield dirpath, dirnames, filenames #LINE# #TAB# #TAB# elif os.path.isfile(os.path.join(dirpath, '__init__.py')): #LINE# #TAB# #TAB# #TAB# yield dirpath, dirnames, filenames"
def process_model(model): #LINE# #TAB# p = BiopaxProcessor() #LINE# #TAB# p.add_model(model) #LINE# #TAB# return p
def register_openers(): #LINE# #TAB# opener = urllib.request.build_opener(*get_handlers()) #LINE# #TAB# urllib.request.install_opener(opener) #LINE# #TAB# return opener
"def jaccard_merge(brands, exemplars): #LINE# #TAB# scores = {} #LINE# #TAB# exemplar_followers = set() #LINE# #TAB# for followers in exemplars.values(): #LINE# #TAB# #TAB# exemplar_followers |= followers #LINE# #TAB# for brand, followers in brands: #LINE# #TAB# #TAB# scores[brand] = _jaccard(followers, exemplar_followers) #LINE# #TAB# return scores"
"def to_jd(year, month, day): #LINE# #TAB# return day + ceil(29.5 * (month - 1)) + (year - 1) * 354 + trunc((3 + #LINE# #TAB# #TAB# 11 * year) / 30) + EPOCH - 1"
"def get_error_message(response): #LINE# #TAB# try: #LINE# #TAB# #TAB# data = response.json() #LINE# #TAB# #TAB# if ""error_description"" in data: #LINE# #TAB# #TAB# #TAB# return data['error_description'] #LINE# #TAB# #TAB# if ""error"" in data: #LINE# #TAB# #TAB# #TAB# return data['error'] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ""Unknown error"""
"def guess_lexer_using_filename(file_name, text): #LINE# #TAB# lexer = None #LINE# #TAB# text = text.strip() #LINE# #TAB# try: #LINE# #TAB# #TAB# lexer = custom_pygments_guess_lexer_for_filename(file_name, text) #LINE# #TAB# except SkipHeartbeat as ex: #LINE# #TAB# #TAB# raise SkipHeartbeat(u(ex)) #LINE# #TAB# except: #LINE# #TAB# #TAB# log.traceback(logging.DEBUG) #LINE# #TAB# return lexer"
def try_import(module): #LINE# #TAB# try: #LINE# #TAB# #TAB# return importlib.import_module(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None
"def stringify_index(index: Union[int, str], count: int) ->str: #LINE# #TAB# if isinstance(index, int): #LINE# #TAB# #TAB# index = str(index) #LINE# #TAB# index = pad_int(index, count) #LINE# #TAB# if index == '': #LINE# #TAB# #TAB# return '0' * count #LINE# #TAB# return index"
"def decrypt_open_sshprivate_key(privkeystring, inp): #LINE# #TAB# key = rsa_key.RSAKey() #LINE# #TAB# key.importKey(privkeystring) #LINE# #TAB# private_key = serialization.load_pem_private_key(key, backend=default_backend()) #LINE# #TAB# decrypted = cipher.decrypt(b64decode(priv_key)) #LINE# #TAB# return decrypted"
"def parse_version(version_string: str) -> Tuple[str, str, str]: #LINE# #TAB# version_string = version_string.strip() #LINE# #TAB# if version_string.startswith(""0.""): #LINE# #TAB# #TAB# release, major, minor = version_string.split(""."") #LINE# #TAB# else: #LINE# #TAB# #TAB# release, major, minor = version_string.split(""."") #LINE# #TAB# return release, major, minor"
"def check_cores(cores): #LINE# #TAB# cores = min(multiprocessing.cpu_count(), cores) #LINE# #TAB# if not six.PY3: #LINE# #TAB# #TAB# cores = 1 #LINE# #TAB# return cores"
"def start_trading(args: Dict[str, Any]) ->int: #LINE# #TAB# session = args['session'] #LINE# #TAB# if session.is_open: #LINE# #TAB# #TAB# start_trading() #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError"
"def get_update_clause_from_dict(dictionary, datetime_format='%Y-%m-%d %H:%M:%S'): #LINE# #TAB# update_clause = '' #LINE# #TAB# update_clause += 'UPDATE'#LINE# #TAB# for key, value in dictionary.items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# if isinstance(value, datetime): #LINE# #TAB# #TAB# #TAB# #TAB# update_clause +='{0} = {1}'.format(key, value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# update_clause +='{0} = {1}'.format(key, value) #LINE# #TAB# update_clause += ')' #LINE# #TAB# return update_clause"
def split_line(line): #LINE# #TAB# commands = line.split(';') #LINE# #TAB# if len(commands) == 1: #LINE# #TAB# #TAB# return commands[0] #LINE# #TAB# elif len(commands) == 2: #LINE# #TAB# #TAB# return commands #LINE# #TAB# return [cmd.strip() for cmd in commands]
def convert_nmea_degrees(nmea_degrees): #LINE# #TAB# if not nmea_degrees: #LINE# #TAB# #TAB# return None #LINE# #TAB# degrees = str(int(nmea_degrees[0:2])) #LINE# #TAB# nmea_degrees = '+' + nmea_degrees[2:] #LINE# #TAB# if '+' in nmea_degrees: #LINE# #TAB# #TAB# degrees = -degrees #LINE# #TAB# elif '-' in nmea_degrees: #LINE# #TAB# #TAB# degrees = nmea_degrees[:-1] #LINE# #TAB# degrees = str(int(degrees)) #LINE# #TAB# return degrees
"def get_team_picks(team_id, gameweek, game='FPL'): #LINE# #TAB# picks = get_team_info(gameweek, team_id) #LINE# #TAB# for pick in picks: #LINE# #TAB# #TAB# if pick['gameweek'] == gameweek: #LINE# #TAB# #TAB# #TAB# return pick #LINE# #TAB# #TAB# elif pick['gameweek'] == gameweek: #LINE# #TAB# #TAB# #TAB# return pick #LINE# #TAB# return None"
"def form_initialized(obj, event): #LINE# #TAB# if hasattr(obj,'signupsheet_configuration') and obj.signupsheet_configuration: #LINE# #TAB# #TAB# obj.signupsheet_configuration = None"
"def get_request_basic_auth(request): #LINE# #TAB# if 'Authorization' not in request.headers: #LINE# #TAB# #TAB# return None #LINE# #TAB# auth = request.headers['Authorization'] #LINE# #TAB# if auth is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not auth.startswith('Basic '): #LINE# #TAB# #TAB# return None #LINE# #TAB# auth_split = auth.split(':') #LINE# #TAB# if len(auth_split) == 2: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# username, password = auth_split #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return 'username:%s' % username #LINE# #TAB# else: #LINE# #TAB# #TAB# return auth"
"def get_weekday_info(vehicle_journey_element): #LINE# #TAB# weekday_info = {} #LINE# #TAB# for i, elem in enumerate(vehicle_journey_element): #LINE# #TAB# #TAB# if elem.tag == 'YEAR': #LINE# #TAB# #TAB# #TAB# weekday_info['year'] = int(elem.attrib['year']) #LINE# #TAB# #TAB# #TAB# weekday_info['month'] = int(elem.attrib['month']) #LINE# #TAB# #TAB# #TAB# weekday_info['day'] = elem.attrib['day'] #LINE# #TAB# return weekday_info"
"def from_directory(buildDir): #LINE# #TAB# errorCode = c_uint() #LINE# #TAB# try: #LINE# #TAB# #TAB# cdb = conf.lib.clang_CompilationDatabase_from_directory(encode( #LINE# #TAB# #TAB# #TAB# buildDir), byref(errorCode)) #LINE# #TAB# except CompilationDatabaseError as e: #LINE# #TAB# #TAB# raise CompilationDatabaseError(int(errorCode.value), #LINE# #TAB# #TAB# #TAB# 'CompilationDatabase loading failed') #LINE# #TAB# return cdb"
"def get_steam_library_folders(): #LINE# #TAB# folders = [] #LINE# #TAB# path = os.path.abspath(os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB#'steams')) #LINE# #TAB# while True: #LINE# #TAB# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# folders.append(path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.mkdir(path) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# folders.reverse() #LINE# #TAB# return folders"
def all_names_for_tag(tag): #LINE# #TAB# longNames = [] #LINE# #TAB# shortNames = [] #LINE# #TAB# if tag.HasAttribute('shortName'): #LINE# #TAB# #TAB# shortNames.append(tag.shortName) #LINE# #TAB# if tag.HasAttribute('longName'): #LINE# #TAB# #TAB# longNames.append(tag.longName) #LINE# #TAB# return longNames
"def register_ns(prefix, ns): #LINE# #TAB# if prefix not in _NAMESPACE_REGISTRY: #LINE# #TAB# #TAB# _NAMESPACE_REGISTRY[prefix] = namespace #LINE# #TAB# if ns not in _NAMESPACE_REGISTRY: #LINE# #TAB# #TAB# _NAMESPACE_REGISTRY[ns] = [] #LINE# #TAB# if not hasattr(ns, 'prefix'): #LINE# #TAB# #TAB# _NAMESPACE_REGISTRY[ns.prefix] = [] #LINE# #TAB# _NAMESPACE_REGISTRY[ns.prefix].append(ns) #LINE# #TAB# _NAMESPACE_REGISTRY[ns.prefix].append(prefix) #LINE# #TAB# _NAMESPACE_REGISTRY[ns.prefix] = ns"
def is_cold_start() ->str: #LINE# #TAB# output = '' #LINE# #TAB# if 'coldstart' in os.environ: #LINE# #TAB# #TAB# output += 'true' #LINE# #TAB# #TAB# output += 'false' #LINE# #TAB# return output
"def galactic_to_equatorial(gl, gb): #LINE# #TAB# ra = gl['ra'] * u.rad #LINE# #TAB# dec = gl['dec'] * u.rad #LINE# #TAB# gl['ra'] = ra #LINE# #TAB# gl['dec'] = dec #LINE# #TAB# gl['dec'] = dec #LINE# #TAB# return gl"
"def logger_level(logger, level): #LINE# #TAB# old_logger_level = logger.getEffectiveLevel() #LINE# #TAB# try: #LINE# #TAB# #TAB# logger.setLevel(level) #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# logger.setLevel(old_logger_level) #LINE# #TAB# #TAB# yield"
def infer_start_activities(dfg): #LINE# #TAB# start_activities = [] #LINE# #TAB# for act in dfg: #LINE# #TAB# #TAB# if 'followed' in act: #LINE# #TAB# #TAB# #TAB# start_activities.append(act['followed']) #LINE# #TAB# #TAB# elif 'unfollowed' in act: #LINE# #TAB# #TAB# #TAB# for act in dfg[act]['unfollowed']: #LINE# #TAB# #TAB# #TAB# #TAB# start_activities.append(act['unfollowed']) #LINE# #TAB# return start_activities
def get_file_extension(file_path): #LINE# #TAB# extension = os.path.splitext(file_path)[1] #LINE# #TAB# return extension
"def get_cli_params(func): #LINE# #TAB# ret = [] #LINE# #TAB# for name, annotation in inspect.signature(func).parameters.items(): #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# param = getattr(annotation, 'arg', None) #LINE# #TAB# #TAB# if param is not None: #LINE# #TAB# #TAB# #TAB# ret.append(name) #LINE# #TAB# return ret"
"def get_content_html(request): #LINE# #TAB# content_hash = uuid.uuid4().hex #LINE# #TAB# response = HttpResponse(content_hash, content_type='text/html') #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=""{}""'.format( #LINE# #TAB# #TAB# content_hash) #LINE# #TAB# return response"
"def resolve_forward_ref(typ): #LINE# #TAB# if is_forward_ref(typ): #LINE# #TAB# #TAB# typ = typ[0] #LINE# #TAB# resolved_typ = typ #LINE# #TAB# if isinstance(typ, str): #LINE# #TAB# #TAB# if typ.startswith('<') and typ.endswith('>'): #LINE# #TAB# #TAB# #TAB# resolved_typ = typ[1:-1] #LINE# #TAB# return resolved_typ"
"def extract_doi(value): #LINE# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# if value.startswith('http'): #LINE# #TAB# #TAB# #TAB# return value[7:] #LINE# #TAB# #TAB# return value #LINE# #TAB# doi = extract_doi_from_url(value) #LINE# #TAB# if not doi: #LINE# #TAB# #TAB# raise ValueError('doi not valid') #LINE# #TAB# return doi"
"def t_number(t): #LINE# #TAB# try: #LINE# #TAB# #TAB# if t.value.count('.') == 0: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = int(t.value) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('[%d]: Number %s is not valid!' % (t.lineno, t.value)) #LINE# #TAB# #TAB# t.value = 0 #LINE# #TAB# return t"
"def convert_to_int32(_bytes: list) ->list: #LINE# #TAB# result = [] #LINE# #TAB# for i in _bytes: #LINE# #TAB# #TAB# result.append(int(i, 16)) #LINE# #TAB# return result"
"def srm87_eqtableiii(T, abc): #LINE# #TAB# Tr = 298.15 #LINE# #TAB# return abc[0] + abc[1] * (T - Tr) + abc[2] * (T - Tr) ** 2"
"def from_esri_code(code): #LINE# #TAB# code = str(code) #LINE# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# crs = from_proj4(proj4) #LINE# #TAB# return crs"
"def calculate_angle(v1, v2): #LINE# #TAB# norm = np.linalg.norm #LINE# #TAB# cosine = np.dot(v1, v2) / (norm(v1) * norm(v2)) #LINE# #TAB# angle = np.arccos(cosine) * 180 / np.pi #LINE# #TAB# return angle"
def import_models(module): #LINE# #TAB# try: #LINE# #TAB# #TAB# module = importlib.import_module(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# raise #LINE# #TAB# models = {} #LINE# #TAB# for name in dir(module): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = importlib.import_module('{}.models'.format(name)) #LINE# #TAB# #TAB# #TAB# models[name] = module #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return models
"def absolute_urls(html): #LINE# #TAB# from bs4 import BeautifulSoup #LINE# #TAB# from yacms.core.request import current_request #LINE# #TAB# request = current_request() #LINE# #TAB# if request is not None: #LINE# #TAB# #TAB# dom = BeautifulSoup(html, ""html.parser"") #LINE# #TAB# #TAB# for tag, attr in ABSOLUTE_URL_TAGS.items(): #LINE# #TAB# #TAB# #TAB# for node in dom.findAll(tag): #LINE# #TAB# #TAB# #TAB# #TAB# url = node.get(attr, """") #LINE# #TAB# #TAB# #TAB# #TAB# if url: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# node[attr] = request.build_absolute_uri(url) #LINE# #TAB# #TAB# html = str(dom) #LINE# #TAB# return html"
"def one_vs_all_func(classes, table, TP, TN, FP, FN, class_name): #LINE# #TAB# one_vs_table = [] #LINE# #TAB# for i in classes: #LINE# #TAB# #TAB# one_vs_table.append(OneVs(table, class_name, i, TP, TN, FP, FN, #LINE# #TAB# #TAB# #TAB# class_name)) #LINE# #TAB# return one_vs_table"
"def reduce_by_keys(orig_dict, keys, default=None): #LINE# #TAB# d = {} #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# d.update(orig_dict[key]) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return d #LINE# #TAB# return default"
"def fca_filter(concept, winlen, min_c, min_z, max_c, max_z, min_neu): #LINE# #TAB# results = [] #LINE# #TAB# count = 0 #LINE# #TAB# for occ in concept: #LINE# #TAB# #TAB# if occ.bin in winlen: #LINE# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# #TAB# #TAB# if min_c <= occ.c and min_z <= occ.z and max_c >= occ.c and max_z <= occ.z: #LINE# #TAB# #TAB# #TAB# #TAB# results.append(concept) #LINE# #TAB# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# #TAB# #TAB# elif min_neu <= occ.neu: #LINE# #TAB# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# return results"
"def get_equal_bins(probs: List[float], num_bins: int=10) ->Bins: #LINE# #TAB# bins = Bins() #LINE# #TAB# for i in range(num_bins): #LINE# #TAB# #TAB# p =probs[i] #LINE# #TAB# #TAB# norm = p.norm() #LINE# #TAB# #TAB# bins.fit(norm) #LINE# #TAB# return bins"
"def make_pieces(files, psize): #LINE# #TAB# #TAB# files_to_concat = [files] #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# phash = hashlib.sha1() #LINE# #TAB# #TAB# #TAB# for i in range(0, len(files_to_concat)): #LINE# #TAB# #TAB# #TAB# #TAB# phash.update(files_to_concat[i]) #LINE# #TAB# #TAB# #TAB# pieces = phash.hexdigest() #LINE# #TAB# #TAB# #TAB# if len(pieces) > psize: #LINE# #TAB# #TAB# #TAB# #TAB# pieces = pieces[:ptsize] #LINE# #TAB# #TAB# #TAB# files_to_concat[i] = pieces #LINE# #TAB# #TAB# return files_to_concat"
def is_true(string) ->bool: #LINE# #TAB# string = str(string) #LINE# #TAB# try: #LINE# #TAB# #TAB# return bool(string) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return string == 'True'
"def locale_negotiator(request): #LINE# #TAB# locale = ""en"" #LINE# #TAB# if request.accept_language: #LINE# #TAB# #TAB# locale = request.accept_language.best_match(LANGUAGES) #LINE# #TAB# #TAB# locale = LANGUAGES.get(locale, ""en"") #LINE# #TAB# return locale"
"def ranksum_statistic(array_one, array_two): #LINE# #TAB# sum_one = sum(array_one) #LINE# #TAB# sum_two = sum(array_two) #LINE# #TAB# exp_diff = 0.0 #LINE# #TAB# for one_dim in range(1, len(array_one)): #LINE# #TAB# #TAB# diff_one = sum(array_one[one_dim]) #LINE# #TAB# #TAB# diff_two = sum(array_two[one_dim]) #LINE# #TAB# #TAB# exp_diff += diff_one / (sum_two - diff_one) #LINE# #TAB# return exp_diff"
"def find_max_with_bin(grid, rectangle): #LINE# #TAB# data = grid[rectangle] #LINE# #TAB# median = np.median(data) #LINE# #TAB# lims = np.abs(data - median) #LINE# #TAB# r = len(grid) // 3 #LINE# #TAB# ind1 = grid.index(max(lims)) #LINE# #TAB# ind2 = grid.index(min(lims)) - 1 #LINE# #TAB# while ind1 > 0: #LINE# #TAB# #TAB# ind1 -= 1 #LINE# #TAB# #TAB# ind2 = grid.index(max(lims)) #LINE# #TAB# while ind2 > 0: #LINE# #TAB# #TAB# ind2 -= 1 #LINE# #TAB# data[ind1] = grid[ind2] #LINE# #TAB# data = np.array(data[ind1:ind2 + lims]) #LINE# #TAB# return data[ind1, ind2]"
"def get_imageblock(filename, format=''): #LINE# #TAB# if not format: #LINE# #TAB# #TAB# format = 'png' #LINE# #TAB# format = format.lower() #LINE# #TAB# filename = os.path.abspath(filename) #LINE# #TAB# rst = '.. code::image::\n\n' + rst #LINE# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# open_func = open #LINE# #TAB# else: #LINE# #TAB# #TAB# open_func = open #LINE# #TAB# block = '\n::image::\n\n' #LINE# #TAB# block += open_func(filename, format) #LINE# #TAB# block += '\n\n' #LINE# #TAB# return block"
"def is_dict(annotation: Any) ->bool: #LINE# #TAB# return get_origin(annotation) is dict and getattr(annotation, '_name', None #LINE# #TAB# #TAB# ) == 'Dict'"
"def message_from_event(event, secret): #LINE# #TAB# existing_message_key = event.get('existing_message_key') #LINE# #TAB# if existing_message_key: #LINE# #TAB# #TAB# message = event.get('existing_message_key') #LINE# #TAB# elif secret: #LINE# #TAB# #TAB# message = event.get('message_secret') #LINE# #TAB# else: #LINE# #TAB# #TAB# message = event.copy() #LINE# #TAB# message['Date'] = timezone.now().strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# message['Date'] = timezone.make_aware(datetime.strptime(message['Date'], #LINE# #TAB# #TAB# '%Y-%m-%d %H:%M:%S')).isoformat() #LINE# #TAB# message_dict = json.loads(message) #LINE# #TAB# return message_dict"
def get_rar_password(skipUserInput): #LINE# #TAB# password = '' #LINE# #TAB# while not password and skipUserInput: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# password = getpass.getpass('Enter the password: ') #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# print('\nPasswords do not match') #LINE# #TAB# return password
"def get_str_errblank(row, name, errors, error_message=None): #LINE# #TAB# if error_message is None: #LINE# #TAB# #TAB# error_message = _('Invalid value for column {}. Requires a non-blank value.' #LINE# #TAB# #TAB# #TAB# ).format(name) #LINE# #TAB# val = row.get(name, '') #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# return val #LINE# #TAB# else: #LINE# #TAB# #TAB# return error_message"
"def get_commands(args): #LINE# #TAB# command_tokens = split(args, True) #LINE# #TAB# commands = [] #LINE# #TAB# for token in command_tokens: #LINE# #TAB# #TAB# if not token: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# command = token.strip() #LINE# #TAB# #TAB# if command: #LINE# #TAB# #TAB# #TAB# commands.append(command) #LINE# #TAB# return commands"
def get_host_name(): #LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# host = socket.gethostname() #LINE# #TAB# else: #LINE# #TAB# #TAB# host = socket.gethostname() #LINE# #TAB# return host
"def get_proctype(): #LINE# #TAB# proctype = platform.machine() #LINE# #TAB# if proctype == 'Linux': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open('/proc/cpuinfo', 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# proctype = f.read().strip() #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return proctype"
"def list_intermediates(geo): #LINE# #TAB# shape_path = maya.cmds.ls(maya.cmds.listRelatives(geo, d=True), type= #LINE# #TAB# #TAB#'shape') #LINE# #TAB# if not shape_path: #LINE# #TAB# #TAB# return [] #LINE# #TAB# intermediate_shapes = [] #LINE# #TAB# for m in shape_path: #LINE# #TAB# #TAB# if maya.cmds.objectType(m) =='shape': #LINE# #TAB# #TAB# #TAB# intermediate_shapes += list_intermediates(m.group(1)) #LINE# #TAB# return intermediate_shapes"
"def get_magic_content_type(input): #LINE# #TAB# if isinstance(input, str): #LINE# #TAB# #TAB# content_type = input #LINE# #TAB# elif isinstance(input, bytes): #LINE# #TAB# #TAB# content_type = input.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# content_type = magic.magic(input) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# content_type = 'bytes' #LINE# #TAB# return content_type"
def convert_date(text): #LINE# #TAB# if text: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return DateTime(*parse_date_text(text)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass
"def setup_blueprint(): #LINE# #TAB# blueprint = Blueprint(__name__, url_prefix='/', template_folder=os.path. #LINE# #TAB# #TAB# join(os.path.dirname(__file__), 'templates')) #LINE# #TAB# blueprint.add_url_rule('/', view_func=view_func, methods=['POST']) #LINE# #TAB# blueprint.add_url_rule('/<path>', view_func=view_func, methods=['GET']) #LINE# #TAB# app.register_blueprint(blueprint) #LINE# #TAB# return blueprint"
def get_hostname(url): #LINE# #TAB# url = url.strip('/') #LINE# #TAB# if url.endswith('/'): #LINE# #TAB# #TAB# url = url[:-1] #LINE# #TAB# hostname = url.split('/')[-1] #LINE# #TAB# config = {'hostname': hostname} #LINE# #TAB# return config
"def create_cnv_brk_sources(store, solution, chromosome_plot_info): #LINE# #TAB# if 'break' not in chromosome_plot_info: #LINE# #TAB# #TAB# chromosome_plot_info['break'] = True #LINE# #TAB# cnvid_source = ColumnDataSource(store, 'cnv_break_' + solution[ #LINE# #TAB# #TAB# 'cnv_break_id']) #LINE# #TAB# chromosome_plot_info['break'] = True #LINE# #TAB# return cnvid_source, chromosome_plot_info"
"def add_argument_parameter(path, kwargs, param, prameter_type): #LINE# #TAB# if param in kwargs: #LINE# #TAB# #TAB# return #LINE# #TAB# if prameter_type == 'query': #LINE# #TAB# #TAB# kwargs[param] = json.dumps(kwargs[param]) #LINE# #TAB# #TAB# param = 'query' #LINE# #TAB# elif prameter_type == 'generator': #LINE# #TAB# #TAB# param = 'generator' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unknown prameter type for %s' % prameter_type) #LINE# #TAB# kwargs[param] = kwargs[param]"
"def calculate_cache_location_for_url(url, postfix=None): #LINE# #TAB# import urlparse #LINE# #TAB# url = urlparse.urlparse(url) #LINE# #TAB# if postfix is not None: #LINE# #TAB# #TAB# url = url.with_suffix(postfix) #LINE# #TAB# path = urlparse.url2pathname(url) #LINE# #TAB# if not path.endswith('/'): #LINE# #TAB# #TAB# path += '/' #LINE# #TAB# return path"
"def get_password(entry=None, username=None, prompt=None, always_ask=False): #LINE# #TAB# entry = entry or dict() #LINE# #TAB# entry['password'] = None #LINE# #TAB# if not entry: #LINE# #TAB# #TAB# entry['password'] = None #LINE# #TAB# if username: #LINE# #TAB# #TAB# username = entry['username'] #LINE# #TAB# if prompt: #LINE# #TAB# #TAB# return getpass.getpass(prompt) #LINE# #TAB# elif always_ask: #LINE# #TAB# #TAB# return getpass.getpass(username) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
def real_to_complex(real_fid): #LINE# #TAB# complex_fid = complex(real_fid) #LINE# #TAB# attempt = 0 #LINE# #TAB# while attempt < 2 ** 32: #LINE# #TAB# #TAB# if real_fid[attempt] == 1: #LINE# #TAB# #TAB# #TAB# attempt += 1 #LINE# #TAB# #TAB# elif real_fid[attempt] == 2: #LINE# #TAB# #TAB# #TAB# attempt -= 2 #LINE# #TAB# #TAB# elif real_fid[attempt] == 3: #LINE# #TAB# #TAB# #TAB# attempt += 1 #LINE# #TAB# return complex_fid
"def to_string(value): #LINE# #TAB# if sys.version_info[0] > 2: #LINE# #TAB# #TAB# if isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# value = value.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
"def set_time_reply(): #LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.Ack(BaseMsgCode.SetTime, AckCode.OK)) #LINE# #TAB# return packet"
"def pick_repo_common(key): #LINE# #TAB# logger.debug('key is: %s', key) #LINE# #TAB# if key == 'default': #LINE# #TAB# #TAB# repo = git.Repo() #LINE# #TAB# elif key == 'gitlab': #LINE# #TAB# #TAB# repo = git.Repo(path=os.getcwd(), search_parent_directories=True) #LINE# #TAB# elif key == 'gitlab-hg': #LINE# #TAB# #TAB# repo = git.Repo(path=os.getcwd(), search_parent_directories=True) #LINE# #TAB# elif key == 'gitlab-travis': #LINE# #TAB# #TAB# repo = git.Repo(path=os.getcwd()) #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.warning('key is not valid: %s', key) #LINE# #TAB# #TAB# repo = None #LINE# #TAB# logger.debug('pick_repo_common: %s', repo) #LINE# #TAB# return repo"
"def strip_tweet(text, remove_url=True): #LINE# #TAB# stripped_text = text.rstrip() #LINE# #TAB# if '\n' in stripped_text: #LINE# #TAB# #TAB# stripped_text = stripped_text.replace('\n','') #LINE# #TAB# #TAB# if remove_url: #LINE# #TAB# #TAB# #TAB# stripped_text = stripped_text.split('#')[0] #LINE# #TAB# return stripped_text"
"def valid_email(x: str) -> bool: #LINE# #TAB# if isinstance(x, str) and re.match(EMAIL_PATTERN, x): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
def get_wake_on_network(): #LINE# #TAB# ret = salt.utils.mac_utils.execute_return_result( #LINE# #TAB# #TAB#'systemsetup -getwakeonnetwork') #LINE# #TAB# return salt.utils.mac_utils.validate_enabled( #LINE# #TAB# #TAB# salt.utils.mac_utils.parse_return(ret)) == 'on'
"def get_ip_address(hostname): #LINE# #TAB# res = socket.getaddrinfo(hostname, 0, socket.AF_UNSPEC, socket. #LINE# #TAB# #TAB# SOCK_STREAM, 0, socket.AI_NUMERICHOST) #LINE# #TAB# if res[0][0] == socket.EAI_NONAME: #LINE# #TAB# #TAB# return #LINE# #TAB# else: #LINE# #TAB# #TAB# return res[0][1]"
"def damgard_jurik_encrypt(m, n, g, s): #LINE# #TAB# result = bytearray(m) #LINE# #TAB# if n > 8: #LINE# #TAB# #TAB# temp = bytearray(b'\x00' * (n - 8)) #LINE# #TAB# #TAB# while temp[-1] == b'\x00': #LINE# #TAB# #TAB# #TAB# temp.append(b'\x00') #LINE# #TAB# #TAB# #TAB# new_m = m[:-1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_m = m[1:-1] #LINE# #TAB# #TAB# j = jurik_encrypt_block(new_m, g, s) #LINE# #TAB# #TAB# result = bytes2str(j) + result #LINE# #TAB# return result"
def m_n(T): #LINE# #TAB# if T == 298.15: #LINE# #TAB# #TAB# return 1.25 #LINE# #TAB# elif T == 4: #LINE# #TAB# #TAB# return 0.5 #LINE# #TAB# elif T == 5: #LINE# #TAB# #TAB# return 0.4 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -T
"def get_trans_co(x2ys, n_trans): #LINE# #TAB# trans_scores = np.zeros((len(x2ys), n_trans)) #LINE# #TAB# for i in range(n_trans): #LINE# #TAB# #TAB# trans_scores[i] = 0.0 #LINE# #TAB# for i in range(len(x2ys)): #LINE# #TAB# #TAB# trans_scores[i] = 0.0 #LINE# #TAB# return trans_scores"
"def linear_mle_tmle(X, y, group_index, sample_weight=None): #LINE# #TAB# intercept = np.mean(y) #LINE# #TAB# d_beta = safe_sparse_dot(X.T, intercept - y) / X.shape[0] #LINE# #TAB# G = len(np.unique(group_index)) #LINE# #TAB# tmle = 1 / np.max([np.linalg.norm(d_beta[np.where(group_index == g)[0]], #LINE# #TAB# #TAB# 2) for g in range(0, G)]) #LINE# #TAB# return intercept, d_beta, tmle"
def is_online(peer): #LINE# #TAB# try: #LINE# #TAB# #TAB# node_stat = peer.get_state() #LINE# #TAB# except RuntimeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if node_stat['state'] == 'online': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# log.debug('node %s is not online' % node_stat['node']) #LINE# #TAB# #TAB# return False
def get_c_dict(colour): #LINE# #TAB# c_dict = {} #LINE# #TAB# for col in colour.colours: #LINE# #TAB# #TAB# c_dict[col.name] = col.value #LINE# #TAB# return c_dict
def is_item_available_for_checkout(item_pid): #LINE# #TAB# if item_pid: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# item = Entity.get(pid=item_pid) #LINE# #TAB# #TAB# except Entity.DoesNotExist: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return item.is_available_for_checkout() #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
def module_getmtime(filename): #LINE# #TAB# filename = os.path.abspath(filename) #LINE# #TAB# if _is_pyc_file(filename): #LINE# #TAB# #TAB# mtime = os.path.getmtime(filename) #LINE# #TAB# elif _is_pyo_file(filename): #LINE# #TAB# #TAB# mtime = os.path.getmtime(filename) #LINE# #TAB# return mtime
"def erequire_ancestor(nested_path: str) ->str: #LINE# #TAB# path = os.path.abspath(nested_path) #LINE# #TAB# while True: #LINE# #TAB# #TAB# _, current_path = os.path.split(path) #LINE# #TAB# #TAB# if os.path.exists(current_path): #LINE# #TAB# #TAB# #TAB# return current_path #LINE# #TAB# #TAB# parent_path = os.path.abspath(os.path.dirname(current_path)) #LINE# #TAB# #TAB# if current_path in path: #LINE# #TAB# #TAB# #TAB# raise require_ancestor(parent_path) #LINE# #TAB# #TAB# path = os.path.abspath(os.path.join(current_path, os.pardir)) #LINE# #TAB# return path"
"def related_used_by(remote, entry): #LINE# #TAB# if not entry.path.startswith('/'): #LINE# #TAB# #TAB# return remote.related(entry) #LINE# #TAB# resources = entry.path.split('/') #LINE# #TAB# required = [] #LINE# #TAB# for resource in resources: #LINE# #TAB# #TAB# if resource.startswith('used_by/'): #LINE# #TAB# #TAB# #TAB# required.append(resource) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# required.append(resource) #LINE# #TAB# return resources"
"def find_masked_rows(m, masking_value=0): #LINE# #TAB# masked_rows = [] #LINE# #TAB# masking_num = 0 #LINE# #TAB# for r in range(m.shape[0]): #LINE# #TAB# #TAB# if masking_value <= m[r, r] and m[r, r]!= masking_value: #LINE# #TAB# #TAB# #TAB# masked_rows.append(r) #LINE# #TAB# #TAB# #TAB# masking_num += 1 #LINE# #TAB# return masked_rows"
"def point_in_rect(p, rect): #LINE# #TAB# return p[0] <= rect[0] and p[1] <= rect[1] and p[0] >= rect[0 #LINE# #TAB# #TAB# ] and p[1] <= rect[1]"
"def equatorial_zone_vertices(vertices, pole, width=5): #LINE# #TAB# return [i for i, v in enumerate(vertices) if np.abs(np.dot(v, pole)) > #LINE# #TAB# #TAB# np.abs(np.cos(np.pi * width / 180))]"
"def fix_reserved_word(word, is_module=False): #LINE# #TAB# if is_module: #LINE# #TAB# #TAB# word = RE_MODULE_WORDS.get(word, word) #LINE# #TAB# if not word: #LINE# #TAB# #TAB# word = '_' #LINE# #TAB# word = word.lower() #LINE# #TAB# for char in word: #LINE# #TAB# #TAB# if char in RE_ENVIRONMENT_WORDS: #LINE# #TAB# #TAB# #TAB# word = word.replace(char, '_') #LINE# #TAB# return word"
"def find_undeclared(nodes, names): #LINE# #TAB# undeclared = set() #LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if node.name not in names: #LINE# #TAB# #TAB# #TAB# undeclared.add(node.name) #LINE# #TAB# return undeclared"
def close_server(port): #LINE# #TAB# global server_max_connections #LINE# #TAB# server_max_connections = port #LINE# #TAB# if server_max_connections > logging.INFO_MAX_CONNECTIONS: #LINE# #TAB# #TAB# logging.basicConfig(level=logging.INFO) #LINE# #TAB# #TAB# server_max_connections -= 1 #LINE# #TAB# #TAB# server.close() #LINE# #TAB# #TAB# server_max_connections = 0 #LINE# #TAB# if port == 1: #LINE# #TAB# #TAB# server.listen(1) #LINE# #TAB# #TAB# server_max_connections = 0 #LINE# #TAB# elif port == 2: #LINE# #TAB# #TAB# server.listen(1) #LINE# #TAB# else: #LINE# #TAB# #TAB# server.close() #LINE# #TAB# #TAB# server_max_connections = 0
def resolve_to_ip_addresses(dns_name: str) ->set: #LINE# #TAB# output = set() #LINE# #TAB# try: #LINE# #TAB# #TAB# resolved = dns.resolver.resolve(dns_name) #LINE# #TAB# #TAB# for address in resolved.items: #LINE# #TAB# #TAB# #TAB# output.add(address.address) #LINE# #TAB# except dns.resolver.NXDOMAIN: #LINE# #TAB# #TAB# pass #LINE# #TAB# return output
"def get_cwd(options): #LINE# #TAB# if not options.cwd: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not os.path.exists(options.cwd): #LINE# #TAB# #TAB# raise exceptions.InvalidCwd( #LINE# #TAB# #TAB# #TAB# ""can't --cwd to invalid path {0!r}"".format(options.cwd)) #LINE# #TAB# return options.cwd"
"def parse_seeds(seeds): #LINE# #TAB# if '|' in seeds: #LINE# #TAB# #TAB# url_seeds = seeds.split('|') #LINE# #TAB# else: #LINE# #TAB# #TAB# url_seeds = seeds #LINE# #TAB# user_seeds = [] #LINE# #TAB# user_seeds = [] #LINE# #TAB# for value in url_seeds: #LINE# #TAB# #TAB# if value[0] in '@': #LINE# #TAB# #TAB# #TAB# user_seeds.append(value[1]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# user_seeds.append(value) #LINE# #TAB# return user_seeds, url_seeds"
"def calc_sha256_from_dir_map(directory: str) ->dict: #LINE# #TAB# sha256_map: dict = {} #LINE# #TAB# if os.path.exists(directory) and os.path.isfile(directory): #LINE# #TAB# #TAB# files = [os.path.join(directory, path) for path in os.listdir( #LINE# #TAB# #TAB# #TAB# directory) if os.path.isfile(os.path.join(directory, path))] #LINE# #TAB# #TAB# for filepath in files: #LINE# #TAB# #TAB# #TAB# sha256_map.update({filepath: sha256_from_file(filepath)}) #LINE# #TAB# return sha256_map"
def rgb_to_vector(image): #LINE# #TAB# if image.pixeltype!= 'unsigned char': #LINE# #TAB# #TAB# image = image.clone('unsigned char') #LINE# #TAB# idim = image.dimension #LINE# #TAB# vec = iio.ANTsVector() #LINE# #TAB# vec.data = image.data #LINE# #TAB# vec.shape = image.dimension #LINE# #TAB# return vec
"def get_netconf_cred_by_name(context, name): #LINE# #TAB# try: #LINE# #TAB# #TAB# query = context.session.query(models.BNPNETCONFCredential) #LINE# #TAB# #TAB# netconf_creds = query.filter_by(name=name).all() #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# LOG.info(_LI('no netconf credential found with name: %s'), name) #LINE# #TAB# #TAB# return #LINE# #TAB# return netconf_creds"
"def merge_dictionaries(base_dict, extra_dict): #LINE# #TAB# merged_dict = dict(base_dict) #LINE# #TAB# merged_dict.update(extra_dict) #LINE# #TAB# return merged_dict"
"def auto_thaw(vault_client, opt): #LINE# #TAB# vault_client.thaw() #LINE# #TAB# return vault_client.temp"
"def get_metadata(ident_hash): #LINE# #TAB# id, version = get_id_n_version(ident_hash) #LINE# #TAB# stmt = _get_sql('get-metadata.sql') #LINE# #TAB# args = dict(id=id, version=version) #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# cursor.execute(stmt, args) #LINE# #TAB# #TAB# #TAB# metadata = cursor.fetchone() #LINE# #TAB# return metadata"
"def get_blast2(pdb_id, chain_id='A', output_form='HTML'): #LINE# #TAB# raw = get_raw_blast(pdb_id, chain_id, output_form) #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse_blast(raw, output_form) #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# return None"
def set_guess(guess): #LINE# #TAB# global _GUESS #LINE# #TAB# _GUESS = guess
"def skip_past_data_dd(data_stream, start): #LINE# #TAB# if _ord(data_stream[start])!= 170 or not _is_bit31_set(data_stream, #LINE# #TAB# #TAB# start + 1): #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# 'Unexpected block format for 0x00 (0x%x) with length and string marker 0x%08x at 0x%x.' #LINE# #TAB# #TAB# #TAB# % (_ord(data_stream[start]), _unpack1('L', data_stream[start + #LINE# #TAB# #TAB# #TAB# 1:start + 5]), start)) #LINE# #TAB# char_count = _unpack1('L', data_stream[start + 1:start + 5]) & 2147483647 #LINE# #TAB# byte_length = char_count * 2 #LINE# #TAB# return start + 5 + byte_length"
"def text_batch_update(batch): #LINE# #TAB# fields = {'from': batch.sender, 'to': sorted(batch.recipients)} #LINE# #TAB# if batch.delivery_report: #LINE# #TAB# #TAB# fields['delivery_report'] = batch.delivery_report #LINE# #TAB# if batch.send_at: #LINE# #TAB# #TAB# fields['send_at'] = _write_datetime(batch.send_at) #LINE# #TAB# if batch.expire_at: #LINE# #TAB# #TAB# fields['expire_at'] = _write_datetime(batch.expire_at) #LINE# #TAB# if batch.tags: #LINE# #TAB# #TAB# fields['tags'] = sorted(batch.tags) #LINE# #TAB# if batch.callback_url: #LINE# #TAB# #TAB# fields['callback_url'] = batch.callback_url #LINE# #TAB# return fields"
"def compute_value(power, wg): #LINE# #TAB# if power not in wg: #LINE# #TAB# #TAB# p1, p2 = power #LINE# #TAB# #TAB# if p1 == 0: #LINE# #TAB# #TAB# #TAB# yy = wg[0, -1] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(yy, p2 / 2).sum() / len(yy) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# xx = wg[-1, 0] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(xx, p1 / 2).sum() / len(xx) #LINE# #TAB# return wg[power]"
"def get_from_cache(url): #LINE# #TAB# if settings.use_cache: #LINE# #TAB# #TAB# response_json = requests.get(url, allow_redirects=True, verify=False) #LINE# #TAB# #TAB# if response_json is not None: #LINE# #TAB# #TAB# #TAB# return response_json #LINE# #TAB# else: #LINE# #TAB# #TAB# response_json = requests.get(url, allow_redirects=True, verify=False) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# response_json = jsonutils.loads(response_json) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# response_json = None #LINE# #TAB# #TAB# return response_json"
"def build_params(dic): #LINE# #TAB# str_params = '' #LINE# #TAB# for key in dic: #LINE# #TAB# #TAB# if isinstance(dic[key], str): #LINE# #TAB# #TAB# #TAB# str_params += '{0}={1}'.format(key, dic[key]) #LINE# #TAB# #TAB# elif isinstance(dic[key], list): #LINE# #TAB# #TAB# #TAB# str_params += '{0}={1}'.format(dic[key], ','.join(dic[key])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# str_params += '{0}={1}'.format(key, dic[key]) #LINE# #TAB# return str_params"
"def get_slot(name, module=None): #LINE# #TAB# if module is None: #LINE# #TAB# #TAB# module = loadConfig() #LINE# #TAB# slot = getattr(module, name, None) #LINE# #TAB# if slot is None: #LINE# #TAB# #TAB# raise ValueError('No such slot: %s' % name) #LINE# #TAB# return slot"
"def obtain_weights_csvlogger_filenames(body_backbone_CNN, image_backbone_CNN): #LINE# #TAB# body_backbone_names = [('body_backbone_' + str(i)) for i in #LINE# #TAB# #TAB# body_backbone_CNN] #LINE# #TAB# image_backbone_names = [('image_backbone_' + str(i)) for i in #LINE# #TAB# #TAB# image_backbone_CNN] #LINE# #TAB# weights_filenames = ['body_backbone_{}.csv'.format(body_backbone_names[0]) #LINE# #TAB# #TAB# ] #LINE# #TAB# csvlogger_filenames = ['image_backbone_{}.csv'.format(image_backbone_names[0]) #LINE# #TAB# #TAB# ] #LINE# #TAB# return weights_filenames, csvlogger_filenames"
"def recv_int(sock: socket.socket) ->int: #LINE# #TAB# data = sock.recv(4) #LINE# #TAB# m = struct.unpack('>i', data) #LINE# #TAB# result = m[0] #LINE# #TAB# return result"
"def covmat2_corrmat(covmatrix): #LINE# #TAB# n, m = covmatrix.shape #LINE# #TAB# corrmat = np.empty((n, m)) #LINE# #TAB# for j in range(n): #LINE# #TAB# #TAB# for k in range(m): #LINE# #TAB# #TAB# #TAB# corrmat[j, k] = covmatrix[j, k] / np.sqrt(np.outer(covmatrix[j, k], covmatrix[ #LINE# #TAB# #TAB# #TAB# #TAB# j, k])) #LINE# #TAB# return corrmat"
"def beauty_print(pr_list): #LINE# #TAB# string = '' #LINE# #TAB# for i, pr in enumerate(pr_list): #LINE# #TAB# #TAB# if pr == 'None': #LINE# #TAB# #TAB# #TAB# string += 'None' #LINE# #TAB# #TAB# elif pr == 'True': #LINE# #TAB# #TAB# #TAB# string += 'True' #LINE# #TAB# #TAB# elif pr == 'False': #LINE# #TAB# #TAB# #TAB# string += 'False' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# string += pr #LINE# #TAB# return string"
"def get_entry_property(entry, prop, default=None, raw=False): #LINE# #TAB# default = default or entry.get(prop, default) #LINE# #TAB# if raw: #LINE# #TAB# #TAB# return default #LINE# #TAB# return default"
"def datatype_to_dtype(datatype): #LINE# #TAB# if datatype in (int, np.integer): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# #TAB# dtype = np.uint8 #LINE# #TAB# elif datatype in (float, np.floating): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# #TAB# dtype = np.float32 #LINE# #TAB# elif datatype in (bool, np.integer): #LINE# #TAB# #TAB# dbrcode = 0 #LINE# #TAB# #TAB# dtype = np.int8 #LINE# #TAB# elif datatype in (np.float16, np.float32, np.float64): #LINE# #TAB# #TAB# dbrcode = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Invalid data type:'+ str(datatype)) #LINE# #TAB# return dbrcode, dtype"
"def make_win(n, mono=False): #LINE# #TAB# if mono: #LINE# #TAB# #TAB# win = np.hanning(n) + 1e-05 #LINE# #TAB# else: #LINE# #TAB# #TAB# win = np.array([np.hanning(n) + 1e-05, np.hanning(n) + 1e-05]) #LINE# #TAB# win = np.transpose(win) #LINE# #TAB# return win"
"def get_upload_enabled(cls) ->bool: #LINE# #TAB# upload_enabled = getattr(cls, 'upload_enabled', None) #LINE# #TAB# if upload_enabled is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# from django.conf import settings #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# from django.conf import settings #LINE# #TAB# #TAB# upload_enabled = settings.GRANT_UPLOAD_ENABLED #LINE# #TAB# return upload_enabled"
"def setup_cmd_parser(cls): #LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, from_date=True, #LINE# #TAB# #TAB# basic_auth=True, archive=True) #LINE# #TAB# group = parser.parser.add_argument_group('NNTP arguments') #LINE# #TAB# group.add_argument('--items-per-page', dest='items_per_page', help= #LINE# #TAB# #TAB# 'Items per page') #LINE# #TAB# group.add_argument('--sleep-time', dest='sleep_time', help= #LINE# #TAB# #TAB# 'Sleep time in case of connection lost') #LINE# #TAB# parser.parser.add_argument('url', help='URL of the NNTP server') #LINE# #TAB# return parser"
"def extract_by_index(sequence: Sequence, indices: Iterable[int]) ->Iterator: #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# yield sequence[index]"
def get_version(): #LINE# #TAB# global _MOLGENIS_VERSION #LINE# #TAB# if _MOLGENIS_VERSION is None: #LINE# #TAB# #TAB# _MOLGENIS_VERSION = _get_version() #LINE# #TAB# return _MOLGENIS_VERSION
"def slot_availability_array(events, slots): #LINE# #TAB# array = np.ones((len(events), len(slots))) #LINE# #TAB# for row, event in enumerate(events): #LINE# #TAB# #TAB# for col, slot in enumerate(slots): #LINE# #TAB# #TAB# #TAB# if slot in event.unavailability or event.duration > slot.duration: #LINE# #TAB# #TAB# #TAB# #TAB# array[row, col] = 0 #LINE# #TAB# return array"
"def find_and_load(name, import_): #LINE# #TAB# spec = find_spec(name, import_) #LINE# #TAB# if spec is not None: #LINE# #TAB# #TAB# module = importlib.import_module(spec.name) #LINE# #TAB# #TAB# spec.loader.exec_module(module) #LINE# #TAB# #TAB# return module #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def update_config_pwd(msg, cfg): #LINE# #TAB# keyfmt = 'password' #LINE# #TAB# msg_type = msg.get('type') #LINE# #TAB# if keyfmt == 'password': #LINE# #TAB# #TAB# msg_type = 'password' #LINE# #TAB# try: #LINE# #TAB# #TAB# cfg['auth'][keyfmt] = msg.get('auth') #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass"
"def get_id(item_name): #LINE# #TAB# path = os.path.join(_ROOT, 'items.json') #LINE# #TAB# items = read_json_file(path) #LINE# #TAB# for item_id in items: #LINE# #TAB# #TAB# if item_name == item_id['name']: #LINE# #TAB# #TAB# #TAB# return item_id #LINE# #TAB# return -1"
"def result_value_above(value: float=None, lower: float=0.0) ->bool: #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# result_value = lower #LINE# #TAB# else: #LINE# #TAB# #TAB# if result_value > lower: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"def usage_percent(used, total, round_=None): #LINE# #TAB# if round_ is None: #LINE# #TAB# #TAB# round_ = int #LINE# #TAB# used, total = float(used), float(total) #LINE# #TAB# if round_ > 100: #LINE# #TAB# #TAB# ret = round(used / total * 100, round_) #LINE# #TAB# else: #LINE# #TAB# #TAB# ret = round(used / total * 100, round_) #LINE# #TAB# return ret"
def set_global_print_private_entries(enabled=False): #LINE# #TAB# global globalPrintPrivateEntries #LINE# #TAB# globalPrintPrivateEntries = enabled
def import_chain(modules: Iterable): #LINE# #TAB# for module in modules: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return importlib.import_module(module) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# break
"def dict_find(d, which_key): #LINE# #TAB# result = [] #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# if which_key in v: #LINE# #TAB# #TAB# #TAB# #TAB# result.append((k, dict_find(v, which_key))) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# result.append((k, v)) #LINE# #TAB# return result"
"def from_file(cls, f): #LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# with f.open() as f: #LINE# #TAB# #TAB# #TAB# obj.load(f) #LINE# #TAB# #TAB# return obj"
def get_current_user(): #LINE# #TAB# current_user = None #LINE# #TAB# try: #LINE# #TAB# #TAB# current_user = getpass.getuser() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# if current_user is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# import pwd #LINE# #TAB# #TAB# #TAB# current_user = pwd.getpwuid(os.getuid()) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return current_user
"def morph_many(waves, gaps): #LINE# #TAB# res = [None] * len(waves) #LINE# #TAB# for i, w in enumerate(waves): #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# res[0] = w #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tmp = morph(w, gaps) #LINE# #TAB# #TAB# #TAB# if tmp is not None: #LINE# #TAB# #TAB# #TAB# #TAB# res[i] = tmp[i] #LINE# #TAB# return res"
"def depends_tsort(data, depends): #LINE# #TAB# res = list(data) #LINE# #TAB# if not depends: #LINE# #TAB# #TAB# return res #LINE# #TAB# for item in depends: #LINE# #TAB# #TAB# req = depends.get(item) #LINE# #TAB# #TAB# if req in res: #LINE# #TAB# #TAB# #TAB# res.remove(req) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res.append(item) #LINE# #TAB# return res"
"def get_text_from_inspire(search='', resultformat='brief', ot=None): #LINE# #TAB# InspireReader = InspireReader(ot=ot) #LINE# #TAB# if resultformat == 'brief': #LINE# #TAB# #TAB# text = InspireReader.extract_text_from_inspire(search) #LINE# #TAB# else: #LINE# #TAB# #TAB# text = InspireReader.extract_text_from_inspire(search) #LINE# #TAB# return text"
"def dflt_sortby_objgoea(goea_res): #LINE# #TAB# #TAB# return [goea.enrichment, #LINE# #TAB# #TAB# #TAB# #TAB# goea.namespace, #LINE# #TAB# #TAB# #TAB# #TAB# goea.p_uncorrected, #LINE# #TAB# #TAB# #TAB# #TAB# goea.depth, #LINE# #TAB# #TAB# #TAB# #TAB# goea.GO]"
"def verify_bucket_access(client, bucket_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# client.get_bucket(bucket_name) #LINE# #TAB# #TAB# return True #LINE# #TAB# except botocore.exceptions.ClientError as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return False"
"def cols_to_normalize(df, columns): #LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# df['_' + col] = df[col] / (1 + df[col]) #LINE# #TAB# return df"
"def sdm_sources(sdmname): #LINE# #TAB# sdm = getsdm(sdmname) #LINE# #TAB# sourcedict = {} #LINE# #TAB# for row in sdm['Field']: #LINE# #TAB# #TAB# src = str(row.fieldName) #LINE# #TAB# #TAB# sourcenum = int(row.sourceId) #LINE# #TAB# #TAB# direction = str(row.referenceDir) #LINE# #TAB# #TAB# (ra,dec) = [float(val) for val in direction.split(' ')[3:]] #LINE# #TAB# #TAB# sourcedict[sourcenum] = {} #LINE# #TAB# #TAB# sourcedict[sourcenum]['source'] = src #LINE# #TAB# #TAB# sourcedict[sourcenum]['ra'] = ra #LINE# #TAB# #TAB# sourcedict[sourcenum]['dec'] = dec #LINE# #TAB# return sourcedict"
def convert_duration_to_string(duration): #LINE# #TAB# seconds = duration.get('seconds') #LINE# #TAB# if seconds is not None: #LINE# #TAB# #TAB# return '{:.2f}s'.format(seconds) #LINE# #TAB# elif seconds is not None: #LINE# #TAB# #TAB# return '{:.2f}s'.format(seconds) #LINE# #TAB# return None
def database_exists(): #LINE# #TAB# try: #LINE# #TAB# #TAB# psycopg2.connect() #LINE# #TAB# except psycopg2.OperationalError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def numeric_part(s): #LINE# #TAB# m = re.match('^([0-9]+)-([0-9]+)$', s) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return int(m.group(1)) #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return float(s) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return s"
"def find_forbidden_combinations(requested_additions, tea_slug=None): #LINE# #TAB# for restricted_addition in requested_additions: #LINE# #TAB# #TAB# if not restricted_addition: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for restricted_combination in ForbiddenCombinations: #LINE# #TAB# #TAB# #TAB# if restricted_addition.add_slug == tea_slug: #LINE# #TAB# #TAB# #TAB# #TAB# return restricted_combination #LINE# #TAB# return []"
def get_file_reference_mode(): #LINE# #TAB# args = sys.argv #LINE# #TAB# return 'runtime' if args[0].startswith('Python') else 'original' if args[0 #LINE# #TAB# #TAB# ].startswith('Python') else 'frozen'
"def fill_notebook(nb, timeout=30): #LINE# #TAB# timeout = timeout or 30 #LINE# #TAB# py_notebook = nbformat.read(timeout=timeout, as_version=4) #LINE# #TAB# for cell in py_notebook['cells']: #LINE# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# for cell_output in cell['outputs']: #LINE# #TAB# #TAB# #TAB# #TAB# cell_output['source'] = cell_output['source'] #LINE# #TAB# #TAB# #TAB# cell['outputs'] = py_notebook.outputs #LINE# #TAB# return py_notebook"
def is_dir(dir_name): #LINE# #TAB# if not os.path.isdir(dir_name): #LINE# #TAB# #TAB# msg = '{0} is not a directory'.format(dir_name) #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(msg) #LINE# #TAB# else: #LINE# #TAB# #TAB# return dir_name
"def get_filtering_args_from_filterset(filterset_class, type): #LINE# #TAB# from..forms.converter import convert_form_field #LINE# #TAB# args = {} #LINE# #TAB# for name, filter_field in six.iteritems(filterset_class.base_filters): #LINE# #TAB# #TAB# field_type = convert_form_field(filter_field.field).Argument() #LINE# #TAB# #TAB# field_type.description = filter_field.label #LINE# #TAB# #TAB# args[name] = field_type #LINE# #TAB# return args"
def fill_list_valueparamir(initialparameters): #LINE# #TAB# default_value = [] #LINE# #TAB# for i in range(len(initialparameters)): #LINE# #TAB# #TAB# default_value.append(initialparameters[i]) #LINE# #TAB# return default_value
def xml_to_penn(tree): #LINE# #TAB# penn = tree.copy() #LINE# #TAB# for child in penn.iter(): #LINE# #TAB# #TAB# if child.tag == 'Document': #LINE# #TAB# #TAB# #TAB# penn.extend(xml_to_penn(child)) #LINE# #TAB# #TAB# elif child.tag == 'Ensembl': #LINE# #TAB# #TAB# #TAB# penn.extend(xml_to_penn(child)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# penn.append(xml_to_penn(child)) #LINE# #TAB# return penn
"def remove_comment_attributes_from_line(oLine): #LINE# #TAB# for sLine in oLine.line.replace(',',''): #LINE# #TAB# #TAB# if sLine.line.startswith(('#', '>')): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# setattr(oLine, sLine, None) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass"
def paradigms_to_alphabet(paradigms: List[paradigm.Paradigm]) ->Set[str]: #LINE# #TAB# symbols = set() #LINE# #TAB# for paradigm in paradigms: #LINE# #TAB# #TAB# symbols.update(*[p.symbol for p in paradigm.paradigms]) #LINE# #TAB# return symbols
def dont_listen(): #LINE# #TAB# global __dont_listen #LINE# #TAB# if __dont_listen: #LINE# #TAB# #TAB# __dont_listen = False
"def steps_to_run(current_step, steps_per_epoch, steps_per_loop): #LINE# #TAB# new_step = int(current_step * steps_per_epoch) #LINE# #TAB# if steps_per_loop > 0: #LINE# #TAB# #TAB# new_step = step * (steps_per_loop - 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_step = step * (steps_per_epoch - steps_per_loop) #LINE# #TAB# return new_step"
"def prepare_reserved_tokens(reserved_tokens): #LINE# #TAB# formatted_tokens = [] #LINE# #TAB# for r in reserved_tokens: #LINE# #TAB# #TAB# if not r: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# name, regex = r.partition(':') #LINE# #TAB# #TAB# if not name: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# name = name.strip() #LINE# #TAB# #TAB# regex = re.compile(regex) #LINE# #TAB# #TAB# formatted_tokens.append((name, regex)) #LINE# #TAB# return formatted_tokens"
"def rm_subtitles(path): #LINE# #TAB# for item in os.listdir(path): #LINE# #TAB# #TAB# full_path = os.path.join(path, item) #LINE# #TAB# #TAB# if os.path.isdir(full_path): #LINE# #TAB# #TAB# #TAB# rm_subtitles(full_path) #LINE# #TAB# #TAB# if os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# os.remove(full_path) #LINE# #TAB# return"
def get_nowait(cls) ->Optional['Action']: #LINE# #TAB# try: #LINE# #TAB# #TAB# return cls._response_queue.get_nowait() #LINE# #TAB# except Empty: #LINE# #TAB# #TAB# return None
"def fromquat_to_vecangle(quat): #LINE# #TAB# normvectpart = np.sqrt(quat[0] ** 2 + quat[1] ** 2 + quat[2] ** 2 + #LINE# #TAB# #TAB# quat[3] ** 2) #LINE# #TAB# angle = np.arccos(quat[3] / normvectpart) * 2.0 #LINE# #TAB# unitvec = np.array(quat[:3]) / np.sin(angle / 2) / normvectpart #LINE# #TAB# return unitvec, angle"
"def read_constaxonomy_file(filepath): #LINE# #TAB# data = pd.read_table(filepath, names=['OTU', 'Taxonomy']) #LINE# #TAB# classifications = data['Taxonomy'] #LINE# #TAB# classifications = classifications.str.split(';', expand=True).drop(6, #LINE# #TAB# #TAB# axis=1) #LINE# #TAB# classifications.columns = list(range(1, 7)) #LINE# #TAB# features_names = data['OTU'] #LINE# #TAB# data = pd.concat([features_names, classifications], axis=1) #LINE# #TAB# data = data.set_index('OTU') #LINE# #TAB# data.index = data.index.rename(None) #LINE# #TAB# data = data.sort_index() #LINE# #TAB# return data"
def non_private_tags_in_dicom_dataset(ds): #LINE# #TAB# non_private_tags = [] #LINE# #TAB# for item in ds.Image: #LINE# #TAB# #TAB# tag = item.Tag #LINE# #TAB# #TAB# if tag!= PRIVATE_TAG: #LINE# #TAB# #TAB# #TAB# non_private_tags.append(tag) #LINE# #TAB# return non_private_tags
"def author_agrees(oligo_dict, contents, nchains): #LINE# #TAB# oligo_id = oligo_dict['author'][0] #LINE# #TAB# for oligo in contents: #LINE# #TAB# #TAB# chains = oligo_dict[oligo_id]['chains'] #LINE# #TAB# #TAB# if chains > 0: #LINE# #TAB# #TAB# #TAB# for s in chains: #LINE# #TAB# #TAB# #TAB# #TAB# if s in oligo_dict[oligo_id]['s']: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# oligo_id = oligo #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return oligo_id"
"def transpose_nested_dictionary(nested_dict): #LINE# #TAB# outer_dict, inner_dict = {} #LINE# #TAB# for k1, v in nested_dict.items(): #LINE# #TAB# #TAB# if k1 not in outer_dict: #LINE# #TAB# #TAB# #TAB# outer_dict[k1] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# inner_dict[k1] = v #LINE# #TAB# return outer_dict, inner_dict"
def game_events(game_id): #LINE# #TAB# events = {} #LINE# #TAB# event_queue = event_queue() #LINE# #TAB# for event in event_queue: #LINE# #TAB# #TAB# if event.game_id == game_id: #LINE# #TAB# #TAB# #TAB# events[event.event_id] = event #LINE# #TAB# return events
"def post_leave(data: Dict[str, Any]) ->None: #LINE# #TAB# status = data['headers']['status'] #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# Memory.add_rooms(data['room']) #LINE# #TAB# EventStatus.leave_room = status"
"def build_option_parser(parser): #LINE# #TAB# parser.add_argument('--os-baremetal-compute-api-version', metavar= #LINE# #TAB# #TAB# '<baremetal-compute-api-version>', default=utils.env( #LINE# #TAB# #TAB# 'OS_BAREMETAL_COMPUTE_API_VERSION', default= #LINE# #TAB# #TAB# DEFAULT_BAREMETAL_COMPUTE_API_VERSION), help=_( #LINE# #TAB# #TAB# 'Baremetal compute API version, default=%s (Env: OS_BAREMETAL_COMPUTE_API_VERSION)' # #LINE# #TAB# #TAB# ) % DEFAULT_BAREMETAL_COMPUTE_API_VERSION) #LINE# #TAB# return parser"
def read_eman1_boxfile(path): #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# if f.readline().strip() == '': #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# #TAB# bboxes = [] #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# if line[0] == '#': #LINE# #TAB# #TAB# #TAB# #TAB# bboxes.append(parse_eman1_boxline(line)) #LINE# #TAB# #TAB# #TAB# elif line[0] == '#': #LINE# #TAB# #TAB# #TAB# #TAB# bboxes.append(parse_eman1_boxline(line)) #LINE# #TAB# return bboxes
def safe_strip(value): #LINE# #TAB# if value is not None: #LINE# #TAB# #TAB# value = value.strip() #LINE# #TAB# return value
"def read_model(model_path, config): #LINE# #TAB# layer = {} #LINE# #TAB# with open(model_path, 'rb') as f: #LINE# #TAB# #TAB# layer_dict = json.load(f) #LINE# #TAB# output = {} #LINE# #TAB# for layer_name in layer_dict: #LINE# #TAB# #TAB# output[layer_name] = {} #LINE# #TAB# #TAB# for key, value in layer_dict[layer_name].items(): #LINE# #TAB# #TAB# #TAB# output[layer_name][key] = value #LINE# #TAB# return output"
"def get_order_clause(archive_table): #LINE# #TAB# order_clause = [ #LINE# #TAB# #TAB# sa.asc(getattr(archive_table, col_name)) for col_name in archive_table._version_col_names #LINE# #TAB# ] #LINE# #TAB# order_clause.append(sa.asc(archive_table.version_id)) #LINE# #TAB# return order_clause"
"def get_relevant_categories(cls): #LINE# #TAB# res = cls.objects.all() #LINE# #TAB# for field in cls._meta.fields: #LINE# #TAB# #TAB# if hasattr(field,'related_categories'): #LINE# #TAB# #TAB# #TAB# if field.related_categories.model_name == cls.model_name: #LINE# #TAB# #TAB# #TAB# #TAB# res = res.filter(related_categories__categories=field. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# relevant_categories) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# res = res.filter(related_categories__model_name=cls. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# model_name, category__in=field.related_categories.all()) #LINE# #TAB# return res"
"def city_nums(): #LINE# fname = pkg_resources.resource_filename(__name__,'resources/CityNumbers.csv') #LINE# with open(fname, 'rU') as csvfile: #LINE# #TAB# reader = csv.reader(csvfile, delimiter = ',') #LINE# #TAB# rows = [row[1] for row in reader] #LINE# city_nums = {} #LINE# for row in reader: #LINE# #TAB# city_nums[row[0]] = row[1] #LINE# return city_nums"
"def p_dictorsetmaker_9(p): #LINE# #TAB# keys, values = p[2] #LINE# #TAB# p[0] = [keys, [p[1]] + values"
"def get_logging_link(scenario_uuid): #LINE# #TAB# r = requests.get(url='{}scenarios/{}'.format(LIZARD_URL, scenario_uuid), #LINE# #TAB# #TAB# headers=get_headers()) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# for result in r.json()['result_set']: #LINE# #TAB# #TAB# if result['result_type']['code'] == 'logging': #LINE# #TAB# #TAB# #TAB# url = result['attachment_url'] #LINE# #TAB# #TAB# #TAB# return url"
"def add_params_docstring(params): #LINE# #TAB# params_docstring = '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# params_docstring += '\n#TAB# - *%s: %s\n' % (key, str(value)) #LINE# #TAB# #TAB# if type(value) is list: #LINE# #TAB# #TAB# #TAB# params_docstring += '\n#TAB# *%s: %s\n#TAB#'% (key, str(value)) #LINE# #TAB# #TAB# elif type(value) is dict: #LINE# #TAB# #TAB# #TAB# params_docstring += '\n#TAB# *%s: %s\n' % (key, str(value)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# params_docstring += '\n#TAB# *%s: %s\n' % (key, value) #LINE# #TAB# return params_docstring"
"def flagged_listdir(): #LINE# #TAB# path = sys.prefix #LINE# #TAB# return [(os.path.join(path, 'flagged'), 'files') for path in os.listdir( #LINE# #TAB# #TAB# path) if os.path.isdir(os.path.join(path, 'flagged'))]"
"def get_last_commit(git_path='git'): #LINE# #TAB# output = subprocess.check_output(['git', 'log', '-1', '--pretty=%H'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=subprocess.DEVNULL, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# shell=True) #LINE# #TAB# if output: #LINE# #TAB# #TAB# return json.loads(output.decode('utf-8')) #LINE# #TAB# return None"
"def get_bone_filename(name, topping): #LINE# #TAB# name_parts = name.split('.') #LINE# #TAB# if topping: #LINE# #TAB# #TAB# name_parts[-1] += '_t' #LINE# #TAB# filename = '{}_{}.{}'.format(name_parts[-1], name_parts[-1]) #LINE# #TAB# return filename"
def register_parser(cls): #LINE# #TAB# FORMATS.append(cls) #LINE# #TAB# return cls
def attribute_set_to_expr(attribute_set): #LINE# #TAB# out = '' #LINE# #TAB# for attr in attribute_set: #LINE# #TAB# #TAB# if attr == 'true': #LINE# #TAB# #TAB# #TAB# out += 'TRUE' #LINE# #TAB# #TAB# elif attr == 'false': #LINE# #TAB# #TAB# #TAB# out += 'FALSE' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out += attribute_to_expr(attr) #LINE# #TAB# return out
def current_resource_name(request): #LINE# #TAB# service = current_service(request) #LINE# #TAB# resource_name = service.viewset.get_name(service.resource) #LINE# #TAB# return resource_name
def get_detailparams() ->dict: #LINE# #TAB# p = request.json['action']['detailParameters'] #LINE# #TAB# return p
"def psi_mgoh_cl_hco3_hmw84(T, P): #LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"def jwk_wrap(key, use="""", kid=""""): #LINE# #TAB# use = use.strip() #LINE# #TAB# if use and not isinstance(use, str): #LINE# #TAB# #TAB# use = [use] #LINE# #TAB# wrapped_key = Key.wrap(key, use, kid) #LINE# #TAB# if isinstance(wrapped_key, CustomKey): #LINE# #TAB# #TAB# if hasattr(wrapped_key, ""kid""): #LINE# #TAB# #TAB# #TAB# kid = wrapped_key.kid #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# kid = wrapped_key.kid #LINE# #TAB# #TAB# wrapped_key = CustomKey(wrapped_key, kid) #LINE# #TAB# return wrapped_key"
"def pearson_score(list1, list2): #LINE# #TAB# size = len(list1) #LINE# #TAB# sum1 = sum(list1) #LINE# #TAB# sum2 = sum(list2) #LINE# #TAB# sum_sq1 = sum([pow(l, 2) for l in list1]) #LINE# #TAB# sum_sq2 = sum([pow(l, 2) for l in list2]) #LINE# #TAB# prod_sum = sum([list1[i] * list2[i] for i in range(size)]) #LINE# #TAB# num = prod_sum - (sum1 * sum2 / float(size)) #LINE# #TAB# den = sqrt((sum_sq1 - pow(sum1, 2.0) / size) * #LINE# #TAB# #TAB# (sum_sq2 - pow(sum2, 2.0) / size)) #LINE# #TAB# return num / den"
def remove_unwanted_headers(allHeadersContent): #LINE# #TAB# brokenHeaders = [] #LINE# #TAB# for h in allHeadersContent: #LINE# #TAB# #TAB# if h not in brokenHeaders: #LINE# #TAB# #TAB# #TAB# brokenHeaders.append(h) #LINE# #TAB# allHeadersContent = dict(brokenHeaders) #LINE# #TAB# for h in brokenHeaders: #LINE# #TAB# #TAB# if h not in allHeadersContent: #LINE# #TAB# #TAB# #TAB# del allHeadersContent[h]
"def by_causes(queryset, cause_string=None): #LINE# #TAB# if cause_string: #LINE# #TAB# #TAB# cause_list = cause_string.split(',') #LINE# #TAB# #TAB# if any(cause in cause_list for cause in cause_list): #LINE# #TAB# #TAB# #TAB# queryset = queryset.filter(**{'%s__icontains' % cause: True}) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# queryset = queryset.filter(**{'%s__icontains' % cause: True}) #LINE# #TAB# return queryset"
"def get_cols(row_content): #LINE# #TAB# cols = [] #LINE# #TAB# subcell_col = [] #LINE# #TAB# prev_bar = None #LINE# #TAB# for _coord, item in row_content: #LINE# #TAB# #TAB# if isinstance(item, LTTextLine): #LINE# #TAB# #TAB# #TAB# subcell_col.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if prev_bar: #LINE# #TAB# #TAB# #TAB# #TAB# bar_ranges = prev_bar, item #LINE# #TAB# #TAB# #TAB# #TAB# col_items = subcell_col if subcell_col else [None] #LINE# #TAB# #TAB# #TAB# #TAB# cols.extend([bar_ranges, col_items]) #LINE# #TAB# #TAB# #TAB# prev_bar = item #LINE# #TAB# #TAB# #TAB# subcell_col = [] #LINE# #TAB# return cols"
"def check_broken_configure_log_links(): #LINE# #TAB# broken_links = set() #LINE# #TAB# for link in CONFIG['LOG_LINKS']: #LINE# #TAB# #TAB# if os.path.exists(link): #LINE# #TAB# #TAB# #TAB# path = os.path.join(CONFIG['LOG_DIR'], link) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.unlink(path) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# broken_links.add(link) #LINE# #TAB# if broken_links: #LINE# #TAB# #TAB# raise BrokenLinks"
def global_instance(cls): #LINE# #TAB# if cls._global_instance is None: #LINE# #TAB# #TAB# with cls._global_instance_lock: #LINE# #TAB# #TAB# #TAB# if cls._global_instance is None: #LINE# #TAB# #TAB# #TAB# #TAB# cls._global_instance = cls() #LINE# #TAB# return cls._global_instance
"def bc_na_s2o3_pm73(T, P): #LINE# #TAB# b0 = 0.0765 #LINE# #TAB# b1 = 0.2672 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.0044 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Na'] * i2c['S2O3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
def get_cmdline_command_name(intfspec): #LINE# #TAB# cmdline = [] #LINE# #TAB# for spec in intfspec: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# spec = spec.split(' ') #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# cmdline.append(spec[0]) #LINE# #TAB# return cmdline
def get_name(model_id): #LINE# #TAB# if model_id in Model.objects.all(): #LINE# #TAB# #TAB# return Model.objects.get(pk=model_id).name #LINE# #TAB# return 'unknown'
"def get_value_from_type(buffer, field_type, _byte_order, is_string=False): #LINE# #TAB# number_length = len(field_type) #LINE# #TAB# if number_length > 0: #LINE# #TAB# #TAB# field_type = struct.unpack_from(field_type, buffer, _byte_order)[0] #LINE# #TAB# #TAB# number = int.from_bytes(number, _byte_order) #LINE# #TAB# else: #LINE# #TAB# #TAB# field_type = struct.unpack_from(field_type, buffer, _byte_order) #LINE# #TAB# value = field_type.to_python_value(buffer) #LINE# #TAB# if is_string: #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
"def parse_args(): #LINE# #TAB# parser = argparse.ArgumentParser() #LINE# #TAB# parser.add_argument('--config', type=str, help= #LINE# #TAB# #TAB# 'path to the config file') #LINE# #TAB# args = parser.parse_args() #LINE# #TAB# return args"
"def readFromCheckpoint(cls, checkpointDir): #LINE# #TAB# #TAB# checkpoint = os.path.join(checkpointDir,'model.checkpoint') #LINE# #TAB# #TAB# modelProto = capnproto.Model() #LINE# #TAB# #TAB# modelProto.readFromCheckpoint(checkpoint) #LINE# #TAB# #TAB# modelProto.checkpoint = checkpoint #LINE# #TAB# #TAB# return modelProto"
"def get_process_name(pid): #LINE# #TAB# try: #LINE# #TAB# #TAB# return os.kill(pid, 0) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
"def remote_decode(data): #LINE# #TAB# while data.find(b'\x00') >= 0: #LINE# #TAB# #TAB# fcommand, data = data.split(b'\x00', 1) #LINE# #TAB# #TAB# if fcommand == b'\x02': #LINE# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# elif fcommand == b'\x03': #LINE# #TAB# #TAB# #TAB# return data"
"def get_base_indentation(code, include_start=False): #LINE# #TAB# if include_start: #LINE# #TAB# #TAB# indent = get_start_indent(code) #LINE# #TAB# else: #LINE# #TAB# #TAB# indent = extract_indent(code) #LINE# #TAB# while indent.startswith('def '): #LINE# #TAB# #TAB# indent = indent[1:] #LINE# #TAB# return indent"
def get_repo_filter(repo=None): #LINE# #TAB# if repo is None: #LINE# #TAB# #TAB# repo = SETTINGS['repo'] #LINE# #TAB# if 'filter' in repo: #LINE# #TAB# #TAB# return SETTINGS['filter'] #LINE# #TAB# return repo
"def valid_function_signature(input_type, func): #LINE# #TAB# func_signature = inspect.signature(func) #LINE# #TAB# num_params = len(func._inputs) #LINE# #TAB# if num_params is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# num_params = len(input_type.fields) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# if num_params!= num_params: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not all(sig.name in func_signature.parameters for sig in #LINE# #TAB# #TAB# input_type.inputs): #LINE# #TAB# #TAB# return None #LINE# #TAB# return func_signature"
"def get_feeds_from_atags(url: str, html: BeautifulSoup) ->list: #LINE# #TAB# feed_list = [] #LINE# #TAB# atags = html.find_all('link', attrs={'rel': 'alternate'}) #LINE# #TAB# for tag in atags: #LINE# #TAB# #TAB# if 'title' in tag.attrs: #LINE# #TAB# #TAB# #TAB# feed_list.append(tag.attrs['title']) #LINE# #TAB# return feed_list"
def configure_frozen_pool(kwargs): #LINE# #TAB# _POOL.frozen = True #LINE# #TAB# for key in kwargs: #LINE# #TAB# #TAB# if key in _POOL_OPTIONS: #LINE# #TAB# #TAB# #TAB# del _POOL_OPTIONS[key] #LINE# #TAB# for key in kwargs: #LINE# #TAB# #TAB# if key not in _POOL_OPTIONS: #LINE# #TAB# #TAB# #TAB# _POOL_OPTIONS[key] = kwargs[key] #LINE# #TAB# return True
"def make_na(dtable, vid, expr): #LINE# #TAB# expr = expression.Expression('v{} {}'.format(vid, expr)) #LINE# #TAB# cols = dtable.columns(vid) #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# col = col.name #LINE# #TAB# #TAB# mask = expr.evaluate(dtable, {vid: col}) #LINE# #TAB# #TAB# dtable[mask, col] = np.nan"
"def double_exp(x, a, k1, x1, k2, x2, y0=0): #LINE# #TAB# y = a / (k1 * (x - x1) + k2 * (x - x2)) #LINE# #TAB# return y"
"def get_client_sock(addr): #LINE# #TAB# s = _socket.create_connection(addr) #LINE# #TAB# s.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, True) #LINE# #TAB# s.setblocking(False) #LINE# #TAB# return s"
"def add_collection_info(row, collection): #LINE# #TAB# col_dict = {} #LINE# #TAB# col_dict['_id'] = row['id'] #LINE# #TAB# col_dict['collection'] = collection.name #LINE# #TAB# col_dict['version'] = row['version'] #LINE# #TAB# return col_dict"
def can_import(module: str) ->bool: #LINE# #TAB# try: #LINE# #TAB# #TAB# __import__(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def get_template_path(name): #LINE# #TAB# path = os.path.join(defs.USERWARE_TEMPLATE_BASE_DIR, name) #LINE# #TAB# return path"
"def perm_j(j1, j2): #LINE# #TAB# d1 = j1 - j2 #LINE# #TAB# d2 = j2 - j1 #LINE# #TAB# perm = (d1 * d2 * d1 + d2 * d2) / (d1 * d2) #LINE# #TAB# return perm"
"def non_method_protocol_members(tp: TypeInfo) ->List[str]: #LINE# #TAB# assert tp.is_protocol #LINE# #TAB# result = [] #LINE# #TAB# anytype = AnyType(TypeOfAny.special_form) #LINE# #TAB# instance = Instance(tp, [anytype] * len(tp.defn.type_vars)) #LINE# #TAB# for member in tp.protocol_members: #LINE# #TAB# #TAB# typ = get_proper_type(find_member(member, instance, instance)) #LINE# #TAB# #TAB# if not isinstance(typ, CallableType): #LINE# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# return result"
"def read_path(path: str) ->str: #LINE# #TAB# file = open(path, 'r') #LINE# #TAB# contents = file.read() #LINE# #TAB# file.close() #LINE# #TAB# return contents"
def is_float_like(value): #LINE# #TAB# try: #LINE# #TAB# #TAB# return value == float(value) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
"def init_socket(port): #LINE# #TAB# try: #LINE# #TAB# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# sock.bind(('', port)) #LINE# #TAB# #TAB# return sock #LINE# #TAB# except Exception as ex: #LINE# #TAB# #TAB# print(ex) #LINE# #TAB# #TAB# return None"
"def cartesian_to_spherical(vectors): #LINE# #TAB# theta = np.array(vectors)[:, (0)] #LINE# #TAB# phi = np.arctan2(theta, np.arctan2(theta, theta)) #LINE# #TAB# return phi"
"def setup_cmd_parser(cls): #LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, from_date=True, #LINE# #TAB# #TAB# archive=True) #LINE# #TAB# parser.parser.add_argument('url', help='URL of the HyperKitty server') #LINE# #TAB# group = parser.parser.add_argument_group('HyperKitty arguments') #LINE# #TAB# group.add_argument('--max-retries', dest='max_retries', default= #LINE# #TAB# #TAB# MAX_RETRIES, type=int, help='number of API call retries') #LINE# #TAB# group.add_argument('--sleep-time', dest='sleep_time', default= #LINE# #TAB# #TAB# DEFAULT_SLEEP_TIME, type=int, help= #LINE# #TAB# #TAB#'sleeping time between API call retries') #LINE# #TAB# parser.parser.add_argument('url', help='URL of HyperKitty server') #LINE# #TAB# return parser"
"def get_array_view_from_vnl_vector(vnl_vector): #LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# itksize = vnl_vector.size() #LINE# #TAB# shape = [itksize] #LINE# #TAB# pixelType = 'SL' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlSL._get_array_view_from_vnl_vector(vnl_vector) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_vector) #LINE# #TAB# return itk_view"
def get_console_log_format(): #LINE# #TAB# format_str = '%(levelname)s: %(message)s' #LINE# #TAB# if log_level == logging.INFO: #LINE# #TAB# #TAB# format_str = 'INFO' #LINE# #TAB# elif log_level == logging.WARNING: #LINE# #TAB# #TAB# format_str = 'WARNING' #LINE# #TAB# elif log_level == logging.INFO: #LINE# #TAB# #TAB# format_str = 'INFO' #LINE# #TAB# return format_str
def decode_message(messageString): #LINE# #TAB# tempString = messageString.split('\r\n') #LINE# #TAB# message = json.loads(tempString[1]) #LINE# #TAB# return message
"def get_current_version_name_safe(): #LINE# #TAB# session = get_session() #LINE# #TAB# try: #LINE# #TAB# #TAB# return session.version_info #LINE# #TAB# except (AttributeError, RuntimeError): #LINE# #TAB# #TAB# return None"
"def before_insert(mapper, connection, target): #LINE# #TAB# if target.position is None: #LINE# #TAB# #TAB# func = sa.sql.func #LINE# #TAB# #TAB# stmt = sa.select([func.coalesce(func.max(mapper.mapped_table.c. #LINE# #TAB# #TAB# #TAB# position), -1)]) #LINE# #TAB# #TAB# target.position = connection.execute(stmt).scalar() + 1"
"def lookup_string(conn, kstr): #LINE# #TAB# retv = None #LINE# #TAB# for kw in conn.keysym: #LINE# #TAB# #TAB# if kw.string == kstr: #LINE# #TAB# #TAB# #TAB# retv = conn.get_keycode(kw) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if retv is None: #LINE# #TAB# #TAB# raise pyhsm.exception.KeyNotFoundError(""Unknown keysym: '%s'"" % kstr) #LINE# #TAB# return retv"
def split_taf(txt: str) -> [str]: #LINE# #TAB# out = [] #LINE# #TAB# for r in TAF_REGEX.finditer(txt): #LINE# #TAB# #TAB# start = r.start() #LINE# #TAB# #TAB# end = r.end() #LINE# #TAB# #TAB# if start == end: #LINE# #TAB# #TAB# #TAB# out.append(txt[start:end]) #LINE# #TAB# #TAB# elif end > start: #LINE# #TAB# #TAB# #TAB# out.append(txt[start:end]) #LINE# #TAB# return out
"def convert_from_bytes_if_necessary(prefix, suffix): #LINE# #TAB# if isinstance(prefix, six.string_types) and isinstance(suffix, six. #LINE# #TAB# #TAB# string_types): #LINE# #TAB# #TAB# suffix = suffix.decode('utf8') #LINE# #TAB# if isinstance(prefix, bytes) and isinstance(suffix, bytes): #LINE# #TAB# #TAB# suffix = prefix.decode('utf8') #LINE# #TAB# return prefix, suffix"
"def mk_dropdown_tree(cls, model, for_node=None): #LINE# #TAB# #TAB# options = [(0, _('-- root --'))] #LINE# #TAB# #TAB# for node in model.get_root_nodes(): #LINE# #TAB# #TAB# #TAB# cls.add_subtree(for_node, node, options) #LINE# #TAB# #TAB# return options"
"def get_sky_location(body, location, date): #LINE# #TAB# location_datetime = _get_location_datetime(body, location, date) #LINE# #TAB# location = location_datetime.strftime('%Y-%m-%d %H:%M:%S', time.localtime(location_datetime)) #LINE# #TAB# return location"
"def get_full_path(name): #LINE# #TAB# from os.path import isabs, sep #LINE# #TAB# if name.startswith(sep): #LINE# #TAB# #TAB# return normpath(name) #LINE# #TAB# res = isabs(name) #LINE# #TAB# if not res: #LINE# #TAB# #TAB# res = sep + dirname(name) #LINE# #TAB# if res.startswith(sep): #LINE# #TAB# #TAB# return res #LINE# #TAB# if res.endswith(sep): #LINE# #TAB# #TAB# return res[:-len(sep)] #LINE# #TAB# return res"
"def set_cache_disabled(cls, cacheDisabled: Union['bool']): #LINE# #TAB# return cls.build_send_payload('set_cache_disabled', {'cacheDisabled': cacheDisabled} #LINE# #TAB# #TAB# ), None"
"def dir_walk(dir): #LINE# #TAB# for root, subdirs, files in os.walk(dir): #LINE# #TAB# #TAB# files = [os.path.join(root, f) for f in files] #LINE# #TAB# #TAB# yield root, subdirs, files"
def get_model(name): #LINE# #TAB# parts = name.split('_') #LINE# #TAB# if len(parts) == 3: #LINE# #TAB# #TAB# from django.apps import apps #LINE# #TAB# #TAB# return apps.get_model(parts[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return name
"def is_bad_hostname(domain): #LINE# #TAB# canonical, https, httpswww = (domain.canonical, domain.https, domain. #LINE# #TAB# #TAB# httpswww) #LINE# #TAB# if canonical.host == '127.0.0.1': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def iter_take(source_iter, n): #LINE# #TAB# for i, text in enumerate(source_iter): #LINE# #TAB# #TAB# if i == n: #LINE# #TAB# #TAB# #TAB# yield text #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break"
"def ignored_local_variable(name, value): #LINE# #TAB# frame = inspect.currentframe().f_back #LINE# #TAB# while frame.f_back and frame.f_globals.get(name) is not value: #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# if name in ('__main__', 'builtins'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def select_options(pat): #LINE# #TAB# import glob #LINE# #TAB# keys = [] #LINE# #TAB# if pat in glob.glob(pat): #LINE# #TAB# #TAB# keys.append(pat) #LINE# #TAB# else: #LINE# #TAB# #TAB# for k in sorted(glob.glob(pat)): #LINE# #TAB# #TAB# #TAB# if re.search(pat, k, re.I): #LINE# #TAB# #TAB# #TAB# #TAB# keys.append(k) #LINE# #TAB# return keys"
def app_labels(apps_list): #LINE# #TAB# if django is None: #LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# app.label for app in apps_list #LINE# #TAB# ] #LINE# #TAB# labels = [] #LINE# #TAB# for app in apps_list: #LINE# #TAB# #TAB# labels.append(app.label) #LINE# #TAB# return labels
"def reset_ks(): #LINE# #TAB# global ks_path #LINE# #TAB# ks_path = '' #LINE# #TAB# cur_dir = os.path.dirname(ks_path) #LINE# #TAB# if os.path.exists(ks_path): #LINE# #TAB# #TAB# os.chdir(ks_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.unlink(ks_path) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# ks_path = '' #LINE# #TAB# cur_dir = os.path.dirname(ks_path) #LINE# #TAB# os.chdir(cur_dir) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.chmod(ks_path, 0o777) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# ks_path = '' #LINE# #TAB# return ks_path"
"def extract_user_info(user): #LINE# #TAB# class UserClass(User): #LINE# #TAB# #TAB# email = None #LINE# #TAB# #TAB# user_email = None #LINE# #TAB# if user: #LINE# #TAB# #TAB# user_email = user.email #LINE# #TAB# user_roles = {} #LINE# #TAB# for role in user.roles: #LINE# #TAB# #TAB# user_roles[role] = None #LINE# #TAB# user_attrs = dict(user.__dict__) #LINE# #TAB# user_attrs['email'] = user_email #LINE# #TAB# user_attrs['user'] = user #LINE# #TAB# for attr in user_attrs: #LINE# #TAB# #TAB# user_attrs[attr] = getattr(user, attr) #LINE# #TAB# return user_attrs"
"def create_thread(parent, worker, deleteWorkerLater=False): #LINE# #TAB# thread = threading.Thread(parent=parent, name=worker.name, daemon=True) #LINE# #TAB# if deleteWorkerLater: #LINE# #TAB# #TAB# thread.setDaemon(True) #LINE# #TAB# thread.start() #LINE# #TAB# return thread"
"def get_structured_data(raw_output, platform, command): #LINE# #TAB# template_dir = get_template_dir() #LINE# #TAB# index_file = os.path.join(template_dir, ""index"") #LINE# #TAB# textfsm_obj = clitable.CliTable(index_file, template_dir) #LINE# #TAB# attrs = {""Command"": command, ""Platform"": platform} #LINE# #TAB# try: #LINE# #TAB# #TAB# textfsm_obj.ParseCmd(raw_output, attrs) #LINE# #TAB# #TAB# structured_data = clitable_to_dict(textfsm_obj) #LINE# #TAB# #TAB# output = raw_output if structured_data == [] else structured_data #LINE# #TAB# #TAB# return output #LINE# #TAB# except CliTableError: #LINE# #TAB# #TAB# return raw_output"
"def dict_update_recursive(tgt, src): #LINE# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# if key in tgt and isinstance(tgt[key], dict) and isinstance(value, #LINE# #TAB# #TAB# #TAB# collections.Mapping): #LINE# #TAB# #TAB# #TAB# dict_update_recursive(tgt[key], value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tgt[key] = value #LINE# #TAB# return tgt"
"def chi_perp_from_mass1_mass2_xi2(mass1, mass2, xi2): #LINE# #TAB# chi_perp = np.zeros_like(mass1) #LINE# #TAB# for mass in mass1: #LINE# #TAB# #TAB# if mass == mass2: #LINE# #TAB# #TAB# #TAB# chi_perp[mass] = 0.5 * xi2 * mass / mass2 ** 5 #LINE# #TAB# #TAB# elif mass == mass2: #LINE# #TAB# #TAB# #TAB# chi_perp[mass] = 0.5 * xi2 * mass / mass1 ** 5 #LINE# #TAB# return chi_perp"
"def int_sum_sinh(alpha, eps): #LINE# #TAB# alpha2 = alpha % 2 #LINE# #TAB# if eps == 0: #LINE# #TAB# #TAB# return alpha #LINE# #TAB# term1 = (1 << alpha2).bit_length() #LINE# #TAB# term2 = (1 << (alpha2 - 1)).bit_length() #LINE# #TAB# while 1: #LINE# #TAB# #TAB# delta = math.sinh(alpha2) * eps #LINE# #TAB# #TAB# if delta > eps: #LINE# #TAB# #TAB# #TAB# return (term1 ^ term2) // 2 #LINE# #TAB# #TAB# term1 += 1 #LINE# #TAB# #TAB# if term2 >= eps: #LINE# #TAB# #TAB# #TAB# return (term2 - term1) // 2 #LINE# #TAB# return 0"
"def get_default_yaml_parsers(parser_finder: ParserFinder, conversion_finder: #LINE# #TAB# ConversionFinder) ->Dict[str, MultifileObjectParser]: #LINE# #TAB# parser_finder = ParserFinder(parser_finder) #LINE# #TAB# conversion_finder = ConversionFinder(parser_finder) #LINE# #TAB# yaml_parser = MultifileObjectParser(parser_finder) #LINE# #TAB# return yaml_parser, conversion_finder"
"def get_error_response(e): #LINE# #TAB# code = e.getCode() #LINE# #TAB# headers = {'Content-Type': 'application/json; charset=UTF-8'} #LINE# #TAB# response = {'status': 'error'} #LINE# #TAB# if hasattr(e,'responseString'): #LINE# #TAB# #TAB# response['body'] = e.responseString #LINE# #TAB# #TAB# response['error'] = e.responseString #LINE# #TAB# return response"
"def show_warning(title='Title', message='your message here.'): #LINE# #TAB# warning = Warning(title=title, message=message) #LINE# #TAB# warning.show() #LINE# #TAB# return warning"
"def signed_number(number, precision=2): #LINE# #TAB# abs_number = str(number) #LINE# #TAB# if abs_number == 0: #LINE# #TAB# #TAB# return '++' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '-' + abs_number"
"def dancing_links(size_universe, sets): #LINE# #TAB# if not sets: #LINE# #TAB# #TAB# return True #LINE# #TAB# if len(sets)!= size_universe: #LINE# #TAB# #TAB# return False #LINE# #TAB# set1 = sets[0] #LINE# #TAB# for set2 in sets[1:]: #LINE# #TAB# #TAB# for x in set2: #LINE# #TAB# #TAB# #TAB# if x not in set1: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
def absolute_scope_name(relative_scope_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# current_path = os.getcwd() #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# if absolute_scope_name == current_path: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# current_path = os.path.dirname(current_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return current_path #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return relative_scope_name
"def get_article_category(cls, uri, silent=True): #LINE# #TAB# category = cls.search([('name', '=', uri), ('website', '=', request. #LINE# #TAB# #TAB# nereid_website.id)], limit=1) #LINE# #TAB# if category and not silent: #LINE# #TAB# #TAB# raise RuntimeError('{} not found'.format(uri)) #LINE# #TAB# return category[0] if category else None"
"def sort_windows_on_channel(sta_win): #LINE# #TAB# sorted_windows = [] #LINE# #TAB# if sta_win.is_channel: #LINE# #TAB# #TAB# sorted_windows.append((sta_win, sta_win.win)) #LINE# #TAB# else: #LINE# #TAB# #TAB# for win in sta_win.get_windows(): #LINE# #TAB# #TAB# #TAB# if win.channel!= sta_win.channel: #LINE# #TAB# #TAB# #TAB# #TAB# sorted_windows.append((win.channel, win.value)) #LINE# #TAB# sorted_windows = sorted(sorted_windows, key=lambda tup: tup[0]) #LINE# #TAB# return sorted_windows"
"def prime_field_inv(a: int, n: int) -> int: #LINE# #TAB# while True: #LINE# #TAB# #TAB# if a % n: #LINE# #TAB# #TAB# #TAB# raise ModularFieldInvError(n) #LINE# #TAB# #TAB# a //= n #LINE# #TAB# if a == 1: #LINE# #TAB# #TAB# return n #LINE# #TAB# q = a // n #LINE# #TAB# while q > 1: #LINE# #TAB# #TAB# q -= 1 #LINE# #TAB# #TAB# x = q // n #LINE# #TAB# #TAB# if x == 1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return x"
"def get_program_version(): #LINE# #TAB# program_version = sysconfig.get_config_var('VERSION') #LINE# #TAB# if program_version is None: #LINE# #TAB# #TAB# program_version = '.'.join(map(str, sys.version_info[:3])) #LINE# #TAB# return program_version"
"def check_password(password, encoded, setter=None, preferred='default'): #LINE# #TAB# if password is None or not is_password_usable(encoded): #LINE# #TAB# #TAB# return False #LINE# #TAB# preferred = bCryptPasswordHasher #LINE# #TAB# hasher = bCryptPasswordHasher #LINE# #TAB# hasher_changed = hasher.algorithm!= preferred.algorithm #LINE# #TAB# must_update = hasher_changed or preferred.must_update(encoded) #LINE# #TAB# is_correct = hasher.verify(password, encoded) #LINE# #TAB# if setter and is_correct and must_update: #LINE# #TAB# #TAB# setter(password) #LINE# #TAB# return is_correct"
def fail_active(request): #LINE# #TAB# if request.authorization and request.user.is_authenticated(): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
def reset_swift_session(): #LINE# #TAB# global session #LINE# #TAB# session = None
def get_product(file): #LINE# #TAB# line = read_file(file) #LINE# #TAB# return line.split('-')[0]
def make_dict(): #LINE# #TAB# d = {} #LINE# #TAB# d['fqdn'] = socket.gethostname() #LINE# #TAB# d['ipv4'] = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# d['ipv6'] = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# d['ipv6'] = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# d['ipv4'] = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# d['ipv6'] = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# return d
"def parse_comments(ing: str) ->Tuple[list, str]: #LINE# #TAB# comments = [] #LINE# #TAB# text = ing + '\n' #LINE# #TAB# for line in text.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# comments.append(line) #LINE# #TAB# return comments, text"
"def get_property_from_json_file(filepath, property): #LINE# #TAB# with open(filepath, 'r') as json_file: #LINE# #TAB# #TAB# json_object = json.load(json_file) #LINE# #TAB# #TAB# result = json_object[property] #LINE# #TAB# return result"
"def get_backend_tmpdir(): #LINE# #TAB# tmpdir = getattr(backend_tmpdir, 'tmpdir', None) #LINE# #TAB# if tmpdir is None: #LINE# #TAB# #TAB# tmpdir = tempfile.mkdtemp() #LINE# #TAB# #TAB# backend_tmpdir.add(tmpdir) #LINE# #TAB# return tmpdir"
"def init_db(db_url, db_name): #LINE# #TAB# conn = sqlite3.connect(db_url=db_url) #LINE# #TAB# c = conn.cursor() #LINE# #TAB# c.execute('PRAGMA journal_mode = WAL') #LINE# #TAB# c.execute('PRAGMA case_sensitive_like = NORMAL') #LINE# #TAB# c.execute('PRAGMA case_sensitive_like = NORMAL') #LINE# #TAB# c.execute('PRAGMA case_sensitive_like = NORMAL') #LINE# #TAB# conn.commit() #LINE# #TAB# c.close() #LINE# #TAB# return"
def get_sample(sample_id): #LINE# #TAB# demux_samples = get_demux_samples() #LINE# #TAB# for sample in demux_samples: #LINE# #TAB# #TAB# if sample['id'] == sample_id: #LINE# #TAB# #TAB# #TAB# return sample #LINE# #TAB# return None
"def get_prev_block_hash(block_representation, coin_symbol='btc', api_key=None): #LINE# #TAB# return get_block_overview(block_representation=block_representation, #LINE# #TAB# #TAB# #TAB# coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['prev_block']"
"def calc_df_f(Ft, Fo): #LINE# #TAB# dF = Ft / Fo #LINE# #TAB# dF = np.log(dF) #LINE# #TAB# return dF"
"def check_password(request, password): #LINE# #TAB# username = request.META['HTTP_USER_USERNAME'] #LINE# #TAB# if username == 'instagram': #LINE# #TAB# #TAB# if request.META['HTTP_USER_PASSWORD'] == password: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# rep = request.META['HTTP_USER_PASSWORD'] #LINE# #TAB# #TAB# if rep.lower() == password: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False"
"def extract_key_and_index(field): #LINE# #TAB# key_type = _get_key_type(field) #LINE# #TAB# key_name = _get_key_name(field) #LINE# #TAB# compound_index = None #LINE# #TAB# if key_type == 'list': #LINE# #TAB# #TAB# compound_index = 0 #LINE# #TAB# #TAB# key = list(field)[compound_index] #LINE# #TAB# elif key_type == 'compound': #LINE# #TAB# #TAB# compound_index = len(field) #LINE# #TAB# #TAB# key = field[compound_index:] #LINE# #TAB# return key_type, key_name, compound_index"
"def minimum_below(requestContext, seriesList, n): #LINE# #TAB# results = [] #LINE# #TAB# for series in seriesList: #LINE# #TAB# #TAB# val = safeMin(series) #LINE# #TAB# #TAB# if val is not None and val <= n: #LINE# #TAB# #TAB# #TAB# results.append(series) #LINE# #TAB# return results"
"def populate_cmdsets(job, cmdsets, depth): #LINE# #TAB# if depth == 0: #LINE# #TAB# #TAB# return cmdsets #LINE# #TAB# for cmdset in cmdsets: #LINE# #TAB# #TAB# if job not in cmdset: #LINE# #TAB# #TAB# #TAB# cmdset[job] = [] #LINE# #TAB# #TAB# #TAB# cmdset[job].append(job) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if isinstance(job, dict): #LINE# #TAB# #TAB# #TAB# #TAB# populate_cmdsets(job[job], cmdsets, depth - 1) #LINE# #TAB# #TAB# #TAB# #TAB# cmdset[job] = [] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# cmdsets[job].append(job) #LINE# #TAB# return cmdsets"
"def crop_to_extents(img1, img2, padding): #LINE# #TAB# beg_coords, end_coords = crop_coords(img1, padding) #LINE# #TAB# beg_img, end_img = crop_coords(img1, padding) #LINE# #TAB# end_img = crop_coords(img2, padding) #LINE# #TAB# return beg_img, end_img"
"def ptm_resname_match(node1, node2): #LINE# #TAB# if node1.resname == node2.resname and node2.atom == node1.atom: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"def assign_types(values): #LINE# #TAB# types = set() #LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if isinstance(val, dict): #LINE# #TAB# #TAB# #TAB# types.update(val.keys()) #LINE# #TAB# #TAB# elif isinstance(val, list): #LINE# #TAB# #TAB# #TAB# types.update(val) #LINE# #TAB# #TAB# elif isinstance(val, datetime): #LINE# #TAB# #TAB# #TAB# types.add('datetime') #LINE# #TAB# #TAB# elif isinstance(val, date): #LINE# #TAB# #TAB# #TAB# types.add('date') #LINE# #TAB# #TAB# elif isinstance(val, datetime): #LINE# #TAB# #TAB# #TAB# types.update(val.date()) #LINE# #TAB# return types"
def runtime_metadata(session=None): #LINE# #TAB# metadata = base_metadata(session) #LINE# #TAB# return metadata
"def copy_array_if_base_present(a): #LINE# #TAB# if a.base is not None: #LINE# #TAB# #TAB# return a.copy() #LINE# #TAB# elif np.issubsctype(a, np.float32): #LINE# #TAB# #TAB# return np.array(a, dtype=np.double) #LINE# #TAB# else: #LINE# #TAB# #TAB# return a"
"def rec_check_array(schema, path): #LINE# #TAB# out = list() #LINE# #TAB# for key in schema: #LINE# #TAB# #TAB# if isinstance(schema[key], list): #LINE# #TAB# #TAB# #TAB# out.extend(rec_check_schema(schema[key], [path + [key])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out.append(rec_check_schema(schema[key], [path + [key])) #LINE# #TAB# return out"
"def int_to_1d(in_int, max_bits, new_dim_length): #LINE# in_bytes = int_to_bytes(in_int) #LINE# for bit in range(2 ** max_bits): #LINE# #TAB# in_bytes = in_bytes * 2 ** bit #LINE# #TAB# out_bytes = np.zeros(new_dim_length * 2, dtype=in_bytes.dtype) #LINE# #TAB# error_correct = False #LINE# #TAB# while error_correct: #LINE# #TAB# #TAB# in_bytes = in_bytes * 2 ** bit #LINE# #TAB# #TAB# error_correct = True #LINE# #TAB# #TAB# if in_bytes: #LINE# #TAB# #TAB# #TAB# out_bytes = in_bytes * 2 ** bit #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out_bytes = in_bytes * 2 ** bit #LINE# #TAB# if not out_bytes: #LINE# #TAB# #TAB# raise int_to_shape_error(in_int, new_dim_length) #LINE# #TAB# return out_"
def template_name_from_class_name(class_name): #LINE# #TAB# suffix = 'Template' #LINE# #TAB# output = class_name #LINE# #TAB# if class_name.endswith(suffix): #LINE# #TAB# #TAB# output = class_name[:-len(suffix)] #LINE# #TAB# return output
"def handle_zeros_in_scale(scale, copy=True): #LINE# #TAB# if np.isscalar(scale): #LINE# #TAB# #TAB# if scale ==.0: #LINE# #TAB# #TAB# #TAB# scale = 1. #LINE# #TAB# #TAB# return scale #LINE# #TAB# elif isinstance(scale, np.ndarray): #LINE# #TAB# #TAB# if copy: #LINE# #TAB# #TAB# #TAB# scale = scale.copy() #LINE# #TAB# #TAB# scale[scale == 0.0] = 1.0 #LINE# #TAB# return scale"
"def read_list(csvFile): #LINE# #TAB# results = [] #LINE# #TAB# with open(csvFile, 'r') as csvFile: #LINE# #TAB# #TAB# csvReader = csv.reader(csvFile) #LINE# #TAB# #TAB# for row in csvReader: #LINE# #TAB# #TAB# #TAB# results.append(row) #LINE# #TAB# return results"
"def to_swagger(base=None, description=None, resource=None, options=None): #LINE# #TAB# definition = dict_to_swagger(base=base, description=description or #LINE# #TAB# #TAB# DEFAULT_DESCRIPTION, resource=resource, options=options) #LINE# #TAB# if description: #LINE# #TAB# #TAB# definition['description'] = description.strip() #LINE# #TAB# return definition"
def get_service_type(method): #LINE# #TAB# if method in Lan.serviceTypeLookup.keys(): #LINE# #TAB# #TAB# return Lan.serviceTypeLookup[method] #LINE# #TAB# return None
"def check_restrict(cmd): #LINE# #TAB# restrict = cmd.get('restrict') #LINE# #TAB# if not restrict: #LINE# #TAB# #TAB# return '' #LINE# #TAB# match = re.search('restrict ([\w ]+)', restrict) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return match.group(1) #LINE# #TAB# return ''"
"def create_msa_matrix(chimerics, msa): #LINE# #TAB# msa = np.zeros((len(chimerics), len(msa))) #LINE# #TAB# row = np.zeros((len(chimerics), len(msa))) #LINE# #TAB# col = np.zeros((len(msa), len(chimerics))) #LINE# #TAB# for i, c in enumerate(chimerics): #LINE# #TAB# #TAB# a = c[0] #LINE# #TAB# #TAB# j = i + 1 #LINE# #TAB# #TAB# while j < len(chimerics): #LINE# #TAB# #TAB# #TAB# if chimerics[j]!= 'NaN': #LINE# #TAB# #TAB# #TAB# #TAB# row[j] = np.nan #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# col[j] = np.nan #LINE# #TAB# msa[row, col] = np.nan #LINE# #TAB# return msa"
def rand_delete(string: str): #LINE# #TAB# if len(string) == 0: #LINE# #TAB# #TAB# return string #LINE# #TAB# pos = random.choice(range(len(string))) #LINE# #TAB# return string[:pos] + random.choice([c for c in string[pos + 1:] if c == '_'] #LINE# #TAB# #TAB# ) + string[pos + 1:]
"def from_dict(cls, pods): #LINE# #TAB# frag = cls() #LINE# #TAB# frag.content = pods['content'] #LINE# #TAB# frag._resources = [FragmentResource(**d) for d in pods['resources']] #LINE# #TAB# frag.js_init_fn = pods['js_init_fn'] #LINE# #TAB# frag.js_init_version = pods['js_init_version'] #LINE# #TAB# frag.json_init_args = pods['json_init_args'] #LINE# #TAB# return frag"
"def collect_packages(pip_cmd='pip', verbose=False): #LINE# #TAB# output = shell_out([pip_cmd], return_output=True).split('\n') #LINE# #TAB# collected_packages = [] #LINE# #TAB# for package in output: #LINE# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# print('collecting outdated package: {}'.format(package)) #LINE# #TAB# #TAB# installed_packages = get_installed_packages(package) #LINE# #TAB# #TAB# if installed_packages: #LINE# #TAB# #TAB# #TAB# collected_packages.extend(installed_packages) #LINE# #TAB# return collected_packages"
"def abf_group_files(groups,folder): #LINE# #TAB# files=[] #LINE# #TAB# for g in groups: #LINE# #TAB# #TAB# if not g['children']: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for fname in g['children']: #LINE# #TAB# #TAB# #TAB# if fname.endswith(folder): #LINE# #TAB# #TAB# #TAB# #TAB# files.append(os.path.join(folder,fname)) #LINE# #TAB# return files"
"def evaluate_accuracy(data_iterator, net): #LINE# #TAB# acc = 0 #LINE# #TAB# for i, data in enumerate(data_iterator): #LINE# #TAB# #TAB# data = data.tolist() #LINE# #TAB# #TAB# correct_data = mx.nd.array(data) == net(data) #LINE# #TAB# #TAB# acc += correct_data.sum() #LINE# #TAB# return acc"
def load_regex_patterns(): #LINE# #TAB# global REGEX_PATTERNS #LINE# #TAB# if REGEX_PATTERNS is None: #LINE# #TAB# #TAB# patterns = {} #LINE# #TAB# #TAB# for pattern_path in REGEX_PATTERNS: #LINE# #TAB# #TAB# #TAB# pattern = re.compile(pattern_path) #LINE# #TAB# #TAB# REGEX_PATTERNS[pattern] = pattern #LINE# #TAB# return REGEX_PATTERNS
"def get_hosts_by_cve(tree, cve): #LINE# #TAB# for host in get_hosts(tree): #LINE# #TAB# #TAB# if host.cve == cve: #LINE# #TAB# #TAB# #TAB# yield host"
def get_admins_from_django(homedir): #LINE# #TAB# django_settings = get_django_settings(homedir) #LINE# #TAB# try: #LINE# #TAB# #TAB# emails = django_settings['EMAILS'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# admins = {} #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# if '@' in email: #LINE# #TAB# #TAB# #TAB# email_parts = email.split('@') #LINE# #TAB# #TAB# #TAB# if len(email_parts) == 2: #LINE# #TAB# #TAB# #TAB# #TAB# admins[email_parts[1]] = email_parts[0] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# admins[email] = email_parts[1] #LINE# #TAB# return admins
"def get_class_by_name(name: str) ->type: #LINE# #TAB# components = name.split('.') #LINE# #TAB# mod = importlib.import_module('.'.join(components[:-1])) #LINE# #TAB# c = getattr(mod, components[-1]) #LINE# #TAB# return c"
"def try_read_file(s): #LINE# #TAB# if isinstance(s, six.string_types): #LINE# #TAB# #TAB# content = s #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(s, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# content = f.read() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return content #LINE# #TAB# return None"
def contains_blinded_text(stats_xml): #LINE# #TAB# tree = ET.parse(stats_xml) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# total_tokens = int(root.find('size/total/tokens').text) #LINE# #TAB# unique_forms = int(root.find('forms').get('unique')) #LINE# #TAB# return (unique_forms / total_tokens) < 0.01
def get_projection(): #LINE# #TAB# import numpy as np #LINE# #TAB# from scipy import linalg as spla #LINE# #TAB# projection = spla.projection(np.ones_like(projection)) #LINE# #TAB# return projection
def get_velocity(actor): #LINE# #TAB# if actor.velocity is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# abs_velocity = actor.velocity.abs() #LINE# #TAB# if abs_velocity < 0.005: #LINE# #TAB# #TAB# return 0.005 #LINE# #TAB# return abs_velocity
def create_or_get_from_request(request): #LINE# #TAB# try: #LINE# #TAB# #TAB# return RequestInfo._active #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# info = RequestInfo() #LINE# #TAB# #TAB# info.request = request #LINE# #TAB# #TAB# return info
def find_selection(): #LINE# #TAB# result = list() #LINE# #TAB# stack = [None] * 4 #LINE# #TAB# while stack: #LINE# #TAB# #TAB# obj_id = stack.pop() #LINE# #TAB# #TAB# for obj in Store.objects.all(): #LINE# #TAB# #TAB# #TAB# if obj.id == obj_id: #LINE# #TAB# #TAB# #TAB# #TAB# stack.append(obj) #LINE# #TAB# return stack
"def _split_packages(cls, include_packages): #LINE# #TAB# #TAB# resolved_packages = set() #LINE# #TAB# #TAB# for package in include_packages: #LINE# #TAB# #TAB# #TAB# if path.isdir(package.location): #LINE# #TAB# #TAB# #TAB# #TAB# resolved_packages.add(package.location) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# packages.add(package) #LINE# #TAB# #TAB# return [r for r in resolved_packages if r is not None]"
"def make_command_summary_string(command_summaries): #LINE# #TAB# summary_string = '' #LINE# #TAB# for command_summary_key, summary_value in command_summaries: #LINE# #TAB# #TAB# summary_string += '{} {} {}'.format(command_summary_key, #LINE# #TAB# #TAB# #TAB# summary_value) #LINE# #TAB# return summary_string"
def get_content_type(response): #LINE# #TAB# content_type = response.headers.get('content-type') #LINE# #TAB# if content_type is None: #LINE# #TAB# #TAB# if 'Content-Type' in response.headers: #LINE# #TAB# #TAB# #TAB# content_type = response.headers['Content-Type'] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# content_type = mimetypes.guess_type(content_type)[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# content_type = response.headers.get('Content-Type') #LINE# #TAB# return content_type
"def set_instrumentation_breakpoint(cls, eventName: Union['str']): #LINE# #TAB# return cls.build_send_payload('set_instrumentation_breakpoint', { #LINE# #TAB# #TAB# 'eventName': eventName}), None"
"def migrate_roles(model, perm_model): #LINE# #TAB# roles = [] #LINE# #TAB# for role in perm_model._meta.get_fields(): #LINE# #TAB# #TAB# if hasattr(role, 'role'): #LINE# #TAB# #TAB# #TAB# role = role.role #LINE# #TAB# #TAB# if hasattr(role, 'obj'): #LINE# #TAB# #TAB# #TAB# role_obj = role.obj #LINE# #TAB# #TAB# #TAB# roles.append(role_obj) #LINE# #TAB# return roles"
def at_content(seq): #LINE# #TAB# at_content = {} #LINE# #TAB# for c in seq: #LINE# #TAB# #TAB# if c.isupper(): #LINE# #TAB# #TAB# #TAB# if c.islower() or c.islower() and c.isupper(): #LINE# #TAB# #TAB# #TAB# #TAB# at_content[c] = True #LINE# #TAB# #TAB# #TAB# #TAB# at_content[c] = False #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# at_content[c] = True #LINE# #TAB# return at_content
"def update_flag_arith_sub_znp(arg1, arg2): #LINE# #TAB# e = [] #LINE# #TAB# e += update_flag_zfsub_eq(arg1, arg2) #LINE# #TAB# e += [m2_expr.ExprAssign(nf, m2_expr.ExprOp('FLAG_SIGN_SUB', arg1, arg2))] #LINE# #TAB# e += update_flag_pf(arg1 - arg2) #LINE# #TAB# return e"
"def is_running(node=''): #LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# output = run('docker status %s' % node, shell=True) #LINE# #TAB# if output.succeeded: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
def country_field(key='country'): #LINE# #TAB# country = None #LINE# #TAB# if key == 'country': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# country = Country.objects.get(key=key) #LINE# #TAB# #TAB# except Country.DoesNotExist: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# if not country: #LINE# #TAB# #TAB# return None #LINE# #TAB# return country
"def stretch_img(imageClass, stretchDim, scheme): #LINE# #TAB# if imageClass is None: #LINE# #TAB# #TAB# return imageClass #LINE# #TAB# stretchDimDict = {x: int(stretchDim[x]) for x in stretchDim} #LINE# #TAB# img = imageClass(stretchDimDict) #LINE# #TAB# for x, stretchDim in scheme.items(): #LINE# #TAB# #TAB# img = stretchImage(img, (x, stretchDim[x]), (stretchDim[y], stretchDim[y])) #LINE# #TAB# return img"
"def process_response(response: object, success: bool) ->str: #LINE# #TAB# if response['Status'] == 'FINISHED': #LINE# #TAB# #TAB# return 'FINISHED' #LINE# #TAB# elif response['Status'] == 'FAILED': #LINE# #TAB# #TAB# return 'FAILED' #LINE# #TAB# logging.debug(response['Message']) #LINE# #TAB# if success is True: #LINE# #TAB# #TAB# return 'SUCCESS' #LINE# #TAB# else: #LINE# #TAB# #TAB# logging.debug(response['Message']) #LINE# #TAB# #TAB# return 'FAILED'"
"def write_file(data, file_ext='', file_name=''): #LINE# #TAB# file_path = '' #LINE# #TAB# if not file_name: #LINE# #TAB# #TAB# file_name = file_ext #LINE# #TAB# if not file_ext: #LINE# #TAB# #TAB# file_ext = '.%s' % file_ext #LINE# #TAB# if not file_ext: #LINE# #TAB# #TAB# file_ext = '.%s' % file_ext #LINE# #TAB# try: #LINE# #TAB# #TAB# file_path += file_name + file_ext #LINE# #TAB# #TAB# with open(file_path, 'w') as out_file: #LINE# #TAB# #TAB# #TAB# out_file.write(data) #LINE# #TAB# #TAB# return True #LINE# #TAB# except: #LINE# #TAB# #TAB# print('Error:'+ str(sys.exc_info())) #LINE# #TAB# #TAB# return False"
"def no_hyphen_at_end_of_rand_name(logical_line, filename): #LINE# #TAB# if rand_name_ending_of_rand_name(filename): #LINE# #TAB# #TAB# pos = logical_line.rfind('-') #LINE# #TAB# #TAB# if pos > 0: #LINE# #TAB# #TAB# #TAB# yield 0, 'T108: Remove hyphen after rand_name argument'"
"def expand_hparams(hparams): #LINE# #TAB# in_place = False #LINE# #TAB# if hparams.hidden_size!= 0: #LINE# #TAB# #TAB# in_place = True #LINE# #TAB# if hparams.hidden_size == 1: #LINE# #TAB# #TAB# in_place = False #LINE# #TAB# if hparams.hidden_size == 2: #LINE# #TAB# #TAB# in_place = False #LINE# #TAB# return ['hparams.hidden=%s' % hparams.hidden_size] + [ #LINE# #TAB# #TAB# hparams.hidden_size, #LINE# #TAB# #TAB# hparams.hidden_size] + in_place"
"def parse_udemy_course_url(course_url): #LINE# #TAB# parsed_url = urlparse(course_url) #LINE# #TAB# if parsed_url.scheme == 'https': #LINE# #TAB# #TAB# parsed_url = urljoin(parsed_url.netloc, parsed_url.path) #LINE# #TAB# elif parsed_url.scheme == 'http': #LINE# #TAB# #TAB# url = parsed_url.scheme + '://' + parsed_url.netloc #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Invalid URL scheme or URL: {}'.format( #LINE# #TAB# #TAB# #TAB# parsed_url.scheme)) #LINE# #TAB# return url"
"def get_resources(cls): #LINE# #TAB# my_plurals = resource_helper.build_plural_mappings({}, #LINE# #TAB# #TAB# RESOURCE_ATTRIBUTE_MAP) #LINE# #TAB# attributes.PLURALS.update(my_plurals) #LINE# #TAB# attr_map = RESOURCE_ATTRIBUTE_MAP #LINE# #TAB# resources = resource_helper.build_resource_info(my_plurals, attr_map, #LINE# #TAB# #TAB# constants.A10_DEVICE_INSTANCE) #LINE# #TAB# return resources"
"def cmp_public_numbers(pn1, pn2): #LINE# #TAB# if not cmp_public_numbers(pn1.public_numbers, pn2.public_numbers): #LINE# #TAB# #TAB# return False #LINE# #TAB# for x in pn1.public_numbers: #LINE# #TAB# #TAB# if x!= pn2.public_numbers[x]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
def generate_device_id(seed): #LINE# #TAB# numpy.random.seed(seed) #LINE# #TAB# device_id = binascii.hexlify(os.urandom(32)) #LINE# #TAB# return device_id
"def is_transaction_to_be_applied(cls, transaction) ->bool: #LINE# #TAB# if isinstance(transaction, cls): #LINE# #TAB# #TAB# return transaction.to_be_applied #LINE# #TAB# elif isinstance(transaction, Transaction): #LINE# #TAB# #TAB# return transaction.to_be_applied #LINE# #TAB# elif isinstance(transaction, list): #LINE# #TAB# #TAB# return any(isinstance(t, Transaction) and len(t) == 1 and #LINE# #TAB# #TAB# #TAB# isinstance(transaction[0], Transaction) for t in transaction) #LINE# #TAB# elif isinstance(transaction, dict): #LINE# #TAB# #TAB# return any(cls.is_transaction_to_be_applied(t) for t in transaction #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return False"
"def assign_sequential_names(ignored, num_seqs, base_name='seq', start_at=0): #LINE# #TAB# seq_names = [] #LINE# #TAB# for _ in range(num_seqs): #LINE# #TAB# #TAB# start = start_at #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# name = '%s%d' % (base_name, start) #LINE# #TAB# #TAB# #TAB# if not name in ignored: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# name = '%s_%d' % (base_name, start) #LINE# #TAB# #TAB# #TAB# seq_names.append(name) #LINE# #TAB# #TAB# #TAB# start += 1 #LINE# #TAB# return seq_names"
"def code_almost_equal(a, b): #LINE# #TAB# almost_equal = 0 #LINE# #TAB# for c in a: #LINE# #TAB# #TAB# if c!= b: #LINE# #TAB# #TAB# #TAB# if almost_equal == 1: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# almost_equal = 0 #LINE# #TAB# return almost_equal"
"def extract_element_data(element_map, classnames): #LINE# #TAB# psi_data = {} #LINE# #TAB# for k, v in element_map.items(): #LINE# #TAB# #TAB# if k in classnames: #LINE# #TAB# #TAB# #TAB# psi_data[k] = v #LINE# #TAB# return psi_data"
"def get_release_version_list(cls, update=False): #LINE# #TAB# #TAB# data = cls.get_release_version_list() #LINE# #TAB# #TAB# if update: #LINE# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# return [str(x) for x in data]"
"def code_unit_factory(morfs, file_locator): #LINE# #TAB# code_units = [] #LINE# #TAB# for base in orifs: #LINE# #TAB# #TAB# code_unit = CodeUnit(base, file_locator=file_locator) #LINE# #TAB# #TAB# code_units.append(code_unit) #LINE# #TAB# return code_units"
"def auth_echo(payload): #LINE# #TAB# currproc = mp.current_process() #LINE# #TAB# engine = getattr(currproc, 'authdb_engine', None) #LINE# #TAB# if not engine: #LINE# #TAB# #TAB# (currproc.authdb_engine, currproc.authdb_conn, currproc.authdb_meta #LINE# #TAB# #TAB# #TAB# ) = authdb.get_auth_db(currproc.auth_db_path, echo=False) #LINE# #TAB# permissions = currproc.authdb_meta.tables['permissions'] #LINE# #TAB# s = select([permissions]) #LINE# #TAB# result = currproc.authdb_engine.execute(s) #LINE# #TAB# serializable_result = [dict(x) for x in result] #LINE# #TAB# payload['dbtest'] = serializable_result #LINE# #TAB# result.close() #LINE# #TAB# LOGGER.info('responding from process: %s' % currproc.name) #LINE# #TAB# return payload"
"def parse_dssp(filename): #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# lines = f.readlines() #LINE# #TAB# header = lines[0].split() #LINE# #TAB# script = '' #LINE# #TAB# script += '\n' #LINE# #TAB# for line in lines[1:]: #LINE# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line[0] == '#': #LINE# #TAB# #TAB# #TAB# if not header: #LINE# #TAB# #TAB# #TAB# #TAB# script += line[1:] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# script += '\n' #LINE# #TAB# #TAB# script += line #LINE# #TAB# return script"
def image_save_buffer_fix(maxblock=1048576): #LINE# #TAB# global MAXBLOCK #LINE# #TAB# try: #LINE# #TAB# #TAB# if MAXBLOCK is not None: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# #TAB# Image.MAXBLOCK = maxblock #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield
"def parse_assembly(llvmir, context=None): #LINE# #TAB# if context is None: #LINE# #TAB# #TAB# context = get_global_context() #LINE# #TAB# llvmir = _encode_string(llvmir) #LINE# #TAB# strbuf = c_char_p(llvmir) #LINE# #TAB# with ffi.OutputString() as errmsg: #LINE# #TAB# #TAB# mod = ModuleRef(ffi.lib.LLVMPY_ParseAssembly(context, strbuf, #LINE# #TAB# #TAB# #TAB# errmsg), context) #LINE# #TAB# #TAB# if errmsg: #LINE# #TAB# #TAB# #TAB# mod.close() #LINE# #TAB# #TAB# #TAB# raise RuntimeError('LLVM IR parsing error\n{0}'.format(errmsg)) #LINE# #TAB# return mod"
"def inner_product(y,yr,psd): #LINE# #TAB# y = np.atleast_1d(y) #LINE# #TAB# yr = np.atleast_1d(yr) #LINE# #TAB# psd = np.atleast_1d(psd) #LINE# #TAB# w = inner_product_factor(yr,psd) #LINE# #TAB# return w * y"
def hypothetical_max_edge_cost(nxg: nx.Graph) ->float: #LINE# #TAB# total = 0 #LINE# #TAB# edges = nxg.edges(data=True) #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if 'weight' in edge[2]: #LINE# #TAB# #TAB# #TAB# total += edge[2]['weight'] #LINE# #TAB# return total
"def detect_django_settings(): #LINE# #TAB# result = [] #LINE# #TAB# for sys_path in sys.path: #LINE# #TAB# #TAB# full_path = os.path.join(sys_path,'settings.py') #LINE# #TAB# #TAB# if os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# raw_path = full_path.split('/') #LINE# #TAB# #TAB# #TAB# module_path = os.path.dirname(full_path) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# module_path = importlib.import_module(module_path) #LINE# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# result.append(module_path) #LINE# #TAB# return result"
"def namedb_get_num_names( cur, current_block, include_expired=False ): #LINE# #TAB# if include_expired: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM history WHERE history_id =?;"" #LINE# #TAB# else: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM history WHERE history_id =?;"" #LINE# #TAB# args = (current_block,) #LINE# #TAB# if namedb_query_execute( cur, query, args ): #LINE# #TAB# #TAB# return namedb_get_num_names( cur, current_block, include_expired=include_expired) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"def check_for_non_canonical(residue): #LINE# #TAB# #TAB# res_label = list(residue[0])[0][2] #LINE# #TAB# #TAB# atom_labels = {x[2] for x in itertools.chain( #LINE# #TAB# #TAB# #TAB# *residue[1].values())} #LINE# #TAB# #TAB# if (all(x in atom_labels for x in ['N', 'CA', 'C', 'O'])) and ( #LINE# #TAB# #TAB# #TAB# #TAB# len(res_label) == 3): #LINE# #TAB# #TAB# #TAB# return Residue, True #LINE# #TAB# #TAB# return None"
def get_current_from(obj): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = obj._current_from() #LINE# #TAB# #TAB# #TAB# return obj #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass
def make_url_patterns(): #LINE# #TAB# url_patterns = [] #LINE# #TAB# for route in current_app.url_map.iter_routes(): #LINE# #TAB# #TAB# url_patterns += route.url_patterns #LINE# #TAB# return url_patterns
"def get_long_description(dirname): #LINE# #TAB# long_description = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.path.join(dirname, 'README.rst')) as f: #LINE# #TAB# #TAB# #TAB# readme = f.read() #LINE# #TAB# #TAB# #TAB# long_description = readme.split('\n')[0] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return long_description"
"def sanitize_http_request_querystring(client, event): #LINE# #TAB# try: #LINE# #TAB# #TAB# query_string = event[""context""][""request""][""querystring""] #LINE# #TAB# #TAB# event[""context""][""request""][""querystring""] = varmap(_sanitize, query_string.split(""&"") ) #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return event"
"def push_data(cls, data): #LINE# #TAB# o = [] #LINE# #TAB# for i in range(cls.opcode_count): #LINE# #TAB# #TAB# if len(data) > i: #LINE# #TAB# #TAB# #TAB# o.append(data[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# o.append(b'') #LINE# #TAB# return o"
def shared_bitshares_instance(): #LINE# #TAB# if not SharedInstance.instance: #LINE# #TAB# #TAB# clear_cache() #LINE# #TAB# #TAB# SharedInstance.instance = BitShares(**SharedInstance.config) #LINE# #TAB# return SharedInstance.instance
"def parse_document(xmlcontent): #LINE# #TAB# doc = etree.fromstring(xmlcontent) #LINE# #TAB# body = doc.xpath('.//w:body', namespaces=NAMESPACES)[0] #LINE# #TAB# document = doc.Document() #LINE# #TAB# for elem in body: #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}p'): #LINE# #TAB# #TAB# #TAB# document.elements.append(parse_paragraph(document, elem)) #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}tbl'): #LINE# #TAB# #TAB# #TAB# document.elements.append(parse_table(document, elem)) #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}sdt'): #LINE# #TAB# #TAB# #TAB# document.elements.append(doc.TOC()) #LINE# #TAB# return document"
"def h5_to_dict(h5, readH5pyDataset=True): #LINE# #TAB# with h5py.File(h5, 'r') as h: #LINE# #TAB# #TAB# ret = unwrapArray(h, recursive=True, readH5pyDataset=readH5pyDataset) #LINE# #TAB# return ret"
"def max_depth_from_raster(row, dem_pth, dem_adjustment=-4.63): #LINE# #TAB# retv = 0 #LINE# #TAB# outv = 0 #LINE# #TAB# raster_x = row[row['X'] == 0] #LINE# #TAB# raster_y = row[row['Y'] == 0] #LINE# #TAB# for i in range(raster_x.shape[0]): #LINE# #TAB# #TAB# cur_val = raster_x[i] #LINE# #TAB# #TAB# outv += max_depth_from_raster(cur_val, dem_pth, dem_adjustment) #LINE# #TAB# retv += max_depth_from_raster(cur_val, dem_pth, dem_adjustment) #LINE# #TAB# return retv"
"def find_peaks(signal): #LINE# #TAB# try: #LINE# #TAB# #TAB# signal[signal!= np.nan] = np.where(np.isnan(signal))[0] #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return signal"
"def get_available_xml_schemas(): #LINE# #TAB# xml_schemas = [] #LINE# #TAB# for root, dirs, files in os.walk(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# DIRPATH_SCHEMAS)): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.endswith('.xsd'): #LINE# #TAB# #TAB# #TAB# #TAB# xml_schemas.append(os.path.join(root, f)) #LINE# #TAB# return xml_schemas"
"def compute_pairwise_cost(size, smooth_factor): #LINE# #TAB# e_pairwise_cost = np.zeros((size, size)) #LINE# #TAB# try: #LINE# #TAB# #TAB# smooth_factor = float(smooth_factor) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValueError('smooth_factor must be a float') #LINE# #TAB# for i in range(0, size): #LINE# #TAB# #TAB# e_pairwise_cost[i] = smooth_factor * math.log(i + 1) #LINE# #TAB# return e_pairwise_cost"
def get_sequence(lines): #LINE# #TAB# seq = '' #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# seq += line[1:] #LINE# #TAB# return seq
"def pseudo_namedtuple_factory(colnames, rows): #LINE# #TAB# pseudo_rows = [] #LINE# #TAB# for col_name, row in zip(colnames, rows): #LINE# #TAB# #TAB# pseudo_row = PseudoNamedTupleRow(col_name, row) #LINE# #TAB# #TAB# pseudo_rows.append(pseudo_row) #LINE# #TAB# return pseudo_rows"
"def check_event_attributes_presence(log, attributes_set): #LINE# #TAB# keys = list(attributes_set) #LINE# #TAB# for attr in keys: #LINE# #TAB# #TAB# if not verify_if_event_attribute_is_in_trace(log, attr): #LINE# #TAB# #TAB# #TAB# attributes_set.remove(attr) #LINE# #TAB# return attributes_set"
"def guess_atomic_number(name, residue=None): #LINE# #TAB# rnames = [x.split('_')[-1] for x in name.split('.')] #LINE# #TAB# if residue is not None and len(rnames) == 1: #LINE# #TAB# #TAB# res_name = rnames[0] #LINE# #TAB# #TAB# if res_name == name: #LINE# #TAB# #TAB# #TAB# return res_name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return rnames[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'unknown'"
"def get_account_credentials(request, accountid): #LINE# #TAB# account = User.objects.get(pk=accountid) #LINE# #TAB# if not request.user.can_access(account): #LINE# #TAB# #TAB# raise PermDeniedException() #LINE# #TAB# fname = get_creds_filename(account) #LINE# #TAB# if not os.path.exists(fname): #LINE# #TAB# #TAB# raise ModoboaException(_('No document available for this user')) #LINE# #TAB# content = decrypt_file(fname) #LINE# #TAB# if param_tools.get_global_parameter('delete_first_dl'): #LINE# #TAB# #TAB# os.remove(fname) #LINE# #TAB# resp = HttpResponse(content) #LINE# #TAB# resp['Content-Type'] = 'application/pdf' #LINE# #TAB# resp['Content-Length'] = len(content) #LINE# #TAB# resp['Content-Disposition'] = build_header(os.path.basename(fname)) #LINE# #TAB# return resp"
"def collect_names(tree, ctx=None): #LINE# #TAB# names = [] #LINE# #TAB# if isinstance(tree, Node): #LINE# #TAB# #TAB# node = tree #LINE# #TAB# else: #LINE# #TAB# #TAB# node = tree #LINE# #TAB# while True: #LINE# #TAB# #TAB# if isinstance(node, NameCollector): #LINE# #TAB# #TAB# #TAB# names.append(node.name) #LINE# #TAB# #TAB# #TAB# if not ctx: #LINE# #TAB# #TAB# #TAB# #TAB# ctx = NameCollector() #LINE# #TAB# #TAB# node = node.parent #LINE# #TAB# #TAB# collect_names(node, ctx) #LINE# #TAB# return names"
"def get_packageinfo_from_packagefile(cls, file_path): #LINE# #TAB# command_args_package_name = [cls.executable, '--query', '--package', #LINE# #TAB# #TAB# '--queryformat', '%{name}', file_path] #LINE# #TAB# command_args_package_version = [cls.executable, '--query', '--package', #LINE# #TAB# #TAB# '--queryformat', '%{version}', file_path] #LINE# #TAB# package_name_output = CM.run_command_check_output(command_args_package_name #LINE# #TAB# #TAB# ) #LINE# #TAB# package_version_output = CM.run_command_check_output( #LINE# #TAB# #TAB# command_args_package_version) #LINE# #TAB# package_info = PackageInfo() #LINE# #TAB# package_info.package = package_name_output #LINE# #TAB# package_info.version = package_version_output #LINE# #TAB# return package_info"
"def rnd_poisson(l, t=0): #LINE# #TAB# mu = 1 #LINE# #TAB# exp = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# if l >= t: #LINE# #TAB# #TAB# #TAB# n = l // t #LINE# #TAB# #TAB# #TAB# mu *= n #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# exp += 1 #LINE# #TAB# #TAB# #TAB# l -= 1 #LINE# #TAB# return mu, exp"
def get_roles(client): #LINE# #TAB# roles = [] #LINE# #TAB# roles += client.list_account_roles() #LINE# #TAB# return roles
"def get_tty_password(confirm): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# password = getpass.getpass('Enter the password: ') #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# print('\nEOFError raised, please try again.\n') #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return password"
"def number_of_set_bits(x): #LINE# #TAB# if x >= 0: #LINE# #TAB# #TAB# return sum(map(lambda y: y & 1, x)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"def filter_exclude(df, s): #LINE# #TAB# df = df.copy() #LINE# #TAB# c = [c for c in df.columns if not s.search(c)] #LINE# #TAB# df = df[c!= s] #LINE# #TAB# return df"
"def as_list(param): #LINE# #TAB# if param is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# elif not isinstance(param, list): #LINE# #TAB# #TAB# return [param] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [param]"
"def calculate_size(name, new_value): #LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += calculate_size_data(new_value) #LINE# #TAB# return data_size"
def format_from_extension(fname): #LINE# #TAB# fname = fname.split('.')[-1] #LINE# #TAB# if not fname: #LINE# #TAB# #TAB# return None #LINE# #TAB# ext = os.path.splitext(fname)[1] #LINE# #TAB# if ext not in SUPPORTED_EXTENSIONS: #LINE# #TAB# #TAB# return None #LINE# #TAB# protocol = SUPPORTED_PROTOCOLS[ext] #LINE# #TAB# if not protocol: #LINE# #TAB# #TAB# return None #LINE# #TAB# return protocol
"def create_route(app, engineio_server, engineio_endpoint): #LINE# #TAB# engineio_server.register_route(engineio_endpoint) #LINE# #TAB# app.router.add_route('GET', view_func=engineio_server.get) #LINE# #TAB# app.router.add_route('POST', view_func=engineio_endpoint, action_func= #LINE# #TAB# #TAB# engineio_server.action) #LINE# #TAB# app.router.add_route('GET', view_func=engineio_server.get) #LINE# #TAB# app.router.add_route('POST', view_func=engineio_endpoint, action_func= #LINE# #TAB# #TAB# engineio_server.post) #LINE# #TAB# return app"
def get_longest_substrings(string_set): #LINE# #TAB# return [string for string in string_set if len(string) > 0 #LINE# #TAB# #TAB# ]
"def add_to_list(my_list, my_element): #LINE# #TAB# if type(my_list) == list: #LINE# #TAB# #TAB# for item in my_list: #LINE# #TAB# #TAB# #TAB# if isinstance(item, my_element): #LINE# #TAB# #TAB# #TAB# #TAB# item = list(item) #LINE# #TAB# #TAB# #TAB# #TAB# add_to_list(item, my_element) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# my_list.append(my_element) #LINE# #TAB# else: #LINE# #TAB# #TAB# my_list.append(my_element) #LINE# #TAB# return my_list"
"def create_mapping(dico): #LINE# #TAB# if isinstance(dico, dict): #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# for key in dico: #LINE# #TAB# #TAB# #TAB# out[dico[key]] = key #LINE# #TAB# #TAB# return out #LINE# #TAB# elif isinstance(dico, list): #LINE# #TAB# #TAB# return [create_mapping(x) for x in dico] #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
"def check_eval(str1): #LINE# #TAB# try: #LINE# #TAB# #TAB# return eval(str1) #LINE# #TAB# except SyntaxError as e: #LINE# #TAB# #TAB# if str1.startswith('""') or str1.startswith(""'""): #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return"
def generate_input(options): #LINE# #TAB# if options.input: #LINE# #TAB# #TAB# fp = open(options.input) if options.input!= '-' else sys.stdin #LINE# #TAB# #TAB# for string in fp.readlines(): #LINE# #TAB# #TAB# #TAB# yield string #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string
def get_link_pages(links): #LINE# #TAB# link_pages = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(links): #LINE# #TAB# #TAB# link_page = [] #LINE# #TAB# #TAB# while i < len(links) and len(link_page) < 10: #LINE# #TAB# #TAB# #TAB# link_page.append(links[i]) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# link_pages.append(link_page) #LINE# #TAB# return link_pages
"def copyright_holder_json(soup): #LINE# #TAB# ""for json output add a full stop if ends in et al"" #LINE# #TAB# holder = None #LINE# #TAB# permissions_tag = raw_parser.article_permissions(soup) #LINE# #TAB# if permissions_tag: #LINE# #TAB# #TAB# holder = node_text(raw_parser.copyright_holder(permissions_tag)) #LINE# #TAB# if holder is not None and holder.endswith('et al'): #LINE# #TAB# #TAB# holder = holder + '.' #LINE# #TAB# return holder"
"def sdl_joysticknumhats(joystick): #LINE# #TAB# joystick_c = unbox(joystick, 'SDL_Joystick *') #LINE# #TAB# rc = lib.sdl_joysticknumhats(joystick_c) #LINE# #TAB# return rc"
"def wrapped_resource(response): #LINE# #TAB# response_content = response.content.decode(response.encoding or 'utf-8') #LINE# #TAB# if isinstance(response_content, list): #LINE# #TAB# #TAB# response_wrapper = ResourceList(content=response_content) #LINE# #TAB# elif isinstance(response_content, dict): #LINE# #TAB# #TAB# response_wrapper = Resource(content=response_content) #LINE# #TAB# else: #LINE# #TAB# #TAB# response_wrapper = Resource(content=response_content) #LINE# #TAB# return response_wrapper"
"def to_list(value): #LINE# #TAB# result = [] #LINE# #TAB# if value: #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# result = [value] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(value) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = [value] #LINE# #TAB# return result"
def get_validator(require=True): #LINE# #TAB# if require: #LINE# #TAB# #TAB# return XsdValidator() #LINE# #TAB# else: #LINE# #TAB# #TAB# return XsdValidator
def get_enum_strings(chid): #LINE# #TAB# if id_is_channel(chid): #LINE# #TAB# #TAB# return [chid_to_string(chid) for chid_ in get_channel_states(chid)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"def create_subformat(id_type, id_value, quality): #LINE# #TAB# if id_type == 'book': #LINE# #TAB# #TAB# new_id = 'book' #LINE# #TAB# elif id_type == 'book2': #LINE# #TAB# #TAB# new_id = 'book2' #LINE# #TAB# elif id_type == 'book3': #LINE# #TAB# #TAB# new_id = 'book3' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError('Unknown subformat type') #LINE# #TAB# new_id = id_value if quality is None else quality #LINE# #TAB# new_id = new_id.replace('-', '') #LINE# #TAB# return new_id"
"def parse_pandoc(attrs): #LINE# #TAB# attrs = dict(attrs) #LINE# #TAB# if 'pandoc' not in attrs: #LINE# #TAB# #TAB# return #LINE# #TAB# for key, value in attrs.items(): #LINE# #TAB# #TAB# if not re.match('^\\s*package\\s*\\[', key): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# package = re.match('^\\s*package\\s*\\[', key) #LINE# #TAB# #TAB# if not package: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# package = package.groups()[0] #LINE# #TAB# #TAB# value = getattr(package, key, None) #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# attrs[key] = value #LINE# #TAB# return attrs"
"def initialize_params(params): #LINE# #TAB# if params is not None: #LINE# #TAB# #TAB# params.update({k: v for k, v in params.items() if not k. #startswith('_')}) #LINE# #TAB# return params"
"def validate_type(data_dict: dict, type_name: str) ->dict: #LINE# #TAB# if 'type' not in data_dict: #LINE# #TAB# #TAB# raise ValueError(f""'{type_name}' not found in dict."") #LINE# #TAB# for field in data_dict.keys(): #LINE# #TAB# #TAB# if field.get('type')!= type_name: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# f""Field '{field.get('type')}' can't be {type_name}.') #LINE# #TAB# return data_dict"
"def trim_trailing(scores, threshold): #LINE# #TAB# threshold = abs(threshold) #LINE# #TAB# position = 0 #LINE# #TAB# for position, basescore in enumerate(scores): #LINE# #TAB# #TAB# if basescore >= threshold: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return position, 0"
"def create_knn(fname): #LINE# #TAB# nn_model, lookup_table = None, [] #LINE# #TAB# with open(fname, 'rb') as f: #LINE# #TAB# #TAB# nn_model = pickle.load(f) #LINE# #TAB# #TAB# lookup_table = list(nn_model.lookup_table) #LINE# #TAB# return nn_model, lookup_table"
"def make_plotdir(outdir): #LINE# #TAB# plotdir = os.path.join(outdir, 'plot') #LINE# #TAB# os.makedirs(plotdir) #LINE# #TAB# return plotdir"
"def encode_key(value): #LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# return value #LINE# #TAB# value = value.replace('/', '%20') #LINE# #TAB# if not value.startswith('/'): #LINE# #TAB# #TAB# return quote(value) #LINE# #TAB# value = value.replace('\\', '/') #LINE# #TAB# if len(value) > 1 and not value.startswith('/'): #LINE# #TAB# #TAB# return quote(value) #LINE# #TAB# return value"
"def run_example(path): #LINE# #TAB# p = Popen(path, shell=True, stdout=PIPE, stderr=PIPE) #LINE# #TAB# r = p.communicate()[0] #LINE# #TAB# p.close() #LINE# #TAB# return r"
def toidpic_old(txt): #LINE# #TAB# try: #LINE# #TAB# #TAB# return toidpic(txt) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return txt
"def ber_decode_multiple(content, berdecoder): #LINE# #TAB# result = [] #LINE# #TAB# while len(content) > 0: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj, content = berdecode(content) #LINE# #TAB# #TAB# #TAB# result.append(obj) #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return result"
"def read_int1_d(): #LINE# #TAB# count = stdio.readInt() #LINE# #TAB# a = create1D(count, None) #LINE# #TAB# for i in range(count): #LINE# #TAB# #TAB# a[i] = stdio.readInt() #LINE# #TAB# return a"
"def maybe_download_corpora(tmp_dir): #LINE# #TAB# downloaded = 0 #LINE# #TAB# urls = CORPORA_URLS #LINE# #TAB# if not os.path.exists(tmp_dir): #LINE# #TAB# #TAB# os.mkdir(tmp_dir) #LINE# #TAB# for url in urls: #LINE# #TAB# #TAB# _download_corpora(url, tmp_dir) #LINE# #TAB# #TAB# downloaded += 1 #LINE# # return downloaded"
"def to_list(val): #LINE# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# return val.split() #LINE# #TAB# else: #LINE# #TAB# #TAB# return val"
"def hamming_distance(left_bytes, right_bytes): #LINE# #TAB# result = 0 #LINE# #TAB# left_bytes_len = len(left_bytes) #LINE# #TAB# right_bytes_len = len(right_bytes) #LINE# #TAB# for i in range(left_bytes_len): #LINE# #TAB# #TAB# if left_bytes_len[i]!= right_bytes_len[i]: #LINE# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# return result"
"def remap_oscar_credit_card(card): #LINE# #TAB# name = card.name #LINE# #TAB# description = card.description #LINE# #TAB# if card.satoshis: #LINE# #TAB# #TAB# number = card.satoshis[0] #LINE# #TAB# #TAB# currency = card.satoshis[1] #LINE# #TAB# #TAB# payload = {'amount': card.amount, 'currency': currency, 'department': #LINE# #TAB# #TAB# #TAB# card.department} #LINE# #TAB# else: #LINE# #TAB# #TAB# number = 1 #LINE# #TAB# #TAB# payload = {'amount': card.amount, 'currency': card.currency, 'department': #LINE# #TAB# #TAB# #TAB# card.department} #LINE# #TAB# return {'card_id': card.id, 'name': name, 'description': description}"
def safe_worker_check(): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# break
def tcl_prep_otaver(ota=None): #LINE# #TAB# if ota is None: #LINE# #TAB# #TAB# ota = tcl_get_starting_otaver() #LINE# #TAB# global ota_full_check #LINE# #TAB# if ota is not None: #LINE# #TAB# #TAB# ota_full_check = _otas_full_check(ota) #LINE# #TAB# #TAB# ota_full_check = _otas_full_check(ota_full_check) #LINE# #TAB# return ota_full_check
"def next_possible_entity(text): #LINE# #TAB# entities = [] #LINE# #TAB# while text and text[0].isalpha(): #LINE# #TAB# #TAB# entities.append('&') #LINE# #TAB# #TAB# text = text[1:] #LINE# #TAB# for part in text.split(' '): #LINE# #TAB# #TAB# if part[0] not in ['&', '&']: #LINE# #TAB# #TAB# #TAB# entities.append(' ') #LINE# #TAB# #TAB# if part[0] == '&': #LINE# #TAB# #TAB# #TAB# entities.append(')') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# entities.append(part) #LINE# #TAB# for entity in entities: #LINE# #TAB# #TAB# if entity[0]!= '&': #LINE# #TAB# #TAB# #TAB# yield entity"
def _clear(cls): #LINE# #TAB# #TAB# cls._status = None #LINE# #TAB# #TAB# pass
"def lock_release(lock): #LINE# #TAB# if lock is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# lock.release() #LINE# #TAB# #TAB# #TAB# logger.debug('Released lock %s', str(lock)) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except (ThreadError, RuntimeError): #LINE# #TAB# #TAB# #TAB# return False"
"def create_configfield_ref_target_node(target_id, env, lineno): #LINE# #TAB# node = nodes.reference(env, lineno, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'configfield', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# target_id=target_id, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# lineno=lineno) #LINE# #TAB# return node"
"def object_as_dict(obj): #LINE# #TAB# if not obj: #LINE# #TAB# #TAB# return None #LINE# #TAB# d = {} #LINE# #TAB# for f in obj.__table__.c: #LINE# #TAB# #TAB# value = getattr(obj, f.name) #LINE# #TAB# #TAB# if type(value) == datetime: #LINE# #TAB# #TAB# #TAB# value = value.isoformat() #LINE# #TAB# #TAB# d[f.name] = value #LINE# #TAB# return d"
"def is_transform_generator(fn): #LINE# #TAB# try: #LINE# #TAB# #TAB# return fn.__dict__.get('is_transform_generator', False) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False"
"def is_list_of_strings(vals): #LINE# #TAB# if not isinstance(vals, (list, tuple, set)): #LINE# #TAB# #TAB# return False #LINE# #TAB# for v in vals: #LINE# #TAB# #TAB# if not isinstance(v, str): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"def y_pred_deviance_derivative(coef, X, y, weights, family, link): #LINE# #TAB# if link: #LINE# #TAB# #TAB# y_pred = y_pred_link(coef, X, y, weights) #LINE# #TAB# #TAB# deriv = -y_pred_derivative(coef, X, y, weights, family) #LINE# #TAB# else: #LINE# #TAB# #TAB# y_pred = y_pred_data(coef, X, y, weights, family) #LINE# #TAB# #TAB# deriv = -y_pred_derivative(family, X, y, weights, link) #LINE# #TAB# return y_pred, deriv"
"def assert_equal_type(logical_line): #LINE# #TAB# if asse_equal_type_re.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'SL317: assertEqual(type(A), B) sentences not allowed'"
def is_special(ip_address): #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.inet_aton(ip_address) #LINE# #TAB# #TAB# return True #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# socket.inet_aton(ip_address.strip()) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return True
"def get_all_regions(): #LINE# #TAB# client = boto3.client('ec2', region_name=DEFAULT_REGION) #LINE# #TAB# response = client.describe_regions() #LINE# #TAB# return response['Regions']"
"def nb_name_from_path(config, path): #LINE# #TAB# for nb_name in config.notebook_names: #LINE# #TAB# #TAB# if nb_name.endswith(os.sep): #LINE# #TAB# #TAB# #TAB# nb_name = nb_name[:-len(os.sep)] #LINE# #TAB# return path"
"def truncate_string(value, max_width=None): #LINE# #TAB# if isinstance(value, six.string_types) and max_width is not None and len(value #LINE# #TAB# #TAB# ) > max_width: #LINE# #TAB# #TAB# return value[:max_width - 3] + '...' #LINE# #TAB# return value"
"def config_get_int(section, option, raise_exception=True, default=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# return __CONFIG.getint(section, option) #LINE# #TAB# except (ConfigParser.NoOptionError, ConfigParser.NoSectionError) as err: #LINE# #TAB# #TAB# if raise_exception and default is None: #LINE# #TAB# #TAB# #TAB# raise err #LINE# #TAB# #TAB# return default"
def decompose_bytes_to_bit_arr(arr): #LINE# #TAB# bit_arr = [] #LINE# #TAB# for i in range(len(arr)): #LINE# #TAB# #TAB# for j in range(8): #LINE# #TAB# #TAB# #TAB# bit_arr.append(arr[i * 8 + j] & 1) #LINE# #TAB# return bit_arr
"def open_conn(host, db, user, password, retries=0, sleep=0.5): #LINE# #TAB# if not retries: #LINE# #TAB# #TAB# retries = retry_count() #LINE# #TAB# if not retries: #LINE# #TAB# #TAB# return None #LINE# #TAB# conn = mysql.connect(host=host, db=db, user=user, passwd=password, #LINE# #TAB# #TAB# retries=retries) #LINE# #TAB# conn.row_factory = mysql.Row #LINE# #TAB# conn.execute('PRAGMA foreign_keys = ON') #LINE# #TAB# conn.row_factory = mysql.Row #LINE# #TAB# sleep(sleep) #LINE# #TAB# while retries: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# conn.execute('SELECT * FROM sqlite_master;') #LINE# #TAB# #TAB# except mysql.OperationalError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# retries.append(0) #LINE# #TAB# return conn"
"def parse_example_command(example_command): #LINE# #TAB# inputs = [] #LINE# #TAB# for line in example_command.split('\n'): #LINE# #TAB# #TAB# parts = line.split('=') #LINE# #TAB# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# input = parts[0] #LINE# #TAB# #TAB# name = parts[1] #LINE# #TAB# #TAB# type_ = parts[2] #LINE# #TAB# #TAB# inputs.append({'name': name, 'type': type_}) #LINE# #TAB# return inputs"
"def get_content_type_from_url_params(app_name, model_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = ContentType.objects.get(app_name=app_name, model_name= #LINE# #TAB# #TAB# #TAB# model_name) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if content_type.app_name not in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# raise Http404"
"def get_model_from_url_params(app_name, model_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# c = app_name.lower() #LINE# #TAB# #TAB# c = model_name.lower() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if not c.startswith('django.'): #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = ContentType.objects.get(app_name=app_name, model=c) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if content_type not in ContentType.objects.all(): #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# return content_type"
"def get_ravello_application(env): #LINE# #TAB# app_name = env.config.get('RAVOR_APPLICATION_NAME', env.config.get( #LINE# #TAB# #TAB# 'RAVOR_APPLICATION_NAME', '')) #LINE# #TAB# app_path = env.config.get('RAVOR_APPLICATION_PATH', '').strip() # # # if app_path == '': #LINE# #TAB# #TAB# app_path = env.config.get('RAVOR_APPLICATION_PATH', '' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# app = RavelloApplication(app_name) #LINE# #TAB# env.config['RAVOR_APPLICATION_PATH'] = app_path #LINE# #TAB# return app"
"def handle_suds_generic_fault(error): #LINE# #TAB# exception = type(error) #LINE# #TAB# if exception is SudsGenericException: #LINE# #TAB# #TAB# exception_data = exception.data #LINE# #TAB# #TAB# raise Exception( #LINE# #TAB# #TAB# #TAB# error.__class__.__name__, #LINE# #TAB# #TAB# #TAB# exception_data, #LINE# #TAB# #TAB# ) #LINE# #TAB# return error"
"def acl_name(name, direction): #LINE# #TAB# if direction =='read': #LINE# #TAB# #TAB# name = f'acl-{name}' #LINE# #TAB# elif direction == 'write': #LINE# #TAB# #TAB# name = f'acl-{name}' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError( #LINE# #TAB# #TAB# #TAB# f'ACLs are not supported by Arista {name}') #LINE# #TAB# return name"
"def mem_ds(res, extent, srs=None, dtype=gdal.GDT_Float32): #LINE# #TAB# dst_ns = int((extent[2] - extent[0])/res + 0.99) #LINE# #TAB# dst_nl = int((extent[3] - extent[1])/res + 0.99) #LINE# #TAB# m_ds = gdal.GetDriverByName('MEM').Create('', dst_ns, dst_nl, 1, dtype) #LINE# #TAB# m_gt = [extent[0], res, 0, extent[3], 0, -res] #LINE# #TAB# m_ds.SetGeoTransform(m_gt) #LINE# #TAB# if srs is not None: #LINE# #TAB# #TAB# m_ds.SetProjection(srs.ExportToWkt()) #LINE# #TAB# return m_ds"
"def load_column_names(filename): #LINE# #TAB# colnames = [] #LINE# #TAB# indices = {} #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for i, line in enumerate(f): #LINE# #TAB# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# if i % 2 == 0: #LINE# #TAB# #TAB# #TAB# #TAB# colnames.append(line[i]) #LINE# #TAB# #TAB# #TAB# #TAB# indices[i] = i #LINE# #TAB# f.close() #LINE# #TAB# return colnames, indices"
"def recast_esys_mapdata(esys_mapdata): #LINE# #TAB# eigvalue = [] #LINE# #TAB# eigenvectors = [] #LINE# #TAB# for i in range(len(esys_mapdata[0])): #LINE# #TAB# #TAB# eig = esys_mapdata[0][i] #LINE# #TAB# #TAB# for j in range(len(esys_mapdata[1])): #LINE# #TAB# #TAB# #TAB# eig = esys_mapdata[1][j] #LINE# #TAB# #TAB# #TAB# eigvalue.append(eig) #LINE# #TAB# #TAB# #TAB# eigenvectors.append(eig) #LINE# #TAB# return eigvalue, eigenvectors"
def retrieve_grains_cache(proxy=None): #LINE# #TAB# global __grains_cache #LINE# #TAB# if __grains_cache: #LINE# #TAB# #TAB# return __grains_cache #LINE# #TAB# if proxy and salt.utils.napalm.is_proxy(__grains__): #LINE# #TAB# #TAB# grains = proxy.get_grains() #LINE# #TAB# else: #LINE# #TAB# #TAB# grains = salt.utils.napalm.get_grains(__grains__) #LINE# #TAB# __grains_cache = dict() #LINE# #TAB# if not proxy and salt.utils.napalm.is_minion(__grains__): #LINE# #TAB# #TAB# __grains_cache[__grains__['name']] = grains #LINE# #TAB# return __grains_cache
def is_ecma_regex(regex): #LINE# #TAB# parts = regex.split('/') #LINE# #TAB# if len(parts) == 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(parts) < 3: #LINE# #TAB# #TAB# raise ValueError('Given regex isn\'t ECMA regex nor Python regex.') #LINE# #TAB# parts.pop() #LINE# #TAB# parts.append('') #LINE# #TAB# raw_regex = '/'.join(parts) #LINE# #TAB# if raw_regex.startswith('/') and raw_regex.endswith('/'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
def class_parser(arg): #LINE# #TAB# parsed = OrderedDict(type=get_type_desc(arg)) #LINE# #TAB# parsed['name'] = arg.__name__ #LINE# #TAB# parsed['signature'] = str(signature_func(arg)) #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getfullargspec(arg)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getargspec(arg)) #LINE# #TAB# parsed['isbuiltin'] = inspect.isbuiltin(arg) #LINE# #TAB# return parsed
def resolve_template(template): #LINE# #TAB# template_path = _find_template(template) #LINE# #TAB# if template_path is None: #LINE# #TAB# #TAB# raise TemplateDoesNotExist('Could not find template: %s' % template) #LINE# #TAB# template_abspath = os.path.abspath(template_path) #LINE# #TAB# if os.path.isfile(template_abspath): #LINE# #TAB# #TAB# return template_abspath #LINE# #TAB# return template_path
"def build_review_config(ini_config, app_config=None): #LINE# #TAB# ret = ReviewConfig() #LINE# #TAB# ret.set_ini_file(ini_config) #LINE# #TAB# if not app_config: #LINE# #TAB# #TAB# app_config = ReviewConfig() #LINE# #TAB# ret.set_app_config(app_config) #LINE# #TAB# ret.set_review_interval(ini_config['REVIEW_INTERVAL']) #LINE# #TAB# ret.set_review_interval(ini_config['REVIEW_INTERVAL']) #LINE# #TAB# ret.set_review_interval(ini_config['REVIEW_INTERVAL']) #LINE# #TAB# return ret"
def asset_type_list(): #LINE# #TAB# from aiida.orm.utils.decorators import parse_asset_type #LINE# #TAB# asset_types = [] #LINE# #TAB# for row in parse_asset_type(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# asset_types.append(row.asset_type) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# asset_types.append(row.asset_type) #LINE# #TAB# return asset_types
"def deregister_decompressor(cls, decompressor): #LINE# #TAB# compression_method = decompressor.COMPRESSION_METHOD.lower() #LINE# #TAB# if compression_method not in cls._decompressors: #LINE# #TAB# #TAB# raise KeyError('Decompressor for compression method: {0:s} not set.'.format( #LINE# #TAB# #TAB# #TAB# decompressor.COMPRESSION_METHOD)) #LINE# #TAB# del cls._decompressors[compression_method]"
def get_checksum(data): #LINE# #TAB# sum = 0 #LINE# #TAB# for chunk in data: #LINE# #TAB# #TAB# sum += chunk[0] + chunk[1] #LINE# #TAB# return sum % 256
"def merge_dicts(x: dict, y: dict) ->dict: #LINE# #TAB# z = x.copy() #LINE# #TAB# z.update(y) #LINE# #TAB# return z"
"def cache_get_last_in_slice(url_dict, start_int, total_int, authn_subj_list): #LINE# #TAB# try: #LINE# #TAB# #TAB# return cache_get_last_in_slice(url_dict, start_int, total_int, #LINE# #TAB# #TAB# #TAB# authn_subj_list) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None"
"def psi_na_cl_no3_pk74(T, P): #LINE# #TAB# psi = -0.0094 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
def transform_bytecode(code_object): #LINE# #TAB# code_object.exec_() #LINE# #TAB# return code_object
"def name_matches(name, matches): #LINE# #TAB# for pattern in matches: #LINE# #TAB# #TAB# if fnmatch.fnmatch(name, pattern): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def update_filter_sdas(sdas, mib, xi_complement, reachability_plot): #LINE# #TAB# n_sdas = len(sdas) #LINE# #TAB# filter_sdas = np.zeros(n_sdas) #LINE# #TAB# for i in range(n_sdas): #LINE# #TAB# #TAB# filter_sdas[i] = mib[i] + xi_complement[i] #LINE# #TAB# #TAB# if sdas[i] > n_sdas[mib]: #LINE# #TAB# #TAB# #TAB# filter_sdas[i] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# filter_sdas[i] = 1 #LINE# #TAB# return filter_sdas"
"def run_commands(env, work_dir, commands, process_start, process_finish): #LINE# #TAB# data = {} #LINE# #TAB# if not isinstance(commands, (list, tuple)): #LINE# #TAB# #TAB# commands = [commands] #LINE# #TAB# for command in commands: #LINE# #TAB# #TAB# p = subprocess.Popen(command, shell=False, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# env=env) #LINE# #TAB# #TAB# data[process_start] = p.communicate(input=command, stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=p.stderr) #LINE# #TAB# process_finish(data) #LINE# #TAB# return data"
"def get_diff_cls(value): #LINE# #TAB# if value in _diff_classes: #LINE# #TAB# #TAB# return _diff_classes[value] #LINE# #TAB# cls = None #LINE# #TAB# if value not in _diff_classes: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cls = type(value) #LINE# #TAB# #TAB# except (AttributeError, TypeError): #LINE# #TAB# #TAB# #TAB# raise ValueError('%s is not a valid differential class' % value) #LINE# #TAB# if cls is None: #LINE# #TAB# #TAB# raise ValueError('%s is not a valid differential class' % value) #LINE# #TAB# return cls"
"def load_input(name): #LINE# #TAB# fp = open(name, 'r+') #LINE# #TAB# it = fp.read() #LINE# #TAB# fp.close() #LINE# #TAB# return it"
"def pick_one(gen: GeneratorType, strategy='random'): #LINE# #TAB# try: #LINE# #TAB# #TAB# elem = next(gen) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# elem = next(gen) #LINE# #TAB# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if strategy == 'random': #LINE# #TAB# #TAB# #TAB# elem = random.choice(elem) #LINE# #TAB# return elem"
"def extract_paragraphs(section, min_paragraph_length=140): #LINE# #TAB# tags = extract_tags(section, min_paragraph_length) #LINE# #TAB# paragraphs = [] #LINE# #TAB# for tag in tags: #LINE# #TAB# #TAB# length = len(tag['text']) #LINE# #TAB# #TAB# if length > min_paragraph_length: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# paragraphs.append(tag['text']) #LINE# #TAB# return paragraphs"
"def to_bin(data, width): #LINE# #TAB# if width > 2 ** width - 1: #LINE# #TAB# #TAB# raise ValueError('width must be an integer greater than 2 ** width') #LINE# #TAB# if data < 0: #LINE# #TAB# #TAB# data = 1 << width #LINE# #TAB# result = np.zeros(data.shape, dtype=np.uint8) #LINE# #TAB# result[0] = data & 1 #LINE# #TAB# result[-1] = 1 #LINE# #TAB# return result"
"def is_list_of_n_doubles(field, num, message): #LINE# #TAB# field.setText(str(field.text()).replace(',','')) #LINE# #TAB# try: #LINE# #TAB# #TAB# if len(str(field.text()).split())!= num: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# #TAB# for txt in str(field.text()).split(): #LINE# #TAB# #TAB# #TAB# if float(txt) % int(txt)!= 0: #LINE# #TAB# #TAB# #TAB# #TAB# _show_consistency(field, message, True) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# _show_consistency(field, message, False) #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"def get_transversals(base, gens): #LINE# #TAB# if not base: #LINE# #TAB# #TAB# return [] #LINE# #TAB# stabs = _distribute_gens_by_base(base, gens) #LINE# #TAB# orbits, transversals = _orbits_transversals_from_bsgs(base, stabs) #LINE# #TAB# transversals = [{x: h._array_form for x, h in y.items()} for y in #LINE# #TAB# #TAB# transversals] #LINE# #TAB# return transversals"
"def import_path(fullpath): #LINE# #TAB# path, filename = os.path.split(fullpath) #LINE# #TAB# filename, ext = os.path.splitext(filename) #LINE# #TAB# sys.path.insert(0, path) #LINE# #TAB# module = importlib.import_module(filename, path) #LINE# #TAB# importlib.reload(module) #LINE# #TAB# del sys.path[0] #LINE# #TAB# return module"
"def alien_filter(name, location, size, unsize): #LINE# #TAB# if name.startswith('python') and name.endswith('.py'): #LINE# #TAB# #TAB# return '' #LINE# #TAB# if not os.path.exists(location): #LINE# #TAB# #TAB# return '' #LINE# #TAB# location = os.path.dirname(location) #LINE# #TAB# newname = name #LINE# #TAB# newlocation = os.path.join(location, newname) #LINE# #TAB# if newlocation == location: #LINE# #TAB# #TAB# return '' #LINE# #TAB# return newname, newlocation, size, unsize"
"def build_from_config(cls, config, dependencies): #LINE# #TAB# task_config = cls.load_from_config(config) #LINE# #TAB# sequence = cls.build_sequence(config, dependencies) #LINE# #TAB# return task_config, sequence"
"def models_compatible(model_a: ModuleModel, model_b: ModuleModel) ->bool: #LINE# #TAB# model_a_names = set(model_a.module_names) #LINE# #TAB# model_b_names = set(model_b.module_names) #LINE# #TAB# return model_a_names == model_b_names"
"def archive_files(files, zip_filename, base_path=''): #LINE# #TAB# files_to_archive = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# full_path = os.path.join(base_path, zip_filename) #LINE# #TAB# #TAB# archive(f, full_path) #LINE# #TAB# #TAB# files_to_archive.append(f) #LINE# #TAB# return files_to_archive"
def additions_umount(mount_point): #LINE# #TAB# try: #LINE# #TAB# #TAB# os.unlink(mount_point) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass
"def gen_renewing_time(lease_time, elapsed=0): #LINE# #TAB# if elapsed: #LINE# #TAB# #TAB# time_string = '%0.2f' % elapsed #LINE# #TAB# else: #LINE# #TAB# #TAB# time_string = '%0.2f' % lease_time #LINE# #TAB# logger.debug('renewing time: %s', time_string) #LINE# #TAB# return time_string"
"def dt_to_float_ordinal(dt): #LINE# #TAB# tzi = getattr(dt, 'tzinfo', None) #LINE# #TAB# if tzi is not None: #LINE# #TAB# #TAB# dt = dt.astimezone(UTC) #LINE# #TAB# base = float(dt.toordinal()) #LINE# #TAB# cdate = datetime.datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, #LINE# #TAB# #TAB# dt.second, dt.microsecond) #LINE# #TAB# base += 12 #LINE# #TAB# return base"
"def set_node(node, value): #LINE# #TAB# parent = node.parent #LINE# #TAB# if parent is not None: #LINE# #TAB# #TAB# parent.value = value #LINE# #TAB# else: #LINE# #TAB# #TAB# parent = node.parent #LINE# #TAB# if parent is None: #LINE# #TAB# #TAB# return #LINE# #TAB# for child in parent.children: #LINE# #TAB# #TAB# child.value = value"
"def semantic_version(tag): #LINE# #TAB# try: #LINE# #TAB# #TAB# version = list(map(int, tag.split('.'))) #LINE# #TAB# #TAB# assert len(version) == 3 #LINE# #TAB# #TAB# return tuple(version) #LINE# #TAB# except Exception as exc: #LINE# #TAB# #TAB# raise CommandError( #LINE# #TAB# #TAB# #TAB# 'Could not parse ""%s"", please use'#LINE# #TAB# #TAB# #TAB# 'MAJOR.MINOR.PATCH' % tag #LINE# #TAB# #TAB# ) from exc"
def query_clean(query): #LINE# #TAB# model_queryset = query.model.objects.all() #LINE# #TAB# cleaned_qs = [] #LINE# #TAB# for obj in model_queryset: #LINE# #TAB# #TAB# if not obj.pk: #LINE# #TAB# #TAB# #TAB# cleaned_qs.append(obj) #LINE# #TAB# return cleaned_qs
"def rcm_vertex_order(vertices_resources, nets): #LINE# #TAB# vertices_neighbours = _get_vertices_neighbours(nets) #LINE# #TAB# for subgraph_vertices in _get_connected_subgraphs(vertices_resources, #LINE# #TAB# #TAB# vertices_neighbours): #LINE# #TAB# #TAB# cm_order = _cuthill_mckee(subgraph_vertices, vertices_neighbours) #LINE# #TAB# #TAB# for vertex in reversed(cm_order): #LINE# #TAB# #TAB# #TAB# yield vertex"
def as_dicts(cursor): #LINE# #TAB# columns = list(cursor.description) #LINE# #TAB# return [row.as_dict() for row in cursor.fetchall()]
"def get_yaml_parser_roundtrip(): #LINE# #TAB# yaml_parser_roundtrip = (yaml.SafeLoader, yaml.SafeLoader, #LINE# #TAB# #TAB# yaml.SafeLoader) #LINE# #TAB# return yaml_parser_roundtrip"
"def get_missing_reqs(dist, installed_dists): #LINE# #TAB# installed_dists_by_name = {} #LINE# #TAB# for installed_dist in installed_dists: #LINE# #TAB# #TAB# installed_dists_by_name[installed_dist.project_name] = installed_dist # # missing = [] #LINE# #TAB# for requirement in dist.requires(): #LINE# #TAB# #TAB# present_dist = installed_dists_by_name.get(requirement.project_name) #LINE# #TAB# #TAB# if present_dist: #LINE# #TAB# #TAB# #TAB# missing.append(requirement) #LINE# #TAB# return missing"
"def remove_suffix(text: str, suffix: str) ->str: #LINE# #TAB# if text.endswith(suffix): #LINE# #TAB# #TAB# return text[:-(len(suffix))] #LINE# #TAB# return text"
"def rgb_from_xyz(xyz): #LINE# #TAB# rgb = tuple(map(int, xyz)) #LINE# #TAB# clipped = False #LINE# #TAB# if len(xyz)!= 3: #LINE# #TAB# #TAB# xyz = xyz[0] * 256 + xyz[1] * 256 + xyz[2] * 256 #LINE# #TAB# #TAB# clipped = True #LINE# #TAB# return rgb, clipped"
def encode_syncsafe32(i): #LINE# #TAB# if i >= 2 ** 28: #LINE# #TAB# #TAB# raise ValueError('value of {} is too large'.format(i)) #LINE# #TAB# elif i < 0: #LINE# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# value = 0 #LINE# #TAB# for x in range(4): #LINE# #TAB# #TAB# if i & 128 == 0: #LINE# #TAB# #TAB# #TAB# value |= (i & 127) << x * 7 #LINE# #TAB# #TAB# #TAB# i >>= 8 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# return value
"def read_allocations(module): #LINE# #TAB# filename = module.params['allocation_file'] #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# content = yaml.safe_load(f) #LINE# #TAB# except IOError as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# return {} #LINE# #TAB# #TAB# module.fail_json(msg= #LINE# #TAB# #TAB# #TAB# 'Failed to open allocation file %s for reading' % filename) #LINE# #TAB# except yaml.YAMLError as e: #LINE# #TAB# #TAB# module.fail_json(msg='Failed to parse allocation file %s as YAML' % #LINE# #TAB# #TAB# #TAB# filename) #LINE# #TAB# if content is None: #LINE# #TAB# #TAB# content = {} #LINE# #TAB# return content"
"def get_socket(timeout=3): #LINE# #TAB# sock = _socket.socket(socket.AF_INET, socket.SOCK_DGRAM, timeout) #LINE# #TAB# sock.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, True) #LINE# #TAB# return sock"
"def get_directory_modules(directory, flush_local_modules=False): #LINE# #TAB# modules = [] #LINE# #TAB# for path, _, files in os.walk(directory): #LINE# #TAB# #TAB# if path == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if flush_local_modules: #LINE# #TAB# #TAB# #TAB# mod = '__init__' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mod = os.path.join(path, mod) #LINE# #TAB# #TAB# modules.append((mod, path)) #LINE# #TAB# return modules"
"def parse_command(comm): #LINE# #TAB# if not comm: #LINE# #TAB# #TAB# return None, [] #LINE# #TAB# parsed = shlex.split(comm) #LINE# #TAB# try: #LINE# #TAB# #TAB# args = json.loads(parsed[0]) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# args = None #LINE# #TAB# return parsed['name'], args"
"def get_dvportgroup_out_shaping(pg_name, pg_default_port_config): #LINE# #TAB# log.trace('Retrieving portgroup out shaping policy') #LINE# #TAB# port_groups = pg_default_port_config.get('portgroups', []) #LINE# #TAB# if port_groups: #LINE# #TAB# #TAB# return port_groups[pg_name]['out_shaping_policy'] #LINE# #TAB# return []"
"def get_auth_from_url(url): #LINE# #TAB# parsed = urlparse(url) #LINE# #TAB# try: #LINE# #TAB# #TAB# auth = unquote(parsed.username), unquote(parsed.password) #LINE# #TAB# except (AttributeError, TypeError): #LINE# #TAB# #TAB# auth = '', '' #LINE# #TAB# return auth"
"def magic_file(filename): #LINE# #TAB# matches = [] #LINE# #TAB# with open(filename) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# matches.append((int(line[1]), float(line[2])))) #LINE# #TAB# matches = sorted(matches, key=lambda x: x[1]) #LINE# #TAB# return matches, [(x[0], x[1]) for x in matches]"
"def cycled_latin_square(elements): #LINE# #TAB# permutation = [] #LINE# #TAB# if type(elements) is int: #LINE# #TAB# #TAB# elements = int(elements) #LINE# #TAB# while elements: #LINE# #TAB# #TAB# permutation.append(random.randint(1, len(elements))) #LINE# #TAB# #TAB# elements = elements - 1 #LINE# #TAB# return permutation"
"def _get_atomsection(mol2_lst): #LINE# #TAB# #TAB# started = False #LINE# #TAB# #TAB# for idx, s in enumerate(mol2_lst): #LINE# #TAB# #TAB# #TAB# if s.startswith('@<TRIPOS>ATOM'): #LINE# #TAB# #TAB# #TAB# #TAB# first_idx = idx + 1 #LINE# #TAB# #TAB# #TAB# #TAB# started = True #LINE# #TAB# #TAB# #TAB# elif started and s.startswith('@<TRIPOS>'): #LINE# #TAB# #TAB# #TAB# #TAB# last_idx_plus1 = idx #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# return mol2_lst[first_idx:last_idx_plus1]"
"def set_tree_text(dictnode_tree, src): #LINE# #TAB# for key in dictnode_tree: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# dictnode_tree[key]['text'] = src[key] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# tail = dictnode_tree[key]['tail'] #LINE# #TAB# #TAB# dictnode_tree[key]['tail'] = tail"
"def merge_returning(options: dict, base_context: Context) ->ReturnType: #LINE# #TAB# try: #LINE# #TAB# #TAB# returning = options['returning'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return base_context.returning if base_context else ReturnType.Records #LINE# #TAB# else: #LINE# #TAB# #TAB# if type(returning) is str: #LINE# #TAB# #TAB# #TAB# return ReturnType(returning) #LINE# #TAB# #TAB# return returning"
"def to_index_row_dict(df, index_col=None, use_ordered_dict=True): #LINE# #TAB# if use_ordered_dict: #LINE# #TAB# #TAB# dict_inst = OrderedDict #LINE# #TAB# if index_col is None: #LINE# #TAB# #TAB# columns = df.columns #LINE# #TAB# else: #LINE# #TAB# #TAB# columns = [index_col] #LINE# #TAB# data = [] #LINE# #TAB# for row in df.iterrows(): #LINE# #TAB# #TAB# temp = OrderedDict() #LINE# #TAB# #TAB# for key in columns: #LINE# #TAB# #TAB# #TAB# temp[key] = row[key] #LINE# #TAB# #TAB# data.append(temp) #LINE# #TAB# return data"
"def milliseconds_offset(cls, timestamp, now=None): #LINE# #TAB# #TAB# if now is None: #LINE# #TAB# #TAB# #TAB# now = datetime.datetime.utcnow() #LINE# #TAB# #TAB# offset = timestamp - now #LINE# #TAB# #TAB# return int(offset.total_seconds() * 1000) // 1000"
"def generate_dot(nicknames, relations, name, format, program, directed=False): #LINE# #TAB# if directed: #LINE# #TAB# #TAB# dot = dotpy.DiGraph() #LINE# #TAB# #TAB# dot.add_edges_from([nicknames, relations]) #LINE# #TAB# else: #LINE# #TAB# #TAB# dot = dotpy.DiGraph() #LINE# #TAB# dot.add_source(name) #LINE# #TAB# dot.add_attributes(format) #LINE# #TAB# description = generate_dot_description(nicknames, relations, name, format, program) #LINE# #TAB# graph = dot.to_dot() #LINE# #TAB# return graph, description"
"def remove_i_columns(df): #LINE# #TAB# all_columns = list(filter(lambda el: el[-2:] == '_I', df.columns)) #LINE# #TAB# for column in all_columns: #LINE# #TAB# #TAB# del df[column]"
"def remove_nones(in_dict): #LINE# #TAB# if in_dict is not None and is_dict(in_dict): #LINE# #TAB# #TAB# return in_dict.copy() #LINE# #TAB# nested_dict = {} #LINE# #TAB# for key, value in in_dict.items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# if value!= '': #LINE# #TAB# #TAB# #TAB# #TAB# nested_dict[key] = value #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# remove_nones(nested_dict) #LINE# #TAB# return nested_dict"
def create_logging_handler(debug: bool) ->logging.Handler: #LINE# #TAB# handler = logging.StreamHandler() #LINE# #TAB# if debug: #LINE# #TAB# #TAB# handler.setLevel(logging.DEBUG) #LINE# #TAB# return handler
"def drop_from_dict(d: dict, skip: List[object]) ->dict: #LINE# #TAB# ret = {} #LINE# #TAB# for key, value in d.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# value = drop_from_dict(value, skip) #LINE# #TAB# #TAB# ret[key] = value #LINE# #TAB# return ret"
"def load_frontends(config, callback, internal_attributes): #LINE# #TAB# frontend_modules = _load_plugins(config.get(""CUSTOM_PLUGIN_MODULE_PATHS""), config[""BASE""], internal_attributes, callback) #LINE# #TAB# for backend_module in frontend_modules: #LINE# #TAB# #TAB# module_name = backend_module.split(""."")[-1] #LINE# #TAB# #TAB# module = import_module(module_name) #LINE# #TAB# #TAB# callback(module, config, internal_attributes) #LINE# #TAB# return frontend_modules"
"def func_source_data(func): #LINE# #TAB# func_name = func.__code__.co_filename #LINE# #TAB# func_file = func.__code__.co_filename #LINE# #TAB# lineno = -1 #LINE# #TAB# source = func_file #LINE# #TAB# data = {'file': func_name, 'line': lineno,'source': source} #LINE# #TAB# if hasattr(func, 'im_func'): #LINE# #TAB# #TAB# data['func_name'] = func.im_func.__name__ #LINE# #TAB# #TAB# data['line'] = func.im_lineno #LINE# #TAB# return data"
def is_not_pickle_safe_gl_model_class(obj_class): #LINE# #TAB# n = 0 #LINE# #TAB# if _is_pickle_safe(obj_class): #LINE# #TAB# #TAB# return True #LINE# #TAB# if _is_tensorflow_gl_model_class(obj_class): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"def get_db_references(cls, entry): #LINE# #TAB# #TAB# return [models.DbReference(reference=x.text) for x in entry.iterfind( #LINE# #TAB# #TAB# #TAB# './dbReference')]"
def average_histories(histories): #LINE# #TAB# average_histories = [] #LINE# #TAB# for history in histories: #LINE# #TAB# #TAB# average_histories.append(average_histories.pop(0)) #LINE# #TAB# return average_histories
def yices_error_string(): #LINE# #TAB# string = yices_error_report_string() #LINE# #TAB# string = '\n'.join(string) #LINE# #TAB# return string
"def collect_files(dirpath, cond=lambda fullname: True): #LINE# #TAB# if not os.path.isdir(dirpath): #LINE# #TAB# #TAB# return [] #LINE# #TAB# files = [] #LINE# #TAB# for entry in os.listdir(dirpath): #LINE# #TAB# #TAB# full_path = os.path.join(dirpath, entry) #LINE# #TAB# #TAB# if os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# files.append(full_path) #LINE# #TAB# #TAB# #TAB# collect_files(full_path, cond=cond) #LINE# #TAB# return files"
def zig_zag_encode(value): #LINE# #TAB# out = 0 #LINE# #TAB# while value > 0: #LINE# #TAB# #TAB# out = (out << 1) + value #LINE# #TAB# #TAB# value >>= 1 #LINE# #TAB# return out
"def load_params(jsonParams): #LINE# #TAB# with open(jsonParams, 'r') as f: #LINE# #TAB# #TAB# jsonFile = f.read() #LINE# #TAB# parsedJSON = json.loads(jsonFile) #LINE# #TAB# return parsedJSON[blockNames.ControlFileParams.generalParams], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.spawningBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.simulationBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.clusteringBlockname]"
"def methods_of(obj): #LINE# #TAB# result = {} #LINE# #TAB# for i in dir(obj): #LINE# #TAB# #TAB# if callable(getattr(obj, i)) and not i.startswith('_'): #LINE# #TAB# #TAB# #TAB# result[i] = getattr(obj, i) #LINE# #TAB# return result"
def vocabulary_skip(format='insdc'): #LINE# #TAB# skip = [] #LINE# #TAB# if format == 'insdc': #LINE# #TAB# #TAB# skip.append('VOCAB') #LINE# #TAB# if format == 'gff3': #LINE# #TAB# #TAB# skip.append('GFF3') #LINE# #TAB# return skip
"def p_dictorsetmaker_star2_2(p): #LINE# #TAB# keys, values = p[2] #LINE# #TAB# p[0] = keys, [p[1]] + values"
"def theta_ca_k_hmw84(T, P): #LINE# #TAB# theta = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"def chars_after(chars, match): #LINE# #TAB# if match.end >= len(match.input_string): #LINE# #TAB# #TAB# return True #LINE# #TAB# return match.input_string[match.end] in chars"
"def create_vpnservice(cls, subnet_id, router_id, name=None): #LINE# #TAB# body = cls.client.create_vpnservice(subnet_id=subnet_id, router_id= #LINE# #TAB# #TAB# router_id, name=name) #LINE# #TAB# vpnservice = body['vpnservice'] #LINE# #TAB# cls.vpnservices.append(vpnservice) #LINE# #TAB# return vpnservice"
"def get_rule_table(rules): #LINE# #TAB# table = formatting.Table(['Id', 'KeyName'], 'Rules') #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# table.add_row([rule['id'], rule['keyName']]) #LINE# #TAB# return table"
"def schema_map(schema): #LINE# #TAB# global _cached_mappings #LINE# #TAB# if _cached_mappings is None: #LINE# #TAB# #TAB# _cached_mappings = {} #LINE# #TAB# for key, item in schema.items(): #LINE# #TAB# #TAB# _cached_mappings[key] = ICachedItemMapper(item) #LINE# #TAB# return _cached_mappings"
def is_uri(uri): #LINE# #TAB# try: #LINE# #TAB# #TAB# op = op.realpath(uri) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def assemble_remotes(resource): #LINE# #TAB# resources = [] #LINE# #TAB# resources.append(resource) #LINE# #TAB# if os.path.isdir(resource): #LINE# #TAB# #TAB# dirs = os.listdir(resource) #LINE# #TAB# #TAB# files = os.listdir(resource) #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.endswith('.pyc'): #LINE# #TAB# #TAB# #TAB# #TAB# f = f[:-5] #LINE# #TAB# #TAB# #TAB# resource = os.path.join(resource, f) #LINE# #TAB# rpm_urls = [] #LINE# #TAB# for rpm_url in resources: #LINE# #TAB# #TAB# if os.path.isfile(rpm_url): #LINE# #TAB# #TAB# #TAB# rpm_url = rpm_url.rstrip('/') #LINE# #TAB# #TAB# rpm_urls.append(rpm_url) #LINE# #TAB# return"
"def check_command(command): #LINE# #TAB# res = True #LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# stderr=subprocess.PIPE) #LINE# #TAB# #TAB# result = p.wait() #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# res = False #LINE# #TAB# finally: #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# p.stdin.close() #LINE# #TAB# #TAB# #TAB# p.stdout = None #LINE# #TAB# return res"
"def fetch_first_result(fget, fset, fdel, apply_func, value_not_found=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# result = apply_func(fget, fset, fdel) #LINE# #TAB# #TAB# if result is not None: #LINE# #TAB# #TAB# #TAB# return result #LINE# #TAB# #TAB# elif value_not_found is not None: #LINE# #TAB# #TAB# #TAB# raise ValueError(value_not_found) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return value_not_found #LINE# #TAB# return fget, fdel, value_not_found"
"def white_strip(q, f, x, add_v): #LINE# #TAB# s1 = np.sum(q * x) #LINE# #TAB# s2 = np.sum(q * x) #LINE# #TAB# if s1!= s2: #LINE# #TAB# #TAB# return q, f, x #LINE# #TAB# q, f = q.T, f #LINE# #TAB# x = x.astype(np.float64) #LINE# #TAB# if q.shape[0]!= x.shape[1]: #LINE# #TAB# #TAB# return q, f, x #LINE# #TAB# add_v(q, x) #LINE# #TAB# return q, f, x"
"def ftest_p(f, df_num, df_den): #LINE# #TAB# n = f.shape[0] #LINE# #TAB# denom = df_den.values.sum() #LINE# #TAB# num = float(n) / denom #LINE# #TAB# if num == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# elif num > 0.0 and denom > 0.0: #LINE# #TAB# #TAB# return 1.0 #LINE# #TAB# return 0.0"
"def wrap_class(request_handler, validator): #LINE# #TAB# #TAB# for method, check in validator.items(): #LINE# #TAB# #TAB# #TAB# if check(request_handler): #LINE# #TAB# #TAB# #TAB# #TAB# return request_handler #LINE# #TAB# #TAB# return validator"
"def prepare_path(path): #LINE# #TAB# if isinstance(path, Path): #LINE# #TAB# #TAB# pass #LINE# #TAB# elif isinstance(path, str): #LINE# #TAB# #TAB# path = Path(path) #LINE# #TAB# elif not isinstance(path, Path): #LINE# #TAB# #TAB# return path #LINE# #TAB# if isinstance(path, (list, tuple)): #LINE# #TAB# #TAB# path = [path] #LINE# #TAB# return path"
"def jac_uniform(X, cells): #LINE# #TAB# n = X.shape[0] #LINE# #TAB# J = np.zeros(n) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(n): #LINE# #TAB# #TAB# #TAB# J[i, j] = X[i, j] / cells[j] #LINE# #TAB# return J"
"def tsuite_exit(trun, tsuite): #LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:tsuite:exit { name: %r }"" % tsuite[""name""]) #LINE# #TAB# rcode = 0 #LINE# #TAB# for hook in tsuite[""hooks""][""exit""]:#TAB# #LINE# #TAB# #TAB# rcode = script_run(trun, hook) #LINE# #TAB# #TAB# if rcode: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:tsuite:exit { rcode: %r }"" % rcode, rcode) #LINE# #TAB# return rcode"
"def yield_accessible_unix_file_names(path): #LINE# #TAB# for file_name in os.listdir(path): #LINE# #TAB# #TAB# full_path = os.path.join(path, file_name) #LINE# #TAB# #TAB# if os.access(full_path, os.X_OK) and os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# yield file_name"
"def get_ns_path(nspath=None, nsname=None, nspid=None): #LINE# #TAB# if nsname and nspid: #LINE# #TAB# #TAB# nspath = os.path.join(nspath, '%s/%s' % (nsname, nspid)) #LINE# #TAB# elif nspath is None: #LINE# #TAB# #TAB# nspath = os.getcwd() #LINE# #TAB# else: #LINE# #TAB# #TAB# nspath = nspath #LINE# #TAB# return nspath"
"def group_items(items, groupids): #LINE# #TAB# if groupids is not None: #LINE# #TAB# #TAB# for group_id in groupids: #LINE# #TAB# #TAB# #TAB# if group_id in items: #LINE# #TAB# #TAB# #TAB# #TAB# yield items[group_id] #LINE# #TAB# #TAB# #TAB# #TAB# del items[group_id]"
"def get_type(schema: Schema) ->List[str]: #LINE# #TAB# if 'type' in schema: #LINE# #TAB# #TAB# return schema['type'] #LINE# #TAB# elif isinstance(schema, dict): #LINE# #TAB# #TAB# types = [] #LINE# #TAB# #TAB# for key, value in schema.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# #TAB# types = value #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# types.append(key) #LINE# #TAB# #TAB# return types #LINE# #TAB# elif isinstance(schema, str): #LINE# #TAB# #TAB# return [schema] #LINE# #TAB# return []"
"def iter_child_nodes(node): #LINE# #TAB# while hasattr(node, 'childNodes'): #LINE# #TAB# #TAB# node = node.childNodes[0] #LINE# #TAB# yield node"
def tendermint_version_is_compatible(running_tm_ver): #LINE# #TAB# if running_tm_ver == 'latest': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def get_sample_weight(samples, metric): #LINE# #TAB# samples = np.asarray(samples) #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# l1 = np.mean(samples, axis=0) #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# l2 = np.mean(samples, axis=0) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Metric must be ""l1"" or ""l2"".') #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# l1 = np.linalg.norm(samples) #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# l2 = np.linalg.norm(samples) #LINE# #TAB# return l1 / l2"
"def iterate_attributes(cls): #LINE# #TAB# for key, value in cls.__dict__.items(): #LINE# #TAB# #TAB# if isinstance(value, property): #LINE# #TAB# #TAB# #TAB# yield key, value"
"def check_result2(state, col_names=None, sort=False, match='exact'): #LINE# #TAB# print('Checking results...') #LINE# #TAB# res = state.copy() #LINE# #TAB# if col_names is not None: #LINE# #TAB# #TAB# for k in col_names: #LINE# #TAB# #TAB# #TAB# if match in res[k]: #LINE# #TAB# #TAB# #TAB# #TAB# res[k].sort(key=lambda x: x[0]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res[k] = res[k].copy() #LINE# #TAB# if sort: #LINE# #TAB# #TAB# _sort_results(state, res) #LINE# #TAB# return state"
"def encode_keys(variants): #LINE# #TAB# keys = variants['key'] #LINE# #TAB# position = keys >> np.uint64(4) & 2 ** 28 - 1 #LINE# #TAB# typ = keys >> np.uint64(1) & 7 #LINE# #TAB# het = keys & 1 #LINE# #TAB# ret = append_fields(variants, data=(position, typ, het), names=( #LINE# #TAB# #TAB# 'position', 'typ', 'het')) #LINE# #TAB# return ret"
def get_status_code(url): #LINE# #TAB# try: #LINE# #TAB# #TAB# return requests.head(url).status_code #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# return -1
"def classical_damage(riskinputs, riskmodel, param, monitor): #LINE# #TAB# positive_risk_vec = riskinputs[:, (0)] #LINE# #TAB# positive_model = riskmodel(positive_risk_vec) #LINE# #TAB# monitor.log(param.num_classical_damages) #LINE# #TAB# j = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# y = monitor.log(param.num_classical_damages[0]) #LINE# #TAB# #TAB# if monitor.log(param.num_classical_damages[1]): #LINE# #TAB# #TAB# #TAB# y = monitor.log(param.num_classical_damages[2]) #LINE# #TAB# #TAB# if monitor.log(param.num_classical_damages[3]): #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# j += 1 #LINE# #TAB# return y"
"def RegisterPlugin(cls, plugin_class): #LINE# #TAB# plugin_name = plugin_class.NAME.lower() #LINE# #TAB# if plugin_name in cls._plugin_classes: #LINE# #TAB# raise KeyError('Plugin class already set for name: {0:s}.'.format( #LINE# #TAB# #TAB# plugin_class.NAME)) #LINE# #TAB# cls._plugin_classes[plugin_name] = plugin_class"
"def get_details(app='groupproject', env='dev', region='us-east-1'): #LINE# #TAB# data = get_details(app=app, env=env, region=region) #LINE# #TAB# try: #LINE# #TAB# #TAB# data['deployment_details'] = data['deployment'].get('details') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# data['deployment_details'] = {} #LINE# #TAB# return data"
"def calc_periodic_potn(V, eps, dx): #LINE# #TAB# delta_V = np.diff(V) #LINE# #TAB# delta_dx = dx / eps #LINE# #TAB# potential = np.zeros(len(V)) #LINE# #TAB# for i in range(len(V)): #LINE# #TAB# #TAB# potential[i] = delta_V / np.sum(delta_V ** 2) * V / delta_dx #LINE# #TAB# return potential"
"def create_floatingip(cls, external_network_id): #LINE# #TAB# floating_ip = cls.routers.create_floatingip(external_network_id= #LINE# #TAB# #TAB# external_network_id) #LINE# #TAB# floating_ip.id = external_network_id #LINE# #TAB# return floating_ip"
"def col_align(rows, left=True): #LINE# #TAB# output = [] #LINE# #TAB# if type(rows) == tuple: #LINE# #TAB# #TAB# for row in rows: #LINE# #TAB# #TAB# #TAB# output.append(col_align(row, left)) #LINE# #TAB# else: #LINE# #TAB# #TAB# for row in rows: #LINE# #TAB# #TAB# #TAB# output.append(col_align(row, left)) #LINE# #TAB# return output"
"def format_dwgsim_params(params: Mapping) ->str: #LINE# #TAB# formatted_params = '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# key = snakecase_to_kebab_case(key) #LINE# #TAB# #TAB# if key == 'extra': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif value is True: #LINE# #TAB# #TAB# #TAB# formatted_params += f' --{key}' #LINE# #TAB# #TAB# elif value is False: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# formatted_params += f' --{key} {value}' #LINE# #TAB# return formatted_params"
"def export_pairwise_gene_match_report(df: pd.DataFrame, outdir: Path) ->Path: #LINE# #TAB# outfile = outdir / 'pairwise_gene_match_report.tsv' #LINE# #TAB# df.to_csv(outfile, sep='\t', index=False) #LINE# #TAB# return outfile"
"def get_file_diff(tree, files_to_diff): #LINE# #TAB# files_to_diff_list = [] #LINE# #TAB# for file_name in files_to_diff: #LINE# #TAB# #TAB# diff_files = tree.get_file_diff(file_name) #LINE# #TAB# #TAB# if len(diff_files) > 0: #LINE# #TAB# #TAB# #TAB# log.debug('Found {0} files on Kolibri Studio...'.format( #LINE# #TAB# #TAB# #TAB# #TAB# file_name)) #LINE# #TAB# #TAB# #TAB# files_to_diff_list.append(diff_files) #LINE# #TAB# return files_to_diff_list"
"def result_folder(project_folder: str) ->str: #LINE# #TAB# path = os.path.join(project_folder,'results') #LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# return path"
"def pad_series(ts, pad, start, end): #LINE# #TAB# span = ts.span #LINE# #TAB# if pad and span < start: #LINE# #TAB# #TAB# ts0 = ts.shift(pad, axis=0) #LINE# #TAB# #TAB# ts1 = ts.shift(pad, axis=1) #LINE# #TAB# elif pad and span > end: #LINE# #TAB# #TAB# ts0 = ts.shift(pad, axis=0) #LINE# #TAB# #TAB# ts1 = ts.shift(pad, axis=1) #LINE# #TAB# elif pad and start < span < end: #LINE# #TAB# #TAB# ts0 = ts.shift(pad, axis=0) #LINE# #TAB# #TAB# ts1 = ts.shift(pad, axis=1) #LINE# #TAB# return ts0, ts1"
"def get_token(): #LINE# #TAB# token = os.environ.get('GH_TOKEN', None) #LINE# #TAB# if not token: #LINE# #TAB# #TAB# token_file = 'github.token' #LINE# #TAB# #TAB# f = open(token_file, 'r') #LINE# #TAB# #TAB# lines = f.readlines() #LINE# #TAB# #TAB# f.close() #LINE# #TAB# #TAB# token = lines[0] #LINE# #TAB# return token"
"def single_clustering(Nu, Nc): #LINE# #TAB# assert Nu > 1 #LINE# #TAB# assert Nc > 1 #LINE# #TAB# clust = 0 #LINE# #TAB# for clust_idx in range(Nu): #LINE# #TAB# #TAB# if clust == 0 and Nc == Nc: #LINE# #TAB# #TAB# #TAB# clust = 1 #LINE# #TAB# #TAB# elif clust == 1: #LINE# #TAB# #TAB# #TAB# clust = 0 #LINE# #TAB# return clust"
"def maybe_shorten_name(powerline, name): #LINE# #TAB# max_size = powerline.segment_conf('cwd','max_dir_size') #LINE# #TAB# if max_size: #LINE# #TAB# #TAB# return name[:max_size] #LINE# #TAB# return name"
"def format_space(experiment): #LINE# #TAB# space_string = SPACE_TEMPLATE.format(title=format_title('Space'), experiment #LINE# #TAB# #TAB# =experiment, space=format_space_dict(experiment)) #LINE# #TAB# return space_string"
"def sanitize_data(data): #LINE# #TAB# sanitized_data = {} #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# sanitized_data[key] = [sanitize_data(x) for x in value] #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# sanitized_data[key] = sanitize_data(value) #LINE# #TAB# return sanitized_data"
"def json_int_dttm_ser(obj): #LINE# #TAB# val = base_json_conv(obj) #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# return val #LINE# #TAB# if isinstance(obj, datetime): #LINE# #TAB# #TAB# obj = datetime_to_epoch(obj) #LINE# #TAB# elif isinstance(obj, date): #LINE# #TAB# #TAB# obj = (obj - EPOCH.date()).total_seconds() * 1000 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError('Unserializable object {} of type {}'.format(obj, #LINE# #TAB# #TAB# #TAB# type(obj))) #LINE# #TAB# return obj"
"def build_model_resnet50_avg(lock_base_model: bool): #LINE# #TAB# model = _build_model_resnet50_base_model(lock_base_model) #LINE# #TAB# if not model: #LINE# #TAB# #TAB# return model, None #LINE# #TAB# return model, 0.0"
def parse_buffer_to_png(data): #LINE# #TAB# #TAB# return [Image.open(BytesIO(image_data + b'\xff\xd9')) for image_data in #LINE# #TAB# #TAB# #TAB# data.split(b'\xff\xd9')[:-1]]
def eof_check(fhpatch): #LINE# #TAB# is_eof = False #LINE# #TAB# try: #LINE# #TAB# #TAB# for fp in fhpatch: #LINE# #TAB# #TAB# #TAB# if not fp.read(1): #LINE# #TAB# #TAB# #TAB# #TAB# is_eof = True #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return is_eof
def get_len(): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# l = int(input('Enter sequence length: ')) #LINE# #TAB# #TAB# #TAB# if l < 0: #LINE# #TAB# #TAB# #TAB# #TAB# sequence_length = 0 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# sequence_length = int(input('Enter length: ')) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# print('Error: sequence length must be an integer') #LINE# #TAB# #TAB# #TAB# sys.exit() #LINE# #TAB# return sequence_length
"def list_actions(cls): #LINE# #TAB# actions = ['start','stop','status'] #LINE# #TAB# for func_name in dir(cls): #LINE# #TAB# #TAB# func = getattr(cls, func_name) #LINE# #TAB# #TAB# if not hasattr(func, '__call__') or getattr(func, #LINE# #TAB# #TAB# #TAB# '__daemonocle_exposed__', False) is not True: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# action = func_name.replace('_', '-') #LINE# #TAB# #TAB# if action not in actions: #LINE# #TAB# #TAB# #TAB# actions.append(action) #LINE# #TAB# return actions"
"def load_accounting_tags(path=ACCOUNTING_GROUPS_FILE): #LINE# #TAB# accounting_tags = {} # #LINE# #TAB# with open(path, 'r') as accounting_group_file: #LINE# #TAB# #TAB# for line in accounting_group_file.readlines(): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# value = ast.literal_eval(line) #LINE# #TAB# #TAB# #TAB# #TAB# accounting_tags[value.strip()] = value #LINE# #TAB# #TAB# #TAB# except (SyntaxError, ValueError): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return accounting_tags"
"def load_java_obj(cls, clazz): #LINE# #TAB# #TAB# java_obj = super(Product, cls).load_java_obj(clazz) #LINE# #TAB# #TAB# if java_obj is None: #LINE# #TAB# #TAB# #TAB# raise ClassificationError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Product class not loaded, cannot load the Java object') #LINE# #TAB# #TAB# return java_obj"
"def parse_rate(rate): #LINE# #TAB# if isinstance(rate, str): #LINE# #TAB# #TAB# if rate.startswith('c'): #LINE# #TAB# #TAB# #TAB# return int(rate.split('c')[1]) * 10 #LINE# #TAB# #TAB# if rate.startswith('s'): #LINE# #TAB# #TAB# #TAB# return int(rate.split('s')[1]) * 10 #LINE# #TAB# #TAB# return rate #LINE# #TAB# return -1"
"def fix_module(job): #LINE# #TAB# modules = settings.RQ_JOBS_MODULE #LINE# #TAB# if not type(modules) == tuple: #LINE# #TAB# #TAB# modules = [modules] #LINE# #TAB# for module in modules: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module_match = importlib.import_module(module) #LINE# #TAB# #TAB# #TAB# if hasattr(module_match, job.task): #LINE# #TAB# #TAB# #TAB# #TAB# job.task = '{}.{}'.format(module, job.task) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return job"
"def convert_html_subscripts_to_latex(text): #LINE# #TAB# text = text.replace('&lt;', '<') #LINE# #TAB# text = text.replace('&gt;', '>') #LINE# #TAB# text = text.replace('&quot;', '""') #LINE# #TAB# return text"
"def strip_strings(obj): #LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for key in obj.keys(): #LINE# #TAB# #TAB# #TAB# obj[key] = strip_strings(obj[key]) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# obj[item] = strip_strings(item) #LINE# #TAB# elif isinstance(obj, tuple): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# for item2 in strip_strings(item): #LINE# #TAB# #TAB# #TAB# #TAB# obj[item2] = strip_strings(item2) #LINE# #TAB# return obj"
"def get_model_prop_by_name(cls, model_cls, prop_name): #LINE# #TAB# model_prop = None #LINE# #TAB# try: #LINE# #TAB# #TAB# model_prop = getattr(model_cls, prop_name) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return model_prop"
def clear_inbox(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# Lizard.inbox = [] #LINE# #TAB# #TAB# #TAB# del Lizard.inbox_queue #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# Lizard.inbox.pop() #LINE# #TAB# #TAB# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass
def google_analytics_factory(context): #LINE# #TAB# global _GOOGLE_ANALYTICS_MANAGER #LINE# #TAB# if _GOOGLE_ANALYTICS_MANAGER is None: #LINE# #TAB# #TAB# _GOOGLE_ANALYTICS_MANAGER = adapter.GoogleAnalyticsAdapter(context) #LINE# #TAB# return _GOOGLE_ANALYTICS_MANAGER
def reset_gmt_datadir(old_gmt_datadir): #LINE# #TAB# if os.environ.get('gmt_datadir'): #LINE# #TAB# #TAB# os.environ['gmt_datadir'] = old_gmt_datadir
"def open_config_files(default_config_path): #LINE# #TAB# config_files = [] #LINE# #TAB# if os.path.exists(default_config_path): #LINE# #TAB# #TAB# for root, dirs, files in os.walk(default_config_path): #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# if filename.endswith('.json'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# config_file = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# json_config_file = open(config_file, 'r').read() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# config_files.append(json_config_file) #LINE# #TAB# return config_files"
"def file_contents(file_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# file_obj = open(file_name, 'r') #LINE# #TAB# #TAB# contents = file_obj.read() #LINE# #TAB# #TAB# file_obj.close() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# contents = '' #LINE# #TAB# return contents"
"def cluster_get(context, cluster, show_progress=False): #LINE# #TAB# clusters = [c for c in context['clusters'] if c['name'] == cluster] #LINE# #TAB# if not clusters: #LINE# #TAB# #TAB# return None #LINE# #TAB# if show_progress: #LINE# #TAB# #TAB# op = 'get %s' % cluster #LINE# #TAB# #TAB# if op in progress: #LINE# #TAB# #TAB# #TAB# x = progress[op] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# x = [c for c in clusters if c['name'] == cluster] #LINE# #TAB# else: #LINE# #TAB# #TAB# x = [c for c in clusters if c['name'] == cluster] #LINE# #TAB# cluster = context['clusters'][cluster] #LINE# #TAB# return cluster"
"def encode_kw11(to_encode): #LINE# #TAB# encoded = {} #LINE# #TAB# for key, value in to_encode.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# #TAB# encoded[key] = value #LINE# #TAB# return encoded"
"def resblock_body(x, num_filters, num_blocks): #LINE# #TAB# x = ZeroPadding2D(((1, 0), (1, 0)))(x) #LINE# #TAB# x = darknet_conv2d_bn_leaky(num_filters, (3, 3), strides=(2, 2))(x) #LINE# #TAB# for i in range(num_blocks): #LINE# #TAB# #TAB# y = compose(darknet_conv2d_bn_leaky(num_filters // 2, (1, 1)), #LINE# #TAB# #TAB# #TAB# darknet_conv2d_bn_leaky(num_filters, (3, 3)))(x) #LINE# #TAB# #TAB# x = Add()([x, y]) #LINE# #TAB# return x"
def eg_sim_annulus_planet(c): #LINE# #TAB# i_simulation = LifetimeSimulation(c) #LINE# #TAB# i_simulation.simulate_annulus_planet() #LINE# #TAB# return 1
"def frac2cart_all(frac_coordinates, lattice_array): #LINE# #TAB# coordinates = deepcopy(frac_coordinates) #LINE# #TAB# for coord in range(coordinates.shape[0]): #LINE# #TAB# #TAB# coordinates[coord] = cartisian_from_fractional(coordinates[coord], #LINE# #TAB# #TAB# #TAB# lattice_array) #LINE# #TAB# return coordinates"
"def get_apcor(expnum, ccd, version='p', prefix=None): #LINE# #TAB# if ccd == 'aperture': #LINE# #TAB# #TAB# return get_apcor(expnum, version, prefix=prefix) #LINE# #TAB# else: #LINE# #TAB# #TAB# if prefix: #LINE# #TAB# #TAB# #TAB# return get_apcor(expnum, ccd, version, prefix=prefix) #LINE# #TAB# #TAB# parts = expnum.split('_') #LINE# #TAB# #TAB# key = parts[-1] #LINE# #TAB# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# #TAB# return get_apcor(expnum, ccd, version, prefix=prefix) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return get_apcor(expnum, ccd, version, prefix=prefix) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None"
"def binary_yen_stack(stack): #LINE# #TAB# out = [] #LINE# #TAB# for i, item in enumerate(stack): #LINE# #TAB# #TAB# out.append(item) #LINE# #TAB# return out"
"def drop_prefix(s, start): #LINE# #TAB# if s.startswith(start): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# i = s.index(start) #LINE# #TAB# #TAB# #TAB# return s[i + 1:] #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return s"
"def normalize_glyph_left_margin(value): #LINE# #TAB# if not isinstance(value, (int, float)) and value is not None: #LINE# #TAB# #TAB# raise TypeError(""Glyph left margin must be an "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# "":ref:`type-int-float`, not %s."" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# % type(value).__name__) #LINE# #TAB# return value"
"def subsample_freq_dist_nonzero(counts, n, dtype=uint): #LINE# #TAB# if counts.sum() <= n: #LINE# #TAB# #TAB# return counts #LINE# #TAB# cumsum = np.cumsum(counts, dtype=dtype) #LINE# #TAB# nz = counts.nonzero()[0] #LINE# #TAB# result = cumsum[nz] #LINE# #TAB# counts[nz] = result #LINE# #TAB# return counts"
"def decode_dict(d): #LINE# #TAB# new = {} #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# new[unicode(k)] = decode_dict(v) #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# new[unicode(k)] = [decode_dict(v) for v in v] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new[unicode(k)] = v #LINE# #TAB# return new"
"def model_to_task_arg(o): #LINE# #TAB# if issubclass(type(o), PyMacaronModel): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return o.to_json() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# return o #LINE# #TAB# return o"
def get_server_url(request): #LINE# #TAB# method = request.method.lower() #LINE# #TAB# parts = method.split('.') #LINE# #TAB# url = '/'.join(parts[:-1]) #LINE# #TAB# try: #LINE# #TAB# #TAB# server_url = url #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# server_url = request.get_full_url() #LINE# #TAB# return server_url
def validate_structures(structures: List[Structure]): #LINE# #TAB# for s in structures: #LINE# #TAB# #TAB# tables = [s.get_lattice() for s in s] #LINE# #TAB# #TAB# for row in tables: #LINE# #TAB# #TAB# #TAB# if row.lattice!= row.lattice: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError('Structure does not have the same lattice') #LINE# #TAB# return
"def crypto_hash_sha512(message): #LINE# #TAB# output = ffi.new('unsigned char[]', crypto_hash_sha512_BYTES) #LINE# #TAB# rc = lib.crypto_hash_sha512(output, message, crypto_hash_sha512_BYTES) #LINE# #TAB# ensure(rc == 0, 'Unexpected library error', raising=exc.RuntimeError) #LINE# #TAB# return output[0]"
"def max_days_in_month(year, month): #LINE# #TAB# if month > 13: #LINE# #TAB# #TAB# raise ValueError(""Incorrect month index"") #LINE# #TAB# if month in (IYYAR, TAMMUZ, ELUL, TEVETH, VEADAR): #LINE# #TAB# #TAB# return 29 #LINE# #TAB# if month == ADAR and not leap(year): #LINE# #TAB# #TAB# return 29 #LINE# #TAB# if month == HESHVAN and (year_days(year) % 10)!= 5: #LINE# #TAB# #TAB# return 29 #LINE# #TAB# if month == KISLEV and (year_days(year) % 10)!= 3: #LINE# #TAB# #TAB# return 29 #LINE# #TAB# return 30"
"def get_output_from_url_chunks_iter(url): #LINE# #TAB# for chunk in url_chunks_iter(url): #LINE# #TAB# #TAB# chunk = chunk.replace('\r', '').replace('\n', '') #LINE# #TAB# #TAB# chunk = chunk.replace('\r', '').replace('\n', '') #LINE# #TAB# #TAB# yield chunk"
def co_prime(l): #LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if l[i]!= l[i + 1]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"def update_control_board_calibration(control_board, fitted_params): #LINE# #TAB# fitted_params = numpy.asarray(fitted_params) #LINE# #TAB# old_calibration = control_board.calibration #LINE# #TAB# for param_name in fitted_params: #LINE# #TAB# #TAB# calibration = fitted_params[param_name] #LINE# #TAB# #TAB# control_board.calibration = calibration #LINE# #TAB# #TAB# old_calibration.parameters[param_name] = fitted_params[param_name] #LINE# #TAB# return old_calibration"
"def get_total_stats(instance): #LINE# #TAB# country_stats = instance.get_country_stats() #LINE# #TAB# total_deaths_per_country = {} #LINE# #TAB# for country in country_stats.keys(): #LINE# #TAB# #TAB# total_deaths_per_country[country] = {} #LINE# #TAB# #TAB# for death in country_stats[country]: #LINE# #TAB# #TAB# #TAB# total_deaths_per_country[country][death] = get_total_deaths( #LINE# #TAB# #TAB# #TAB# #TAB# death, instance) #LINE# #TAB# return {'country_stats': country_stats, 'total_deaths_per_country': #LINE# #TAB# #TAB# total_deaths_per_country}"
def get_game(tree): #LINE# #TAB# game = {} #LINE# #TAB# for card in tree.findall('card'): #LINE# #TAB# #TAB# attrs = card.attrib #LINE# #TAB# #TAB# if 'name' in attrs: #LINE# #TAB# #TAB# #TAB# game['name'] = attrs['name'] #LINE# #TAB# #TAB# if 'height' in attrs: #LINE# #TAB# #TAB# #TAB# game['height'] = attrs['height'] #LINE# #TAB# #TAB# if 'width' in attrs: #LINE# #TAB# #TAB# #TAB# game['width'] = int(attrs['width']) #LINE# #TAB# #TAB# if 'height' in attrs: #LINE# #TAB# #TAB# #TAB# game['height'] = int(attrs['height']) #LINE# #TAB# return game
"def replace_lines_in_files(search_string, replacement_line): #LINE# #TAB# paths = _s.dialogs.MultipleFiles('DIS AND DAT|*.*') #LINE# #TAB# if paths == []: #LINE# #TAB# #TAB# return #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# _shutil.copy(path, path + '.backup') #LINE# #TAB# #TAB# lines = read_lines(path) #LINE# #TAB# #TAB# for n in range(0, len(lines)): #LINE# #TAB# #TAB# #TAB# if lines[n].find(search_string) >= 0: #LINE# #TAB# #TAB# #TAB# #TAB# print(lines[n]) #LINE# #TAB# #TAB# #TAB# #TAB# lines[n] = replacement_line.strip() + '\n' #LINE# #TAB# #TAB# write_to_file(path, join(lines, '')) #LINE# #TAB# return"
def get_lowest_fn(p): #LINE# #TAB# f = p.function #LINE# #TAB# terminals = f.terminals #LINE# #TAB# if len(terminals) > 0: #LINE# #TAB# #TAB# for fn in terminals: #LINE# #TAB# #TAB# #TAB# if fn.has_terminals(): #LINE# #TAB# #TAB# #TAB# #TAB# return fn #LINE# #TAB# f = next(iter(terminals)) #LINE# #TAB# f = fn.function #LINE# #TAB# if f is not None: #LINE# #TAB# #TAB# return f #LINE# #TAB# del f #LINE# #TAB# return p
"def calc_padding_for_alignment(align, base): #LINE# #TAB# if align & Alignment.H_CENTER: #LINE# #TAB# #TAB# num_pad = align - 1 #LINE# #TAB# elif align & Alignment.RIGHT: #LINE# #TAB# #TAB# num_pad = align - 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# num_pad = align #LINE# #TAB# assert num_pad == 0 #LINE# #TAB# padding = 0 #LINE# #TAB# while len(base) % num_pad!= 0: #LINE# #TAB# #TAB# padding += 1 #LINE# #TAB# #TAB# base = base >> num_pad #LINE# #TAB# return padding"
def guard_cancel(analysis_request): #LINE# #TAB# unassigned = [] #LINE# #TAB# for analysis in analysis_request.getAnalyses(): #LINE# #TAB# #TAB# status = api.get_workflow_status_of(analysis) #LINE# #TAB# #TAB# if status not in unassigned: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if status == 'unassigned': #LINE# #TAB# #TAB# #TAB# unassigned.append(True) #LINE# #TAB# #TAB# elif not api.get_workflow_status_of(analysis) == 'unassigned': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
def is_localhost(host): #LINE# #TAB# if host == 'localhost': #LINE# #TAB# #TAB# return True #LINE# #TAB# elif host == '0.0.0.0': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def super_accepter(arg, lookup_dict): #LINE# #TAB# if isinstance(arg, (list, tuple)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# assert len(arg) == 1 #LINE# #TAB# #TAB# #TAB# res = arg[0] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# res = arg #LINE# #TAB# elif isinstance(arg, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res = lookup_dict[arg] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# res = lookup_dict[arg] #LINE# #TAB# else: #LINE# #TAB# #TAB# res = arg #LINE# #TAB# return res"
"def get_index_fields(connection, db, tbl, index): #LINE# #TAB# index_fields = [] #LINE# #TAB# sql = ""SELECT name FROM %s WHERE type='table' AND name='{tbl}'"" #LINE# #TAB# result = connection.execute(sql, (db, tbl, index)) #LINE# #TAB# if result: #LINE# #TAB# #TAB# for row in result: #LINE# #TAB# #TAB# #TAB# index_fields.append(row[0]) #LINE# #TAB# return index_fields"
def mkdir_p(path): #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise
def get_project_repository(project_element): #LINE# #TAB# repository_handler = None #LINE# #TAB# repository_name = project_element.attrib.get('name') #LINE# #TAB# if repository_name: #LINE# #TAB# #TAB# repository_handler = ProjectRepositoryHandler(repository_name) #LINE# #TAB# #TAB# project_element.attrib['repository'] = repository_handler #LINE# #TAB# return repository_handler
"def get_table_from_table_code(table_code: int, table_name: str, is_msc: #LINE# #TAB# bool=False) ->Table: #LINE# #TAB# if is_msc: #LINE# #TAB# #TAB# table = msc_tables[table_code] #LINE# #TAB# elif table_code == 1: #LINE# #TAB# #TAB# table = table_name #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Table code {} could not be detected. Please check that the code is correct.' #LINE# #TAB# #TAB# #TAB#.format(table_code)) #LINE# #TAB# return table"
def get_recipes_in_node(node): #LINE# #TAB# recipes = set() #LINE# #TAB# run_list = node.run_list #LINE# #TAB# for filename in run_list: #LINE# #TAB# #TAB# if filename.endswith('.py'): #LINE# #TAB# #TAB# #TAB# recipe = filename[:-5] #LINE# #TAB# #TAB# #TAB# if recipe not in recipes: #LINE# #TAB# #TAB# #TAB# #TAB# recipes.add(recipe) #LINE# #TAB# return recipes
"def split_original(name): #LINE# #TAB# assert name.endswith('.py') #LINE# #TAB# parts = name.split('/') #LINE# #TAB# base = parts[0] #LINE# #TAB# ext = '/'.join(parts[1:]) #LINE# #TAB# return {'base': base, 'ext': ext}"
"def image_to_tensor(image: Image) ->np.ndarray: #LINE# #TAB# mean = [0.485, 0.456, 0.406] #LINE# #TAB# std = [0.229, 0.224, 0.225] #LINE# #TAB# image = image.convert('RGB') #LINE# #TAB# tensor = np.asarray(image) #LINE# #TAB# tensor = crop_and_resize(tensor, 224) #LINE# #TAB# tensor = tensor / 255.0 #LINE# #TAB# tensor[..., 0] -= mean[0] #LINE# #TAB# tensor[..., 1] -= mean[1] #LINE# #TAB# tensor[..., 2] -= mean[2] #LINE# #TAB# tensor[..., 0] /= std[0] #LINE# #TAB# tensor[..., 1] /= std[1] #LINE# #TAB# tensor[..., 2] /= std[2] #LINE# #TAB# assert tensor.shape == (224, 224, 3) #LINE# #TAB# return tensor"
"def open_openpyxl_template(template_file): #LINE# #TAB# openpyxl_content = load_workbook(template_file, read_only=True) #LINE# #TAB# worksheet = openpyxl_content.sheet_names[0] #LINE# #TAB# return worksheet"
def arduino_default_path(): #LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# root = os.getenv('APPDATA') #LINE# #TAB# elif sys.platform == 'darwin': #LINE# #TAB# #TAB# root = os.getenv('APPDATA') #LINE# #TAB# else: #LINE# #TAB# #TAB# root = os.getenv('XDG_DATA_HOME') or os.path.expanduser('~') #LINE# #TAB# return root
"def convert_line(line: str, prop_dict: Dict[str, str]) ->None: #LINE# #TAB# for column in line.split('\t'): #LINE# #TAB# #TAB# value = column.strip() #LINE# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# if column in prop_dict: #LINE# #TAB# #TAB# #TAB# #TAB# prop_dict[column].append(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# prop_dict[column] = ''"
def parse_duration(s: str) ->int: #LINE# #TAB# if ':' not in s: #LINE# #TAB# #TAB# raise ValueError(f'Invalid duration: {s}') #LINE# #TAB# t = int(s.split(':')[0]) #LINE# #TAB# if t <= 0: #LINE# #TAB# #TAB# return t #LINE# #TAB# elif t >= 60: #LINE# #TAB# #TAB# return t + 60 #LINE# #TAB# elif t <= 60: #LINE# #TAB# #TAB# return t #LINE# #TAB# else: #LINE# #TAB# #TAB# return t
def clean_env_variables(): #LINE# #TAB# try: #LINE# #TAB# #TAB# os.environ['ID'] = os.environ['ID'] #LINE# #TAB# #TAB# os.environ['LOGNAME'] = os.environ['LOGNAME'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass
"def persistent_attribute_names_of(cls): #LINE# #TAB# names = [] #LINE# #TAB# for attr in dir(cls): #LINE# #TAB# #TAB# if not attr.startswith('_'): #LINE# #TAB# #TAB# #TAB# if inspect.isroutine(getattr(cls, attr)): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# attr = getattr(cls, attr) #LINE# #TAB# #TAB# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# #TAB# names.append(attr) #LINE# #TAB# return names"
"def get_list_devices_by_hostname(search_string, database_name='devices.db'): #LINE# #TAB# conn = sqlite3.connect(database_name) #LINE# #TAB# c = conn.cursor() #LINE# #TAB# cur = conn.cursor() #LINE# #TAB# query = 'SELECT * FROM devices WHERE op =? AND device_name =? ORDER BY device_name' #LINE# #TAB# cur.execute(query, (search_string,)) #LINE# #TAB# rows = cur.fetchall() #LINE# #TAB# conn.close() #LINE# #TAB# return rows"
"def get_end_date(obj): #LINE# #TAB# if obj.dtend is not None and isinstance(obj.dtend, datetime): #LINE# #TAB# #TAB# return obj.dtend #LINE# #TAB# elif obj.duration is not None and isinstance(obj.duration, int): #LINE# #TAB# #TAB# end_date = obj.duration #LINE# #TAB# elif obj.dtend and isinstance(obj.dtend[0], datetime): #LINE# #TAB# #TAB# end_date = obj.dtend[0] #LINE# #TAB# elif obj.dtend and isinstance(obj.dtend[1], datetime): #LINE# #TAB# #TAB# end_date = obj.dtend[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError('Invalid duration type %s' % type(obj)) #LINE# #TAB# return end_date"
def typealiases_parser(typealiases): #LINE# #TAB# typaliases = {} #LINE# #TAB# if typealiases: #LINE# #TAB# #TAB# for line in typealiases.splitlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line and not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# typaliases[line] = '' #LINE# #TAB# #TAB# #TAB# typaliases[line].strip() #LINE# #TAB# return typaliases
"def get_setting(section, name, default=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# value = getattr(settings, section, name) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return value"
"def get_formatted_default_menu_choice(state): #LINE# #TAB# choices = state.default_menu_choices #LINE# #TAB# if choices is None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# elif isinstance(choices, six.string_types): #LINE# #TAB# #TAB# return choices[0] #LINE# #TAB# elif isinstance(choices, tuple): #LINE# #TAB# #TAB# return '|'.join(choices) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
def default_logger(name): #LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# logger.propagate = False #LINE# #TAB# return logger
"def set_child_joined_alias(child, alias_map): #LINE# #TAB# try: #LINE# #TAB# #TAB# child.alias = alias_map[child.alias] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass"
def get_oidc_datas(request_parameters): #LINE# #TAB# return {param_name: request_parameters[param_name] for param_name in #LINE# #TAB# #TAB# OIDC_DESCRIPTORS if param_name in request_parameters}
"def len_header(filename): #LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# a = f.read(blimpy_header_size) #LINE# #TAB# #TAB# if a == b'#': #LINE# #TAB# #TAB# #TAB# return 4 #LINE# #TAB# #TAB# elif a == b'#': #LINE# #TAB# #TAB# #TAB# return 2 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 1"
"def plot_swim_speed(exp_ind, swim_speed): #LINE# #TAB# if len(exp_ind) > 1: #LINE# #TAB# #TAB# ax1, ax2 = plt.subplots(1, 1, figsize=(18, 10)) #LINE# #TAB# #TAB# ax1.plot(exp_ind[0], swim_speed[exp_ind[-1]], 'b') #LINE# #TAB# #TAB# ax1.set_xlim(0, exp_ind[-1] + 1) #LINE# #TAB# #TAB# ax1.set_ylim(exp_ind[-1] + 1, exp_ind[-2] + 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# ax1.plot(exp_ind, swim_speed) #LINE# #TAB# ax1.set_xlabel('Time [s]', fontsize=12) #LINE# #TAB# ax1.set_ylabel('Speed [m/s]') #LINE# #TAB# return ax1"
"def get_object(cls, api_token, cert_id): #LINE# #TAB# #TAB# cert = cls(token=api_token, id=cert_id) #LINE# #TAB# #TAB# cert.load() #LINE# #TAB# #TAB# return cert"
"def expand_file_name(path, doc_root): #LINE# #TAB# if not path: #LINE# #TAB# #TAB# return #LINE# #TAB# path = os.path.abspath(os.path.expanduser(path)) #LINE# #TAB# expanded_path = os.path.join(doc_root, os.path.basename(path)) #LINE# #TAB# return expanded_path"
def get_int_from_str(txt): #LINE# #TAB# try: #LINE# #TAB# #TAB# return int(txt) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
"def is_none(type, value): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if hasattr(type, '__origin__'): #LINE# #TAB# #TAB# type.__origin__ = None #LINE# #TAB# return type.__args__ == value"
def reset_context(): #LINE# #TAB# global _context #LINE# #TAB# _context = None
"def match_index(indices, indir): #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# full_index = os.path.join(indir, index) #LINE# #TAB# #TAB# if os.path.exists(full_index): #LINE# #TAB# #TAB# #TAB# return full_index #LINE# #TAB# return None"
def add_output_path(path: str = None) -> str: #LINE# #TAB# global _paths #LINE# #TAB# path = path or os.getcwd() #LINE# #TAB# if path not in _paths: #LINE# #TAB# #TAB# _paths.append(path) #LINE# #TAB# return path
"def find_open_port_starting_at(start_port: int): #LINE# #TAB# with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s: #LINE# #TAB# #TAB# s.bind((start_port, 0)) #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# s.close() #LINE# #TAB# #TAB# #TAB# start_port += 1 #LINE# #TAB# #TAB# return s.getsockname()[1]"
"def update_mtime(file_path): #LINE# #TAB# old_mtime = os.path.getmtime(file_path) #LINE# #TAB# os.chmod(file_path, stat.S_IMODE(os.stat(file_path).st_mtime) | stat.S_IEXEC) #LINE# #TAB# return old_mtime, old_mtime"
"def set_current_documentation_page(name): #LINE# #TAB# curr_documentation = DOCIAL_PAGES.get(name, None) #LINE# #TAB# if curr_documentation is not None: #LINE# #TAB# #TAB# curr_documentation['page_name'] = name #LINE# #TAB# global DOCIAL_PAGES #LINE# #TAB# DOCIAL_PAGES[name] = curr_documentation"
"def load_default_config(ipython_dir=None): #LINE# #TAB# config = load_default_config_file(ipython_dir=ipython_dir) #LINE# #TAB# filename = os.path.join(ipython_dir, DEFAULT_CONFIG_FILENAME) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'r') as config_file: #LINE# #TAB# #TAB# #TAB# config.load(config_file) #LINE# #TAB# except IOError as e: #LINE# #TAB# #TAB# raise ConfigError( #LINE# #TAB# #TAB# #TAB# 'Could not load default config file: {}'.format(e)) #LINE# #TAB# return config"
"def format_options(options): #LINE# #TAB# if options: #LINE# #TAB# #TAB# lines = '\n'.join(['- %s : %s' % (k, v) for k, v in options.items()]) #LINE# #TAB# #TAB# return '\n'.join(lines) #LINE# #TAB# return ''"
"def os_info(): #LINE# #TAB# name = platform.system() #LINE# #TAB# version_str = platform.release() #LINE# #TAB# if '-' in name: #LINE# #TAB# #TAB# return name, version_str #LINE# #TAB# elif name == 'nt': #LINE# #TAB# #TAB# return 'nt', platform_str #LINE# #TAB# elif name =='mac': #LINE# #TAB# #TAB# return'mac', platform_str #LINE# #TAB# elif name == 'windows': #LINE# #TAB# #TAB# return 'windows', platform_str #LINE# #TAB# else: #LINE# #TAB# #TAB# return name, version_str"
"def rotate_circular_record(record, n_bases): #LINE# #TAB# new_record = SeqRecord(record.seq) #LINE# #TAB# new_record.seq = record.seq[:n_bases] + record.seq[n_bases:] #LINE# #TAB# return new_record"
"def reduce_point_density(points, radius, priority=None): #LINE# #TAB# if priority is None: #LINE# #TAB# #TAB# priority = {'mean_squared_error': _reduce_mean_squared_error, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB#'std_error': _reduce_std_error} #LINE# #TAB# mask = _reduce_point_mask(points, radius, priority) #LINE# #TAB# return mask"
def get_one_aminame(inst_img_id): #LINE# #TAB# try: #LINE# #TAB# #TAB# aminame = Image_Name.objects.get(img_id=inst_img_id).aminame #LINE# #TAB# except: #LINE# #TAB# #TAB# raise Exception('Image_Name for image_id {} not found'.format( #LINE# #TAB# #TAB# #TAB# inst_img_id)) #LINE# #TAB# return aminame
def parse_err_query_response(response: str) ->str: #LINE# #TAB# try: #LINE# #TAB# #TAB# err_query_params = response.split(':') #LINE# #TAB# #TAB# return err_query_params[1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return response
"def exec_contract(cls, node: ast.AST) ->Optional[Callable]: #LINE# #TAB# output = None #LINE# #TAB# try: #LINE# #TAB# #TAB# module = ast.get_module(node.module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# for stmt in module.body: #LINE# #TAB# #TAB# if isinstance(stmt, ast.FunctionDef) and stmt.name == 'contract': #LINE# #TAB# #TAB# #TAB# output = stmt.func #LINE# #TAB# return output"
"def get_blueprint_routes(app, base_path): #LINE# #TAB# return { #LINE# #TAB# #TAB# route.name: dict( #LINE# #TAB# #TAB# #TAB# [( #LINE# #TAB# #TAB# #TAB# #TAB# (blueprint.name, blueprint.path.replace(base_path, '')) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# for blueprint in app.blueprints #LINE# #TAB# #TAB# ] #LINE# #TAB# }"
"def import_string(dotted_path: str) ->ty.Any: #LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
def list_resource_names(admin_required=None): #LINE# #TAB# all_resource_managers = list() #LINE# #TAB# if admin_required is None or admin_required == False: #LINE# #TAB# #TAB# return all_resource_managers #LINE# #TAB# for service in resource_manager_registry._registry.values(): #LINE# #TAB# #TAB# resource_manager_names = service.resource_names #LINE# #TAB# #TAB# for resource_manager_name in resource_manager_registry._registry[service #LINE# #TAB# #TAB# #TAB# ].resource_managers: #LINE# #TAB# #TAB# #TAB# if resource_manager_name not in admin_required: #LINE# #TAB# #TAB# #TAB# #TAB# all_resource_managers.append(resource_manager_name) #LINE# #TAB# return all_resource_managers
def get_base_input(test=False): #LINE# #TAB# from django.forms.widgets import widgets #LINE# #TAB# base_input = None #LINE# #TAB# try: #LINE# #TAB# #TAB# from django.forms.widgets.base_input import DateTimeBaseInput #LINE# #TAB# #TAB# base_input = widgets.DateTimeBaseInput #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# if test: #LINE# #TAB# #TAB# #TAB# raise import_exception #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return base_input
"def find_glob_matches(in_files, metadata): #LINE# #TAB# out_files = [] #LINE# #TAB# for in_file in in_files: #LINE# #TAB# #TAB# if 'glob' in metadata: #LINE# #TAB# #TAB# #TAB# globbed = metadata['glob'] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# globbed = fnmatch.filter(in_file, metadata['glob']) #LINE# #TAB# #TAB# if globbed: #LINE# #TAB# #TAB# #TAB# out_files.append((in_file, globbed)) #LINE# #TAB# return out_files"
"def db_connect_tester(): #LINE# #TAB# import sqlite3 #LINE# #TAB# from.modules import sqlite3 #LINE# #TAB# db_url = sqlite3.connect('database_url=%s' % database_url) #LINE# #TAB# --no_gtfs_zip_needed = ['--no_gtfs_zip_needed'] #LINE# #TAB# c = sqlite3.connect(db_url) #LINE# #TAB# yield c, db_url"
"def resolve_annotations(raw_annotations: Dict[str, AnyType], module_name: Optional[str]) -> Dict[str, AnyType]: #LINE# #TAB# if module_name is not None: #LINE# #TAB# #TAB# annotations = {'__module__': module_name, '__annotations__': raw_annotations} #LINE# #TAB# resolved = {} #LINE# #TAB# for name, value in raw_annotations.items(): #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if inspect.isclass(value) and issubclass(value, TypeVar): #LINE# #TAB# #TAB# #TAB# resolved[name] = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# resolved[name] = value #LINE# #TAB# return resolved"
"def use_rule(name, rule): #LINE# #TAB# global use_rules #LINE# #TAB# use_rules.update(rule) #LINE# #TAB# if not name in use_rules.keys(): #LINE# #TAB# #TAB# return False #LINE# #TAB# for pat in rule['patterns']: #LINE# #TAB# #TAB# r = re.compile(pat) #LINE# #TAB# #TAB# if r.match(name): #LINE# #TAB# #TAB# #TAB# use_rules[name] = r #LINE# #TAB# if 'fallback' in rule: #LINE# #TAB# #TAB# use_rules[name]['fallback'] = rule['fallback']"
def get_root_dir(): #LINE# #TAB# path = os.path.abspath(os.curdir) #LINE# #TAB# while path!= os.sep: #LINE# #TAB# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# return path #LINE# #TAB# #TAB# path = os.path.dirname(path) #LINE# #TAB# return path
"def mark_missing_in_source(config, testcases_root, svn_dir): #LINE# #TAB# if testcases_root: #LINE# #TAB# #TAB# _mark_missing_in_source(config, testcases_root, svn_dir) #LINE# #TAB# return svn_dir"
"def season_game_logs(season): #LINE# #TAB# file_name = 'game_logs.txt' #LINE# #TAB# z = get_zip_file(GOOGLE_HOME + '/' + str(season) + '/' + file_name) #LINE# #TAB# data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='""') #LINE# #TAB# data.columns = gamelog_columns #LINE# #TAB# return data"
"def validate_function_ref_variables(value, input_names): #LINE# #TAB# if isinstance(value, ast.FunctionDef): #LINE# #TAB# #TAB# if value.name in ['return']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif isinstance(value, ast.Tuple): #LINE# #TAB# #TAB# #TAB# value = tuple(validate_function_ref_variables(v, input_names) for v in value) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def get_events_for_mod(mod: util.BcmlMod) ->{}: #LINE# #TAB# events = {} #LINE# #TAB# events_keys = get_mod_events(mod) #LINE# #TAB# for event_key in events_keys: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# events[event_key] = get_logged_event(mod, event_key) #LINE# #TAB# #TAB# except BcmlError: #LINE# #TAB# #TAB# #TAB# events[event_key] = {} #LINE# #TAB# return events"
"def deserialize_dict(data, boxed_type): #LINE# #TAB# res = boxed_type() #LINE# #TAB# for key, value in six.iteritems(data): #LINE# #TAB# #TAB# if isinstance(value, boxed_type): #LINE# #TAB# #TAB# #TAB# res[key] = deserialize(value, boxed_type) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res[key] = value #LINE# #TAB# return res"
def get_unique_repositories(repo_list): #LINE# #TAB# unique_list = list() #LINE# #TAB# for repo in repo_list: #LINE# #TAB# #TAB# if repo not in unique_list: #LINE# #TAB# #TAB# #TAB# unique_list.append(repo) #LINE# #TAB# return unique_list
"def find_hashtags(text): #LINE# #TAB# hash_tags = [] #LINE# #TAB# for match in re.finditer(hashtag_pattern, text): #LINE# #TAB# #TAB# hashtag = match.group() #LINE# #TAB# #TAB# if hashtag: #LINE# #TAB# #TAB# #TAB# hash_tags.append(hashag) #LINE# #TAB# text = re.sub(' +','', text) #LINE# #TAB# return hash_tags"
"def get_nexus_subject_param(vicon, name, param): #LINE# #TAB# params = vicon.GetNexusParameters(name) #LINE# #TAB# for sub in params: #LINE# #TAB# #TAB# if sub.GetSubjectParam(): #LINE# #TAB# #TAB# #TAB# return sub.GetSubjectParam() #LINE# #TAB# return None"
"def check_dtype(a, dtype=np.floating): #LINE# #TAB# if not isinstance(a, np.ndarray): #LINE# #TAB# #TAB# raise TypeError('Input array type must be of type {}'.format(dtype)) #LINE# #TAB# if a.dtype!= dtype: #LINE# #TAB# #TAB# raise TypeError('Input array type must be of type {}'.format(dtype)) #LINE# #TAB# return a"
"def reserve_memory(mem_size_str='1GB', reserve_time=3600): #LINE# #TAB# logger.debug('reserve_memory: %s' % mem_size_str) #LINE# #TAB# start_time = time.time() #LINE# #TAB# if time.time() - start_time < reserve_time: #LINE# #TAB# #TAB# size = int(time.time()) #LINE# #TAB# else: #LINE# #TAB# #TAB# size = int(round(mem_size_str, 2)) #LINE# #TAB# if size < start_time: #LINE# #TAB# #TAB# size += reserve_time - start_time #LINE# #TAB# else: #LINE# #TAB# #TAB# size = int(round(mem_size_str, 2)) #LINE# #TAB# logger.debug('reserve_memory: %s' % size) #LINE# #TAB# return size"
"def channel_names(fn, usecols=None): #LINE# #TAB# nchan = get_nchan(fn) #LINE# #TAB# cols = {} #LINE# #TAB# for i in range(nchan): #LINE# #TAB# #TAB# for col in range(nchan): #LINE# #TAB# #TAB# #TAB# cols[col] = nchan[col] #LINE# #TAB# dnames = {} #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# dnames[col] = nchan #LINE# #TAB# if usecols is not None: #LINE# #TAB# #TAB# dnames = dict(dnames.items()) #LINE# #TAB# return dnames"
"def sanitize_cloud(cloud: str) -> str: #LINE# #TAB# if len(cloud) < 4: #LINE# #TAB# #TAB# return cloud #LINE# #TAB# if not cloud[3].isdigit() and cloud[3] not in ('/', '-'): #LINE# #TAB# #TAB# if cloud[3] == 'O': #LINE# #TAB# #TAB# #TAB# cloud = cloud[:3] + '0' + cloud[4:] #LINE# #TAB# #TAB# elif cloud[3]!= 'U': #LINE# #TAB# #TAB# #TAB# cloud = cloud[:3] + cloud[4:] + cloud[3] #LINE# #TAB# return cloud"
"def mapping_to_sets(mapping): #LINE# #TAB# smap = {} #LINE# #TAB# for k, v in mapping.items(): #LINE# #TAB# #TAB# smap[v] = set(v) #LINE# #TAB# return smap"
"def find_str(s, char): #LINE# #TAB# start = s.find(char, 0) #LINE# #TAB# if start == -1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return start + 1"
"def px4_update(IMU, ATT): #LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# px4_state.update(IMU) #LINE# #TAB# if ATT is None: #LINE# #TAB# #TAB# ATT = ATT.value #LINE# #TAB# px4_state.update_ATT(ATT) #LINE# #TAB# r1, r2, r3 = px4_state.get_r3() #LINE# #TAB# px4_state.update_r1(r1, r2) #LINE# #TAB# px4_state.update_r3(r3, r4) #LINE# #TAB# px4_state.update_bcr(IMU) #LINE# #TAB# return 1"
def get_user_shell(): #LINE# #TAB# import pwd #LINE# #TAB# pwuid = pwd.getpwuid(os.getuid()) #LINE# #TAB# if not pwuid: #LINE# #TAB# #TAB# return '/bin/sh' #LINE# #TAB# else: #LINE# #TAB# #TAB# return pwuid
def get_gateway_dict(vpc): #LINE# #TAB# return {get_name(gateway): gateway for gateway in vpc.vpcs.all()}
"def get_overlay_spec(o, k, v): #LINE# #TAB# s = '' #LINE# #TAB# if o.HasElement('type'): #LINE# #TAB# #TAB# s += o.GetElement('type').GetLabel() + ':' + v #LINE# #TAB# elif o.HasElement('group'): #LINE# #TAB# #TAB# s += o.GetElement('group').GetLabel() + ':' + v #LINE# #TAB# elif o.HasElement('key'): #LINE# #TAB# #TAB# s += o.GetElement('key').GetValue() #LINE# #TAB# return s"
"def get_image_label(name, default=""not_found.png""): #LINE# #TAB# label = QLabel() #LINE# #TAB# label.setPixmap(QPixmap(get_image_path(name, default))) #LINE# #TAB# return label"
"def colorize_headlines_visitor(c, p, item): #LINE# #TAB# if p.h.startswith('!= '): #LINE# #TAB# #TAB# f = item.font(0) #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# #TAB# item.setFont(0, f) #LINE# #TAB# raise leoPlugins.TryNext"
"def iter_documents(obj, path, ext): #LINE# #TAB# path = path[:-1] #LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for key in obj.keys(): #LINE# #TAB# #TAB# #TAB# if path[-1] == '/': #LINE# #TAB# #TAB# #TAB# #TAB# path = path[:-1] #LINE# #TAB# #TAB# #TAB# #TAB# document = obj[key] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# document = obj #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# document = iter_documents(item, path, ext) #LINE# #TAB# else: #LINE# #TAB# #TAB# document = obj #LINE# #TAB# #TAB# yield document, path, ext"
"def to_lists(x): #LINE# #TAB# if isinstance(x[0], tuple): #LINE# #TAB# #TAB# return list(x) #LINE# #TAB# elif isinstance(x[0], list): #LINE# #TAB# #TAB# return list(x) #LINE# #TAB# else: #LINE# #TAB# #TAB# return [x[0]]"
"def read_gtfs_trips(textfile_path, textfile): #LINE# #TAB# if textfile!= 'trip.txt': #LINE# #TAB# #TAB# raise ValueError('{} is not a proper GTFS file name'.format(textfile)) #LINE# #TAB# df = pd.read_csv(os.path.join(textfile_path, textfile), dtype={ #LINE# #TAB# #TAB# 'trip_id': object}, low_memory=False) #LINE# #TAB# if len(df) == 0: #LINE# #TAB# #TAB# raise ValueError('{} has no records'.format(os.path.join( #LINE# #TAB# #TAB# #TAB# textfile_path, textfile))) #LINE# #TAB# df['trip_id'] = pd.to_numeric(df['trip_id']) #LINE# #TAB# df['trip_text'] = pd.to_string(df['trip_text']) #LINE# #TAB# return df"
"def git_templates(): #LINE# #TAB# from shutil import which #LINE# #TAB# if which('git-templates'): #LINE# #TAB# #TAB# templates = which('git-templates') #LINE# #TAB# #TAB# if templates is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# elif sys.platform == 'darwin': #LINE# #TAB# #TAB# #TAB# templates = os.path.join(os.path.expanduser('~'), '.git', 'templates') #LINE# #TAB# #TAB# elif os.path.exists(templates): #LINE# #TAB# #TAB# #TAB# return templates #LINE# #TAB# return None"
def micro_service_filter(cls): #LINE# #TAB# try: #LINE# #TAB# #TAB# return cls.__subclasses__([cls]) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return cls
def postorder_count(node): #LINE# #TAB# if node.left is not None: #LINE# #TAB# #TAB# return sum(postorder_count(node.left)) #LINE# #TAB# elif node.right is not None: #LINE# #TAB# #TAB# return sum(postorder_count(node.right)) #LINE# #TAB# return 1
"def clone_plugin_data(dashboard_entry, request=None): #LINE# #TAB# plugin_data = {} #LINE# #TAB# plugin_data['dashboard_entry'] = dashboard_entry #LINE# #TAB# plugin_data['user'] = dashboard_entry.user #LINE# #TAB# plugin_data['content'] = dashboard_entry.content #LINE# #TAB# if request: #LINE# #TAB# #TAB# plugin_data['request'] = clone_request(request) #LINE# #TAB# return plugin_data"
"def read_coords_params(cls, fits, extname): #LINE# #TAB# hdu_coords = fits.getdata(extname) #LINE# #TAB# param_coords = [] #LINE# #TAB# for point in hdu_coords: #LINE# #TAB# #TAB# param_coords.append(np.array(point.data)) #LINE# #TAB# return param_coords"
"def theta_b3o3oh4_hco3_fw86(T, P): #LINE# #TAB# theta = 0.12 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"def get_section_list(school_id): #LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_manager.get_api_url(), #LINE# #TAB# #TAB# school_id) #LINE# #TAB# resp = requests.get(url) #LINE# #TAB# code_value = resp.status_code #LINE# #TAB# data = resp.data.decode('utf-8') #LINE# #TAB# if code_value == 200: #LINE# #TAB# #TAB# return code_value, data #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return json.loads(data) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None, None"
"def create_loan_replace_item_blueprint(app): #LINE# #TAB# blueprint = Blueprint( #LINE# #TAB# #TAB# 'loan_replace_item', #LINE# #TAB# #TAB# __name__, #LINE# #TAB# #TAB# template_folder='templates', #LINE# #TAB# #TAB# static_folder=os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# #TAB# 'loan-replace-item.html') #LINE# #TAB# ) #LINE# #TAB# return blueprint"
"def fix_numeric(ns: Mapping[str, Value]) ->Dict[str, Value]: #LINE# #TAB# if not namespace: #LINE# #TAB# #TAB# return ns #LINE# #TAB# for key, value in ns.items(): #LINE# #TAB# #TAB# if isinstance(value, Numbered): #LINE# #TAB# #TAB# #TAB# namespace[key] = value.fix_numeric() #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# namespace[key] = fix_numeric(value) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# namespace[key] = [fix_numeric(item) for item in value] #LINE# #TAB# return ns"
"def conditional_str(prefix, obj, suffix): #LINE# #TAB# if obj is not None: #LINE# #TAB# #TAB# return prefix + str(obj) + suffix #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
"def compute_exit_code(config, exception=None): #LINE# #TAB# if exception is not None: #LINE# #TAB# #TAB# raise exception #LINE# #TAB# if config.is_server: #LINE# #TAB# #TAB# return 2 #LINE# #TAB# return 0"
"def nltk_isri_stemmer(input_dict): #LINE# #TAB# return {'tagger': {'object': nltk.stem.arial.isri.stemmer(), #LINE# #TAB# #TAB# 'function':'stem'}}"
"def from_base_n(base_num, base=None, alphabet=None): #LINE# #TAB# if not alphabet: #LINE# #TAB# #TAB# alphabet = alphabet_from_num(base_num) #LINE# #TAB# out = '' #LINE# #TAB# for num in base_num: #LINE# #TAB# #TAB# if num.isdigit(): #LINE# #TAB# #TAB# #TAB# out += alphabet[int(num)] #LINE# #TAB# #TAB# elif num.isdigit(): #LINE# #TAB# #TAB# #TAB# out += int(num) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('Unrecognised base number') #LINE# #TAB# return out"
def remove_duckling_entities(entity_predictions): #LINE# #TAB# new_entity_predictions = [] #LINE# #TAB# for p in entity_predictions: #LINE# #TAB# #TAB# if p['type']!= 'duckling': #LINE# #TAB# #TAB# #TAB# new_entity_predictions.append(p) #LINE# #TAB# return new_entity_predictions
"def reset_attributes(cls): #LINE# #TAB# return {k: v.original_value for k, v in vars(cls).items() if not k. #LINE# #TAB# #TAB# startswith('_')}"
"def to_numpy(nd4j_array): #LINE# #TAB# buff = nd4j_array.data() #LINE# #TAB# address = buff.pointer().address() #LINE# #TAB# dtype = get_context_dtype() #LINE# #TAB# mapping = { #LINE# #TAB# #TAB# 'double': ctypes.c_double, #LINE# #TAB# #TAB# 'float': ctypes.c_float #LINE# #TAB# } #LINE# #TAB# Pointer = ctypes.POINTER(mapping[dtype]) #LINE# #TAB# pointer = ctypes.cast(address, Pointer) #LINE# #TAB# np_array = np.ctypeslib.as_array(pointer, tuple(nd4j_array.shape())) #LINE# #TAB# return np_array"
"def get_creator_by_name(name): #LINE# #TAB# if name in creator_cache: #LINE# #TAB# #TAB# return creator_cache[name] #LINE# #TAB# pkg_name, obj_name = name.rsplit('.', 1) #LINE# #TAB# sys.path.append(os.path.join(pkg_name, obj_name)) #LINE# #TAB# func = get_module_by_name(name) #LINE# #TAB# function = getattr(func, '__module__', '') #LINE# #TAB# creator_cache[name] = function #LINE# #TAB# return function"
"def get_attribute(data: dict, attribute_name: str): #LINE# #TAB# value = data.get(attribute_name) #LINE# #TAB# return value"
"def check_imc_creds(auth, url): #LINE# #TAB# LOGGER.debug('Checking IMC credentials for %s', url) #LINE# #TAB# succeeded, _ = auth.check_credentials(url) #LINE# #TAB# if succeeded: #LINE# #TAB# #TAB# LOGGER.debug('Authentication successful') #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# LOGGER.debug('Authentication failed') #LINE# #TAB# #TAB# return False"
"def load_csv(base_file, csv_name, sep=',', convert_float=False): #LINE# #TAB# filepath = os.path.dirname(os.path.abspath(base_file)) #LINE# #TAB# filename = os.path.join(filepath, csv_name) #LINE# #TAB# engine = 'python' if sep!= ',' else 'c' #LINE# #TAB# float_precision = {} #LINE# #TAB# if engine == 'c': #LINE# #TAB# #TAB# float_precision = {'float_precision': 'high'} #LINE# #TAB# data = pd.read_csv(filename, sep=sep, engine=engine, **float_precision) #LINE# #TAB# if convert_float: #LINE# #TAB# #TAB# data = data.astype(float) #LINE# #TAB# return data"
"def search_mailingaddr_valid(cls, name, clause): #LINE# #TAB# tab_qu = cls.get_mailing_addr_sql() #LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# qu1 = tab_qu.select(tab_qu.id_line, where=Operator(tab_qu.mailing_addr, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"def cycle_vector(vector, vector_length, shift): #LINE# #TAB# result = numpy.empty(vector_length) #LINE# #TAB# result[:-shift] = vector[0:-shift] #LINE# #TAB# for i in range(vector_length): #LINE# #TAB# #TAB# result[i] = vector[i + shift:i + shift] #LINE# #TAB# return result"
"def camel_to_underscore(name): #LINE# #TAB# new_name = '' #LINE# #TAB# for i, char in enumerate(name): #LINE# #TAB# #TAB# if char.isupper(): #LINE# #TAB# #TAB# #TAB# if i > 0: #LINE# #TAB# #TAB# #TAB# #TAB# new_name += '_' #LINE# #TAB# #TAB# #TAB# new_name += char.lower() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_name += char #LINE# #TAB# return new_name"
"def get_toolkit_to_apic_classmap(cls): #LINE# #TAB# return {'fvCEp': Endpoint, 'fvStCEp': Endpoint, 'fvCrtrn': #LINE# #TAB# #TAB# AttributeCriterion}"
"def parse_hex(value): #LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value, 16) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return value"
def get_query_type_from_type_string(type_string): #LINE# #TAB# if type_string!='string': #LINE# #TAB# #TAB# parts = type_string.split('.') #LINE# #TAB# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# #TAB# query_type_string = parts[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# query_type_string = '.'.join(parts) #LINE# #TAB# else: #LINE# #TAB# #TAB# query_type_string = type_string #LINE# #TAB# return query_type_string
"def calculate_contact_map_eigen(contactMap): #LINE# #TAB# m = len(contactMap) #LINE# #TAB# eigenvalues = np.zeros(m) #LINE# #TAB# eigv = np.zeros(m) #LINE# #TAB# for c in contactMap: #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# #TAB# while i < m: #LINE# #TAB# #TAB# #TAB# if c in contactMap[c]: #LINE# #TAB# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# #TAB# #TAB# eigv[:, (i)] = np.linalg.eigvalsh(contactMap[c][i]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# eigv[:, (i)] = 0 #LINE# #TAB# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# return eigenvalues, eigv"
"def numpy_to_vtk_points(nodes, points=None, dtype='<f', deep=1): #LINE# #TAB# vtk_points = None #LINE# #TAB# if points is None: #LINE# #TAB# #TAB# vtk_points = {} #LINE# #TAB# #TAB# for i, point in enumerate(nodes): #LINE# #TAB# #TAB# if isinstance(point, np.ndarray): #LINE# #TAB# #TAB# #TAB# vtk_points[i] = numpy_support.numpy_to_vtk_points(point, deep) #LINE# #TAB# #TAB# elif isinstance(point, np.ndarray): #LINE# #TAB# #TAB# #TAB# vtk_points[i] = numpy_support.numpy_to_vtk_points(point, deep) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# vtk_points[i] = point #LINE# #TAB# return vtk_points"
def check_macs(ip_address): #LINE# #TAB# global nmap #LINE# #TAB# print('checking mac address...') #LINE# #TAB# match = nmap.ip_address.match(ip_address) #LINE# #TAB# if match == None: #LINE# #TAB# #TAB# print('ip address not found') #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# print('found mac address: %s' % match) #LINE# #TAB# #TAB# return 1
"def suggest_parser(cls, ext, text): #LINE# #TAB# if ext == '': #LINE# #TAB# #TAB# return cls.defaultShebangParser #LINE# #TAB# parser = cls.suggestParserForExtension(ext, text) #LINE# #TAB# if parser is None: #LINE# #TAB# #TAB# cls.registerShebangKeyword(cls.defaultShebangKeyword) #LINE# #TAB# #TAB# parser = cls.defaultShebangParser #LINE# #TAB# return parser"
"def from_path(cls, path): #LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# raise TypeError('Path is not a directory') #LINE# #TAB# source_list = cls() #LINE# #TAB# for root, _, files in os.walk(path): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if f.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# source_file = Source.from_file(os.path.join(root, f)) #LINE# #TAB# #TAB# #TAB# #TAB# source_list.append(source_file) #LINE# #TAB# return source_list"
"def get_chapter_urls(soup): #LINE# #TAB# preload_images = soup.find_all('preload_images') #LINE# #TAB# urls = [] #LINE# #TAB# for img in preload_images: #LINE# #TAB# #TAB# if img.get('name') == 'image': #LINE# #TAB# #TAB# #TAB# urls.append(img.get('src')) #LINE# #TAB# filenames = [f.split('.')[-1] for f in images] #LINE# #TAB# return urls, filenames"
"def is_point(pointlist): #LINE# #TAB# try: #LINE# #TAB# #TAB# if not isinstance(pointlist, list) or not isinstance(pointlist[0], float #LINE# #TAB# #TAB# #TAB# ) or not isinstance(pointlist[1], float): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# for x, y in pointlist: #LINE# #TAB# #TAB# #TAB# if not is_point(x, y): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
def utc_to_gps(timestamp): #LINE# #TAB# td = datetime.datetime.utcfromtimestamp(timestamp) #LINE# #TAB# return (td.hour * 3600 + td.minute * 60 + td.second + td. #LINE# #TAB# #TAB# microsecond) / 1000000.0
def check_dir_existence(path: str) ->bool: #LINE# #TAB# import os #LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as exception: #LINE# #TAB# #TAB# if exception.errno!= errno.ENOENT: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
def sanitise_name(name): #LINE# #TAB# name = name.lower() #LINE# #TAB# if name.startswith('p_'): #LINE# #TAB# #TAB# name = name[4:] #LINE# #TAB# if name.endswith('.py'): #LINE# #TAB# #TAB# name = name[:-7] #LINE# #TAB# if '-' in name: #LINE# #TAB# #TAB# name = name[:-2] #LINE# #TAB# return name
def slice_required_len(slice_obj): #LINE# #TAB# if slice_obj.step and slice_obj.step!= 1: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.start is None and slice_obj.stop is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.start and slice_obj.stop < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.stop and slice_obj.start > slice_obj.stop: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.start and slice_obj.start < slice_obj.stop: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.stop and slice_obj.stop < slice_obj.start: #LINE# #TAB# #TAB# return None #LINE# #TAB# return slice_obj.stop + 1
"def get_last_update(): #LINE# #TAB# from datetime import datetime #LINE# #TAB# date_str =''.join([datetime.datetime.now(), datetime.datetime.now()]) #LINE# #TAB# date_str = date_str[:-10] #LINE# #TAB# return date_str"
"def get_version(package_name, ignore_cache=False): #LINE# #TAB# try: #LINE# #TAB# #TAB# version = pkg_resources.get_distribution(package_name).version #LINE# #TAB# #TAB# if version == 'unknown': #LINE# #TAB# #TAB# #TAB# return 'unknown' #LINE# #TAB# #TAB# elif ignore_cache: #LINE# #TAB# #TAB# #TAB# return 'unknown' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return version #LINE# #TAB# except: #LINE# #TAB# #TAB# return 'unknown'"
"def dct_aicc(yk, theory_var=None): #LINE# #TAB# NF = len(yk) #LINE# #TAB# aic = np.zeros(NF) #LINE# #TAB# aic[-1] = 0.0 #LINE# #TAB# for K in range(1, NF - 1): #LINE# #TAB# #TAB# aic[K] = aic[K] + 2 * (K + 1) / (NF - K + 2) #LINE# #TAB# aic = aic + 1.0 #LINE# #TAB# return aic"
"def dp_unnormalise(y, normalisation_parameters): #LINE# #TAB# norms = zip(y, normalisation_parameters) #LINE# #TAB# norms = norms[0] #LINE# #TAB# new_y = np.copy(y) #LINE# #TAB# for k in range(norms.shape[0]): #LINE# #TAB# #TAB# if np.abs(norms[k] < 1e-08) and np.abs(norms[k] > 1e-08): #LINE# #TAB# #TAB# #TAB# new_y[k] = y[k] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_y[k] = 1.0 #LINE# #TAB# return new_y"
"def to_vararray(var_instance, bounds): #LINE# #TAB# assert isinstance(var_instance, SymbolVAR) #LINE# #TAB# from symbols import BOUNDLIST #LINE# #TAB# from symbols import VARARRAY #LINE# #TAB# assert isinstance(bounds, BOUNDLIST) #LINE# #TAB# var_instance.__class__ = VARARRAY #LINE# # var_instance.class_ = CLASS.array #LINE# #TAB# var_instance.bounds = bounds #LINE# #TAB# return var_instance"
"def should_run_cmake(commands, cmake_with_sdist): #LINE# #TAB# for cmd in commands: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if which(cmd): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if cmake_with_sdist: #LINE# #TAB# #TAB# paths = find_cmake_paths(cmake_with_sdist) #LINE# #TAB# #TAB# for path in paths: #LINE# #TAB# #TAB# #TAB# if any(command.exists() for command in paths): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def multilocus_d_est(Ht_est, Hs_est, n): #LINE# #TAB# n = float(n) #LINE# #TAB# Ht_est = sum(Ht_est) / float(len(Ht_est)) #LINE# #TAB# Hs_est = sum(Hs_est) / float(len(Hs_est)) #LINE# #TAB# if (n - 1.0) * (Ht_est - Hs_est) == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# d_est = multilocus_D(Ht_est, Hs_est) #LINE# #TAB# #TAB# return d_est"
"def bc_sm_cl_pm73(T, P): #LINE# #TAB# b0 = 1.622 * 2 / 3 #LINE# #TAB# b1 = 8.231 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.09057 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['SM'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"def get_labels(labels): #LINE# #TAB# label_u = unique_labels(labels) #LINE# #TAB# label_u_line = [(i + '_line') for i in label_u] #LINE# #TAB# return label_u, label_u_line"
"def glob_session(ses_path): #LINE# #TAB# ses_path = pathlib.Path(ses_path) #LINE# #TAB# if not ses_path.exists(): #LINE# #TAB# #TAB# return [] #LINE# #TAB# pattern = ses_path.joinpath('*') #LINE# #TAB# glob_result = glob.glob(pattern) #LINE# #TAB# if glob_result: #LINE# #TAB# #TAB# if isinstance(glob_result[0], str): #LINE# #TAB# #TAB# #TAB# glob_result[0] = glob_result[0].replace('\\', '/') #LINE# #TAB# #TAB# glob_result[1] = glob_result[1].replace('\\', '/') #LINE# #TAB# glob_result = glob.glob(ses_path.joinpath('*')) #LINE# #TAB# if not glob_result: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return glob_result"
"def psi_k_na_cl_pp87ii(T, P): #LINE# #TAB# psi = -0.0102 #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"def from_string(config_string, based_on=None, filename='<string>'): #LINE# #TAB# config = get_default(based_on=based_on, filename=filename) #LINE# #TAB# parser = ConfigParser() #LINE# #TAB# if isinstance(config_string, dict): #LINE# #TAB# #TAB# parser.read_string(config_string) #LINE# #TAB# config.parse(config_string) #LINE# #TAB# return config"
"def is_last_page(grab): #LINE# #TAB# try: #LINE# #TAB# #TAB# next_link = grab.xpath_one('//a[contains(@class, ""b-pager__next"")]') #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# logging.debug('No results found') #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"def dists_from_mean_slow(qs): #LINE# #TAB# dists = [] #LINE# #TAB# for u in qs: #LINE# #TAB# #TAB# mean = np.mean(u) #LINE# #TAB# #TAB# dists.append((u, mean)) #LINE# #TAB# return dists"
"def get_pixel_coords(): #LINE# #TAB# pixel_coords = np.array([[0, 0, 0], [90, 90, 90], [0, 90, 90], [0, 90, 90], #LINE# #TAB# #TAB# dtype=np.float64) #LINE# #TAB# return pixel_coords"
"def package_search(data_dict): #LINE# #TAB# if 'package_search' not in data_dict: #LINE# #TAB# #TAB# return {} #LINE# #TAB# override = data_dict.get('package_search', False) #LINE# #TAB# if not override: #LINE# #TAB# #TAB# result_list = packages.package_search(data_dict) #LINE# #TAB# #TAB# result_list = sorted(result_list, key=lambda x: x['metadata_modified_date'], #LINE# #TAB# #TAB# #TAB# reverse=True) #LINE# #TAB# #TAB# data_dict['limit'] = result_list['limit'] #LINE# #TAB# else: #LINE# #TAB# #TAB# pass #LINE# #TAB# return {'result': result_dict}"
"def create_kernel(shape): #LINE# #TAB# image_shape = np.array(shape, dtype=np.int) #LINE# #TAB# kernel = np.zeros(image_shape) #LINE# #TAB# for i in range(shape): #LINE# #TAB# #TAB# x = i * np.ones_like(image_shape) #LINE# #TAB# #TAB# y = i * np.ones_like(image_shape) #LINE# #TAB# #TAB# kernel[y, x] = 1 #LINE# #TAB# return kernel"
def reverse_graph(y): #LINE# #TAB# graph = {} #LINE# #TAB# edges = y.graph #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if edge[0] == y[1]: #LINE# #TAB# #TAB# #TAB# edges[edge[0]] = edge[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# edges[edge[0]] = edge[1] #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if edge[0] == y[1]: #LINE# #TAB# #TAB# #TAB# edges[edge[0]] = edge[1] #LINE# #TAB# return graph
"def output_graph(graph, root=None): #LINE# #TAB# root = root or '.' #LINE# #TAB# result = graph.depth_first_search(root) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# import traceback #LINE# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# #TAB# return '' #LINE# #TAB# output = '\n\n' #LINE# #TAB# output += '\n' + '\n'.join(result) #LINE# #TAB# output += '\n-->' + graph.to_dot() + '\n\n' #LINE# #TAB# output += '\n-->' + '\n\n' #LINE# #TAB# return output"
"def is_uri_to_be_filtered(uri, filter_list): #LINE# #TAB# uri_to_be_filtered = False #LINE# #TAB# for filter_name in filter_list: #LINE# #TAB# #TAB# if filter_name.lower() == uri.lower(): #LINE# #TAB# #TAB# #TAB# uri_to_be_filtered = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return uri_to_be_filtered"
def fetch_framework_var(attr_name): #LINE# #TAB# framework_var = framework_objects.get_framework_variable(attr_name) #LINE# #TAB# if not framework_var: #LINE# #TAB# #TAB# new_attr = framework_objects.get_framework_variable(attr_name) #LINE# #TAB# #TAB# framework_var = new_attr #LINE# #TAB# else: #LINE# #TAB# #TAB# attr = framework_objects.get_framework_variable(attr_name) #LINE# #TAB# #TAB# framework_var = framework_objects.get_framework_variable(attr) #LINE# #TAB# return framework_var
"def run_command(path, command): #LINE# #TAB# command_path = os.path.abspath(command) #LINE# #TAB# if not path.endswith('/'): #LINE# #TAB# #TAB# path += '/' #LINE# #TAB# cmd_path = os.path.join(path, command) #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_call(cmd_path, shell=True) #LINE# #TAB# #TAB# output = subprocess.check_output(cmd_path, shell=True) #LINE# #TAB# #TAB# output = output.decode('utf-8') #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"def serialize_request_payload(data: dict) ->dict: #LINE# #TAB# json_data = json.dumps(data, ensure_ascii=False) #LINE# #TAB# if isinstance(json_data, str): #LINE# #TAB# #TAB# datetime_payload = data #LINE# #TAB# elif isinstance(json_data, bytes): #LINE# #TAB# #TAB# datetime_payload = data.isoformat() #LINE# #TAB# else: #LINE# #TAB# #TAB# datetime_payload = json_data #LINE# #TAB# return {'datetime': datetime_payload}"
def load_file(path): #LINE# #TAB# if path.is_file(): #LINE# #TAB# #TAB# with path.open() as fp: #LINE# #TAB# #TAB# #TAB# return yaml.safe_load(fp) #LINE# #TAB# return {}
def remove_supporting(written_files: List[str]): #LINE# #TAB# for written_file in written_files: #LINE# #TAB# #TAB# if os.path.splitext(written_file)[1] == '.py': #LINE# #TAB# #TAB# #TAB# os.remove(written_file) #LINE# #TAB# return written_files
"def get_spark_memory_config(memory=SparkDefault.MEMORY): #LINE# #TAB# if isinstance(memory, str): #LINE# #TAB# #TAB# memory = memory.split('.') #LINE# #TAB# config = [] #LINE# #TAB# for element in memory: #LINE# #TAB# #TAB# if element =='spark': #LINE# #TAB# #TAB# #TAB# config.append({'spark': spark_id,'memory': memory}) #LINE# #TAB# #TAB# elif element =='mldd': #LINE# #TAB# #TAB# #TAB# config.append({'spark': spark_id,'memory': memory}) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# config.append({'spark': spark_id,'memory': memory}) #LINE# #TAB# config.sort(key=lambda x: x[0]) #LINE# #TAB# return config"
"def minpy_getitem_grad(arr, index, g): #LINE# #TAB# ret = minpy_array_grad(arr, index, g) #LINE# #TAB# if ret is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# ret[0] = g #LINE# #TAB# return ret"
"def remove_negative_entries(A): #LINE# #TAB# assert A.shape[0] >= 0 #LINE# #TAB# new_row_indices = A.indices #LINE# #TAB# new_col_indices = A.indices #LINE# #TAB# for idx in range(A.shape[0]): #LINE# #TAB# #TAB# if A[idx, idx] < 0: #LINE# #TAB# #TAB# #TAB# new_row_indices[idx, idx] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_row_indices[idx, idx] = 0 #LINE# #TAB# A = A.tocsc() #LINE# #TAB# return A, new_row_indices, new_col_indices"
"def apply_items_index(mcs, items): #LINE# #TAB# new_items = [] #LINE# #TAB# for idx, val in enumerate(items): #LINE# #TAB# #TAB# if val is not None: #LINE# #TAB# #TAB# #TAB# items[idx], new_items.append(val) #LINE# #TAB# return mcs, new_items"
"def item_sectie_adapter(obj, request): #LINE# #TAB# return {'niscode': obj.niscode, 'naam': obj.naam, 'gewest': {'id': obj.gewest.id, #LINE# #TAB# #TAB# 'naam': obj.gewest.naam}}"
def get_security_policy(user: Optional[IPrincipal]=None) ->ISecurityPolicy: #LINE# #TAB# global _security_policy #LINE# #TAB# if _security_policy is None: #LINE# #TAB# #TAB# _security_policy = ISecurityPolicy() #LINE# #TAB# if user is not None: #LINE# #TAB# #TAB# _security_policy.setUser(user) #LINE# #TAB# return _security_policy
"def byte_list_to_u16le_list(byteData): #LINE# #TAB# halfWordList = [] #LINE# #TAB# for byte in byteData: #LINE# #TAB# #TAB# halfWord = struct.unpack('<H', byte)[0] #LINE# #TAB# #TAB# halfWordList.append(halfWord) #LINE# #TAB# return halfWordList"
"def std_nlnlike2d(params, endog, n_bkg_exp, signal_2d, background_2d): #LINE# #TAB# n_sig = params[0] #LINE# #TAB# n_bkg = params[1] #LINE# #TAB# alpha = n_bkg_exp * n_sig #LINE# #TAB# bkg = np.tile(background_2d, (n_bkg, n_bkg)) #LINE# #TAB# s = signal_2d.pdf(endog) #LINE# #TAB# b = background_2d.pdf(endog) #LINE# #TAB# sumlogl = np.sum(np.log(n_sig * s + b * b)) #LINE# #TAB# sumlogl -= np.sum(np.log(np.arange(1, n_bkg + 1))) #LINE# #TAB# return -sumlogl"
"def get_flow_file(flow_name): #LINE# #TAB# command = ( #LINE# #TAB# #TAB# ""flow -f {0} -e {1} -d {2} -e '{3}' -d {4}"" #LINE# #TAB# #TAB#.format(flow_name, flow_name, os.path.basename(flow_name))) #LINE# #TAB# try: #LINE# #TAB# #TAB# flow_file = subprocess.check_output(command, stderr=subprocess.STDOUT) #LINE# #TAB# #TAB# if flow_file.strip(): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.remove(flow_file) #LINE# #TAB# #TAB# #TAB# except (subprocess.CalledProcessError, OSError): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return flow_file #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# return None"
def add_sbo(model): #LINE# #TAB# sbo_model = '' #LINE# #TAB# if model.description!= '': #LINE# #TAB# #TAB# sbo_model +='' #LINE# #TAB# for i in model.demands: #LINE# #TAB# #TAB# sbo_model += i.sbo_term() #LINE# #TAB# for i in model.exchanges: #LINE# #TAB# #TAB# if i.description!= '': #LINE# #TAB# #TAB# #TAB# sbo_model +='' #LINE# #TAB# #TAB# sbo_model += i.sbo_term() #LINE# #TAB# return sbo_model
"def wkt_to_rectangle(extent): #LINE# #TAB# bbox = None #LINE# #TAB# try: #LINE# #TAB# #TAB# srs = pyproj.SpatialReference() #LINE# #TAB# #TAB# srs.ImportFromWkt(extent) #LINE# #TAB# except pyproj.error as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# xmin, xmax = extent[0], extent[1] #LINE# #TAB# ymin, ymax = extent[2], extent[3] #LINE# #TAB# bbox = Rect() #LINE# #TAB# bbox.xmin = xmin #LINE# #TAB# bbox.ymin = xmax #LINE# #TAB# bbox.ymax = ymax #LINE# #TAB# bbox.width = xmax #LINE# #TAB# bbox.height = ymax #LINE# #TAB# return bbox"
"def get_model_var_scope(module_scope, model_name): #LINE# #TAB# module_scope_name = module_scope[:-1] #LINE# #TAB# model_scope_name = model_name #LINE# #TAB# while model_scope_name in dir(module_scope): #LINE# #TAB# #TAB# model_scope_name = '%s.%s' % (module_scope_name, model_name) #LINE# #TAB# if not module_scope_name.endswith('.'): #LINE# #TAB# #TAB# module_scope_name = module_scope[:-1] #LINE# #TAB# if module_scope_name == '': #LINE# #TAB# #TAB# return None, None #LINE# #TAB# parent_scope = getattr(module_scope, model_scope_name) #LINE# #TAB# if parent_scope is not None: #LINE# #TAB# #TAB# return parent_scope, model_scope_name #LINE# #TAB# return module_scope, model_scope_name"
"def flip_layers(nparray): #LINE# #TAB# if nparray % 2 == 1: #LINE# #TAB# #TAB# new_img = np.zeros((nparray, nparray, 3), dtype=np.uint8) #LINE# #TAB# #TAB# new_img[(...), ::-1] = np.uint8(nparray % 2) #LINE# #TAB# #TAB# new_img[(...), ::-1] = np.uint8(nparray % 2) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_img = np.zeros((nparray, nparray), dtype=np.uint8) #LINE# #TAB# #TAB# new_img[(...), ::-1] = np.uint8(nparray % 2) #LINE# #TAB# return new_img"
def epoch_to_human_time(epoch_time): #LINE# #TAB# date_time = datetime.datetime.fromtimestamp(epoch_time) #LINE# #TAB# human_time = date_time.strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# return human_time
"def cache_root(custom_directory): #LINE# #TAB# if custom_directory: #LINE# #TAB# #TAB# dataset_root = os.path.join(custom_directory, 'dataset.cache') #LINE# #TAB# else: #LINE# #TAB# #TAB# dataset_root = os.path.join(os.getcwd(), 'dataset') #LINE# #TAB# if not os.path.exists(dataset_root): #LINE# #TAB# #TAB# os.makedirs(dataset_root) #LINE# #TAB# return dataset_root"
def architecture_name_from_target(target): #LINE# #TAB# target_is_architecture = target.is_architecture() #LINE# #TAB# if target_is_architecture: #LINE# #TAB# #TAB# for arch in target_is_architecture: #LINE# #TAB# #TAB# #TAB# return arch.name #LINE# #TAB# return None
def print_character(ordchr): #LINE# #TAB# if ordchr in string.printable: #LINE# #TAB# #TAB# return ordchr #LINE# #TAB# else: #LINE# #TAB# #TAB# return '.'
"def get_snapshot_from_tree(name, root): #LINE# #TAB# try: #LINE# #TAB# #TAB# snapshot_tree = get_snapshot_tree(name, root) #LINE# #TAB# #TAB# if snapshot_tree: #LINE# #TAB# #TAB# #TAB# return snapshot_tree #LINE# #TAB# except: #LINE# #TAB# #TAB# return None"
"def norme_vec(vec1): #LINE# #TAB# n = np.linalg.norm(vec1, axis=-1) #LINE# #TAB# return n"
def find_faderport_output_name(number=0): #LINE# #TAB# ins = [i for i in mido.get_output_names() if i.lower().startswith('faderport')] #LINE# #TAB# if 0 <= number < len(ins): #LINE# #TAB# #TAB# return ins[number] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"def make_name(name: str) ->str: #LINE# #TAB# long_name = name.split('_')[0] #LINE# #TAB# long_name = long_name.replace('-', '_') #LINE# #TAB# long_name = long_name.replace('T', '_') #LINE# #TAB# return long_name"
"def parse_gnu_time_file(filename, isdatetime=False): #LINE# #TAB# df = pd.read_csv(filename, sep='\t', header=None) #LINE# #TAB# if isdatetime: #LINE# #TAB# #TAB# df = df.dt.to_datetime() #LINE# #TAB# return df"
def has_ctrl_handler(control): #LINE# #TAB# _handler = get_ctrl_handler(control) #LINE# #TAB# return _handler is not None
"def simple_call_string(function_name, argument_list): #LINE# #TAB# call = function_name + '(' + ', '.join([(var + '=' + repr(value)) for #LINE# #TAB# #TAB# var, value in argument_list]) + ')' #LINE# #TAB# return call"
"def run_console_command(command): #LINE# #TAB# process = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# _, _, terminal_output, err = process.communicate() #LINE# #TAB# if process.returncode!= 0: #LINE# #TAB# #TAB# error = str(process.returncode) #LINE# #TAB# return error, terminal_output"
"def get_fk(from_table, to_table): #LINE# #TAB# fk_list = [x.name for x in from_table.foreign_keys] #LINE# #TAB# for fk in to_table.foreign_keys: #LINE# #TAB# #TAB# if fk.target_table == to_table: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# fk_list.extend(get_fk(fk.from_table, fk.to_table)) #LINE# #TAB# return fk_list"
"def cmp_mtime(a, b): #LINE# #TAB# abs_mtime = os.path.getmtime(a) #LINE# #TAB# abs_file_name = os.path.getfile(b) #LINE# #TAB# if abs_mtime > os.path.getmtime(abs_file_name): #LINE# #TAB# #TAB# return -1 #LINE# #TAB# elif abs_mtime < os.path.getmtime(a): #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"def get_job_class(job_class=None): #LINE# #TAB# RQ = getattr(settings, 'RQ', {}) #LINE# #TAB# if job_class is None: #LINE# #TAB# #TAB# job_class = RQ.get('JOB_CLASS', default_job_class) #LINE# #TAB# if isinstance(job_class, str): #LINE# #TAB# #TAB# job_class = import_module(job_class) #LINE# #TAB# return job_class"
"def guess_request_country(request): #LINE# #TAB# if hasattr(request, 'country'): #LINE# #TAB# #TAB# return request.country #LINE# #TAB# if hasattr(request,'session'): #LINE# #TAB# #TAB# country = request.session.get('country') #LINE# #TAB# #TAB# if country and country in COUNTRIES: #LINE# #TAB# #TAB# #TAB# return country #LINE# #TAB# if hasattr(request, 'country'): #LINE# #TAB# #TAB# country = request.country #LINE# #TAB# if country: #LINE# #TAB# #TAB# return country #LINE# #TAB# if hasattr(settings, 'COUNTRIES'): #LINE# #TAB# #TAB# for country in settings.COUNTRIES: #LINE# #TAB# #TAB# #TAB# if country in request.country: #LINE# #TAB# #TAB# #TAB# #TAB# return country #LINE# #TAB# return None"
"def get_create_gridcal_folder(): #LINE# #TAB# gridcal_folder = os.path.join(os.path.expanduser('~'), '.gridcal') #LINE# #TAB# if not os.path.exists(gridcal_folder): #LINE# #TAB# #TAB# os.makedirs(gridcal_folder) #LINE# #TAB# return gridcal_folder"
def load_config(path: str) ->dict: #LINE# #TAB# config = load_config_file(path) #LINE# #TAB# if config.get('extension') == 'yaml': #LINE# #TAB# #TAB# config = yaml.safe_load(config.read()) #LINE# #TAB# elif config.get('extension') == 'json': #LINE# #TAB# #TAB# config = json.safe_load(config.read()) #LINE# #TAB# return config
def checkResponse(request): #LINE# #TAB# try: #LINE# #TAB# #TAB# response = request.response #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# if request.status_code == 404: #LINE# #TAB# #TAB# #TAB# raise InvalidRequest() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise InvalidRequest() #LINE# #TAB# if 400 <= response.status_code < 600: #LINE# #TAB# #TAB# raise InvalidRequest() #LINE# #TAB# if 500 <= response.status_code < 600: #LINE# #TAB# #TAB# raise InvalidRequest() #LINE# #TAB# return True
"def import_matlab_data(filename): #LINE# #TAB# n = 0 #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# while 1: #LINE# #TAB# #TAB# #TAB# data = f.read() #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# return n"
"def row_canonicalize_street_and_number(sd, row): #LINE# #TAB# if row[0].isdigit(): #LINE# #TAB# #TAB# s = row[1].lower() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n = float(row[2]) #LINE# #TAB# #TAB# #TAB# if s.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# return s #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return s #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return row[0] #LINE# #TAB# if row[1].isdigit(): #LINE# #TAB# #TAB# return row[1].lower(), float(row[2]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return row"
"def log_dir(): #LINE# #TAB# module_dir = os.path.dirname(sys.modules[__name__].__file__) #LINE# #TAB# log_dir = os.path.join(module_dir, 'logs') #LINE# #TAB# if not os.path.exists(log_dir): #LINE# #TAB# #TAB# os.makedirs(log_dir) #LINE# #TAB# return log_dir"
"def get_info(path): #LINE# #TAB# with open(path) as fh: #LINE# #TAB# #TAB# contents = fh.read() #LINE# #TAB# info_dict = {} #LINE# #TAB# for line in contents.splitlines(): #LINE# #TAB# #TAB# if '=' not in line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key, value = line.split('=', 1) #LINE# #TAB# #TAB# info_dict[key.strip()] = value.strip() #LINE# #TAB# return info_dict"
"def dolphin_label_hits(label): #LINE# #TAB# query = db.session.query(Dolphin).filter(Dolphin.label == label).group_by( #LINE# #TAB# #TAB# Dolphin.label).order_by(desc(func.score), desc(func.score)) #LINE# #TAB# for result in query: #LINE# #TAB# #TAB# yield result"
"def find_packages(library_name): #LINE# #TAB# packages = [] #LINE# #TAB# for module_name in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# if library_name.startswith(module_name + '.'): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# packages.append(os.path.relpath(os.path.dirname(module_name), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# library_name + '.py')) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# return packages"
"def get_valid_type(value, type_): #LINE# #TAB# if type_ == int: #LINE# #TAB# #TAB# return int #LINE# #TAB# elif type_ == float: #LINE# #TAB# #TAB# return float #LINE# #TAB# elif type_ == bool: #LINE# #TAB# #TAB# return bool #LINE# #TAB# elif type_ == list: #LINE# #TAB# #TAB# return [get_valid_type(v, type_) for v in value] #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return type_(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return value"
"def save_text(text, end='\n'): #LINE# #TAB# text = text[0:-1] #LINE# #TAB# text = text.rstrip('\n') #LINE# #TAB# start = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# sent = text.rfind(end) #LINE# #TAB# #TAB# sent = sent[0:len(text) - 1] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(text, 'w') as f: #LINE# #TAB# #TAB# #TAB# #TAB# yaml.dump(text, f, default_flow_style=False) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# start += len(text) #LINE# #TAB# return text"
def check_augs(augs): #LINE# #TAB# already_loaded = False #LINE# #TAB# for a in augs: #LINE# #TAB# #TAB# if a not in _AUG_CACHE: #LINE# #TAB# #TAB# #TAB# _load_auggs(a) #LINE# #TAB# #TAB# #TAB# already_loaded = True #LINE# #TAB# return already_loaded
def find_english_word(term): #LINE# #TAB# word = '' #LINE# #TAB# for row in English: #LINE# #TAB# #TAB# if row.lower() == term.lower(): #LINE# #TAB# #TAB# #TAB# word = row.word #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return word
"def sniff_field_spelling(mlog, source): #LINE# #TAB# position_field_type_default = position_field_types[0] #LINE# #TAB# msg = mlog.recv_match(source) #LINE# #TAB# mlog._rewind() #LINE# #TAB# position_field_selection = [spelling for spelling in #LINE# #TAB# #TAB# position_field_types if hasattr(msg, spelling[0])] #LINE# #TAB# return position_field_selection[0 #LINE# #TAB# #TAB# ] if position_field_selection else position_field_type_default"
"def get_redis(): #LINE# #TAB# global REDIS #LINE# #TAB# if REDIS is None: #LINE# #TAB# #TAB# host, port = REDIS_PUBSUB['address'] #LINE# #TAB# #TAB# db = REDIS_PUBSUB['db'] #LINE# #TAB# #TAB# password = REDIS_PUBSUB['password'] #LINE# #TAB# #TAB# REDIS = redis.Redis(host, port, db=db, password=password) #LINE# #TAB# return REDIS"
def primary_key(cls): #LINE# #TAB# if cls.__from_class__: #LINE# #TAB# #TAB# cls = cls.__from_class__ #LINE# #TAB# return cls.__table__.primary_key.columns.values()[0].name
"def range_moments(minval, maxval): #LINE# #TAB# mn = float(minval) #LINE# #TAB# mx = float(maxval) #LINE# #TAB# if mn < mx: #LINE# #TAB# #TAB# mn = mx - mn #LINE# #TAB# return mn, mx"
"def truncate_impulse(impulse, ntaps, window='hanning'): #LINE# #TAB# if window == 'hanning': #LINE# #TAB# #TAB# length = int(ntaps / window * impulse) #LINE# #TAB# elif window == 'hanning': #LINE# #TAB# #TAB# length = int(ntaps / window * impulse) #LINE# #TAB# else: #LINE# #TAB# #TAB# length = impulse #LINE# #TAB# return impulse[length < ntaps]"
def expects_none(options): #LINE# #TAB# return len(options) > 0 and options[0] == 'count' and options[1 #LINE# #TAB# #TAB# ]!= 'zero'
"def display_oneliners(term, top_margin, offset): #LINE# #TAB# oneliners_lines = term.split('\n') #LINE# #TAB# bottom_margin = max(top_margin, offset - oneliners_lines[0]) #LINE# #TAB# adj_margin = 0 #LINE# #TAB# for line in oneliners_lines[1:]: #LINE# #TAB# #TAB# print(term.rjust(adj_margin,''), end='') #LINE# #TAB# #TAB# bottom_margin += oneliners_lines[0][:offset] #LINE# #TAB# #TAB# adj_margin += oneliners_lines[0][offset:] #LINE# #TAB# return bottom_margin, adj_margin"
"def dict_to_obs(obs_dict): #LINE# #TAB# if not isinstance(obs_dict, Dict): #LINE# #TAB# #TAB# obs_dict = obs_dict.items() #LINE# #TAB# obs = {} #LINE# #TAB# for k, v in obs_dict.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obs[k] = v.tolist() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# obs[k] = v #LINE# #TAB# return obs"
"def extract_guides(func: Callable) ->Dict[str, Tuple[Callable]]: #LINE# #TAB# annotations = get_annotations(func) #LINE# #TAB# guides: Dict[str, Tuple[Callable]] = {} #LINE# #TAB# for keyword, value in annotations.items(): #LINE# #TAB# #TAB# if keyword.startswith('guid:'): #LINE# #TAB# #TAB# #TAB# guides[keyword[4:]] = extract_guides(value) #LINE# #TAB# return guides"
"def update_conf(conf, target, project, repository): #LINE# #TAB# if'repository' in target: #LINE# #TAB# #TAB# r = target.get('repository') #LINE# #TAB# #TAB# if r is not None: #LINE# #TAB# #TAB# #TAB# conf['repository'] = r #LINE# #TAB# #TAB# conf['project'] = project #LINE# #TAB# #TAB# conf['repository'] = repository #LINE# #TAB# elif target is not None: #LINE# #TAB# #TAB# conf['project'] = project #LINE# #TAB# #TAB# conf['repository'] = repository"
"def correlations_to_indices(correlations, basis): #LINE# #TAB# local_state_indices = [] #LINE# #TAB# indices = [] #LINE# #TAB# for i, c in enumerate(correlations): #LINE# #TAB# #TAB# for j, s in enumerate(basis.states): #LINE# #TAB# #TAB# #TAB# if s == 0: #LINE# #TAB# #TAB# #TAB# #TAB# local_state_indices.append(i) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# local_state_indices.append(j) #LINE# #TAB# #TAB# indices.append(np.argmax(local_state_indices)) #LINE# #TAB# return indices"
"def split_msg(msgs, max_len): #LINE# #TAB# msg = [] #LINE# #TAB# for i, msg_out in enumerate(msgs): #LINE# #TAB# #TAB# if len(msg_out) > max_len: #LINE# #TAB# #TAB# #TAB# return msg #LINE# #TAB# #TAB# if len(msg_out) == 0: #LINE# #TAB# #TAB# #TAB# if len(msg_out[-1]) > max_len: #LINE# #TAB# #TAB# #TAB# #TAB# msg_out[-1] = b'\x00' * (max_len - len(msg_out[-1])) #LINE# #TAB# #TAB# #TAB# msg.append(msg_out[-1]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# msg.append(msg_out[-1]) #LINE# #TAB# return msg"
"def process_dat(filepath): #LINE# #TAB# dat = [] #LINE# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if ':' in line: #LINE# #TAB# #TAB# #TAB# #TAB# ss = line.strip().split(':') #LINE# #TAB# #TAB# #TAB# #TAB# dat.append(json.loads(ss)) #LINE# #TAB# return dat"
"def to_date(string, fmt=None, output='str'): #LINE# #TAB# try: #LINE# #TAB# #TAB# date = dateutil.parser.parse(string) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# date = string #LINE# #TAB# if fmt is None: #LINE# #TAB# #TAB# fmt = '%Y-%m-%dT%H:%M:%S.%f' #LINE# #TAB# date = date.strftime(fmt) #LINE# #TAB# if output =='str': #LINE# #TAB# #TAB# return date.isoformat() #LINE# #TAB# return date"
"def checksum_field_bytes(ctx: Dict[str, Any]) ->bytearray: #LINE# #TAB# bytes = bytearray() #LINE# #TAB# for key, value in ctx.items(): #LINE# #TAB# #TAB# if key =='sha256': #LINE# #TAB# #TAB# #TAB# bytes.extend(checksum_field_bytes(value)) #LINE# #TAB# #TAB# elif key =='sha512': #LINE# #TAB# #TAB# #TAB# bytes.extend(checksum_field_bytes(value)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# bytes.extend(key.encode('utf-8')) #LINE# #TAB# return bytes"
"def setup_auth_handler(rootdir): #LINE# #TAB# user_dir = os.path.join(rootdir, 'user') #LINE# #TAB# if not os.path.exists(user_dir): #LINE# #TAB# #TAB# return False #LINE# #TAB# auth_file = open(user_dir + 'auth.json', 'r') #LINE# #TAB# if not os.path.exists(auth_file): #LINE# #TAB# #TAB# return False #LINE# #TAB# handler = HTTPBasicAuthHandler(auth_file) #LINE# #TAB# os.chdir(user_dir) #LINE# #TAB# try: #LINE# #TAB# #TAB# pyramid_auth.setup(handler, user_dir) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def is_volatile(type_): #LINE# #TAB# nake_type = remove_alias(type_) #LINE# #TAB# if isinstance(nake_type, cpptypes.volatile_t): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(nake_type, cpptypes.pointer_t): #LINE# #TAB# #TAB# return is_volatile(nake_type.base) #LINE# #TAB# elif isinstance(nake_type, cpptypes.pointer_t): #LINE# #TAB# #TAB# return is_volatile(nake_type.base) #LINE# #TAB# return False"
"def expand_groups(grp): #LINE# #TAB# data = {'groups': grp} #LINE# #TAB# if '.' in grp: #LINE# #TAB# #TAB# data['groups'] = [x.replace('.', '_') for x in grp.split('.')] #LINE# #TAB# elif '.' in grp: #LINE# #TAB# #TAB# data['groups'] = [x.replace('.', '_') for x in grp.split('.')] #LINE# #TAB# return data"
"def get_class_device_info(school_id, sn): #LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_card_server_url(), sn) #LINE# #TAB# resp = do_get_request(url=url, token=class_card_server_token(), #LINE# #TAB# #TAB# school_id=school_id) #LINE# #TAB# if resp is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = resp.json() #LINE# #TAB# #TAB# #TAB# if isinstance(data['value'], list): #LINE# #TAB# #TAB# #TAB# #TAB# return data['value'][0] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return None"
"def get_fields_in_model(instance): #LINE# #TAB# meta = getattr(instance, '_meta', None) #LINE# #TAB# if not meta: #LINE# #TAB# #TAB# return instance._meta.get_fields() #LINE# #TAB# fields = [] #LINE# #TAB# for field in meta.get_fields(): #LINE# #TAB# #TAB# if not field.auto_created: #LINE# #TAB# #TAB# #TAB# fields.append(field.name) #LINE# #TAB# return fields"
"def get_version_info(): #LINE# #TAB# contents = read_file(os.path.join('version.py', '__init__.py')) #LINE# #TAB# version_info = {} #LINE# #TAB# version_info['major'] = int(contents.split('.')[0]) #LINE# #TAB# version_info['minor'] = int(contents.split('.')[1]) #LINE# #TAB# version_info['patch'] = int(contents.split('(')[2]) #LINE# #TAB# return version_info"
"def generate_row_data(results): #LINE# #TAB# for key, value in results.items(): #LINE# #TAB# #TAB# if key!= 'timestamp': #LINE# #TAB# #TAB# #TAB# yield re.sub('\\s+','', key) #LINE# #TAB# #TAB# yield re.sub('\\s+','', key) #LINE# #TAB# data = [] #LINE# #TAB# for row in results['rows']: #LINE# #TAB# #TAB# row_data = [] #LINE# #TAB# #TAB# for value in row['values']: #LINE# #TAB# #TAB# #TAB# row_data.append(value) #LINE# #TAB# #TAB# yield row_data"
"def split_concept_cd(concept_cd, join_char='+'): #LINE# #TAB# index = concept_cd.find(join_char) #LINE# #TAB# if index == -1: #LINE# #TAB# #TAB# category_cd = concept_cd #LINE# #TAB# #TAB# data_label = None #LINE# #TAB# else: #LINE# #TAB# #TAB# category_cd = concept_cd[:index] #LINE# #TAB# #TAB# data_label = concept_cd[index + 1:] #LINE# #TAB# return category_cd, data_label"
"def rsa_verify(xml, signature, key, c14n_exc=True): #LINE# #TAB# hasher = RSA.new(key) #LINE# #TAB# try: #LINE# #TAB# #TAB# hasher.verify(xml, signature) #LINE# #TAB# except InvalidSignature: #LINE# #TAB# #TAB# if c14n_exc: #LINE# #TAB# #TAB# #TAB# raise InvalidSignature #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def sigma_fwhm(sigma, shape='gaus'): #LINE# #TAB# if shape == 'gaus': #LINE# #TAB# #TAB# sigma = sigma.copy() #LINE# #TAB# elif shape == 'window': #LINE# #TAB# #TAB# sigma = sigma.copy() #LINE# #TAB# if sigma.ndim == 2: #LINE# #TAB# #TAB# sigma_fwhm = sigma / 2.0 #LINE# #TAB# elif sigma.ndim == 3: #LINE# #TAB# #TAB# sigma_fwhm = sigma / 3.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unknown sigma shape: {}'.format(shape)) #LINE# #TAB# return sigma_fwhm"
"def order_name(name): #LINE# #TAB# if len(name) > 20: #LINE# #TAB# #TAB# return '%s..%s' % (name[:20], '...') #LINE# #TAB# else: #LINE# #TAB# #TAB# return name"
"def write_json(hr_data, filename): #LINE# #TAB# if hr_data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(filename, 'w') as f: #LINE# #TAB# #TAB# json.dump(hr_data, f, indent=2, separators=(',', ': ')) #LINE# #TAB# return filename"
"def hash_file_partially(path, size=524288): #LINE# #TAB# hash_obj = hashlib.sha512() #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# data = f.read(size) #LINE# #TAB# #TAB# #TAB# #TAB# hash_obj.update(data) #LINE# #TAB# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# hash_obj.update(data[:size]) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return hash_obj"
"def get_config(config_dir=None): #LINE# #TAB# config_path = get_config_path(config_dir=config_dir) #LINE# #TAB# default_config = DEFAULT_CONFIG #LINE# #TAB# if os.path.exists(config_path): #LINE# #TAB# #TAB# with open(config_path, 'r') as config_file: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# config = json.load(config_file) #LINE# #TAB# #TAB# #TAB# #TAB# return config #LINE# #TAB# #TAB# #TAB# except ValueError as e: #LINE# #TAB# #TAB# #TAB# #TAB# print(e) #LINE# #TAB# return default_config"
"def normalize_params(params): #LINE# #TAB# if isinstance(params, dict): #LINE# #TAB# #TAB# params = {normalize_param(key): normalize_params(value) for key, #LINE# #TAB# #TAB# #TAB# value in params.items()} #LINE# #TAB# return params"
def get_taglist(node): #LINE# #TAB# taglist = [] #LINE# #TAB# for child in node.childNodes: #LINE# #TAB# #TAB# taglist.append(get_tag(child)) #LINE# #TAB# return taglist
"def iter_global_packages(): #LINE# #TAB# for directory in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(directory, '__init__.py')): #LINE# #TAB# #TAB# #TAB# yield directory"
"def get_order(account, order_url): #LINE# #TAB# url = order_url.format(account=account, url=order_url) #LINE# #TAB# data = requests.get(url) #LINE# #TAB# if data.status_code == 200: #LINE# #TAB# #TAB# raw_order = data.json() #LINE# #TAB# #TAB# return raw_order #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.warning('Could not retrieve order data for %s' % order_url) #LINE# #TAB# #TAB# return None"
"def register_notifications(): #LINE# #TAB# notifications = [] #LINE# #TAB# notifications.append(NotificationHandler(path='/')) #LINE# #TAB# for path, _, cls in pkg_resources.iter_entry_points( #LINE# #TAB# #TAB# 'chattie.notifications'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cls = cls() #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not hasattr(cls,'register_notifications'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# notifications.append(cls) #LINE# #TAB# cls.register_notifications() #LINE# #TAB# return notifications"
"def usable_id(cls, id): #LINE# #TAB# try: #LINE# #TAB# #TAB# qry_id = int(id) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# qry_id = None #LINE# #TAB# if not qry_id: #LINE# #TAB# #TAB# msg = 'unknown identifier %s' % id #LINE# #TAB# #TAB# cls.error(msg) #LINE# #TAB# return qry_id"
def is_run_from_ipython(): #LINE# #TAB# try: #LINE# #TAB# #TAB# __IPYTHON__ #LINE# #TAB# #TAB# return True #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False
"def get_route_name(resource_uri): #LINE# #TAB# resource_uri = resource_uri.replace('/', '%2F') #LINE# #TAB# resource_uri = resource_uri.replace('?', '') #LINE# #TAB# resource_uri = resource_uri.replace('?', '') #LINE# #TAB# return resource_uri"
"def site_default_id(): #LINE# #TAB# if hasattr(site, 'default_id'): #LINE# #TAB# #TAB# return site.default_id #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"def compute_covariance_matrix(points): #LINE# #TAB# covariance_matrix = np.zeros((points.shape[0], points.shape[1])) #LINE# #TAB# for boundary in points: #LINE# #TAB# #TAB# for i in range(points.shape[1]): #LINE# #TAB# #TAB# #TAB# covariance_matrix[i, i] = compute_covariance_row(boundary) #LINE# #TAB# return covariance_matrix"
def dir_exists(dirpath): #LINE# #TAB# if dirpath is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not os.path.isdir(dirpath): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def print_bandwidth_before_after_permutation(A, permutation): #LINE# #TAB# len_A = A.shape[0] #LINE# #TAB# if not len_A == len(permutation): #LINE# #TAB# #TAB# return #LINE# #TAB# before = A[:, 0] #LINE# #TAB# after = A[:, 1] #LINE# #TAB# for i in range(len_A): #LINE# #TAB# #TAB# if permutation[i] == 1: #LINE# #TAB# #TAB# #TAB# print('bandwidth before:', before[i], 'before:', after[i]) #LINE# #TAB# #TAB# elif permutation[i] == 0: #LINE# #TAB# #TAB# #TAB# print('bandwidth after:', after[i], 'after:', before[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# print('bandwidth before:', before[i], 'after:', after[i]) #LINE# #TAB# return"
"def datetime_to_json(date): #LINE# #TAB# return {'year': date.year,'month': date.month, 'day': date.day, #LINE# #TAB# #TAB# 'hour': date.hour,'minute': date.minute,'second': date.second}"
"def base_quality_single_threaded(fastq, pkl): #LINE# #TAB# logger.debug('base_quality_single_threaded') #LINE# #TAB# with open(fastq, 'rb') as f: #LINE# #TAB# #TAB# r = pickle.load(f, pickle.HIGHEST_PROTOCOL) #LINE# #TAB# base_quality_single_thread(r, fastq, pkl) #LINE# #TAB# return r"
"def get_component(filename, default='global'): #LINE# #TAB# if hasattr(filename,'read'): #LINE# #TAB# #TAB# filename = filename.read() #LINE# #TAB# parts = filename.split('.') #LINE# #TAB# if len(parts) >= 3: #LINE# #TAB# #TAB# if parts[1] in'modules' or parts[2] in 'base.py': #LINE# #TAB# #TAB# #TAB# return parts[0] #LINE# #TAB# if len(parts) >= 2: #LINE# #TAB# #TAB# if parts[1] in 'base.py': #LINE# #TAB# #TAB# #TAB# return parts[1] #LINE# #TAB# if len(parts) >= 1: #LINE# #TAB# #TAB# if parts[0] in 'base.py': #LINE# #TAB# #TAB# #TAB# return parts[0] #LINE# #TAB# return default"
"def parse_technical_string_params(cls, val): #LINE# #TAB# ttype, params, mode = _extract_technical_string_parts(val) #LINE# #TAB# if len(params) not in (1, 2): #LINE# #TAB# #TAB# raise InvalidTechnicalException( #LINE# #TAB# #TAB# #TAB# 'Invalid %s technical string: %s, 1 or 2 args allowed' % (ttype, #LINE# #TAB# #TAB# #TAB# val)) #LINE# #TAB# result = dict(window=int(params[0]), min_periods=1) #LINE# #TAB# if len(params) == 2: #LINE# #TAB# #TAB# result['min_periods'] = int(params[1]) #LINE# # # # return result"
"def get_context(): #LINE# #TAB# context = {} #LINE# #TAB# context['settings'] = settings.DJANGO_SETTINGS_MODULE #LINE# #TAB# context['settings']['local_path'] = os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# 'templates') #LINE# #TAB# context['settings']['current_path'] = os.path.join(os.getcwd(), settings. #LINE# #TAB# #TAB# DJANGO_SETTINGS_MODULE) #LINE# #TAB# context['arch'] = _get_arch() #LINE# #TAB# context['platform'] = platform.platform() #LINE# #TAB# context['current_path'] = os.path.abspath(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# 'templates')) #LINE# #TAB# return context"
def filter_python_files(files): #LINE# #TAB# python_files = [] #LINE# #TAB# for file_info in files: #LINE# #TAB# #TAB# if is_python(file_info): #LINE# #TAB# #TAB# #TAB# python_files.append(file_info) #LINE# #TAB# return python_files
def fix_descriptions(description: Optional[str]) ->Optional[str]: #LINE# #TAB# if description is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# description = description.strip() #LINE# #TAB# if not description or description == '': #LINE# #TAB# #TAB# return description #LINE# #TAB# description = description.capitalize() #LINE# #TAB# if description.endswith('.'): #LINE# #TAB# #TAB# return description + '.' #LINE# #TAB# return description
def clean_for_doc(nb): #LINE# #TAB# for ws in nb.worksheets: #LINE# #TAB# #TAB# for cell in ws.cells: #LINE# #TAB# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# cell.metadata['source'] = ''
"def to_unicode(s): #LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# return s.decode('utf8') #LINE# #TAB# return s"
def no_env(key): #LINE# #TAB# old = mgr.env #LINE# #TAB# mgr.env[key] = False #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# mgr.env[key] = old
"def _fix_docs(this_abc, child_class): #LINE# #TAB# #TAB# if sys.version_info >= (3, 5): #LINE# #TAB# #TAB# #TAB# return child_class #LINE# #TAB# #TAB# if not issubclass(child_class, this_abc): #LINE# #TAB# #TAB# #TAB# raise KappaError('Cannot fix docs of class that is not decendent.') #LINE# #TAB# #TAB# for name in dir(child_class): #LINE# #TAB# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if name in this_abc.__abstractmethods__: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# child_func = getattr(this_abc, name) #LINE# #TAB# #TAB# #TAB# child_func.__doc__ = child_func.__doc__"
"def clean_direction(dir_list, preprocess=False): #LINE# #TAB# ret = [] #LINE# #TAB# for dir in dir_list: #LINE# #TAB# #TAB# if dir in DIR_STRS: #LINE# #TAB# #TAB# #TAB# if preprocess: #LINE# #TAB# #TAB# #TAB# #TAB# ret.append(dir) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ret.append(dir) #LINE# #TAB# return ret"
def reset_global_log_properties(): #LINE# #TAB# global _log_properties #LINE# #TAB# _log_properties = logging.Config
def instantiate_references_json(references_json): #LINE# #TAB# references = {} #LINE# #TAB# for model_reference in references_json: #LINE# #TAB# #TAB# references[model_reference[0]] = instantiate_reference(model_reference[1]) #LINE# #TAB# return references
def get_current(cls) ->'Context': #LINE# #TAB# if cls._context is None: #LINE# #TAB# #TAB# raise ContextIsNotInitializedError #LINE# #TAB# return cls._context
"def shuffle_pages(num, scheme): #LINE# #TAB# np.random.shuffle(range(num)) #LINE# #TAB# pages = [] #LINE# #TAB# for i in range(num): #LINE# #TAB# #TAB# pages.append(scheme[i]) #LINE# #TAB# num = int(num) #LINE# #TAB# return pages"
def watch_for_pystol_timeouts(stop): #LINE# #TAB# global _pystol_timeouts #LINE# #TAB# while not _pystol_timeouts: #LINE# #TAB# #TAB# if stop: #LINE# #TAB# #TAB# #TAB# _pystol_timeouts = [] #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# time.sleep(0.1) #LINE# #TAB# #TAB# if time.time() > pystol_timeouts: #LINE# #TAB# #TAB# #TAB# _pystol_timeouts.append(time.time()) #LINE# #TAB# #TAB# del _pystol_timeouts
"def list_lzh (archive, compression, cmd, verbosity, interactive): #LINE# #TAB# opts = 't' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# return [cmd, opts, archive]"
"def extract_sets_from_connections(connections): #LINE# #TAB# s1 = collections.defaultdict(set) #LINE# #TAB# s2 = collections.defaultdict(set) #LINE# #TAB# for c1, c2 in connections: #LINE# #TAB# #TAB# s1[c1].add(c2) #LINE# #TAB# #TAB# s2[c2].add(c1) #LINE# #TAB# return s1, s2"
"def setup_active_outlink_matrix(shape, node_status=None, return_count=True): #LINE# #TAB# links = outlinks(shape, node_status=node_status) #LINE# #TAB# if return_count: #LINE# #TAB# #TAB# return links, active_outlink_count_per_node(shape) #LINE# #TAB# else: #LINE# #TAB# #TAB# return links"
"def add_path(tdict, path): #LINE# #TAB# path = path.split('.') #LINE# #TAB# node = tdict #LINE# #TAB# for p in path: #LINE# #TAB# #TAB# if p not in node: #LINE# #TAB# #TAB# #TAB# node[p] = Node(p) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# node[p] = {} #LINE# #TAB# #TAB# node = node[p] #LINE# #TAB# return node"
"def mysql_large_prefix_check(engine): #LINE# #TAB# if not str(engine.url).startswith('mysql'): #LINE# #TAB# #TAB# return False #LINE# #TAB# variables = dict(engine.execute( #LINE# #TAB# #TAB#'show variables where variable_name like ""innodb_large_prefix"" or variable_name like ""innodb_file_format"";' #LINE# #TAB# #TAB# ).fetchall()) #LINE# #TAB# if variables.get('innodb_file_format', 'Barracuda' #LINE# #TAB# #TAB# ) == 'Barracuda' and variables.get('innodb_large_prefix', 'ON' #LINE# #TAB# #TAB# ) == 'ON': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
def get_component(id): #LINE# #TAB# if id not in cache: #LINE# #TAB# #TAB# return None #LINE# #TAB# return cache[id]
"def normalize_row_probability(x): #LINE# #TAB# x = np.asarray(x) #LINE# #TAB# row_probability = np.sum(x.T, axis=0) #LINE# #TAB# if row_probability == 0: #LINE# #TAB# #TAB# return x #LINE# #TAB# else: #LINE# #TAB# #TAB# return x / row_probability"
def to_isostring(dt): #LINE# #TAB# if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) > timedelta(0): #LINE# #TAB# #TAB# logging.warning( #LINE# #TAB# #TAB# #TAB# 'Warning: aware datetimes are interpreted as if they were naive') #LINE# #TAB# return dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'
"def get_coords(x, y, w, h, pagesize): #LINE# #TAB# page_height = int(w / 2) #LINE# #TAB# page_width = int(h / 2) #LINE# #TAB# if page_height > pagesize: #LINE# #TAB# #TAB# h = pagesize #LINE# #TAB# coords = {} #LINE# #TAB# coords['x'] = x - page_width #LINE# #TAB# coords['y'] = y - page_height #LINE# #TAB# coords['width'] = w #LINE# #TAB# coords['height'] = h #LINE# #TAB# return coords"
"def parse_control_file(data, args): #LINE# #TAB# for line in data.splitlines(): #LINE# #TAB# #TAB# m = re_control_file.match(line) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# steps = m.group(1) #LINE# #TAB# #TAB# #TAB# if len(steps) == 1: #LINE# #TAB# #TAB# #TAB# #TAB# yield steps[0] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# print(steps) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# print(line) #LINE# #TAB# for step in steps: #LINE# #TAB# #TAB# yield step"
"def ignored_by_git(filename): #LINE# #TAB# filename = os.path.abspath(filename) #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#') and line.endswith('.git'): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
def country_code(code): #LINE# #TAB# country_code = code.split('-')[0] #LINE# #TAB# if len(country_code)!= 2: #LINE# #TAB# #TAB# raise voluptuous.Invalid('Invalid country code: {}'.format(code)) #LINE# #TAB# return country_code
"def bam_sum(bam, out_txt): #LINE# #TAB# out_file = out_txt + '.sum.txt' #LINE# #TAB# cmd ='samtools view -h -f 2 > {0} > {1}'.format(bam, out_file) #LINE# #TAB# run(cmd) #LINE# #TAB# return out_file"
"def generate_packer_filename(provider, region, builder): #LINE# #TAB# metadata = get_metadata(provider, region, builder) #LINE# #TAB# filename = os.path.join( #LINE# #TAB# #TAB# metadata['packer_dir'], #LINE# #TAB# #TAB# '{0}-{1}.pack'.format( #LINE# #TAB# #TAB# #TAB# metadata['package'], #LINE# #TAB# #TAB# #TAB# builder.name, #LINE# #TAB# #TAB# ) #LINE# #TAB# ) #LINE# #TAB# return filename"
def p_seen_union(p): #LINE# #TAB# if p[1] is None: #LINE# #TAB# #TAB# p[0] = p[2] = [] #LINE# #TAB# else: #LINE# #TAB# #TAB# p[1].append(p[3]) #LINE# #TAB# p[0] = p[1]
"def shorten_url(url, cols, shorten): #LINE# #TAB# cols = ((cols - 6) *.85) #LINE# #TAB# if shorten is False or len(url) < cols: #LINE# #TAB# #TAB# return url #LINE# #TAB# split = int(cols *.5) #LINE# #TAB# return url[:split] + ""..."" + url[-split:]"
"def get_by_name(cls, request_handler, name) ->Optional['Report']: #LINE# #TAB# response = request_handler.make_request('GET', '/reports') #LINE# #TAB# matches = cls.search([('name', '=', name)]) #LINE# #TAB# if matches: #LINE# #TAB# #TAB# report = cls(request_handler, matches) #LINE# #TAB# #TAB# return report #LINE# #TAB# return None"
"def from_schema(cls, tag, schema): #LINE# #TAB# instance = cls(tag) #LINE# #TAB# for key, value in schema.items(): #LINE# #TAB# #TAB# setattr(instance, key, value) #LINE# #TAB# return instance"
"def plot_graph(graph): #LINE# #TAB# import matplotlib.pyplot as plt #LINE# #TAB# lines = [] #LINE# #TAB# c = 0 #LINE# #TAB# for node in graph: #LINE# #TAB# #TAB# for u, v in node.edges(): #LINE# #TAB# #TAB# #TAB# line = [] #LINE# #TAB# #TAB# #TAB# for v in v: #LINE# #TAB# #TAB# #TAB# #TAB# line.append('#TAB# ') #LINE# #TAB# #TAB# #TAB# cmap = plt.get_cmap(u) #LINE# #TAB# #TAB# #TAB# line.append('#TAB# ') #LINE# #TAB# #TAB# #TAB# plt.plot(line, cmap, linewidth=2) #LINE# #TAB# #TAB# c += 1 #LINE# #TAB# plt.show() #LINE# #TAB# return lines"
def is_desktop_locked() -> bool: #LINE# #TAB# locked = False #LINE# #TAB# if is_remote_desktop(): #LINE# #TAB# #TAB# desktop_lock = pygame.is_desktop_locked() #LINE# #TAB# #TAB# if desktop_lock: #LINE# #TAB# #TAB# #TAB# pygame.cancel() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# desktop_lock = True #LINE# #TAB# else: #LINE# #TAB# #TAB# pygame.init() #LINE# #TAB# #TAB# desktop_lock = pygame.is_desktop_locked() #LINE# #TAB# return desktop_lock
"def get_catalog_definitions(): #LINE# #TAB# d = {} #LINE# #TAB# for k, v in globals().items(): #LINE# #TAB# #TAB# if not k.startswith('_'): #LINE# #TAB# #TAB# #TAB# field = k.replace('_', '-') #LINE# #TAB# #TAB# #TAB# d[field] = v #LINE# #TAB# return d"
"def uwsgi_main(config_file): #LINE# #TAB# assert not os.path.exists(config_file #LINE# #TAB# #TAB# ), 'config file does not exist' #LINE# #TAB# default_conf = configparser.ConfigParser(allow_no_value=True) #LINE# #TAB# if not default_conf.read(config_file): #LINE# #TAB# #TAB# default_conf.read(config_file) #LINE# #TAB# app = UWSGIApplication(config_file=config_file) #LINE# #TAB# app.register_blueprint(blueprint) #LINE# #TAB# return app"
"def get_job_status(response): #LINE# #TAB# slurm_status_mapping = {'RUNNING': RUNNING, 'CANCELLED': CANCELLED, #LINE# #TAB# #TAB# 'COMPLETED': COMPLETED, 'CONFIGURING': PENDING, 'COMPLETING': #LINE# #TAB# #TAB# RUNNING, 'FAILED': FAILED, 'NODE_FAIL': FAILED, 'PENDING': PENDING, #LINE# #TAB# #TAB# 'PREEMPTED': FAILED, 'SUSPENDED': PENDING, 'TIMEOUT': FAILED} #LINE# #TAB# for line in response.split('\n'): #LINE# #TAB# #TAB# if line.strip() in slurm_status_mapping: #LINE# #TAB# #TAB# #TAB# return slurm_status_mapping[line.strip()] #LINE# #TAB# return None"
"def interpolate_range_edges(ranges): #LINE# #TAB# edges = [] #LINE# #TAB# if ranges is None: #LINE# #TAB# #TAB# return edges #LINE# #TAB# if isinstance(ranges, pd.Series): #LINE# #TAB# #TAB# if ranges.min() < 0: #LINE# #TAB# #TAB# #TAB# edges = range(ranges.min(), ranges.max()) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# edges = _interpolate_ranges_from_series(ranges) #LINE# #TAB# else: #LINE# #TAB# #TAB# edges = _interpolate_ranges_from_series(ranges) #LINE# #TAB# for start, end in edges: #LINE# #TAB# #TAB# edges.append((start, end)) #LINE# #TAB# return edges"
"def get_lut_id(lut, label, use_lut): #LINE# #TAB# crc = 0 #LINE# #TAB# if label == '-': #LINE# #TAB# #TAB# return crc #LINE# #TAB# for char in label: #LINE# #TAB# #TAB# if chars.isdigit(char): #LINE# #TAB# #TAB# #TAB# crc += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return crc & 255 #LINE# #TAB# if label == '0': #LINE# #TAB# #TAB# return crc >> 8 & 255 #LINE# #TAB# elif label == '1': #LINE# #TAB# #TAB# return crc >> 8 & 255 #LINE# #TAB# else: #LINE# #TAB# #TAB# return crc & 255"
def unicode_string(string): #LINE# #TAB# try: #LINE# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# string = string.decode('base64') #LINE# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# try: #LINE# #TAB# #TAB# string = string.decode('base64') #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# return string
"def conversion_uplift(control: Tuple[float], test: Tuple[float]) ->float: #LINE# #TAB# control, test = control #LINE# #TAB# if abs(control) > abs(test): #LINE# #TAB# #TAB# return 1 #LINE# #TAB# if abs(test) > abs(control): #LINE# #TAB# #TAB# return 0 #LINE# #TAB# return control - test"
"def pack_prms(): #LINE# #TAB# config_dict = {'Paths': prms.Paths.to_dict(), 'FileNames': prms. #LINE# #TAB# #TAB# FileNames.to_dict(), 'Db': prms.Db.to_dict(), 'DbCols': prms.DbCols #LINE# #TAB# #TAB#.to_dict(), 'DataSet': prms.DataSet.to_dict(), 'Reader': prms. #LINE# #TAB# #TAB# Reader.to_dict(), 'Instruments': prms.Instruments.to_dict(), #LINE# #TAB# #TAB# 'Batch': prms.Batch.to_dict()} #LINE# #TAB# return config_dict"
"def get_portgroup_details(session, dvs_name, pg_name): #LINE# #TAB# dvs_info = session._call_method(vim_util, 'get_dvs_details', dvs_name) #LINE# #TAB# if dvs_info is None: #LINE# #TAB# #TAB# raise exception.DvsNotFound(vim_name=dvs_name) #LINE# #TAB# port_group_details = dvs_info.portgroup_details #LINE# #TAB# if pg_name is None: #LINE# #TAB# #TAB# raise exception.PortgroupNotFound(vim_name=pg_name) #LINE# #TAB# return port_group_details['vlans'][0]['id']"
def optional_to_list(option: Optional[_T]) ->List[_T]: #LINE# #TAB# if option is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [option]
"def module_exists(module_name): #LINE# #TAB# try: #LINE# #TAB# #TAB# module_info = imp.find_module(module_name) #LINE# #TAB# #TAB# module = imp.load_module(module_name, *module_info) #LINE# #TAB# #TAB# imp.find_module('__init__', module_name, *module_info) #LINE# #TAB# #TAB# has_module = True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# has_module = False #LINE# #TAB# return has_module"
def check_database(database_name): #LINE# #TAB# if not in_project(database_name): #LINE# #TAB# #TAB# logging.error('The database {} does not exist in project'.format( #LINE# #TAB# #TAB# #TAB# database_name)) #LINE# #TAB# #TAB# return None #LINE# #TAB# return database_name
"def matching_details(song_name, song_title, artist): #LINE# #TAB# match_name = difflib.SequenceMatcher(None, song_name, song_title).ratio() #LINE# #TAB# match_title = difflib.SequenceMatcher(None, song_name, artist + song_title).ratio() #LINE# #TAB# if max(match_name,match_title) >= 0.55: #LINE# #TAB# #TAB# return True, max(match_name,match_title) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False, (match_name + match_title) / 2"
def choose_dialect(dialects): #LINE# #TAB# dialect_lower = {s.lower(): s for s in dialects} #LINE# #TAB# try: #LINE# #TAB# #TAB# return dialect_lower[0] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# dialect = dialects.pop(i) #LINE# #TAB# #TAB# #TAB# if not dialect: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# return dialect
def set_initialized(initialized): #LINE# #TAB# global node_initialized #LINE# #TAB# node_initialized = initialized
def set_run_validators(run): #LINE# #TAB# global _run_validators #LINE# #TAB# _run_validators = run
"def create_pdf(pdf_name: str, pdf_bytes: bytes) ->None: #LINE# #TAB# if not os.path.exists(filedir_pdf): #LINE# #TAB# #TAB# os.makedirs(filedir_pdf) #LINE# #TAB# output_filepath = filedir_pdf + '/' + pdf_name + '.pdf' #LINE# #TAB# if not os.path.exists(output_filepath): #LINE# #TAB# #TAB# os.makedirs(output_filepath) #LINE# #TAB# with open(output_filepath, 'wb') as outfile: #LINE# #TAB# #TAB# outfile.write(pdf_bytes) #LINE# #TAB# return"
"def reduce_deps(graph): #LINE# #TAB# reduced_graph = graph.copy() #LINE# #TAB# for node in graph.nodes(): #LINE# #TAB# #TAB# if isinstance(node, TestCaseGraph): #LINE# #TAB# #TAB# #TAB# reduced_graph.remove_node(node) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for child in node.children: #LINE# #TAB# #TAB# #TAB# if isinstance(child, TestCaseGraph): #LINE# #TAB# #TAB# #TAB# #TAB# reduced_graph.remove_edge(child, test=False) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return reduced_graph"
"def read_dir(dirPath, numLabels, modify=False): #LINE# #TAB# infile = os.listdir(dirPath) #LINE# #TAB# data = [] #LINE# #TAB# if modify: #LINE# #TAB# #TAB# for row in infile: #LINE# #TAB# #TAB# #TAB# if row.endswith('.csv'): #LINE# #TAB# #TAB# #TAB# #TAB# temp = [] #LINE# #TAB# #TAB# #TAB# #TAB# for col in row: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# temp.append(read_csv_row(col, numLabels)) #LINE# #TAB# #TAB# #TAB# data.append(temp) #LINE# #TAB# else: #LINE# #TAB# #TAB# print('Error: No data found.') #LINE# #TAB# data = np.array(data) #LINE# #TAB# return data"
"def get_signed_query_params_v2(credentials, expiration, string_to_sign): #LINE# #TAB# timestamp = int(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')) #LINE# #TAB# signed_string = quote(string_to_sign) #LINE# #TAB# signed_url = settings.SIGNED_URLS.format( #LINE# #TAB# #TAB# username=credentials.username, #LINE# #TAB# #TAB# password=expiration, #LINE# #TAB# #TAB# session=credentials.session, #LINE# #TAB# #TAB# signature=signed_string, #LINE# #TAB# ) #LINE# #TAB# return signed_url"
"def check_selinux_label(api, domain, logger): #LINE# #TAB# config = api.get_config_value('selinux.label') #LINE# #TAB# selinux = config['selinux'] #LINE# #TAB# if selinux: #LINE# #TAB# #TAB# if isinstance(selinux, list): #LINE# #TAB# #TAB# #TAB# for label in selinux: #LINE# #TAB# #TAB# #TAB# #TAB# if label == domain: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif not isinstance(selinux, six.string_types): #LINE# #TAB# #TAB# #TAB# logger.error('Invalid selinux label: %s' % label) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# logger.info('selinux mode: %s' % selinux) #LINE# #TAB# return True"
"def fsync_file(file_path): #LINE# #TAB# file_stats = None #LINE# #TAB# try: #LINE# #TAB# #TAB# st = os.stat(file_path) #LINE# #TAB# #TAB# if st: #LINE# #TAB# #TAB# #TAB# file_stats = st #LINE# #TAB# #TAB# fsync(file_path) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# logger.error('failed to fsync: %s', err) #LINE# #TAB# except Exception as err: #LINE# #TAB# #TAB# logger.error('failed to fsync: %s', err) #LINE# #TAB# return file_stats"
"def namespace_to_dict(obj): #LINE# #TAB# if isinstance(obj, argparse.Namespace): #LINE# #TAB# #TAB# d = dict(obj) #LINE# #TAB# elif isinstance(obj, optparse.Values): #LINE# #TAB# #TAB# d = {key: value for key, value in iteritems(obj)} #LINE# #TAB# return d"
"def is_console_app(): #LINE# #TAB# result = False #LINE# #TAB# if sys.stdout.isatty() or os.name == 'nt': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = True #LINE# #TAB# #TAB# except (OSError, IOError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = True #LINE# #TAB# #TAB# except (OSError, IOError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return result"
"def get_all_existing_loggers(): #LINE# #TAB# global _all_loggers #LINE# #TAB# if _all_loggers is None: #LINE# #TAB# #TAB# _all_loggers = [] #LINE# #TAB# #TAB# return _all_loggers #LINE# #TAB# for name, logger in logging.Logger.manager.loggerDict.items(): #LINE# #TAB# #TAB# if isinstance(logger, logging.PlaceHolder): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# _all_loggers.append((name, logger)) #LINE# #TAB# return _all_loggers"
def get_egg(directory): #LINE# #TAB# for name in os.listdir(directory): #LINE# #TAB# #TAB# if name.endswith('.egg'): #LINE# #TAB# #TAB# #TAB# return name
"def group_dataframe_from_rois(df, rois): #LINE# #TAB# table = [] #LINE# #TAB# for index in range(df.shape[0]): #LINE# #TAB# #TAB# if len(df) == index: #LINE# #TAB# #TAB# #TAB# row_rois = df[rois == index] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# col_rois = [rois[i] for i in range(df.shape[1])] #LINE# #TAB# #TAB# #TAB# row_rois.append(df[col_rois]) #LINE# #TAB# groups = pd.DataFrame.from_dict(table) #LINE# #TAB# return groups"
"def calc_cov_x(infodic, p): #LINE# #TAB# fjac = infodic['fjac'] #LINE# #TAB# ipvt = infodic['ipvt'] #LINE# #TAB# n = len(p) #LINE# #TAB# perm = np.take(np.eye(n), ipvt - 1, 0) #LINE# #TAB# r = np.triu(np.transpose(fjac)[:n, :]) #LINE# #TAB# R = np.dot(r, perm) #LINE# #TAB# try: #LINE# #TAB# #TAB# cov_x = np.linalg.inv(np.dot(np.transpose(R), R)) #LINE# #TAB# except LinAlgError: #LINE# #TAB# #TAB# cov_x = None #LINE# #TAB# return cov_x"
"def moving_average(iterable, n): #LINE# #TAB# value = None #LINE# #TAB# for i in range(0, len(iterable) - n + 1): #LINE# #TAB# #TAB# value = value + iterable[i] #LINE# #TAB# return value"
"def has_app(name, check_platform=True): #LINE# #TAB# if check_platform: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# import _winreg as winreg #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# import winreg #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, name) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
"def fq6_qi_pow(t_x, i): #LINE# #TAB# global frob_coeffs #LINE# #TAB# i %= 2 #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return t_x #LINE# #TAB# a, b, c, d, e, f = fq6_qi_pow(t_x[:6], i) #LINE# #TAB# g, h, i, j, k, l = fq6_mul(fq6_qi_pow(t_x[6:12], i), frob_coeffs[12, i, 1]) #LINE# #TAB# return a, b, c, d, e, f, g, h, i, j, k, l"
"def evenodd_hit(x, y, fills): #LINE# #TAB# n = len(x) #LINE# #TAB# if fills: #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# x, y = x[i], y[i] #LINE# #TAB# else: #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# if x == 0: #LINE# #TAB# #TAB# #TAB# #TAB# fill = fills[i] #LINE# #TAB# #TAB# #TAB# if y == 0: #LINE# #TAB# #TAB# #TAB# #TAB# fill = fills[i] #LINE# #TAB# return fill"
"def div_gf2(a, b): #LINE# #TAB# q1 = mod(a, deg(b)) #LINE# #TAB# q2 = mod(a * q - b, deg(b)) #LINE# #TAB# r1 = mod(a * r + b, deg(b)) #LINE# #TAB# r2 = mod(a * r - b * r1, deg(b)) #LINE# #TAB# return q1, r1 + r2"
"def create_connection(db_file): #LINE# #TAB# try: #LINE# #TAB# #TAB# conn = sqlite3.connect(db_file) #LINE# #TAB# #TAB# return conn #LINE# #TAB# except Error as e: #LINE# #TAB# #TAB# print(bcolors.FAIL, e, bcolors.ENDC) #LINE# #TAB# return None"
"def code_cache_check(cachefname): #LINE# #TAB# if not os.path.exists(cachefname): #LINE# #TAB# #TAB# return False, None #LINE# #TAB# if not os.path.exists(cachefname): #LINE# #TAB# #TAB# return False, None #LINE# #TAB# try: #LINE# #TAB# #TAB# with io.open(cachefname, 'rb') as f: #LINE# #TAB# #TAB# #TAB# cached = f.read() #LINE# #TAB# #TAB# return True, cached #LINE# #TAB# except (IOError, OSError): #LINE# #TAB# #TAB# return False, None"
"def get_adaptive_eval_interval(cur_dev_size, thres_dev_size, base_interval): #LINE# #TAB# if cur_dev_size < thres_dev_size: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# interval = int(cur_dev_size / thres_dev_size) #LINE# #TAB# if interval == 0: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# return interval"
"def important_dates(year): #LINE# #TAB# ddict = {} #LINE# #TAB# if year < 500: #LINE# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# else: #LINE# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# for month in range(1, 13): #LINE# #TAB# #TAB# if date_format not in ddict: #LINE# #TAB# #TAB# #TAB# ddict[date_format] = datetime.date(year, month, 1) #LINE# #TAB# #TAB# elif date_format == '%m/%d/%Y': #LINE# #TAB# #TAB# #TAB# date_format = '%m/%d/%Y' #LINE# #TAB# #TAB# ddict[date_format] = datetime.date(year, month, 1) #LINE# #TAB# return ddict"
"def log_lik_frob(S, D, variance): #LINE# #TAB# lS = np.array(S) #LINE# #TAB# lD = np.array(D) #LINE# #TAB# variance2 = 2 * variance ** 2 #LINE# #TAB# if variance2 > 0: #LINE# #TAB# #TAB# lS -= variance2 * np.log(variance) #LINE# #TAB# nSd = S.shape[0] #LINE# #TAB# nD = D.shape[0] #LINE# #TAB# logL = -0.5 * np.sum(np.log(nSd)) #LINE# #TAB# return logL"
def parse_history_node(node): #LINE# #TAB# if node is not None: #LINE# #TAB# #TAB# pubdate = {} #LINE# #TAB# #TAB# if node.tag == 'pubmed': #LINE# #TAB# #TAB# #TAB# for child in node.iterfind('./entry/pubdate'): #LINE# #TAB# #TAB# #TAB# #TAB# pubdate = parse_entry_node(child) #LINE# #TAB# #TAB# return pubdate #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'date': node.text}
"def length_check(trimmed_aln): #LINE# #TAB# aln_len_check = set() #LINE# #TAB# for aln in SeqIO.parse(trimmed_aln, 'fasta'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# aln_len_check.add(str(aln)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# for name in aln_len_check: #LINE# #TAB# #TAB# if not re.search('X\\d+', name): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if re.search('-\\d+', name): #LINE# #TAB# #TAB# #TAB# aln_len_check.add(name) #LINE# #TAB# return aln_len_check"
"def combined_download(accounts, days=60): #LINE# #TAB# client = ofxclient.Client(institution=None) #LINE# #TAB# out_file = StringIO() #LINE# #TAB# out_file.write('<OFX>') #LINE# #TAB# out_file.write('</OFX>') #LINE# #TAB# for a in accounts: #LINE# #TAB# #TAB# ofxclient_download(a, out_file, days=days) #LINE# #TAB# out_file.seek(0) #LINE# #TAB# out_file.write('</OFX>') #LINE# #TAB# out_file.seek(0) #LINE# #TAB# data = out_file.read() #LINE# #TAB# out_file.close() #LINE# #TAB# return data"
"def reps_bg_diff(mats: Iterable[sp.spmatrix]) ->np.ndarray: #LINE# #TAB# if not isinstance(mats[0], sp.spmatrix): #LINE# #TAB# #TAB# raise ValueError('Input must be an instance of sp.spmatrix.') #LINE# #TAB# if not mats[0].shape == mats[0].shape: #LINE# #TAB# #TAB# raise ValueError('Input must be an instance of sp.spmatrix.') #LINE# #TAB# output = np.zeros((mats[0].shape[0], mats[0].shape[1])) #LINE# #TAB# for mats_row, mats_col in enumerate(mats): #LINE# #TAB# #TAB# output[mats_row, mats_col] = np.median(mats_col, axis=0) #LINE# #TAB# return output"
"def process_profile_input(profile=None, geni_input=None, type_geni='g'): #LINE# #TAB# if profile is None and geni_input is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if type_geni == 'g': #LINE# #TAB# #TAB# if profile is not None: #LINE# #TAB# #TAB# #TAB# return profile #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return geni_input[profile] #LINE# #TAB# elif type_geni == 'b': #LINE# #TAB# #TAB# return profile #LINE# #TAB# else: #LINE# #TAB# #TAB# pass"
"def suits_with_dtype(mn, mx, dtype): #LINE# #TAB# if mn is None and mx is None or dtype is None or mn is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if mx is None or mx == mx: #LINE# #TAB# #TAB# return True #LINE# #TAB# if dtype is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# np.dtype(dtype) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
"def mk_nonce(when=None): #LINE# #TAB# when = when or time.time() #LINE# #TAB# str_nonce = str(time.time()).replace(':', '').replace('.', '').replace('-', '') #LINE# #TAB# nonce = '(' + str_nonce + ')' #LINE# #TAB# if when: #LINE# #TAB# #TAB# timestamp = str(when) #LINE# #TAB# #TAB# nonce = timestamp + '(' + str_nonce #LINE# #TAB# else: #LINE# #TAB# #TAB# timestamp = str(time.time()) #LINE# #TAB# #TAB# nonce = timestamp + '(' + str_nonce #LINE# #TAB# return nonce"
"def range_cardinality(slot: SlotDefinition) ->str: #LINE# #TAB# if isinstance(slot.definition['type'], str): #LINE# #TAB# #TAB# return '{}..{}'.format(slot.definition['type'], slot.definition[ #LINE# #TAB# #TAB# #TAB# 'number']) #LINE# #TAB# if isinstance(slot.definition['type'], list): #LINE# #TAB# #TAB# return '{}..{}'.format(slot.definition['type'], slot.definition[ #LINE# #TAB# #TAB# #TAB# 'number']) #LINE# #TAB# if isinstance(slot.definition['type'], list): #LINE# #TAB# #TAB# return '{}..{}'.format(slot.definition['type'], slot.definition[ #LINE# #TAB# #TAB# #TAB# 'number']) #LINE# #TAB# return ''"
"def is_host_ip6(value): #LINE# #TAB# for test in [lambda x: ipaddress.IPv6Address(x)._prefixlen!= 128, lambda #LINE# #TAB# #TAB# x: ipaddress.IPv6Address(x)._prefixlen!= 128]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return bool(test(value)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
def tileset_info(filename): #LINE# #TAB# tset = _read_tileset_info(filename) #LINE# #TAB# try: #LINE# #TAB# #TAB# info = json.loads(tset.info[filename]) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return None #LINE# #TAB# return info
"def is_valid_combination(cls, lease_id, status): #LINE# #TAB# lease = cls.get_or_bust(lease_id) #LINE# #TAB# event_sign = cls.get_or_bust(status) #LINE# #TAB# if event_sign is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if lease['status']!= status: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(lease['reservations']) > 1 or len(lease['events']) > 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# if any(not event_sign in lease['event_sign'] for event_sign in #LINE# #TAB# #TAB# event_sign): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def init_logs(): #LINE# #TAB# start_time = dt.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M') #LINE# #TAB# logname = os.path.join(os.path.expanduser('~') + '/nanoGUI_' + #LINE# #TAB# #TAB# start_time + '.log') #LINE# #TAB# handlers = [logging.FileHandler(logname)] #LINE# #TAB# logging.basicConfig(format='%(asctime)s %(message)s', handlers=handlers, #LINE# #TAB# #TAB# level=logging.INFO) #LINE# #TAB# logging.info('NanoGUI {} started with NanoPlot {}'.format(__version__, #LINE# #TAB# #TAB# nanoplot.__version__)) #LINE# #TAB# logging.info('Python version is: {}'.format(sys.version.replace('\n','')) #LINE# #TAB# #TAB# ) #LINE# #TAB# return logname"
"def file_exists(filename): #LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(""file '{}' not found"".format(filename)) #LINE# #TAB# if not os.path.isfile(filename): #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(""file '{}' does not exist"".format(filename)) #LINE# #TAB# return filename"
def start_htmap_logger(): #LINE# #TAB# logger = logging.getLogger('htmap') #LINE# #TAB# if not logger.handlers: #LINE# #TAB# #TAB# ch = logging.StreamHandler() #LINE# #TAB# #TAB# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s') #LINE# #TAB# #TAB# ch.setFormatter(formatter) #LINE# #TAB# #TAB# logger.addHandler(ch) #LINE# #TAB# #TAB# logger.setLevel(logging.INFO) #LINE# #TAB# logger.addHandler(ch) #LINE# #TAB# return logger
"def get_removed_fraction(untrimmed_alignment_size, no_sites_trimmed): #LINE# #TAB# trimmed_size = float(untrimmed_alignment_size) #LINE# #TAB# if trimmed_size < no_sites_trimmed: #LINE# #TAB# #TAB# removed_fraction = no_sites_trimmed - trimmed_size #LINE# #TAB# else: #LINE# #TAB# #TAB# removed_fraction = trimmed_size / no_sites_trimmed #LINE# #TAB# return removed_fraction"
"def extract_color(color): #LINE# #TAB# if not color.isdigit(): #LINE# #TAB# #TAB# return None #LINE# #TAB# color = int(color, 16) #LINE# #TAB# r = color >> 16 & 255 #LINE# #TAB# g = color >> 8 & 255 #LINE# #TAB# b = color & 255 #LINE# #TAB# if r > 255 or g > 255 or b > 255: #LINE# #TAB# #TAB# return None #LINE# #TAB# return r, g, b"
"def pack_contsign_as_key_image_as_val(k, v): #LINE# #TAB# b = len(v) #LINE# #TAB# assert len(k) == 2 #LINE# #TAB# c1 = k[0] #LINE# #TAB# if k[1] == 1: #LINE# #TAB# #TAB# if v[1] == 1: #LINE# #TAB# #TAB# #TAB# c1 = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# c2 = 1 #LINE# #TAB# elif v[1] == 1: #LINE# #TAB# #TAB# c1 = 0 #LINE# #TAB# #TAB# c2 = v[0] #LINE# #TAB# return b, c1, c2"
def gen_table_from_list(csv_content): #LINE# #TAB# t = [] #LINE# #TAB# for row in csv_content: #LINE# #TAB# #TAB# c = [] #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# cell = next(row) #LINE# #TAB# #TAB# #TAB# if cell.cell_type == MyTableCell: #LINE# #TAB# #TAB# #TAB# #TAB# c.append(cell) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# t.append(c) #LINE# #TAB# t.sort(key=lambda x: x[0]) #LINE# #TAB# return t
"def fetch_and_parse_school_unit_data(code: str) ->dict: #LINE# #TAB# data = {} #LINE# #TAB# url = SCHOOL_DATA_URL + code #LINE# #TAB# resp = requests.get(url) #LINE# #TAB# resp.raise_for_status() #LINE# #TAB# data = json.loads(resp.text) #LINE# #TAB# if code not in data: #LINE# #TAB# #TAB# logger.error('Error fetching code School unit.') #LINE# #TAB# #TAB# raise ValueError('Error fetching code School unit.') #LINE# #TAB# for code, data in data.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# unit_data = json.loads(data) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# logger.error('Error parsing code School unit.') #LINE# #TAB# #TAB# #TAB# raise ValueError('Error parsing data.') #LINE# #TAB# return unit_data"
def strip_and_uniq(tab): #LINE# #TAB# _list = [] #LINE# #TAB# for element in tab: #LINE# #TAB# #TAB# if element not in _list: #LINE# #TAB# #TAB# #TAB# _list.append(element) #LINE# #TAB# return _list
"def prox_max_entropy(X, step, gamma=1): #LINE# #TAB# T = X.shape[0] #LINE# #TAB# p = gamma * np.exp(X / T) #LINE# #TAB# res = np.zeros(T) #LINE# #TAB# for t in range(T): #LINE# #TAB# #TAB# res[t] = np.max(X[t:t + p]) #LINE# #TAB# return res"
"def add_operatingsystem(cls, options=None): #LINE# #TAB# cls.command_sub = 'add-operatingsystem' #LINE# #TAB# result = cls.execute(cls._construct_command(options)) #LINE# #TAB# return result"
"def diff_mark(m, t): #LINE# #TAB# diff_mark = m - t #LINE# #TAB# return diff_mark"
"def normalize_2d(im, min_value, max_value): #LINE# #TAB# if im.ndim!= 2: #LINE# #TAB# #TAB# raise ValueError('Im must be 2D array') #LINE# #TAB# im = np.asarray(im) #LINE# #TAB# im[im < min_value] = min_value #LINE# #TAB# im[im > max_value] = max_value #LINE# #TAB# return im"
"def calc_z0_and_conv_factor_from_ratio_of_harmonics(z, z2, NA=0.999): #LINE# #TAB# z0 = stats.norm.ppf(z) #LINE# #TAB# z2_mpf = stats.norm.ppf(z2) #LINE# #TAB# conv_factor = stats.norm.ifft(z0 * z2_mpf / NA) #LINE# #TAB# return conv_factor, z2_mpf"
def get_label_vocab(vocab_path): #LINE# #TAB# label_vocab = set() #LINE# #TAB# with open(vocab_path) as file: #LINE# #TAB# #TAB# for line in file: #LINE# #TAB# #TAB# #TAB# label_vocab.add(line.strip()) #LINE# #TAB# label_vocab = list(set(label_vocab)) #LINE# #TAB# return label_vocab
"def internal_cycles(model, allowedreacs=None, reacsbounds={}, tol=1e-10): #LINE# #TAB# if allowedreacs is None: #LINE# #TAB# #TAB# allowedreacs = [model] #LINE# #TAB# while True: #LINE# #TAB# #TAB# for rxn in model: #LINE# #TAB# #TAB# #TAB# if not rxn.reacs: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# newreac = _get_internal_cycles(rxn, allowedreacs=allowedreacs, #LINE# #TAB# #TAB# #TAB# #TAB# reacsbounds=reacsbounds, tol=tol) #LINE# #TAB# #TAB# #TAB# if newreac: #LINE# #TAB# #TAB# #TAB# #TAB# yield newreac"
def set_safe_paths(list_of_paths): #LINE# #TAB# safe_paths = [safe_path(x) for x in list_of_paths] #LINE# #TAB# os.chdir(safe_paths[0]) #LINE# #TAB# return safe_paths
"def ephem_to_timezone(date, tzinfo): #LINE# #TAB# seconds, microseconds = _ephem_convert_to_seconds_and_microseconds(date) #LINE# #TAB# date = dt.datetime.fromtimestamp(seconds, tzinfo) #LINE# #TAB# date = date.replace(microsecond=microseconds) # # # return date"
"def filter_csv_by_n_subjects(participants, n_adult, n_child): #LINE# #TAB# new_csv_files = [] #LINE# #TAB# n_subjects = n_adult + n_child #LINE# #TAB# for participant in participants: #LINE# #TAB# #TAB# if len(participant) <= n_subjects: #LINE# #TAB# #TAB# #TAB# new_csv_files.append(participant) #LINE# #TAB# #TAB# #TAB# n_adult += 1 #LINE# #TAB# #TAB# elif len(participant) > n_child: #LINE# #TAB# #TAB# #TAB# new_csv_files.append(participant) #LINE# #TAB# if len(new_csv_files) > n_adult: #LINE# #TAB# #TAB# return new_csv_files #LINE# #TAB# else: #LINE# #TAB# #TAB# return new_csv_files"
"def get_params_files(paths=ini_paths): #LINE# #TAB# params_files = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# for ini_file in [f for f in ini_paths if f.endswith('.py')]: #LINE# #TAB# #TAB# #TAB# if os.path.isfile(ini_file): #LINE# #TAB# #TAB# #TAB# #TAB# with open(ini_file, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# content = f.read() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# params_files.append(content) #LINE# #TAB# return params_files"
"def log_softmax(x): #LINE# #TAB# x = tf.expand_dims(x, -1) #LINE# #TAB# max_shape = x.shape[-1] #LINE# #TAB# x = tf.log(tf.exp(tf.expand_dims(x, max_shape))) #LINE# #TAB# x = tf.expand_dims(x, -1) #LINE# #TAB# return x"
"def quote_str(s: str) ->str: #LINE# #TAB# if not s: #LINE# #TAB# #TAB# return '""""' #LINE# #TAB# if'' in s: #LINE# #TAB# #TAB# s = '""' + s + '""' #LINE# #TAB# if '""' in s: #LINE# #TAB# #TAB# s = s.replace('""', '""""') #LINE# #TAB# return s"
def format_date(date): #LINE# #TAB# if date: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%d') #LINE# #TAB# if date.year == YEAR and date.month!= 12: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%d') #LINE# #TAB# if date.year == YEAR and date.month!= 12: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%d') #LINE# #TAB# if date.year == YEAR and date.month!= 12: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%d') #LINE# #TAB# if date.year == YEAR and date.month!= 12: #LINE# #TAB# #TAB# return date.strftime('%Y-%m-%d') #LINE# #TAB# return ''
"def unpack_tableswitch(bc, offset): #LINE# #TAB# jump = offset % 4 #LINE# #TAB# if jump: #LINE# #TAB# #TAB# offset += 4 - jump #LINE# #TAB# (default, low, high), offset = _unpack(_struct_iii, bc, offset) #LINE# #TAB# joffs = list() #LINE# #TAB# for _index in range(high - low + 1): #LINE# #TAB# #TAB# j, offset = _unpack(_struct_i, bc, offset) #LINE# #TAB# #TAB# joffs.append(j) #LINE# #TAB# return (default, low, high, joffs), offset"
"def assert_equal_none(logical_line): #LINE# #TAB# res = asse_equal_start_with_none_re.search(logical_line #LINE# #TAB# #TAB# ) or asse_equal_end_with_none_re.search(logical_line) #LINE# #TAB# if res: #LINE# #TAB# #TAB# yield 0, 'N318: assertEqual(A, None) or assertEqual(None, A) sentences not allowed'"
"def to_unicode(text): #LINE# #TAB# if not text or isinstance(text, unicode if PY2 else str): #LINE# #TAB# #TAB# return text #LINE# #TAB# try: #LINE# #TAB# #TAB# return text.decode('UTF-8') #LINE# #TAB# except UnicodeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return text.decode('CP1252') #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return text"
"def all_states(n, big_endian=False): #LINE# #TAB# n = int(n) #LINE# #TAB# states = [] #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# if big_endian: #LINE# #TAB# #TAB# #TAB# states.append(n % 2) #LINE# #TAB# #TAB# #TAB# n //= 2 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# states.append(n % 2) #LINE# #TAB# return states"
"def get_model(): #LINE# #TAB# if not hasattr(g,'model'): #LINE# #TAB# #TAB# g.model = _get_model_() #LINE# #TAB# return g.model"
"def get_request_param(request: web.Request, name: str, error_if_missing: #LINE# #TAB# Optional[Exception]=None) ->Any: #LINE# #TAB# values = request.GET.get(name) #LINE# #TAB# if values is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# raise error_if_missing() #LINE# #TAB# #TAB# except web.MissingParameter: #LINE# #TAB# #TAB# #TAB# raise error_if_missing #LINE# #TAB# return values"
def check_invalid_function_name(tokens: List[TokenInfo]) ->Optional[str]: #LINE# #TAB# func_name = None #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.kind!= TokenKind.NAME: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if func_name is None: #LINE# #TAB# #TAB# #TAB# func_name = token.value.lower() #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return func_name
"def parse_extension_arg(arg, arg_dict): #LINE# #TAB# try: #LINE# #TAB# #TAB# if '.' not in arg: #LINE# #TAB# #TAB# #TAB# return arg_dict #LINE# #TAB# #TAB# tokens = arg.split('.') #LINE# #TAB# #TAB# if len(tokens) == 1: #LINE# #TAB# #TAB# #TAB# key = tokens[0] #LINE# #TAB# #TAB# #TAB# value = tokens[1] #LINE# #TAB# #TAB# elif len(tokens) == 2: #LINE# #TAB# #TAB# #TAB# key, value = tokens #LINE# #TAB# #TAB# #TAB# arg_dict[str(key).strip()] = value.strip() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return arg_dict"
"def deferred_emails(): #LINE# #TAB# if settings.CENTRAL_EMAILS: #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# email = request.form.get('email') #LINE# #TAB# #TAB# #TAB# if not email: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# #TAB# #TAB# if email in _deferred_emails: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# except (AttributeError, ValueError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return False"
"def serach_path(): #LINE# #TAB# serach_path = os.path.dirname(__file__) #LINE# #TAB# serach_path = os.path.join(serach_path, 'bin','serach') #LINE# #TAB# return serach_path"
"def get_flow_matrix(G, nodelist=None): #LINE# #TAB# M = nx.Graph() #LINE# #TAB# if nodelist is None: #LINE# #TAB# #TAB# nodelist = G.nodes() #LINE# #TAB# for n in nodelist: #LINE# #TAB# #TAB# M.add_node(n) #LINE# #TAB# return M"
"def get_high_intensity_peaks(image, mask, num_peaks): #LINE# #TAB# output = image #LINE# #TAB# mask = np.zeros((num_peaks, mask.shape[0]), dtype=np.bool) #LINE# #TAB# for i in range(num_peaks): #LINE# #TAB# #TAB# peak = image[:, :, (i)] #LINE# #TAB# #TAB# if peak[1] > mask[0]: #LINE# #TAB# #TAB# #TAB# output = peak #LINE# #TAB# #TAB# #TAB# mask = mask[1:] #LINE# #TAB# return output"
def get_templates(cls): #LINE# #TAB# dialog = cls(None) #LINE# #TAB# result = dialog.get_templates() #LINE# #TAB# if result is not None: #LINE# #TAB# #TAB# return result #LINE# #TAB# plugin_templates = [] #LINE# #TAB# for item in dialog.get_templates(): #LINE# #TAB# #TAB# if item.name not in plugin_templates: #LINE# #TAB# #TAB# #TAB# plugin_templates.append(item.name) #LINE# #TAB# return plugin_templates
"def calc_downsample(w, h, target=400): #LINE# #TAB# if w > h: #LINE# #TAB# #TAB# downsampler = h / w #LINE# #TAB# #TAB# if int(downsampler < target): #LINE# #TAB# #TAB# #TAB# return int(downsampler) #LINE# #TAB# #TAB# elif int(downsampler > target): #LINE# #TAB# #TAB# #TAB# return target #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return w #LINE# #TAB# else: #LINE# #TAB# #TAB# return w"
"def string_to_bytes(value: str) ->bytes: #LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# return value.encode('utf-8') #LINE# #TAB# elif isinstance(value, bytes): #LINE# #TAB# #TAB# return value #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
def cleanup_nodes(doc): #LINE# #TAB# while doc and len(doc) > 0: #LINE# #TAB# #TAB# for n in doc: #LINE# #TAB# #TAB# #TAB# if not n.isspace(): #LINE# #TAB# #TAB# #TAB# #TAB# del doc[n]
"def disable_tracing_hostname(url, blacklist_hostnames=None): #LINE# #TAB# if blacklist_hostnames is None: #LINE# #TAB# #TAB# blacklist_hostnames = DEFAULT_BLACKLIST_HOSTNAMES #LINE# #TAB# url = re.sub(URL_PATTERN, '', url) #LINE# #TAB# url_lower = url.lower() #LINE# #TAB# for hostname in blacklist_hostnames: #LINE# #TAB# #TAB# if url_lower.startswith(hostname): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def sdl_sempost(sem): #LINE# #TAB# sem_c = unbox(sem, 'SDL_sem *') #LINE# #TAB# rc = lib.sdl_sempost(sem_c) #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return rc"
"def job_binary_create(context, values): #LINE# #TAB# b = JobBinary(context, values) #LINE# #TAB# b.data = values.get('data') #LINE# #TAB# return b"
"def queue_get_all(cls, q): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield q.get_nowait() #LINE# #TAB# #TAB# except queue.Empty: #LINE# #TAB# #TAB# #TAB# break"
"def make_editions_dict(editions): #LINE# #TAB# d = {} #LINE# #TAB# nums = ['1', '2', '3', '4', '5', '6'] #LINE# #TAB# num_counter = 0 #LINE# #TAB# for k, date_dict in editions.items(): #LINE# #TAB# #TAB# d['edition%s' % nums[num_counter]] = k #LINE# #TAB# #TAB# if date_dict['start'] is not None: #LINE# #TAB# #TAB# #TAB# d['start_e%s' % nums[num_counter]] = date_dict['start'].isoformat() #LINE# #TAB# #TAB# if date_dict['end'] is not None: #LINE# #TAB# #TAB# #TAB# d['end_e%s' % nums[num_counter]] = date_dict['end'].isoformat() #LINE# #TAB# #TAB# num_counter += 1 #LINE# #TAB# return d"
"def pack_gossip_notify(data_type): #LINE# #TAB# code = struct.pack('!H', GOSSIP_NOTIFY_CODE) #LINE# #TAB# data = None #LINE# #TAB# return {'code': code, 'data': data}"
"def bracketed_list(l, r, sep, expr, allow_missing_close=False): #LINE# #TAB# closer = sym(r) if not allow_missing_close else p.Optional(sym(r)) #LINE# #TAB# return sym(l) - listMembers(sep, expr) - closer"
"def maybe_download(filename): #LINE# #TAB# if not os.path.exists(WORK_DIRECTORY): #LINE# #TAB# #TAB# os.mkdir(WORK_DIRECTORY) #LINE# #TAB# filepath = os.path.join(WORK_DIRECTORY, filename) #LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# filepath, _ = request.urlretrieve(filepath, filename) #LINE# #TAB# #TAB# print('Successfully downloaded', filename, 'to', filepath) #LINE# #TAB# else: #LINE# #TAB# #TAB# os.remove(filepath) #LINE# #TAB# return filepath"
"def linear_weighted_moving_average(data, period): #LINE# #TAB# catch_errors.check_for_period_error(data, period) #LINE# #TAB# wma = sum(linear_weighted_moving_average(data, period)) #LINE# #TAB# return wma"
def initialize_from_parammat(paramMat): #LINE# #TAB# paramList = [] #LINE# #TAB# for i in range(len(paramMat)): #LINE# #TAB# #TAB# paramList.append(paramMat[i]) #LINE# #TAB# return paramList
"def check_nonnegative(entry: Union[sparse.csr_matrix, np.ndarray]): #LINE# #TAB# if not has_negative_entries(entry): #LINE# #TAB# #TAB# raise ValueError('Only positive values are expected.') #LINE# #TAB# else: #LINE# #TAB# #TAB# return"
def is_unit(ustr): #LINE# #TAB# ustr = tools.bytes2str(ustr) #LINE# #TAB# if is_null_unit(ustr): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# as_unit(ustr) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def classify_atom_occurrences_in_formula(phi: Formula): #LINE# #TAB# if isinstance(phi, Formula): #LINE# #TAB# #TAB# classes = [] #LINE# #TAB# #TAB# for a in phi.atoms: #LINE# #TAB# #TAB# #TAB# if a.is_Symbol: #LINE# #TAB# #TAB# #TAB# #TAB# classes.append(True) #LINE# #TAB# #TAB# #TAB# elif a.is_Integer(): #LINE# #TAB# #TAB# #TAB# #TAB# classes.append(False) #LINE# #TAB# #TAB# #TAB# elif a.is_Symbol(): #LINE# #TAB# #TAB# #TAB# #TAB# classes.append(False) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# logger.warning('Could not classify atom {} because it is not positive or negative.' #LINE# #TAB# #TAB# #TAB# #TAB#.format(phi)) #LINE# #TAB# else: #LINE# #TAB# #TAB# classes = [] #LINE# #TAB# return classes"
"def enumerate_query_by_limit(q, limit=1000): #LINE# #TAB# for offset in count(0, limit): #LINE# #TAB# #TAB# r = q.offset(offset).limit(limit).all() #LINE# #TAB# #TAB# for row in r: #LINE# #TAB# #TAB# #TAB# yield row #LINE# #TAB# #TAB# if len(r) < limit: #LINE# #TAB# #TAB# #TAB# break"
"def load_dict(dict_str, str_ok=False): #LINE# #TAB# o = {} #LINE# #TAB# for c in dict_str.splitlines(): #LINE# #TAB# #TAB# if not str_ok or len(c) > 0: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid format for {!r}: {!r}'.format(c, dict_str)) #LINE# #TAB# #TAB# if c[0] not in o: #LINE# #TAB# #TAB# #TAB# raise ValueError('Invalid format for {!r}: {!r}'.format(c, o[0])) #LINE# #TAB# #TAB# o[c[0]] = c[1] #LINE# #TAB# return o"
def total_seconds(td): #LINE# #TAB# try: #LINE# #TAB# #TAB# return td.total_seconds() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6 #LINE# #TAB# return td
"def prepare_image_choices(images: List[Image]) ->List[Tuple[str, Image]]: #LINE# #TAB# available_images = [] #LINE# #TAB# for image in images: #LINE# #TAB# #TAB# available_images.append((image.name, image)) #LINE# #TAB# return available_images"
"def most_common_item(lst): #LINE# #TAB# lst = [l for l in lst if l] #LINE# #TAB# if lst: #LINE# #TAB# #TAB# return max(set(lst), key=lst.count) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def namedb_preorder_remove( cur, preorder_hash ): #LINE# #TAB# preorder_rows = namedb_select_preorder_rows( cur, preorder_hash ) #LINE# #TAB# ret = False #LINE# #TAB# for preorder_row in preorder_rows: #LINE# #TAB# #TAB# if preorder_row['preorder_hash'] == preorder_hash: #LINE# #TAB# #TAB# #TAB# ret = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# namedb_query_execute( cur, 'BEGIN', ()) #LINE# #TAB# namedb_query_execute( cur, 'END', ()) #LINE# #TAB# return ret"
"def focused_declaration(): #LINE# #TAB# focus_widget = focus_widget() #LINE# #TAB# widget_id = focus_widget.id() #LINE# #TAB# declaration = widget_by_id(focus_widget, widget_id) #LINE# #TAB# return declaration"
"def decode_edges(edges): #LINE# #TAB# if edges == 0: #LINE# #TAB# #TAB# return [], [] #LINE# #TAB# forward = [] #LINE# #TAB# reverse = [] #LINE# #TAB# for i, v in enumerate(edges): #LINE# #TAB# #TAB# if v % 2 == 1: #LINE# #TAB# #TAB# #TAB# forward.append(i) #LINE# #TAB# #TAB# #TAB# reverse.append(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# forward.append(i) #LINE# #TAB# #TAB# #TAB# reverse.append(v) #LINE# #TAB# if reverse: #LINE# #TAB# #TAB# return forward, reverse #LINE# #TAB# else: #LINE# #TAB# #TAB# return forward, reverse"
"def cylinder_inertia(mass, radius, height, transform=None): #LINE# #TAB# if transform is None: #LINE# #TAB# #TAB# transform = odl.Matrix() #LINE# #TAB# x = np.cos(radius) * np.cos(height) #LINE# #TAB# y = np.sin(radius) * np.sin(height) #LINE# #TAB# z = np.cos(radius) #LINE# #TAB# inertia = np.matrix([np.sin(x), np.cos(y), np.sin(z)], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# dtype=np.float64) #LINE# #TAB# if transform is not None: #LINE# #TAB# #TAB# inertia = transform.dot(inertia) #LINE# #TAB# return inertia"
"def filter_repos(repos, ignore_repos): #LINE# #TAB# filtered_repos = [] #LINE# #TAB# for repo in repos: #LINE# #TAB# #TAB# if repo in ignore_repos: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# filtered_repos.append(repo) #LINE# #TAB# return filtered_repos"
"def timestamp_utc(value): #LINE# #TAB# try: #LINE# #TAB# #TAB# return time.strptime(value, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo= #LINE# #TAB# #TAB# #TAB# timezone.utc) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return value"
"def natural_rate(delta: Union[float, datetime.timedelta]): #LINE# #TAB# delta = normalize_timedelta(delta) #LINE# #TAB# return f'{delta.days} per {delta.seconds}'"
"def read_pdf(filename): #LINE# #TAB# pdf = PdfFileReader(filename) #LINE# #TAB# path = pdf.generate_filename() #LINE# #TAB# with open(path, 'rb') as infile: #LINE# #TAB# #TAB# is_data = True #LINE# #TAB# #TAB# while not is_data: #LINE# #TAB# #TAB# #TAB# if infile.hasNext(): #LINE# #TAB# #TAB# #TAB# #TAB# raise StopIteration #LINE# #TAB# #TAB# #TAB# lines = infile.readlines() #LINE# #TAB# #TAB# #TAB# if not lines: #LINE# #TAB# #TAB# #TAB# #TAB# raise StopIteration #LINE# #TAB# #TAB# #TAB# pdf.addPage(lines) #LINE# #TAB# return path"
"def convert_strings_to_list(args, argname): #LINE# #TAB# if isinstance(args[argname], str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return [i for i in args[argname]] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# return args[argname]"
"def get_bearer_auth_credentials(auth): #LINE# #TAB# if auth: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# auth = auth.split(' ', 1)[0].strip() #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# user = auth.split(' ', 1)[0].strip() #LINE# #TAB# #TAB# password = auth.split(':', 1)[1].strip() #LINE# #TAB# #TAB# return {'user': user, 'password': password} #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def extension_for_type(mime_type): #LINE# #TAB# ext = mime_type.split('/')[-1] #LINE# #TAB# if ext in ['.jpeg', '.jpe', '.jpe']: #LINE# #TAB# #TAB# random.seed() #LINE# #TAB# #TAB# ext = '.jpg' #LINE# #TAB# return ext"
"def get_chebi_name_from_id(chebi_id, offline=False): #LINE# #TAB# name = chebi_name_from_id(chebi_id, offline) #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = 'ChEBI'+ chebi_id #LINE# #TAB# return name"
"def get_by_urn(cls, actor_urn): #LINE# #TAB# actor_ref = None #LINE# #TAB# for actor in cls.list(): #LINE# #TAB# #TAB# if actor.urn.value == actor_urn: #LINE# #TAB# #TAB# #TAB# actor_ref = actor #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return actor_ref"
"def get_document_by_bookmark(bookmark: str) ->Dict[str, Any]: #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(bookmark, 'rb') as bookmark_file: #LINE# #TAB# #TAB# #TAB# return json.loads(bookmark_file.read()) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return {}"
"def update_paths(paths, idx): #LINE# #TAB# python_paths = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if not isinstance(path, list): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# python_paths.append(path) #LINE# #TAB# #TAB# for path in idx: #LINE# #TAB# #TAB# #TAB# if path not in python_paths: #LINE# #TAB# #TAB# #TAB# #TAB# python_paths.append(path) #LINE# #TAB# python_paths = python_paths + [path] #LINE# #TAB# return python_paths"
"def make_rw(obj: Any): #LINE# #TAB# if isinstance(obj, np.ndarray): #LINE# #TAB# #TAB# if obj.size == 0: #LINE# #TAB# #TAB# #TAB# return np.rw(obj) #LINE# #TAB# #TAB# return obj #LINE# #TAB# shape = tuple(obj.shape) #LINE# #TAB# if len(shape)!= 2: #LINE# #TAB# #TAB# raise ValueError('RO object should have shape %r' % shape) #LINE# #TAB# if len(shape)!= 2: #LINE# #TAB# #TAB# raise ValueError('RO object should have shape %r' % shape) #LINE# #TAB# r = roo.RW(shape) #LINE# #TAB# r.mask = obj.mask #LINE# #TAB# return r"
"def flatten_material(material): #LINE# #TAB# mat = copy.deepcopy(material) #LINE# #TAB# mat_flattened = [] #LINE# #TAB# for k in list(mat.keys()): #LINE# #TAB# #TAB# if isinstance(mat[k], dict): #LINE# #TAB# #TAB# #TAB# mat_flattened.extend(flatten_material(mat[k])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mat_flattened.append(mat[k]) #LINE# #TAB# return mat_flattened"
"def read_config(fs, settings_path='settings.ini'): #LINE# #TAB# cwd = fs.getcwd() #LINE# #TAB# settings_path = os.path.join(cwd, settings_path) #LINE# #TAB# if not os.path.exists(settings_path): #LINE# #TAB# #TAB# return {} #LINE# #TAB# config = ConfigParser() #LINE# #TAB# config.optionxform = str #LINE# #TAB# config.read(settings_path) #LINE# #TAB# if platform.system().lower().startswith('win'): #LINE# #TAB# #TAB# config.readfp(fs.open(settings_path)) #LINE# #TAB# else: #LINE# #TAB# #TAB# config.readfp(fs.open(settings_path, 'r')) #LINE# #TAB# return config"
"def _is_charge_balanced(struct): #LINE# #TAB# #TAB# charge = 0 #LINE# #TAB# #TAB# for attr in structure.attributes.keys(): #LINE# #TAB# #TAB# #TAB# if not hasattr(struct, attr): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# value = getattr(struct, attr) #LINE# #TAB# #TAB# #TAB# charge += value * value #LINE# #TAB# #TAB# if charge == 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return True"
"def normalize_a(A): #LINE# #TAB# assert type(A) is tf.SparseTensor or type(A) is tf.Tensor #LINE# #TAB# common_layers = get_common_layers(A) #LINE# #TAB# if len(common_layers) == 2: #LINE# #TAB# #TAB# A_norm = A.tocsc().data #LINE# #TAB# else: #LINE# #TAB# #TAB# A_norm = A.tocsr() #LINE# #TAB# #TAB# for _ in range(3): #LINE# #TAB# #TAB# #TAB# A_norm = tf.divide(A_norm, tf.sqrt(A_norm)) #LINE# #TAB# return A_norm"
def set_native_id(idx): #LINE# #TAB# import ctypes as ct #LINE# #TAB# from.util import safe_call as safe_call #LINE# #TAB# from.library import backend #LINE# #TAB# if backend.name()!= 'cuda': #LINE# #TAB# #TAB# raise RuntimeError('Invalid backend loaded') #LINE# #TAB# safe_call(backend.get().afcu_set_native_id(idx)) #LINE# #TAB# return
def get_max_outputs(): #LINE# #TAB# max_outputs = RPR.GetMaxMIDIOutputs() #LINE# #TAB# return max_outputs
def merge_input(input_array): #LINE# #TAB# output_string = '' #LINE# #TAB# for line in input_array: #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# output_string += line + '\n' #LINE# #TAB# output_string = output_string[:-1] #LINE# #TAB# return output_string
"def sort_version_tuples(versions, reverse=False): #LINE# #TAB# items = [] #LINE# #TAB# for version in versions: #LINE# #TAB# #TAB# item = {'major': version.major,'minor': version.minor, #LINE# #TAB# #TAB# #TAB# 'patch': version.patch, 'build': version.build} #LINE# #TAB# #TAB# if reverse: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# item['reverse'] = True #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# item['reverse'] = False #LINE# #TAB# #TAB# #TAB# items.append(item) #LINE# #TAB# return items"
"def at_least_libvips(x, y): #LINE# #TAB# if not isinstance(x, basestring): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not isinstance(y, basestring): #LINE# #TAB# #TAB# return False #LINE# #TAB# if x.splitlines() and not y.splitlines(): #LINE# #TAB# #TAB# return False #LINE# #TAB# found = False #LINE# #TAB# for x_line in x.splitlines(): #LINE# #TAB# #TAB# found = True #LINE# #TAB# #TAB# for y_line in y.splitlines(): #LINE# #TAB# #TAB# #TAB# if not found and 'vips' in y_line: #LINE# #TAB# #TAB# #TAB# #TAB# found = False #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if not found: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def is_compatible(signature, withSignature): #LINE# #TAB# if not signature.startswith(withSignature): #LINE# #TAB# #TAB# raise ValueError('Invalid signature: %r' % withSignature) #LINE# #TAB# if len(signature)!= len(withSignature): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def get_content_length(environ): #LINE# #TAB# content_length = environ.get('CONTENT_LENGTH') #LINE# #TAB# if content_length is None: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# content_length = int(content_length) #LINE# #TAB# #TAB# if content_length < 0: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return content_length"
"def replace_subqueries(ex, tables, table_ctor): #LINE# #TAB# for k, v in tables.items(): #LINE# #TAB# #TAB# if isinstance(v, BaseX): #LINE# #TAB# #TAB# #TAB# ex = replace_subqueries(ex, v, table_ctor) #LINE# #TAB# return ex"
"def src_simple(input_data, output_data, ratio, converter_type, channels): #LINE# #TAB# raw_input_data = ffi.new('float*') #LINE# #TAB# raw_output_data = ffi.new('float*') #LINE# #TAB# converter = _src_simple_converter(converter_type, input_data, output_data, #LINE# #TAB# #TAB# raw_input_data, ratio, channels) #LINE# #TAB# output_data = ffi.new('float*') #LINE# #TAB# result = converter(input_data, output_data, converter) #LINE# #TAB# return result, raw_output_data"
"def normalize_host(scheme, host): #LINE# #TAB# if not host: #LINE# #TAB# #TAB# return host #LINE# #TAB# parsed = urlsplit(host) #LINE# #TAB# if scheme in ['http', 'https']: #LINE# #TAB# #TAB# path = unquote(parsed.path) #LINE# #TAB# #TAB# if path.startswith('/'): #LINE# #TAB# #TAB# #TAB# path = path[1:] #LINE# #TAB# #TAB# if path.endswith('/'): #LINE# #TAB# #TAB# #TAB# path = path[:-1] #LINE# #TAB# #TAB# urlunparse = urlparse(scheme + '://' + path) #LINE# #TAB# #TAB# return urlunparse.scheme + '://' + urlunparse.netloc + urlunparse.path #LINE# #TAB# return host"
"def user_can_create_topic(category, forum, user): #LINE# #TAB# can_create = True #LINE# #TAB# if user.is_authenticated: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# create_topic = Topic.objects.create(category=category, #LINE# #TAB# #TAB# #TAB# #TAB# forum_id=forum.pk, user=user).exists() #LINE# #TAB# #TAB# except Topic.DoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if create_topic: #LINE# #TAB# #TAB# #TAB# can_create = False #LINE# #TAB# return can_create"
"def validate_bids(opts): #LINE# #TAB# bids = opts.bids or [] #LINE# #TAB# if not bids: #LINE# #TAB# #TAB# return bids #LINE# #TAB# if isinstance(opts, dict): #LINE# #TAB# #TAB# for k, v in six.iteritems(opts): #LINE# #TAB# #TAB# #TAB# bids.append(validate_bids(v)) #LINE# #TAB# elif isinstance(bids, list): #LINE# #TAB# #TAB# for bids in bids: #LINE# #TAB# #TAB# #TAB# for item in bids: #LINE# #TAB# #TAB# #TAB# #TAB# bids.append(validate_bids(item)) #LINE# #TAB# else: #LINE# #TAB# #TAB# bids = validate_bids(bids) #LINE# #TAB# return bids"
def get_config(): #LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = '' #LINE# #TAB# cfg.parentdir_prefix ='sphinx_ioam_theme-' #LINE# #TAB# cfg.versionfile_source ='sphinx_ioam_theme/_version.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"def get_flag_suggestions(attempt, longopt_list): #LINE# if len(attempt) <= 2 or not longopt_list: #LINE# #TAB# return [] # option_names = [v.split('=')[0] for v in longopt_list] #LINE# distances = [(_DamerauLevenshtein(attempt, option[0:len(attempt)]), option) #LINE# #TAB# #TAB# #TAB# for option in option_names] #LINE# distances.sort(key=lambda t: t[0]) #LINE# least_errors, _ = distances[0] #LINE# if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len(attempt): #LINE# #TAB# return [] #LINE# suggestions = [] #LINE# for errors, name in distances: #LINE# #TAB# if errors == least_errors: #LINE# #TAB# suggestions.append(name) #LINE# # else: #LINE# #TAB# break #LINE# return suggestions"
def check_for_number(claimset_data): #LINE# #TAB# try: #LINE# #TAB# #TAB# for number in claimset_data: #LINE# #TAB# #TAB# #TAB# if number.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False
def is_linear(cigar_tuple): #LINE# #TAB# if cigar_tuple[0] == 0 or cigar_tuple[1] <= 10: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
def notebook_mode(m): #LINE# #TAB# global NATIVE_MODE #LINE# #TAB# NATIVE_MODE = m
def class_name_from_prefix(prefix): #LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# md5.update(prefix) #LINE# #TAB# md5.hexdigest() #LINE# #TAB# class_name = md5.hexdigest() #LINE# #TAB# return class_name
def load_lib(): #LINE# #TAB# lib_path = _find_lib_path() #LINE# #TAB# lib = ctypes.cdll.LoadLibrary(lib_path[0]) #LINE# #TAB# lib.MXGetLastError.restype = ctypes.c_char_p #LINE# #TAB# return lib
"def init_fluiddb(app, sandbox=False): #LINE# #TAB# app = Flask(app) #LINE# #TAB# FluidDBClient.bind(app, sandbox=sandbox) #LINE# #TAB# return app"
"def cartesian_to_polar(x, y, center): #LINE# #TAB# polar = np.zeros((len(center), 2), dtype=np.float) #LINE# #TAB# polar[:, (0)] = center[0] #LINE# #TAB# polar[:, (1)] = x - polar[:, (0)] #LINE# #TAB# polar[:, (2)] = y - polar[:, (1)] #LINE# #TAB# return polar"
"def anti_aliasing(spec): #LINE# #TAB# xs = [] #LINE# #TAB# ys = [] #LINE# #TAB# if len(spec) == 1: #LINE# #TAB# #TAB# for a in spec: #LINE# #TAB# #TAB# #TAB# if 'alias' in a: #LINE# #TAB# #TAB# #TAB# #TAB# xs.append(spec[a]['alias']) #LINE# #TAB# #TAB# #TAB# #TAB# ys.append(spec[a]['alias']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for a in spec: #LINE# #TAB# #TAB# #TAB# #TAB# if 'alias' in a: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# xs.append(spec[a]['alias']) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ys.append(spec[a]['alias']) #LINE# #TAB# df = pd.DataFrame(xs, columns=spec.columns) #LINE# #TAB# return df"
"def node_to_dict(cls, node, json, json_fields): #LINE# #TAB# if json: #LINE# #TAB# #TAB# d = {'id': node._id, 'label': node._label, 'type': node.__class__.__name__} #LINE# #TAB# #TAB# if json_fields: #LINE# #TAB# #TAB# #TAB# d.update(json_fields(node)) #LINE# #TAB# #TAB# return d #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
def decode_to_string(toDecode): #LINE# #TAB# if not toDecode: #LINE# #TAB# #TAB# return toDecode #LINE# #TAB# return toDecode.decode(sys.stdout.encoding) if six.PY3 else toDecode
"def get_expr_params(operator): #LINE# #TAB# params = [] #LINE# #TAB# for param in operator.params: #LINE# #TAB# #TAB# if isinstance(param, LinParam): #LINE# #TAB# #TAB# #TAB# params.append(param.name) #LINE# #TAB# #TAB# elif isinstance(param, ArrayParam): #LINE# #TAB# #TAB# #TAB# params.extend(get_expr_params(param)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# params.append(param) #LINE# #TAB# return params"
"def setup_platform(hass, config, add_entities, discovery_info=None): #LINE# #TAB# mochad_controller = hass.data[DOMAIN] #LINE# #TAB# devs = config.get(CONF_DEVICES) #LINE# #TAB# add_entities([MochadLight(hass, mochad_controller.ctrl, dev) for dev in #LINE# #TAB# #TAB# devs]) #LINE# #TAB# return True"
"def open_fr(file_name, encoding=ENCODING, encode=True): #LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r', newline='', encoding=encoding) #LINE# #TAB# elif encode: #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r', encoding=encoding) #LINE# #TAB# else: #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r') #LINE# #TAB# return file_obj"
"def translate_indexes(triangle_list, lut): #LINE# #TAB# for i, t in enumerate(triangle_list): #LINE# #TAB# #TAB# v = lut.get(t[0], t[1]) #LINE# #TAB# #TAB# if v is not None: #LINE# #TAB# #TAB# #TAB# yield i, v"
def ctor_overridable(cls): #LINE# #TAB# for name in cls.__dict__: #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
def ghid_transform(unpacked_spo): #LINE# #TAB# spo = unpacked_spo #LINE# #TAB# if spo.algo == '0': #LINE# #TAB# #TAB# return _remove_address(spo.address) #LINE# #TAB# else: #LINE# #TAB# #TAB# return spo
"def transform_filter(actor, transformation): #LINE# #TAB# vtkActor = vtk.vtkActor() #LINE# #TAB# vtkActor.SetTransform(transformation) #LINE# #TAB# vtkActor.GetOutput() #LINE# #TAB# return vtkActor"
"def group_clusters(df, cluster_id, date_col): #LINE# #TAB# clusters = df[cluster_id].unique() #LINE# #TAB# grouped_clusters = [clusters[c] for c in clusters] #LINE# #TAB# grouped_clusters_date = grouped_clusters[date_col].copy() #LINE# #TAB# for i, cluster in enumerate(grouped_clusters_date): #LINE# #TAB# #TAB# if len(cluster) > 1: #LINE# #TAB# #TAB# #TAB# cluster_id_list = [c for c in clusters if c!= cluster_id] #LINE# #TAB# #TAB# #TAB# if len(cluster_id_list) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# for date in cluster_id_list: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# grouped_clusters_date[i] = pd.to_datetime(cluster_id_list[i] + #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# date) #LINE# #TAB# return grouped_clusters_date"
"def command_line(): #LINE# #TAB# import sys #LINE# #TAB# global last_exit_value #LINE# #TAB# if last_exit_value: #LINE# #TAB# #TAB# sys.stderr.write('\n') #LINE# #TAB# #TAB# last_exit_value = 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_call(['namedex', '-h'], stdout=sys.stdout, #LINE# #TAB# #TAB# #TAB# stderr=sys.stderr) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# last_exit_value = 1"
"def trace_full (frame, event, arg): #LINE# #TAB# global IS_TRACEABLE #LINE# #TAB# if not IS_TRACEABLE: #LINE# #TAB# #TAB# return #LINE# #TAB# if arg is None: #LINE# #TAB# #TAB# arg = 'None' #LINE# #TAB# L = [] #LINE# #TAB# while frame: #LINE# #TAB# #TAB# L.append(frame.f_back.f_code.co_filename) #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# for line in L: #LINE# #TAB# #TAB# if line.startswith(arg): #LINE# #TAB# #TAB# #TAB# IS_TRACEABLE = True #LINE# #TAB# #TAB# #TAB# break"
def get_long(): #LINE# #TAB# while True: #LINE# #TAB# #TAB# s = get_random_string(10) #LINE# #TAB# #TAB# if not s: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# #TAB# n = int(s) #LINE# #TAB# #TAB# if n < 1: #LINE# #TAB# #TAB# #TAB# return long(n) #LINE# #TAB# return
"def normalize_node(node, headers=None): #LINE# #TAB# if isinstance(node, str): #LINE# #TAB# #TAB# node = str(node) #LINE# #TAB# elif isinstance(node, dict): #LINE# #TAB# #TAB# node = dict(node) #LINE# #TAB# #TAB# normalize_node(node, headers) #LINE# #TAB# else: #LINE# #TAB# #TAB# if not headers: #LINE# #TAB# #TAB# #TAB# return node #LINE# #TAB# #TAB# for k, v in node.items(): #LINE# #TAB# #TAB# #TAB# node[k] = normalize_value(v, headers) #LINE# #TAB# return node"
def remaining_bytes(char): #LINE# #TAB# if char.is_utf8(): #LINE# #TAB# #TAB# return char.encode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# return b''
"def is_grouping_sane(cls, gtype): #LINE# #TAB# if (gtype == cls.SHUFFLE or gtype == cls.ALL or gtype == cls.LOWEST or #LINE# #TAB# #TAB# gtype == cls.NONE): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(gtype, cls.FIELDS): #LINE# #TAB# #TAB# return gtype.gtype == topology_pb2.Grouping.Value('FIELDS' #LINE# #TAB# #TAB# #TAB# ) and gtype.fields is not None #LINE# #TAB# elif isinstance(gtype, cls.CUSTOM): #LINE# #TAB# #TAB# return gtype.gtype == topology_pb2.Grouping.Value('CUSTOM' #LINE# #TAB# #TAB# #TAB# ) and gtype.python_serialized is not None #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"def dp_calc_legs(g, all_tensors, s, inputs, i1_cut_i2_wo_output, i1_union_i2): #LINE# #TAB# outer_indices = dp_calc_legs_cell(g, all_tensors, s, inputs, #LINE# #TAB# #TAB# i1_cut_i2_wo_output, i1_union_i2) #LINE# #TAB# return outer_indices"
"def addrinfo_or_none(contact_point, port): #LINE# #TAB# try: #LINE# #TAB# #TAB# value = socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, #LINE# #TAB# #TAB# #TAB# socket.SOCK_STREAM) #LINE# #TAB# #TAB# return value #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# log.debug('Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port)) #LINE# #TAB# #TAB# return None"
"def verboselogs_class_transform(cls): #LINE# #TAB# if not hasattr(cls, 'loggers'): #LINE# #TAB# #TAB# return cls #LINE# #TAB# for name in cls.loggers: #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# obj = getattr(cls, name) #LINE# #TAB# #TAB# if hasattr(obj,'verbose'): #LINE# #TAB# #TAB# #TAB# obj.verbose = False #LINE# #TAB# #TAB# if hasattr(obj, 'info'): #LINE# #TAB# #TAB# #TAB# obj.info = [x[0] for x in obj.info] #LINE# #TAB# #TAB# if hasattr(obj,'stream'): #LINE# #TAB# #TAB# #TAB# obj.stream = [x[1] for x in obj.stream]"
"def infer_blob_devices(net): #LINE# #TAB# devices = {} #LINE# #TAB# for param, device in net.params.items(): #LINE# #TAB# #TAB# if 'device' in param.op: #LINE# #TAB# #TAB# #TAB# devices[param.device] = device #LINE# #TAB# return devices"
def get_applicant_from_email(email): #LINE# #TAB# try: #LINE# #TAB# #TAB# applicant = Applicant.objects.get(email=email) #LINE# #TAB# except ObjectDoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# return applicant.applicant_id
"def nc_file_summary(cls, nc_filename): #LINE# #TAB# if not os.path.exists(nc_filename): #LINE# #TAB# #TAB# return 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(nc_filename, 'r') as nc_file: #LINE# #TAB# #TAB# #TAB# byte_count = nc_file.readline().replace('\n', '') #LINE# #TAB# #TAB# #TAB# return byte_count #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# logger.warn('Unable to open nc file: %s' % nc_filename) #LINE# #TAB# #TAB# return 0"
"def equally_accessible_windows(is_accessible, size, start=0, stop=None, step=None): #LINE# #TAB# if step is None: #LINE# #TAB# #TAB# step = 1 #LINE# #TAB# if start is None: #LINE# #TAB# #TAB# start = 0 #LINE# #TAB# if stop is None: #LINE# #TAB# #TAB# stop = start #LINE# #TAB# if step is None: #LINE# #TAB# #TAB# step = 1 #LINE# #TAB# w = [] #LINE# #TAB# is_accessible = is_accessible.any(axis=1) #LINE# #TAB# for i in range(start, stop): #LINE# #TAB# #TAB# w.append(equally_accessible_windows(is_accessible, size, i, start, stop, step)) #LINE# #TAB# return w"
"def parse_params(url): #LINE# #TAB# params = {} #LINE# #TAB# result = parse.urlparse(url) #LINE# #TAB# for key, value in six.iteritems(result.query): #LINE# #TAB# #TAB# if value[0] == '': #LINE# #TAB# #TAB# #TAB# result[key] = value[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[key] = urllib.parse.unquote(value[0]) #LINE# #TAB# if 'domain' in result: #LINE# #TAB# #TAB# result['domain'] = result['domain'] #LINE# #TAB# return result"
def add_get_id(d): #LINE# #TAB# if 'id' not in d: #LINE# #TAB# #TAB# d['id'] = uuid.uuid4().hex #LINE# #TAB# return d
"def get_group(name: str) -> _Group: #LINE# #TAB# global _groups #LINE# #TAB# group = _groups.get(name, _Group()) #LINE# #TAB# return group"
def uninstallation_paths(dist): #LINE# #TAB# for path in dist.get_paths('record-without-.pyc'): #LINE# #TAB# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# #TAB# yield path
"def queue_context_entry(exchange, queue_name, routing=None): #LINE# #TAB# return {'exchange': exchange, 'queue': queue_name, 'routing': routing if #LINE# #TAB# #TAB# routing else queue_name}"
"def parse_credential(value): #LINE# #TAB# try: #LINE# #TAB# #TAB# _, data = value.split('=', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# key, path = data.split('=', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not key or not path: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# path, path = path.split('=', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# (path, path) = path.split('=', 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return key, path"
def largest_div(num_n): #LINE# #TAB# result = 1 #LINE# #TAB# while True: #LINE# #TAB# #TAB# r = int(num_n / result) #LINE# #TAB# #TAB# if r > 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# result = r #LINE# #TAB# return result
"def bot_config(player_config_path: Path, team: Team) -> 'PlayerConfig': #LINE# #TAB# player_config = PlayerConfig(team=team) #LINE# #TAB# for part in player_config_path.parts[1:]: #LINE# #TAB# #TAB# if part.startswith('__'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# player_config.add_part(part) #LINE# #TAB# return player_config"
"def scan_directory_for_bot_configs(root_dir) ->Set[BotConfigBundle]: #LINE# #TAB# bot_configs = set() #LINE# #TAB# for root, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# if filename.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# path = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# bot_configs.add(BotConfigBundle(path)) #LINE# #TAB# return bot_configs"
"def register_credentials(cls, credentials): #LINE# #TAB# if credentials.type_indicator in cls._credentials: #LINE# #TAB# #TAB# raise KeyError('Credential object already set for type indicator: {0:s}.' # #LINE# #TAB# #TAB# #TAB#.format(credentials.type_indicator)) #LINE# #TAB# cls._credentials[credentials.type_indicator] = credentials"
"def from_list(cls, l): #LINE# #TAB# obj = cls() #LINE# #TAB# for item in l: #LINE# #TAB# #TAB# obj.append(Point(item[0], item[1])) #LINE# #TAB# return obj"
"def tls_decrypt(alg, c): #LINE# #TAB# plaintext = ffi.new(""TLSPlaintext"") #LINE# #TAB# buf = c.data #LINE# #TAB# while len(buf) > 0: #LINE# #TAB# #TAB# if isinstance(buf[0], bytes): #LINE# #TAB# #TAB# #TAB# buf = buf[0] #LINE# #TAB# #TAB# #TAB# c.data = ffi.decompress(buf, alg.decompress(c.data)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# c.next() #LINE# #TAB# #TAB# #TAB# buf = ffi.decompress(buf, alg.decompress(c.data)) #LINE# #TAB# plaintext.data = ffi.buffer(plaintext, len(buf)) #LINE# #TAB# return plaintext"
"def set_contourf_properties(stroke_width, fcolor, fill_opacity, contour_levels, contourf_idx, unit): #LINE# #TAB# global contourf_properties #LINE# #TAB# contourf_properties['stroke_width'] = stroke_width #LINE# #TAB# if unit == 'pt': #LINE# #TAB# #TAB# contourf_properties['fill_opacity'] = fill_opacity #LINE# #TAB# if contour_levels: #LINE# #TAB# #TAB# contourf_properties['contour_levels'] = contour_levels #LINE# #TAB# contourf_properties['contourf_idx'] = contourf_idx"
"def major_axis(points): #LINE# #TAB# U, S, V = np.linalg.svd(points) #LINE# #TAB# axis = util.dot(S, V) #LINE# #TAB# return axis[0]"
"def new_from_memory(cls, data): #LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj.from_memory(data) #LINE# #TAB# #TAB# except GIError: #LINE# #TAB# #TAB# #TAB# obj.from_bytes(data) #LINE# #TAB# #TAB# return obj"
"def build_wsgi_app(controller=None, transactional=False): #LINE# #TAB# request_hooks = [hooks.JSONErrorHook()] #LINE# #TAB# if transactional: #LINE# #TAB# #TAB# request_hooks.append(hooks.OSVmExpireTransactionHook()) #LINE# #TAB# wsgi_app = pecan.Pecan(controller or versions.AVAILABLE_VERSIONS[ #LINE# #TAB# #TAB# versions.DEFAULT_VERSION](), hooks=request_hooks, force_canonical=False #LINE# #TAB# #TAB# ) #LINE# #TAB# repositories.clear() #LINE# #TAB# return wsgi_app"
def my_penetrance(geno): #LINE# #TAB# f0 = geno[0] #LINE# #TAB# f1 = geno[1] #LINE# #TAB# f2 = geno[2] #LINE# #TAB# penetrance = math.sin(f0 + 0.3 * (f1 - f2)) #LINE# #TAB# penetrance = penetrance / math.sqrt(f1 + 0.3 * (f2 - f1)) #LINE# #TAB# return penetrance
"def gill_king(mat, eps=1e-16): #LINE# #TAB# N = mat.shape[0] #LINE# #TAB# M = np.diag(mat) #LINE# #TAB# K = np.zeros(N) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# ch = np.dot(mat[i, :], mat[i - 1, :]) #LINE# #TAB# #TAB# if np.abs(ch) > eps: #LINE# #TAB# #TAB# #TAB# K[i] = -eps #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# K[i] = 1 #LINE# #TAB# return M, K"
"def select_sources(cat_table, cuts): #LINE# #TAB# try: #LINE# #TAB# #TAB# tmp = cat_table.copy() #LINE# #TAB# #TAB# for cut in cuts: #LINE# #TAB# #TAB# #TAB# if np.any(tmp[cut.key] == cut.key for cut in cuts): #LINE# #TAB# #TAB# #TAB# #TAB# tmp = tmp[cut.key] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# tmp = cat_table.loc[(tmp['key'] == cut.key) & (cat_table.loc[cut.key] == #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# cut.key)].copy() #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise #LINE# #TAB# return tmp"
"def export_csv(request): #LINE# #TAB# response = HttpResponse(content_type='text/csv') #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=weights.csv' #LINE# #TAB# with open(os.path.join(settings.MEDIA_ROOT, 'weights.csv'), 'w') as csv_file: #LINE# #TAB# #TAB# reader = csv.reader(csv_file, delimiter=',', quotechar='""') #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# csv_file.write(row) #LINE# #TAB# return response"
def locate_all_or_nothing_cycle(player): #LINE# #TAB# cycle = itertools.cycle(player) #LINE# #TAB# while not cycle: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cycle.next() #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# return cycle
"def all_catalogs(cls): #LINE# #TAB# for language in cls.all_languages(): #LINE# #TAB# #TAB# for topic in cls.all_topics(): #LINE# #TAB# #TAB# #TAB# if language == topic: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# catalog = TopicCatalog(language=language, topic=topic) #LINE# #TAB# #TAB# #TAB# if catalog.valid: #LINE# #TAB# #TAB# #TAB# #TAB# yield catalog #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield None"
def default_start_stop_activation_time(has_start_stop): #LINE# #TAB# d = defaults.dfl.functions #LINE# #TAB# if (not has_start_stop or d.ENABLE_ALL_FUNCTIONS or d. #LINE# #TAB# #TAB# default_start_stop_activation_time.ENABLE): #LINE# #TAB# #TAB# return d.default_start_stop_activation_time.threshold #LINE# #TAB# return sh.NONE
"def related_categories(context, article=None, limit=None): #LINE# #TAB# if article: #LINE# #TAB# #TAB# categories = article.related_categories() #LINE# #TAB# else: #LINE# #TAB# #TAB# categories = Category.objects.filter(published=True).select_related('category') #LINE# #TAB# return categories[:limit]"
def compute_escape(pos): #LINE# #TAB# n_steps = 0 #LINE# #TAB# escape = 0 #LINE# #TAB# for i in range(len(pos)): #LINE# #TAB# #TAB# if pos[i] > pos[i + 1] and pos[i] < pos[i + 2]: #LINE# #TAB# #TAB# #TAB# escape += 1 #LINE# #TAB# #TAB# elif pos[i] > pos[i + 1] and pos[i] > pos[i + 1]: #LINE# #TAB# #TAB# #TAB# n_steps += 1 #LINE# #TAB# return n_steps
"def glu_new_nurbs_renderer(baseFunction): #LINE# #TAB# global _nurbsRenderer #LINE# #TAB# if not _nurbsRenderer: #LINE# #TAB# #TAB# source = ctypes.create_string_buffer(GLU_MAX_NURBS_RENDERER_BUFFER_SIZE) #LINE# #TAB# #TAB# result = _lib.glu_new_nurbs_renderer(source, ctypes.byref( #LINE# #TAB# #TAB# #TAB# _nurbsRenderer)) #LINE# #TAB# #TAB# if baseFunction!= glu_null: #LINE# #TAB# #TAB# #TAB# baseFunction(result) #LINE# #TAB# #TAB# _nurbsRenderer = result #LINE# #TAB# return _nurbsRenderer"
"def from_map(name, table, inobj): #LINE# #TAB# obj = Rule(name, table.name, inobj.pop('description', None), inobj. #LINE# #TAB# #TAB# pop('owner', None), inobj.pop('privileges', []), inobj.pop( #LINE# #TAB# #TAB# 'ruleset', None), inobj.pop('handler', None), inobj.pop( #LINE# #TAB# #TAB# 'validator', None)) #LINE# #TAB# obj.fix_privileges() #LINE# #TAB# obj.set_oldname(inobj) #LINE# #TAB# return obj"
"def gen_child_nodes(dx_nodes, dx_node): #LINE# #TAB# for node in dx_nodes: #LINE# #TAB# #TAB# if isinstance(node, DiffxNode): #LINE# #TAB# #TAB# #TAB# for child in dx_node.children: #LINE# #TAB# #TAB# #TAB# #TAB# yield child #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield node"
"def collective_overlap(particle, other, a, side, normalize=True): #LINE# #TAB# a = np.asarray(a) #LINE# #TAB# other = np.asarray(other) #LINE# #TAB# overlaps = np.zeros(len(particle), dtype=float) #LINE# #TAB# for i, p in enumerate(particle): #LINE# #TAB# #TAB# if side == 'left': #LINE# #TAB# #TAB# #TAB# overlaps[i] = a.max() - a.min() #LINE# #TAB# #TAB# elif side == 'right': #LINE# #TAB# #TAB# #TAB# overlaps[i] = a.max() - a.min() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('`side` must be \'left\' or \'right\'.') #LINE# #TAB# if normalize: #LINE# #TAB# #TAB# overlap = normalize_overlap(overlaps) #LINE# #TAB# return overlap"
"def update_branch(profile, name, sha): #LINE# #TAB# ref = ""heads/"" + name #LINE# #TAB# data = refs.get_ref(profile, ref) #LINE# #TAB# new_sha = data.get(""sha"") #LINE# #TAB# new_sha = ""heads/"" + sha #LINE# #TAB# data.update({""head"": ref, ""new"": new_sha}) #LINE# #TAB# return new_sha"
"def camel_case_to_pep8(name): #LINE# #TAB# new_name = '' #LINE# #TAB# for pos, char in enumerate(name): #LINE# #TAB# #TAB# if char.isupper(): #LINE# #TAB# #TAB# #TAB# if pos == 0: #LINE# #TAB# #TAB# #TAB# #TAB# new_name += '_' #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if char.islower(): #LINE# #TAB# #TAB# #TAB# new_name += char.lower() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_name += char #LINE# #TAB# return new_name"
"def get_auth_header(token: Optional[str]) ->Dict[str, str]: #LINE# #TAB# if token: #LINE# #TAB# #TAB# return {'Authorization': 'Bearer {}'.format(token)} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
"def make_name_absolute(model_name, name): #LINE# #TAB# absolute_name = os.path.join(model_name, name) #LINE# #TAB# if not absolute_name.startswith('.'): #LINE# #TAB# #TAB# absolute_name = '.' + absolute_name #LINE# #TAB# if not absolute_name.endswith('.'): #LINE# #TAB# #TAB# absolute_name += '.' #LINE# #TAB# return absolute_name"
def deal_caps(x: Collection[str]) ->Collection[str]: #LINE# #TAB# res = [] #LINE# #TAB# for t in x: #LINE# #TAB# #TAB# if t == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if t[0].isupper() and len(t) > 1 and t[1:].islower(): #LINE# #TAB# #TAB# #TAB# res.append(TK_MAJ) #LINE# #TAB# #TAB# res.append(t.lower()) #LINE# #TAB# return res
"def fake_data(num_images): #LINE# #TAB# num_images_to_fake = num_images #LINE# #TAB# if num_images_to_fake == 8: #LINE# #TAB# #TAB# return np.hstack([[random.randint(0, 255) for _ in range(num_images_to_fake) #LINE# #TAB# #TAB# #TAB# ] * num_images_to_fake]) #LINE# #TAB# elif num_images_to_fake == 10: #LINE# #TAB# #TAB# return np.hstack([random.randint(0, 255) for _ in range(num_images_to_fake)]) #LINE# #TAB# else: #LINE# #TAB# #TAB# x = np.arange(num_images_to_fake) #LINE# #TAB# #TAB# y = np.arange(num_images_to_fake) #LINE# #TAB# #TAB# return x, y"
"def read_pickle(cls, file_path: str): #LINE# #TAB# with open(file_path, 'rb') as fh: #LINE# #TAB# #TAB# clf = pickle.load(fh) #LINE# #TAB# _LOGGER.info('Loaded classifier from ""%s"".', file_path) #LINE# #TAB# return clf"
"def _parse_dict(cls, value): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# return {cls._parse_list(item): cls._parse_dict(item) for item in value} #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# #TAB# return {cls._parse_str(key): cls._parse_dict(val) for key, val in value.items()}"
"def resolve_reviewers_as_ids(reviewers, team_instance): #LINE# #TAB# resolved_reviewers = [] #LINE# #TAB# for reviewer in reviewers: #LINE# #TAB# #TAB# identity_ref = resolve_identity_ref(reviewer, team_instance) #LINE# #TAB# #TAB# resolved_reviewers.append(identity_ref) #LINE# #TAB# #TAB# vote = resolve_reviewer_vote(reviewer, team_instance) #LINE# #TAB# #TAB# if vote: #LINE# #TAB# #TAB# #TAB# resolved_reviewers += [identity_ref, vote] #LINE# #TAB# return resolved_reviewers"
"def get_filepath(resource_id): #LINE# #TAB# if not resource_id: #LINE# #TAB# #TAB# return None #LINE# #TAB# resource_dir = get_resource_dir(resource_id) #LINE# #TAB# filepath = os.path.join(resource_dir, '{0}.html'.format(resource_id)) #LINE# #TAB# return filepath"
def p_hook(t): #LINE# #TAB# if len(t[3]) == 0: #LINE# #TAB# #TAB# t[0] = t[1] + [t[3]] #LINE# #TAB# elif len(t) == 1: #LINE# #TAB# #TAB# t[0] = t[3] #LINE# #TAB# else: #LINE# #TAB# #TAB# assert False
def foundation_rotation_reduction_factor_millen(cor_norm_rot): #LINE# #TAB# g_min = cor_norm_rot[0] #LINE# #TAB# g_max = cor_norm_rot[1] #LINE# #TAB# if g_min > 0.0 and g_max < 0.0: #LINE# #TAB# #TAB# k = 1 #LINE# #TAB# elif g_max > 0.0 and g_min < 0.0: #LINE# #TAB# #TAB# k = 2 #LINE# #TAB# elif g_max > 0.0 and g_max < 0.0: #LINE# #TAB# #TAB# k = 3 #LINE# #TAB# return k
def get_key(secret_key): #LINE# #TAB# result = keyring.generate_key(secret_key) #LINE# #TAB# if not result: #LINE# #TAB# #TAB# raise Exception('Could not generate a key') #LINE# #TAB# return result
"def cmd_check_upgrade(args): #LINE# #TAB# pkg_list = args['packages'] #LINE# #TAB# for repo, pkg_name in pkg_list: #LINE# #TAB# #TAB# if os.path.exists(os.path.join(repo, pkg_name)): #LINE# #TAB# #TAB# #TAB# if not shutil.which(pkg_name): #LINE# #TAB# #TAB# #TAB# #TAB# logger.error('Unable to upgrade package %s to %s', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# pkg_name, repo) #LINE# #TAB# #TAB# #TAB# #TAB# sys.exit(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# logger.debug('Package %s is older than any existing %s', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pkg_name, repo) #LINE# #TAB# return 0"
def mkdir_p(dir): #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(dir) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(dir): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise
"def create_customer(name, email, phone): #LINE# #TAB# customer = Customer(name=name, email=email, phone=phone) #LINE# #TAB# customer.save() #LINE# #TAB# return customer"
def count_convnd(node): #LINE# #TAB# padding = 0 #LINE# #TAB# n = len(node) #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# if n % 2 == 0: #LINE# #TAB# #TAB# #TAB# padding += 1 #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# return padding
def delete_desktop_entry(): #LINE# #TAB# desktop_entry = _get_desktop_entry() #LINE# #TAB# desktop_entry.delete() #LINE# #TAB# return desktop_entry
def set_logger(logger): #LINE# #TAB# global _logger #LINE# #TAB# _logger = logger
"def ensure_string(cls, data): #LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(data, six.string_types): #LINE# #TAB# #TAB# return data #LINE# #TAB# if isinstance(data, collections.Mapping): #LINE# #TAB# #TAB# lines = data.values() #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# if not isinstance(line, six.string_types): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return str(line) #LINE# #TAB# #TAB# #TAB# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return line #LINE# #TAB# return data"
"def compatible_archs(flags, up): #LINE# #TAB# if up: #LINE# #TAB# #TAB# return [Architecture(arch) for arch in arch_api.list_architectures() if arch. #LINE# #TAB# #TAB# #TAB# compatible(flags)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [Architecture(arch) for arch in arch_api.list_architectures() if arch. #LINE# #TAB# #TAB# #TAB# compatible(flags)]"
def function_argument_names(argument_spec): #LINE# #TAB# args = argument_spec.args #LINE# #TAB# if not args: #LINE# #TAB# #TAB# return None #LINE# #TAB# return [x[1] for x in args]
"def batch_for_sv(samples): #LINE# #TAB# if not isinstance(samples[0], (list, tuple)): #LINE# #TAB# #TAB# samples = [samples] #LINE# #TAB# samples = utils.to_single_data(samples) #LINE# #TAB# if tz.get_in([""config"", ""algorithm"", ""samtools"", ""variantcaller""], samples): #LINE# #TAB# #TAB# out = [] #LINE# #TAB# #TAB# for data in samples: #LINE# #TAB# #TAB# #TAB# data = tz.get_in([""config"", ""algorithm"", ""variantcaller""], data) #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# out.append((data, dd.get_sample_name(data))) #LINE# #TAB# #TAB# return out #LINE# #TAB# else: #LINE# #TAB# #TAB# return samples"
"def get_config_data(): #LINE# #TAB# return {'zzz_subversionPage': [QCoreApplication.translate('VcsPySvnPlugin', #LINE# #TAB# #TAB# 'Subversion'), os.path.join('VcsPlugins', 'vcsPySvn', #LINE# #TAB# #TAB# 'vcsPySvn', 'icons', 'preferences-subversion.svg'), #LINE# #TAB# #TAB# createConfigurationPage, 'vcsPage', None]}"
def translate_ambiguous(seq): #LINE# #TAB# ambiguous = [c for c in seq if c in '{{'] #LINE# #TAB# #TAB# ] #LINE# #TAB# res = '' #LINE# #TAB# for s in ambiguous: #LINE# #TAB# #TAB# if s in '}}': #LINE# #TAB# #TAB# #TAB# res +='' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res += s #LINE# #TAB# return res
"def discard_nth_base(ctx, iterator, step): #LINE# #TAB# assert step > 0 #LINE# #TAB# first, last = next(iterator), step #LINE# #TAB# for item in iterator: #LINE# #TAB# #TAB# if item!= first: #LINE# #TAB# #TAB# #TAB# yield item"
def api_data_to_frame(data: list) ->pd.DataFrame: #LINE# #TAB# columns = list(data[0].keys()) #LINE# #TAB# data = pd.DataFrame(data) #LINE# #TAB# for i in range(len(columns)): #LINE# #TAB# #TAB# data[columns[i]] = data[i][columns[i]] #LINE# #TAB# return data
"def load_cytoband(filename): #LINE# #TAB# df = pd.read_csv(filename, sep='\t', comment='#', header=None) #LINE# #TAB# if df.empty: #LINE# #TAB# #TAB# df = pd.DataFrame() #LINE# #TAB# else: #LINE# #TAB# #TAB# for line in df.split('\n'): #LINE# #TAB# #TAB# #TAB# if line.strip() == '': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# df[line] = df[line].astype(str) #LINE# #TAB# return df"
"def tags_test(ctx): #LINE# #TAB# import unicodedata #LINE# #TAB# with open('src/robotide/editor/tags.py', 'r') as f: #LINE# #TAB# #TAB# c = unicodedata.normalize('NFC', f.read()) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# c.update(ctx.obj['tags']) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass"
def read_varint(raw_hex): #LINE# #TAB# val = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# i = raw_hex[0] #LINE# #TAB# #TAB# val |= (raw_hex[i] & 127) << 7 #LINE# #TAB# #TAB# if i & 128: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return val
"def extra_object_info(obj): #LINE# #TAB# extra_info = '' #LINE# #TAB# if hasattr(obj, 'extra_info'): #LINE# #TAB# #TAB# extra_info += str(obj.extra_info) #LINE# #TAB# if hasattr(obj, 'package_name'): #LINE# #TAB# #TAB# extra_info += str(obj.package_name) #LINE# #TAB# return extra_info"
"def normalize_medscan_name(name): #LINE# #TAB# name = name.replace('-', '_') #LINE# #TAB# name = name.replace('_', '') #LINE# #TAB# for suffix in ['complex', 'complex-', 'complexes']: #LINE# #TAB# #TAB# if name.endswith(suffix): #LINE# #TAB# #TAB# #TAB# name = name[:-len(suffix)] #LINE# #TAB# return name"
"def check_condition(condition, user): #LINE# #TAB# if isinstance(condition, dict): #LINE# #TAB# #TAB# for keypath, value in condition.items(): #LINE# #TAB# #TAB# #TAB# if keypath not in user: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# if not condition[keypath]: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(condition, list): #LINE# #TAB# #TAB# for value in user: #LINE# #TAB# #TAB# #TAB# if value not in condition[keypath]: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def merge_dict(dict_a, dict_b): #LINE# #TAB# if not isinstance(dict_a, dict) or not isinstance(dict_b, dict): #LINE# #TAB# #TAB# return dict_b #LINE# #TAB# for key in dict_b: #LINE# #TAB# #TAB# if key in dict_a: #LINE# #TAB# #TAB# #TAB# dict_a[key] = merge_dict(dict_a[key], dict_b[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dict_a[key] = dict_b[key] #LINE# #TAB# for key in dict_a: #LINE# #TAB# #TAB# dict_a[key] = dict_a[key] #LINE# #TAB# return dict_a"
"def parse_element_types(str_input): #LINE# #TAB# bit_object_list, rest0 = get_bit_object(str_input) #LINE# #TAB# yt_url, rest1 = get_youtube_url(rest0) #LINE# #TAB# time_codes, rest2 = get_time_codes(rest1) #LINE# #TAB# titles, rest3 = get_titles(rest2) #LINE# #TAB# if rest3: #LINE# #TAB# #TAB# tags = rest3.split('.') #LINE# #TAB# else: #LINE# #TAB# #TAB# tags = [] #LINE# #TAB# return time_codes, titles, yt_url, tags, bit_object_list"
def workid_from_url(url): #LINE# #TAB# r = requests.get(url) #LINE# #TAB# if r.status_code == 200: #LINE# #TAB# #TAB# workid = r.json()['workid'] #LINE# #TAB# #TAB# if workid: #LINE# #TAB# #TAB# #TAB# return workid #LINE# #TAB# return ''
"def get_mi_vector(MI_FS, k, F, s, are_data_binned, n_jobs=1): #LINE# #TAB# MIs = Parallel(n_jobs=n_jobs)(delayed(_get_mi)(f, s, MI_FS, are_data_binned) for f in #LINE# #TAB# #TAB# F) #LINE# #TAB# return MIs"
"def no_mutable_default_args(logical_line): #LINE# #TAB# m = MUTABLE_DEFAULT_ARGS_RE.search(logical_line) #LINE# #TAB# if not m: #LINE# #TAB# #TAB# return #LINE# #TAB# msg = ( #LINE# #TAB# #TAB# 'S360: Use of mutable type in function definition is not allowed in sahara code' #LINE# #TAB# #TAB# ) #LINE# #TAB# yield 0, msg"
def parse_degradations_args(degradations_args): #LINE# #TAB# if not degradations_args: #LINE# #TAB# #TAB# return [] #LINE# #TAB# degradations = [] #LINE# #TAB# for degradations_arg in degradations_args: #LINE# #TAB# #TAB# degradations.append(Degradation(**degradations_arg)) #LINE# #TAB# return degradations
"def parse_references(xml): #LINE# #TAB# references = [] #LINE# #TAB# ref_finder = HTMLReferenceFinder(xml) #LINE# #TAB# for elm, uri_attr in ref_finder: #LINE# #TAB# #TAB# type_ = _discover_uri_type(elm.get(uri_attr)) #LINE# #TAB# #TAB# references.append(Reference(elm, type_, uri_attr)) #LINE# #TAB# return references"
"def golden_ratio(figwidth=5): #LINE# #TAB# goldenfraction = 0.2 #LINE# #TAB# if float(figwidth) / float(goldenfraction) > 0.8: #LINE# #TAB# #TAB# width = float(figwidth) / float(goldenfraction) #LINE# #TAB# return width, goldenfraction"
"def get_ratio(filter_function, iterable): #LINE# #TAB# result = 0 #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if filter_function(item): #LINE# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# return result"
"def compile_patterns_in_dictionary(dictionary): #LINE# #TAB# for key in dictionary: #LINE# #TAB# #TAB# if isinstance(dictionary[key], str): #LINE# #TAB# #TAB# #TAB# dictionary[key] = re.sub(pattern_compiled_regex, compile_pattern, dictionary[key]) #LINE# #TAB# return dictionary"
"def get_disk_image_by_name(pbclient, location, image_name): #LINE# #TAB# all_images = pbclient.list_images() #LINE# #TAB# matching = [i for i in all_images['items'] if i['properties']['name'] == #LINE# #TAB# #TAB# image_name and i['properties']['imageType'] == 'HDD' and i[ #LINE# #TAB# #TAB# 'properties']['location'] == location] #LINE# #TAB# return matching"
"def fromquat_to3rotangles(initquat): #LINE# #TAB# rot = np.zeros(4, dtype=np.float64) #LINE# #TAB# rot[:3, (0)] = initquat[:3] * np.sin(np.pi / 2) #LINE# #TAB# rot[:3, (1)] = initquat[:3, (2)] * np.cos(np.pi / 2) #LINE# #TAB# rot[:3, (0)] = initquat[:3, (1)] * np.sin(np.pi / 2) #LINE# #TAB# rot[:3, (2)] = initquat[:3, (0)] * np.cos(np.pi / 2) #LINE# #TAB# return rot"
"def python_code(ss, arrays=None): #LINE# #TAB# if arrays is None: #LINE# #TAB# #TAB# arrays = [] #LINE# #TAB# py_code = '' #LINE# #TAB# for ss_type in ss: #LINE# #TAB# #TAB# if ss_type == 'function': #LINE# #TAB# #TAB# #TAB# py_code += '\n' #LINE# #TAB# #TAB# elif type(ss_type) == list: #LINE# #TAB# #TAB# #TAB# py_code += '\n' #LINE# #TAB# #TAB# #TAB# for a in arrays: #LINE# #TAB# #TAB# #TAB# #TAB# py_code += '#TAB# #TAB#'+ str(a) + '\n' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('Type {} is not recognized'.format(ss_type)) #LINE# #TAB# return py_code"
"def get_auth_key(context): #LINE# #TAB# user = getattr(context, 'user', None) #LINE# #TAB# if user: #LINE# #TAB# #TAB# return user.get_auth_key() #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def create_auth(cloud_name): #LINE# #TAB# auth = keystoneclient.PasswordAuth(cloud_name, salt=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# password=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# domain=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# access_key=True, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# secret=None) #LINE# #TAB# return auth"
"def are_files_or_str_iter(ctx, param, s): #LINE# #TAB# result = [] #LINE# #TAB# if are_files(s): #LINE# #TAB# #TAB# result = [f for f in s if os.path.isfile(f)] #LINE# #TAB# if len(result) > 1: #LINE# #TAB# #TAB# result = result[0] #LINE# #TAB# return result"
"def percentile_biases(encoders, trainX, percentile=50): #LINE# #TAB# sorted_encoders = _sort_encoders(encoders) #LINE# #TAB# percentile = int(percentile / 100) #LINE# #TAB# biases = [] #LINE# #TAB# for enc in sorted_encoders: #LINE# #TAB# #TAB# X = enc.trainX #LINE# #TAB# #TAB# biases.append(np.percentile(X, percentile, axis=0)) #LINE# #TAB# return biases"
"def p_ex_expression(tok): #LINE# #TAB# if len(tok) == 4: #LINE# #TAB# #TAB# tok[0] = LogicalBinOpRule(tok[2], tok[1], tok[3]) #LINE# #TAB# else: #LINE# #TAB# #TAB# tok[0] = tok[1]"
"def lint_file(filename, reg_score, reg_warning, reg_error): #LINE# #TAB# data = LintData(filename) #LINE# #TAB# score = LintScore(data, reg_score) #LINE# #TAB# if score >= reg_warning: #LINE# #TAB# #TAB# log.warn(reg_warning) #LINE# #TAB# if score >= reg_error: #LINE# #TAB# #TAB# log.warn(reg_error) #LINE# #TAB# if score >= reg_warning: #LINE# #TAB# #TAB# log.warning(reg_warning) #LINE# #TAB# log.error(reg_error) #LINE# #TAB# return score"
"def from_url_with_cache(cls, url, cache_dir='cache', filename=None): #LINE# #TAB# #TAB# if filename is None: #LINE# #TAB# #TAB# #TAB# filename = url.split('?')[0] + '.png' #LINE# #TAB# #TAB# cache_file = os.path.join(cache_dir, filename) #LINE# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# #TAB# with open(cache_file, 'wb') as f: #LINE# #TAB# #TAB# #TAB# #TAB# pickle.dump(url, f) #LINE# #TAB# #TAB# image = cls.from_url(url) #LINE# #TAB# #TAB# if filename is not None: #LINE# #TAB# #TAB# #TAB# image.save(cache_file) #LINE# #TAB# #TAB# return image"
"def normalize_hosts_and_ports(hosts_and_ports): #LINE# #TAB# if isinstance(hosts_and_ports, tuple): #LINE# #TAB# #TAB# host, port = hosts_and_ports #LINE# #TAB# #TAB# hosts = list(map(lambda x: normalize_port(x), hosts_and_ports)) #LINE# #TAB# #TAB# ports = list(map(lambda x: normalize_host(x), ports)) #LINE# #TAB# #TAB# return list(zip(hosts, ports)) #LINE# #TAB# elif isinstance(hosts_and_ports, int): #LINE# #TAB# #TAB# return hosts_and_ports, None #LINE# #TAB# else: #LINE# #TAB# #TAB# return hosts_and_ports"
"def build_stack_vrt(in_file_list, out_file): #LINE# #TAB# assert type(in_file_list) == list #LINE# #TAB# data = [] #LINE# #TAB# for in_file in in_file_list: #LINE# #TAB# #TAB# data.extend(rasterio.read_raster(in_file, 'vrt')) #LINE# #TAB# vrt_file = out_file + '.vrt' #LINE# #TAB# if not file_exists(vrt_file): #LINE# #TAB# #TAB# with open(vrt_file, 'w') as out_file: #LINE# #TAB# #TAB# #TAB# json.dump(data, out_file, indent=2) #LINE# #TAB# return vrt_file"
"def split_scenarios(scentime, scencmd): #LINE# #TAB# scenarios = [] #LINE# #TAB# for line in scencmd.splitlines(): #LINE# #TAB# #TAB# if line.startswith(scentime): #LINE# #TAB# #TAB# #TAB# scenario, _, content = line.partition(' ') #LINE# #TAB# #TAB# #TAB# scenarios.append(scenario) #LINE# #TAB# #TAB# elif len(line) > 1: #LINE# #TAB# #TAB# #TAB# content = line[1:] #LINE# #TAB# #TAB# #TAB# if not content: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# content = '\n'.join(content.split('\n')) #LINE# #TAB# #TAB# scenarios += [content] #LINE# #TAB# return scenarios"
def find_packages(): #LINE# #TAB# packages = [] #LINE# #TAB# for directory in os.listdir(PYTHON_DIR): #LINE# #TAB# #TAB# if directory[-1]!= '/': #LINE# #TAB# #TAB# #TAB# for package in os.listdir(directory): #LINE# #TAB# #TAB# #TAB# #TAB# if package.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# packages.append(package[:-3]) #LINE# #TAB# packages.sort() #LINE# #TAB# return packages
"def average_data(counts, observable): #LINE# #TAB# observable = np.asarray(observable) #LINE# #TAB# N = len(counts) #LINE# #TAB# if N == 0: #LINE# #TAB# #TAB# return float('nan') #LINE# #TAB# mean = counts[0] * observable.sum() / N #LINE# #TAB# for i in range(1, N - 1): #LINE# #TAB# #TAB# mean += counts[i] * observable[i] / (N - 1 - i) #LINE# #TAB# return mean"
"def is_relative_url(url): #LINE# #TAB# if url.startswith(""#""): #LINE# #TAB# #TAB# return False #LINE# #TAB# if url.find(""://"") > 0 or url.startswith(""//""): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
def check_diff_3b(sync): #LINE# #TAB# error = False #LINE# #TAB# for value in sync[1:]: #LINE# #TAB# #TAB# if value > 150: #LINE# #TAB# #TAB# #TAB# error = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return error
"def is_valid_node_ip(ip): #LINE# #TAB# ip = str(ip) #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.inet_pton(socket.AF_INET, ip) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return False #LINE# #TAB# if ip.count('.') == 1: #LINE# #TAB# #TAB# ip = ip[:-1] #LINE# #TAB# try: #LINE# #TAB# #TAB# socket.inet_aton(ip) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def default_value(argument, default): #LINE# #TAB# if argument is not None: #LINE# #TAB# #TAB# return default_value(argument, default) #LINE# #TAB# else: #LINE# #TAB# #TAB# return argument"
"def xfs_info_get_kv(serialized): #LINE# #TAB# if not serialized: #LINE# #TAB# #TAB# return {} #LINE# #TAB# ret = {} #LINE# #TAB# for line in serialized.split(b'\n'): #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# kv = line.strip().split() #LINE# #TAB# #TAB# if len(kv)!= 2: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key, value = kv #LINE# #TAB# #TAB# ret[key.strip()] = value.strip() #LINE# #TAB# return ret"
def labels_to_01(y: np.ndarray) ->np.ndarray: #LINE# #TAB# if y is None: #LINE# #TAB# #TAB# raise ValueError('Labels must not be None!') #LINE# #TAB# return y * 2 - 1
"def data_only_container(name, volumes): #LINE# #TAB# container = container_path(name, volumes) #LINE# #TAB# if not container.exists(): #LINE# #TAB# #TAB# container.mkdir(parents=True, exist_ok=True) #LINE# #TAB# return container"
"def encode_unicode_or_identity(value): #LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# return value"
"def get_tempo(artist, title): #LINE# #TAB# if not isinstance(artist, str): #LINE# #TAB# #TAB# artist = artist.replace(' ', '') #LINE# #TAB# if not isinstance(title, str): #LINE# #TAB# #TAB# title = title.replace(' ', '') #LINE# #TAB# tempo = None #LINE# #TAB# if artist == title: #LINE# #TAB# #TAB# tempo = get_tempo_lyr(artist) #LINE# #TAB# if tempo is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return tempo"
def get_query_type(key): #LINE# #TAB# if key in _UPDATE_QUERY_TYPE: #LINE# #TAB# #TAB# return _UPDATE_QUERY_TYPE[key] #LINE# #TAB# return None
"def templates_for_host(request, templates): #LINE# #TAB# if not isinstance(templates, (list, tuple)): #LINE# #TAB# #TAB# templates = [templates] #LINE# #TAB# device = device_from_request(request) #LINE# #TAB# device_templates = [] #LINE# #TAB# for template in templates: #LINE# #TAB# #TAB# if device: #LINE# #TAB# #TAB# #TAB# device_templates.append('%s/%s' % (device, template)) #LINE# #TAB# #TAB# device_templates.append(template) #LINE# #TAB# return device_templates"
def find_carbon_sources(model): #LINE# #TAB# active = [] #LINE# #TAB# for rxn in model.reactions: #LINE# #TAB# #TAB# if not rxn.reaction_rule: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if is_carbon_source(rxn): #LINE# #TAB# #TAB# #TAB# active.append(rxn) #LINE# #TAB# return active
def get_any_bitbox02_bootloaders() ->List[DeviceInfo]: #LINE# #TAB# devices = get_bitbox02_bootloaders() #LINE# #TAB# devices.extend(get_bitbox02_v2_bootloaders()) #LINE# #TAB# return devices
def get_sep(type=''): #LINE# #TAB# import py2to3.sep #LINE# #TAB# if type in py2to3.sep: #LINE# #TAB# #TAB# return py2to3.sep[type] #LINE# #TAB# return py2to3.sep[type]
def get_featured_groups(count=1): #LINE# #TAB# result = [] #LINE# #TAB# fgroup = get_favourite_group(count=count) #LINE# #TAB# for org in fgroup: #LINE# #TAB# #TAB# result.append(org) #LINE# #TAB# return result
def add_default_exposure_class(layer): #LINE# #TAB# if not layer.has_exposure_class(): #LINE# #TAB# #TAB# return #LINE# #TAB# exposure_classes = layer.exposure_classes.copy() #LINE# #TAB# for key in exposure_classes: #LINE# #TAB# #TAB# if key not in exposure_classes[key]: #LINE# #TAB# #TAB# #TAB# exposure_classes[key] = exposure_classes[key] #LINE# #TAB# layer.exposure_classes = exposure_classes
"def legendre_symbol(a, p): #LINE# #TAB# if a == 1: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# elif a % p == 0: #LINE# #TAB# #TAB# for i in range(1, p): #LINE# #TAB# #TAB# #TAB# if a % i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# return i #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1"
"def connectivity_matrix(cm): #LINE# #TAB# if cm.size == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# if cm.ndim!= 2: #LINE# #TAB# #TAB# raise ValueError(""Connectivity matrix must be 2-dimensional."") #LINE# #TAB# if cm.shape[0]!= cm.shape[1]: #LINE# #TAB# #TAB# raise ValueError(""Connectivity matrix must be square."") #LINE# #TAB# if not np.all(np.logical_or(cm == 1, cm == 0)): #LINE# #TAB# #TAB# raise ValueError(""Connectivity matrix must contain only binary "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""values."") #LINE# #TAB# return True"
"def is_broker_action_done(action, rid=None, unit=None): #LINE# #TAB# action_status = action.get_status() #LINE# #TAB# rid = rid or action_status.get('rid') #LINE# #TAB# unit = unit or action_status.get('unit') #LINE# #TAB# if unit: #LINE# #TAB# #TAB# action_status['unit'] = unit #LINE# #TAB# if rid: #LINE# #TAB# #TAB# action_status['rid'] = rid #LINE# #TAB# if action_status['status'] == 'terminated': #LINE# #TAB# #TAB# return True #LINE# #TAB# elif action_status['status'] =='rejected': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"def retrieve_flags(flag_dict, flag_filter): #LINE# #TAB# return [(f[0], f[1]) for f in list(flag_dict.items()) if isinstance(f[0 #LINE# #TAB# #TAB# ], (str, bytes)) and f[0].startswith(flag_filter)]"
"def create_stat(func, stat, set_, val, param, min_, max_, rand=False): #LINE# #TAB# if rand: #LINE# #TAB# #TAB# stat_id = str(stat) #LINE# #TAB# else: #LINE# #TAB# #TAB# stat_id = str(stat) #LINE# #TAB# stat['stat_id'] = stat_id #LINE# #TAB# stat['values'] = list(zip(set_, val, min_, max_)) #LINE# #TAB# return stat"
def check_pnr_structure(pnr): #LINE# #TAB# pnr = PNR_structure(pnr) #LINE# #TAB# if not pnr.is_valid: #LINE# #TAB# #TAB# raise ValueError('Structure %s is incorrect' % pnr) #LINE# #TAB# return
"def find_local_modules(plugin_dir): #LINE# #TAB# return [fname for fname in os.listdir(plugin_dir) if os.path.isdir(os.path. #LINE# #TAB# #TAB# join(plugin_dir, fname)) and not fname.startswith('_')]"
"def compute_histogram(values, edges, use_orig_distr=False): #LINE# #TAB# hist, bin_edges = np.histogram(values, edges, density=True) #LINE# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# hist = distr_to_numpy(hist) #LINE# #TAB# hist = hist[0] if use_orig_distr else hist #LINE# #TAB# return hist"
def to_underscore(val): #LINE# #TAB# val = to_pascal_case(val) #LINE# #TAB# ret = '' #LINE# #TAB# for item in val: #LINE# #TAB# #TAB# if item[0].isupper(): #LINE# #TAB# #TAB# #TAB# ret += '_' + item[1].lower() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret += item[0].lower() #LINE# #TAB# return ret
"def sim_single_spingroup_old(loc_ind, freq_offset, phantom, seq): #LINE# #TAB# from. import sim #LINE# #TAB# signal = sim.SimSingleSpingroup(loc_ind, freq_offset, phantom, seq) #LINE# #TAB# return signal"
"def hash_parameters(words, minimize_indices=False): #LINE# #TAB# #TAB# words_hashed, indices_hashed = _generate_hash_words(words) #LINE# #TAB# #TAB# if minimize_indices: #LINE# #TAB# #TAB# #TAB# indices_hashed = _minimize_indices(indices_hashed, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# minimize_indices=minimize_indices) #LINE# #TAB# #TAB# return words_hashed, indices_hashed"
"def deserialize_retries(retries: dict) ->Dict[Retry, RetryConfig]: #LINE# #TAB# if not retries: #LINE# #TAB# #TAB# return retries #LINE# #TAB# retries_config = dict() #LINE# #TAB# for retry_name, retry_config in retries.items(): #LINE# #TAB# #TAB# if isinstance(retry_config, RetryConfig): #LINE# #TAB# #TAB# #TAB# retries_config[retry_name] = RetryConfig(name=retry_name, #LINE# #TAB# #TAB# #TAB# #TAB# retry_config=retry_config) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# retries_config[retry_name] = retry_config #LINE# #TAB# return retries_config"
"def identify_hook(path): #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# f.read() #LINE# #TAB# #TAB# version_line = f.readline() #LINE# #TAB# #TAB# if version_line.startswith('# THERAPIST'): #LINE# #TAB# #TAB# #TAB# return version_line.split()[2]"
"def get_win_folder_from_registry(csidl_name): #LINE# #TAB# import _winreg #LINE# #TAB# shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': #LINE# #TAB# #TAB# 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData'}[csidl_name] #LINE# #TAB# key = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders' #LINE# #TAB# #TAB# ) #LINE# #TAB# dir, _type = _winreg.QueryValueEx(key, shell_folder_name) #LINE# #TAB# return dir"
"def is_in_path_env(path): #LINE# #TAB# for root, _, files in os.walk(path): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# if file.startswith('python') or file.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def iterate_from_vcf(infile, sample): #LINE# #TAB# for line in read_vcf(infile, sample=sample): #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith(""#""): #LINE# #TAB# #TAB# #TAB# tmp = line.split(""\t"") #LINE# #TAB# #TAB# #TAB# yield tmp[0], tmp[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# with open(infile, ""r"") as in_handle: #LINE# #TAB# #TAB# #TAB# #TAB# for g in parse_vcf(in_handle, line): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield g"
"def activate_extensions(extension_names, verbose=False): #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print('Activating %s...' % extension_names) #LINE# #TAB# active_extensions = [] #LINE# #TAB# for extension_name in extension_names: #LINE# #TAB# #TAB# [activate_extension(extension_name, verbose=verbose) for extension_name in #LINE# #TAB# #TAB# #TAB# extension_names]"
"def qs_as_graph(qs, limit=100): #LINE# #TAB# if limit: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# qs = queryset.only('id') #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# qs = queryset.values_list('id', flat=True) #LINE# #TAB# #TAB# qs = qs.order_by('-id') #LINE# #TAB# g = netvis_graph(qs) #LINE# #TAB# if limit: #LINE# #TAB# #TAB# g.limit(limit) #LINE# #TAB# return g"
"def width_gumbel(widths, num_samples): #LINE# #TAB# temp = np.zeros(num_samples) #LINE# #TAB# for i in range(num_samples): #LINE# #TAB# #TAB# temp[i] = random.randint(0, len(widths)) #LINE# #TAB# return temp"
"def get_unused_node_id(graph, initial_guess='unknown', _format='{}<%d>'): #LINE# #TAB# node_id = initial_guess #LINE# #TAB# while node_id in graph.nodes_iter(): #LINE# #TAB# #TAB# node_id = _format.format(node_id) #LINE# #TAB# return node_id"
"def ft_n_p(data, axis='F1'): #LINE# #TAB# if data.dim == 1: #LINE# #TAB# #TAB# raise NPKError('Not implemented in 1D', data=data) #LINE# #TAB# todo = data.test_axis(axis) #LINE# #TAB# data.revf(axis=todo).fft(axis=todo) #LINE# #TAB# return data"
"def from_environ(cls, environ): #LINE# #TAB# #TAB# environ_copy = environ.copy() #LINE# #TAB# #TAB# collection = cls(environ_copy['wsgi.url_encoding'], environ_copy[ #LINE# #TAB# #TAB# #TAB# 'wsgi.url_path'], environ_copy['wsgi.path_info']) #LINE# #TAB# #TAB# for key, value in collection.items(): #LINE# #TAB# #TAB# #TAB# setattr(collection, key, value) #LINE# #TAB# #TAB# return collection"
"def grid_formatter(col_types, nan_display='', overrides=None): #LINE# #TAB# json_formatter = json.JSONFormatter() #LINE# #TAB# if overrides is None: #LINE# #TAB# #TAB# overrides = {} #LINE# #TAB# for col_type in col_types: #LINE# #TAB# #TAB# with suppress(TypeError): #LINE# #TAB# #TAB# #TAB# if col_type == 'datetime64[ns]': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# value = get_col_value(col_type, override=overrides) #LINE# #TAB# #TAB# #TAB# json_formatter.col_types[col_type] = value #LINE# #TAB# data = json.dumps(json_formatter.col_types, indent=2) #LINE# #TAB# return data, nan_display"
"def transform_streams_for_comparison(outputs): #LINE# #TAB# for i, output in enumerate(outputs): #LINE# #TAB# #TAB# output['streams'][i] = output['stream'] #LINE# #TAB# return outputs"
"def do_sorted_xmlattr(_environment, d, autospace=True): #LINE# #TAB# attrs = list(d.keys()) #LINE# #TAB# for k in attrs: #LINE# #TAB# #TAB# if not autospace: #LINE# #TAB# #TAB# #TAB# return k #LINE# #TAB# #TAB# value = d[k] #LINE# #TAB# #TAB# if type(value) is list: #LINE# #TAB# #TAB# #TAB# sorted_xmlattr(d, value, autospace=autospace) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sorted_xmlattr(d, value, autospace=autospace) #LINE# #TAB# return d"
"def show_text_dialog(parent, text, caption): #LINE# #TAB# dialog = scrolled_message_dialog.ScrolledMessageDialog(parent) #LINE# #TAB# dialog.setText(text) #LINE# #TAB# dialog.exec_() #LINE# #TAB# return dialog"
def iter_object_acl(root): #LINE# #TAB# for child in iter_children(root): #LINE# #TAB# #TAB# for acl in iter_acl(child): #LINE# #TAB# #TAB# #TAB# yield acl
"def find_station_codes_by_city(city_name, token): #LINE# #TAB# aqi = load_aqi() #LINE# #TAB# logging.info('Looking for stations for city %s...' % city_name) #LINE# #TAB# if city_name: #LINE# #TAB# #TAB# stations = aqi.find_stations_by_city(city_name, token) #LINE# #TAB# #TAB# if not stations: #LINE# #TAB# #TAB# #TAB# logging.warn('No stations found.') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for station in stations: #LINE# #TAB# #TAB# #TAB# #TAB# yield station"
"def make_palette(color1, color2, N, hsv=True): #LINE# #TAB# new_color = np.linspace(0, 1, N) #LINE# #TAB# if hsv: #LINE# #TAB# #TAB# new_color = cv2.hsv_to_rgb(color1, color2, 0.0, 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_color = cv2.color(color1, color2, 0.0) #LINE# #TAB# new_color[0] = color1 #LINE# #TAB# new_color[1] = np.linspace(0, 1, N - 1) #LINE# #TAB# if N > 1: #LINE# #TAB# #TAB# new_color = new_color.transpose((1, 2, 0)) #LINE# #TAB# return new_color"
"def set_repository(repository, should_list=False): #LINE# #TAB# global _REPOSITORY #LINE# #TAB# _REPOSITORY = repository #LINE# #TAB# if should_list: #LINE# #TAB# #TAB# _REPOSITORY = [repository.split('/', 1)[1] for repository in #LINE# #TAB# #TAB# #TAB# _REPOSITORY.split('/')]"
"def convert_string_to_unicode(string): #LINE# #TAB# if six.PY3 and isinstance(string, bytes): #LINE# #TAB# #TAB# return string.decode('utf-8') #LINE# #TAB# return string"
"def recursive_dict_merge(left, right, create_copy=True): #LINE# #TAB# if create_copy: #LINE# #TAB# #TAB# left = copy.deepcopy(left) #LINE# #TAB# for key, value in right.items(): #LINE# #TAB# #TAB# if key in left and isinstance(left[key], dict) and isinstance( #LINE# #TAB# #TAB# #TAB# value, collections.Mapping): #LINE# #TAB# #TAB# #TAB# recursive_dict_merge(left[key], value, create_copy=True) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# left[key] = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# left[key] = value #LINE# #TAB# return left"
def alias_it(s): #LINE# #TAB# if s.has_group_by_clause: #LINE# #TAB# #TAB# return s.alias #LINE# #TAB# else: #LINE# #TAB# #TAB# return s
"def int_to_json(value): #LINE# #TAB# if isinstance(value, int): #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
"def center_mass(data): #LINE# #TAB# cm = np.zeros((data.shape[0], 2), dtype=np.float64) #LINE# #TAB# for i in range(data.shape[0]): #LINE# #TAB# #TAB# cm[(i), :] = (data[(i), :] + data[:(i), :]) / 2.0 #LINE# #TAB# return cm"
"def remove_component(cls, component): #LINE# #TAB# while component is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cls._components.remove(component) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass"
"def parse_extras(header): #LINE# #TAB# kwargs = {} #LINE# #TAB# if 'extras' in header: #LINE# #TAB# #TAB# extras = json.loads(header['extras']) #LINE# #TAB# #TAB# for item in extras: #LINE# #TAB# #TAB# #TAB# scene_key = item.get('scene_key', None) #LINE# #TAB# #TAB# #TAB# if scene_key!='scene': #LINE# #TAB# #TAB# #TAB# #TAB# kwargs[scene_key] = item #LINE# #TAB# return kwargs"
"def get_search_fields(request, doc_types): #LINE# #TAB# fields = OrderedDict() #LINE# #TAB# for doc_type in doc_types: #LINE# #TAB# #TAB# request_field = getattr(request, doc_type, None) #LINE# #TAB# #TAB# if request_field is not None: #LINE# #TAB# #TAB# #TAB# if hasattr(request_field,'search_fields'): #LINE# #TAB# #TAB# #TAB# #TAB# fields[request_field.col_name] = request_field.search_fields.get( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# request_field.col_name) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# fields[request_field.col_name] = None #LINE# #TAB# return fields"
"def d_opt_libsvm(filename): #LINE# #TAB# _, ext = os.path.splitext(filename) #LINE# #TAB# libsvm_dataset = load_libsvm(ext) #LINE# #TAB# datasets = libsvm_dataset['datasets'] #LINE# #TAB# opt_libsvm = [] #LINE# #TAB# if len(datasets) > 1: #LINE# #TAB# #TAB# for idx, dataset in enumerate(datasets): #LINE# #TAB# #TAB# #TAB# opt_libsvm.append((idx, dataset)) #LINE# #TAB# d_opt_libsvm = [] #LINE# #TAB# for idx, dataset in enumerate(datasets): #LINE# #TAB# #TAB# idx = d_opt_libsvm.append((idx, dataset)) #LINE# #TAB# #TAB# if len(d_opt_libsvm) > 0: #LINE# #TAB# #TAB# #TAB# yield d_opt_libsvm, idx, dataset #LINE# #TAB# #TAB# #TAB# d_opt_libsvm = []"
"def version_str_to_tuple(version_string): #LINE# #TAB# if version_string is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# version_parts = version_string.split('-') #LINE# #TAB# major, minor, revision, prerelease = version_parts #LINE# #TAB# return major, minor, revision, prerelease"
def get_logger(name): #LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# handler = logging.StreamHandler() #LINE# #TAB# formatter = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(levelname)s - %(name)s - %(message)s') #LINE# #TAB# handler.setFormatter(formatter) #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# logger.setLevel(logging.INFO) #LINE# #TAB# return logger
def folder_exists_or_create(f): #LINE# #TAB# if not os.path.exists(f): #LINE# #TAB# #TAB# os.makedirs(f) #LINE# #TAB# return f
def seldon_message_to_json(message_proto: prediction_pb2.SeldonMessage) ->Dict: #LINE# #TAB# message_json = json_format.MessageToJson(message_proto) #LINE# #TAB# message_dict = json.loads(message_json) #LINE# #TAB# return message_dict
"def traverse_aggregation(agg): #LINE# #TAB# if isinstance(agg, Aggregation): #LINE# #TAB# #TAB# yield agg.left, agg.right #LINE# #TAB# else: #LINE# #TAB# #TAB# for elem in agg.items: #LINE# #TAB# #TAB# #TAB# if isinstance(elem, tuple): #LINE# #TAB# #TAB# #TAB# #TAB# for sub_elem in traverse_aggregation(elem): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield elem, sub_elem #LINE# #TAB# #TAB# #TAB# yield elem"
"def run_vw_command(cmd): #LINE# #TAB# proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, #LINE# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# stdout, stderr = proc.communicate() #LINE# #TAB# if proc.returncode!= 0: #LINE# #TAB# #TAB# raise subprocess.CalledProcessError(stdout, stderr) #LINE# #TAB# return stdout, stderr"
"def resolve_outcomes(state, updates, dist): #LINE# #TAB# outcomes = state.outcomes + updates.outcomes #LINE# #TAB# random.shuffle(outcomes) #LINE# #TAB# new_state = copy.deepcopy(state) #LINE# #TAB# for u, v in zip(outcomes, dist): #LINE# #TAB# #TAB# if u in state.outcomes: #LINE# #TAB# #TAB# #TAB# state.outcomes[u] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# state.outcomes[u] = v #LINE# #TAB# return new_state"
"def add_url_to_context(jinja_context: dict, new_filename: str) ->dict: #LINE# #TAB# md5 = jinja_context.get('md5', None) #LINE# #TAB# if md5 is None: #LINE# #TAB# #TAB# jinja_context['md5'] = os.path.join(os.getcwd(), new_filename) #LINE# #TAB# context = jinja_context.copy() #LINE# #TAB# context['md5'] = md5 #LINE# #TAB# return context"
def log_level(lookfor): #LINE# #TAB# if not lookfor: #LINE# #TAB# #TAB# return logging.INFO #LINE# #TAB# elif lookfor.startswith('0') or lookfor.startswith('4'): #LINE# #TAB# #TAB# return logging.DEBUG #LINE# #TAB# elif lookfor.startswith('INFO'): #LINE# #TAB# #TAB# return logging.INFO #LINE# #TAB# elif lookfor.startswith('WARNING'): #LINE# #TAB# #TAB# return logging.WARNING #LINE# #TAB# elif lookfor.startswith('ERROR'): #LINE# #TAB# #TAB# return logging.ERROR #LINE# #TAB# else: #LINE# #TAB# #TAB# return logging.CRITICAL
"def int_hex(r, g, b): #LINE# #TAB# r = int(r, 16) #LINE# #TAB# g = int(g, 16) #LINE# #TAB# b = int(b, 16) #LINE# #TAB# if r > 255: #LINE# #TAB# #TAB# return '#' + r + '0' #LINE# #TAB# if g > 255: #LINE# #TAB# #TAB# return r + '0' #LINE# #TAB# if b > 255: #LINE# #TAB# #TAB# return b + '0' #LINE# #TAB# return r + '0'"
"def bac_metric(solution, prediction): #LINE# #TAB# return (sum([(solution[i] == p) for i, p in enumerate(prediction)]) / #LINE# #TAB# #TAB# sum([(solution[i] == p) for i in range(len(solution))])) ** 0.5"
"def memory_limit_in_gigabytes(): #LINE# #TAB# limit = 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# with open('/proc/meminfo', 'r') as meminfo: #LINE# #TAB# #TAB# #TAB# for line in meminfo: #LINE# #TAB# #TAB# #TAB# #TAB# if line.startswith('limit'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# limit = int(line.split()[2]) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return limit"
"def auth_with_refresh_token(session, refresh_token): #LINE# #TAB# token_request_data = { #LINE# #TAB# #TAB# 'client_id': OAUTH2_CLIENT_ID, #LINE# #TAB# #TAB# 'client_secret': OAUTH2_CLIENT_SECRET, #LINE# #TAB# #TAB# 'grant_type':'refresh_token', #LINE# #TAB# #TAB#'refresh_token': refresh_token, #LINE# #TAB# } #LINE# #TAB# res = _make_token_request(session, token_request_data) #LINE# #TAB# return res['access_token']"
"def is_ascii(s): #LINE# #TAB# try: #LINE# #TAB# #TAB# s.encode('ascii','strict') #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"def parse_object(response, infotype): #LINE# #TAB# if infotype in ('idletime','refcount'): #LINE# #TAB# #TAB# return int_or_none(response) #LINE# #TAB# return response"
"def parse_program(dictionary): #LINE# #TAB# identifier = parse_identifier(dictionary['identifier']) #LINE# #TAB# creator_id = dictionary['creator_id'] #LINE# #TAB# name = dictionary['name'] #LINE# #TAB# version = dictionary['version'] #LINE# #TAB# is_release_version = dictionary['is_release_version'] #LINE# #TAB# description = dictionary['description'] #LINE# #TAB# required_external_domains = dictionary['required_external_domains'] #LINE# #TAB# docker_image_name = dictionary['docker_image_name'] #LINE# #TAB# rating_numerator = dictionary['rating_numerator'] #LINE# #TAB# rating_count = dictionary['rating_count'] #LINE# #TAB# res = Program(identifier, creator_id, name, version, is_release_version, #LINE# #TAB# #TAB# description, required_external_domains, docker_image_name, #LINE# #TAB# #TAB# rating_numerator, rating_count) #LINE# #TAB# return res"
"def read_in_test_prompt(filename): #LINE# #TAB# if filename is not None: #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# text = f.read() #LINE# #TAB# #TAB# return text #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
"def get_path_style(path, fill=True): #LINE# #TAB# style = {} #LINE# #TAB# style['alpha'] = path.get_alpha() #LINE# #TAB# if style['alpha'] is None: #LINE# #TAB# #TAB# style['alpha'] = 1 #LINE# #TAB# style['edgecolor'] = export_color(path.get_edgecolor()) #LINE# #TAB# if fill: #LINE# #TAB# #TAB# style['facecolor'] = export_color(path.get_facecolor()) #LINE# #TAB# else: #LINE# #TAB# #TAB# style['facecolor'] = 'none' #LINE# #TAB# style['edgewidth'] = path.get_linewidth() #LINE# #TAB# style['dasharray'] = get_dasharray(path) #LINE# #TAB# style['zorder'] = path.get_zorder() #LINE# #TAB# return style"
"def get_settings_folder(): #LINE# #TAB# if IS_NOTEBOOK: #LINE# #TAB# #TAB# folder = os.path.join(os.path.expanduser('~'), '.settings') #LINE# #TAB# elif LINUX: #LINE# #TAB# #TAB# folder = os.getcwd() #LINE# #TAB# else: #LINE# #TAB# #TAB# folder = os.curdir #LINE# #TAB# return folder"
"def plot_path(path): #LINE# #TAB# fig = plt.figure(figsize=(15, 8)) #LINE# #TAB# ax = plt.subplot() #LINE# #TAB# ax.savefig(path) #LINE# #TAB# plt.close(fig) #LINE# #TAB# return fig"
"def mh_digest (data): #LINE# #TAB# mh = libmh.MinHash() #LINE# #TAB# if not isinstance(data, bytes): #LINE# #TAB# #TAB# data = bytes(data, 'utf-8') #LINE# #TAB# for d in data: #LINE# #TAB# #TAB# mh.update(d) #LINE# #TAB# return mh.digest()[::-1]"
"def verify_cookie_value(name, value): #LINE# #TAB# try: #LINE# #TAB# #TAB# conn = http.client.HTTPConnection() #LINE# #TAB# #TAB# conn.set_cookie(name, value) #LINE# #TAB# #TAB# res = conn.send_request() #LINE# #TAB# #TAB# conn.close() #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(""Failed verification of cookie '{}'"".format(name)) #LINE# #TAB# #TAB# raise e"
"def copy_directory(src, dest): #LINE# #TAB# if not os.path.exists(dest): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shutil.copytree(src, dest) #LINE# #TAB# #TAB# except shutil.Error as e: #LINE# #TAB# #TAB# #TAB# if not os.path.exists(dest): #LINE# #TAB# #TAB# #TAB# #TAB# raise e"
"def map_to_pixel(mX, mY, geoTransform): #LINE# #TAB# mX = np.asarray(mX) #LINE# #TAB# mY = np.asarray(mY) #LINE# #TAB# pX = ((mX - geoTransform[0]) / geoTransform[3]) - 0.5 #LINE# #TAB# pY = ((mY - geoTransform[1]) / geoTransform[5]) - 0.5 #LINE# #TAB# return pX, pY"
def negate_builtin_atom(atom): #LINE# #TAB# if atom.symbol == 'X': #LINE# #TAB# #TAB# return atom #LINE# #TAB# if atom.symbol == 'Y': #LINE# #TAB# #TAB# return atom.offset #LINE# #TAB# if atom.symbol == 'Z': #LINE# #TAB# #TAB# return atom.offset #LINE# #TAB# return atom
def reset_clipboard(request): #LINE# #TAB# response = request.response #LINE# #TAB# response.clipboard.clear() #LINE# #TAB# return response
def set_mode(passwd): #LINE# #TAB# if passwd is None: #LINE# #TAB# #TAB# return #LINE# #TAB# process = psutil.Process(os.getpid()) #LINE# #TAB# process.mode = passwd #LINE# #TAB# psutil.Process(os.getpid()) #LINE# #TAB# return process
def bluetooth_validate(address): #LINE# #TAB# try: #LINE# #TAB# #TAB# unhexlify(address) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def log_lower_gamma(a, x): #LINE# #TAB# if x > 0.0: #LINE# #TAB# #TAB# return log(a) + np.log(1 - x) #LINE# #TAB# else: #LINE# #TAB# #TAB# return np.inf"
def install_activate_reload(ctx): #LINE# #TAB# ctx.ctrl.sendline('n') #LINE# #TAB# ctx.ctrl.recvline('') #LINE# #TAB# ctx.device.reload() #LINE# #TAB# return
"def unserialize_bookmark(bookmark): #LINE# #TAB# identifier = base64.b64decode(bookmark) #LINE# #TAB# if not identifier.startswith(b'/'): #LINE# #TAB# #TAB# identifier = b'/' + identifier #LINE# #TAB# return identifier[1:], identifier[2:]"
"def abicomp_data(options): #LINE# #TAB# paths = options.paths #LINE# #TAB# output = subprocess.check_output(['abicomp', '-v'], cwd=os.getcwd()) #LINE# #TAB# if not path.exists(output): #LINE# #TAB# #TAB# os.makedirs(output) #LINE# #TAB# with open(output, 'w') as f: #LINE# #TAB# #TAB# t = StringIO() #LINE# #TAB# #TAB# t.write(json.dumps(t, indent=2)) #LINE# #TAB# with open(output, 'w') as f: #LINE# #TAB# #TAB# t.write(json.dumps(t, indent=2)) #LINE# #TAB# return output"
"def deregister_helper(cls, resolver_helper): #LINE# #TAB# type_indicator = resolver_helper.type_indicator #LINE# #TAB# if type_indicator not in cls._resolver_helpers: #LINE# #TAB# #TAB# raise KeyError('Resolver helper object not set for type indicator: {0:s}.' #LINE# #TAB# #TAB# #TAB#.format(type_indicator)) #LINE# #TAB# del cls._resolver_helpers[type_indicator]"
"def sub_fq(R1, R2, percent): #LINE# #TAB# subset = [] #LINE# #TAB# for i in range(len(R1)): #LINE# #TAB# #TAB# if round(len(R1[i]) * percent / 100, 2) == 0: #LINE# #TAB# #TAB# #TAB# subset.append(R1[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# subset.append(R2[i]) #LINE# #TAB# return subset"
"def tenant_provisioned(tenant_id): #LINE# #TAB# session = db.get_reader_session() #LINE# #TAB# with session.begin(): #LINE# #TAB# #TAB# res = any(session.query(m).filter(m.tenant_id == tenant_id).count() for #LINE# #TAB# #TAB# #TAB# m in [models_v2.Network, models_v2.Port]) #LINE# #TAB# return res"
def expensive_task_gen(num=8700): #LINE# #TAB# tasks = [] #LINE# #TAB# start = time.time() #LINE# #TAB# while True: #LINE# #TAB# #TAB# if time.time() - start < num: #LINE# #TAB# #TAB# #TAB# for n in range(num): #LINE# #TAB# #TAB# #TAB# #TAB# yield {} #LINE# #TAB# #TAB# #TAB# tasks.append(task()) #LINE# #TAB# #TAB# #TAB# if time.time() - start >= num: #LINE# #TAB# #TAB# #TAB# #TAB# break
def score_c_to_5_classes(score_c): #LINE# #TAB# if score_c == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# score_c_5 = sum(1 for _ in range(5)) #LINE# #TAB# return score_c_5 / 5
"def template_renderer(f): #LINE# #TAB# old_formatters = jinja2.options.template_renderer_old_formatters #LINE# #TAB# jinja2.options.template_renderer_old_formatters = {v.name: v for v in f} #LINE# #TAB# if hasattr(f,'render'): #LINE# #TAB# #TAB# jinja2.options.template_renderer_old_formatters[f.name] = f #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# jinja2.options.template_renderer_old_formatters[f.name] = old_formatters[f. #LINE# #TAB# #TAB# #TAB# #TAB# name]"
"def no_overlap(y, z, y1, z1, y2, z2, pt_id, pt_1_id, pt_2_id): #LINE# #TAB# for y in range(len(y)): #LINE# #TAB# #TAB# if y1 <= y[pt_id] < z1: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif y2 <= y[pt_id] < y1: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif z1 <= z2[pt_id] < z1: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"def get_window(title, exact=False): #LINE# #TAB# for window in get_windows(): #LINE# #TAB# #TAB# if title in window.title: #LINE# #TAB# #TAB# #TAB# if not exact: #LINE# #TAB# #TAB# #TAB# #TAB# return window #LINE# #TAB# #TAB# return None #LINE# #TAB# return window"
"def get_resource(remote): #LINE# #TAB# resp = remote.get(path='users.json') #LINE# #TAB# resp.raise_for_status() #LINE# #TAB# data = resp.json() #LINE# #TAB# user_info = {} #LINE# #TAB# groups = {} #LINE# #TAB# for item in data.get('users', []): #LINE# #TAB# #TAB# user_info[item.get('name')] = item.get('email') #LINE# #TAB# #TAB# groups[item.get('group', [])] = item.get('email') #LINE# #TAB# return user_info, groups"
"def fix_logging_path(config, main_section): #LINE# #TAB# file_path = config.get(main_section, 'log.file') #LINE# #TAB# path = os.path.expandvars(os.path.expanduser(file_path)) #LINE# #TAB# if not path.startswith(os.path.expanduser('~')): #LINE# #TAB# #TAB# path = os.path.join(os.path.dirname(path), file_path) #LINE# #TAB# config[main_section] = path"
def get_root(node): #LINE# #TAB# while node.left is not None: #LINE# #TAB# #TAB# node = node.left #LINE# #TAB# return node
"def state_fidelity(state0: State, state1: State) -> bk.BKTensor: #LINE# #TAB# new_state0 = bk.BKTensor(0) #LINE# #TAB# new_state1 = bk.BKTensor(0) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# new_state0.update(state0[i]) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# new_state1.update(state1[i]) #LINE# #TAB# return new_state0.fidelity"
"def closest_point(p, a, b): #LINE# #TAB# ap = [(a[i] - b[i]) for i in range(3)] #LINE# #TAB# ab = [(b[i] - a[i]) for i in range(3)] #LINE# #TAB# bc = [(a[i] - b[i]) for i in range(3)] #LINE# #TAB# dist = np.sum(ap + ab) #LINE# #TAB# return dist < 0"
"def get_request_body(request_body): #LINE# #TAB# result = request_body #LINE# #TAB# if request_body is None: #LINE# #TAB# #TAB# result = '' #LINE# #TAB# elif isinstance(request_body, dict): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = request_body.toxml() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if result is not None: #LINE# #TAB# #TAB# return result #LINE# #TAB# return request_body"
"def single_encode(input, errors='strict'): #LINE# #TAB# if not isinstance(input, text_type): #LINE# #TAB# #TAB# input = text_type(input, sys.getdefaultencoding(), errors) #LINE# #TAB# length = len(input) #LINE# #TAB# output = unicodedata.normalize('NFC', input) #LINE# #TAB# for i in range(length): #LINE# #TAB# #TAB# output = output.replace(single_replace_char[i], output[i]) #LINE# #TAB# return output"
def has_tests(module: str): #LINE# #TAB# try: #LINE# #TAB# #TAB# mod = importlib.import_module(module) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
"def from_rgb(r, g=None, b=None): #LINE# #TAB# c = r if isinstance(r, list) else [r, g, b] #LINE# #TAB# best = {} #LINE# #TAB# for index, item in enumerate(colors): #LINE# #TAB# #TAB# d = __distance(item, c) #LINE# #TAB# #TAB# if not best or d <= best['distance']: #LINE# #TAB# #TAB# #TAB# best = {'distance': d, 'index': index} #LINE# #TAB# if 'index' in best: #LINE# #TAB# #TAB# return best['index'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1"
"def get_ip(): #LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# try: #LINE# #TAB# #TAB# s.connect(('8.8.8.8', 80)) #LINE# #TAB# #TAB# ip = s.getsockname()[0] #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# ip = '127.0.0.1' #LINE# #TAB# finally: #LINE# #TAB# #TAB# s.close() #LINE# #TAB# return ip"
"def autocorr_func1(mags, lag, maglen, magmed, magstd): #LINE# #TAB# lagindex = nparange(maglen-lag) #LINE# #TAB# products = (mags[lagindex] - magmed) * (mags[lagindex+lag] - magmed) #LINE# #TAB# autocovarfunc = npsum(products)/lagindex.size #LINE# #TAB# varfunc = npsum( #LINE# #TAB# #TAB# (mags[lagindex] - magmed)*(mags[lagindex] - magmed) #LINE# #TAB# )/mags.size #LINE# #TAB# acorr = autocovarfunc/varfunc #LINE# #TAB# return acorr"
"def find_mask(display, symbol): #LINE# #TAB# mask = 0 #LINE# #TAB# x = ord(symbol) #LINE# #TAB# while x > 0: #LINE# #TAB# #TAB# if x & 128 == 0: #LINE# #TAB# #TAB# #TAB# x = ord(symbol) #LINE# #TAB# #TAB# mask |= 1 #LINE# #TAB# #TAB# if x < 32: #LINE# #TAB# #TAB# #TAB# if symbol == 'X': #LINE# #TAB# #TAB# #TAB# #TAB# mask |= 2 #LINE# #TAB# #TAB# #TAB# if symbol == 'Z': #LINE# #TAB# #TAB# #TAB# #TAB# mask |= 3 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# mask |= 4 #LINE# #TAB# return mask"
"def iq_to_envelope(iq_array: np.ndarray) ->np.ndarray: #LINE# #TAB# env = np.abs(iq_array) #LINE# #TAB# if env.ndim == 2: #LINE# #TAB# #TAB# env = hilbert_transform(env) #LINE# #TAB# else: #LINE# #TAB# #TAB# env = np.outer(hilbert_transform(env), iq_array) #LINE# #TAB# return env"
def handles_request_head(request_func): #LINE# #TAB# request_func._handles_request_head = True #LINE# #TAB# return request_func
"def proc_depends_setter(this, value): #LINE# #TAB# dependencies = this.__dict__.get('dependencies', None) #LINE# #TAB# if dependencies: #LINE# #TAB# #TAB# for dep in dependencies: #LINE# #TAB# #TAB# #TAB# proc_depends = ProcessDependssetter(dep, value) #LINE# #TAB# #TAB# #TAB# if proc_depends is not None: #LINE# #TAB# #TAB# #TAB# #TAB# dependencies.remove(proc_depends) #LINE# #TAB# #TAB# #TAB# #TAB# for param in proc_depends: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# param.setter(this, value) #LINE# #TAB# return value"
"def build_reference_fasta_map(reference_fasta_map_param): #LINE# #TAB# if reference_fasta_map_param is None: #LINE# #TAB# #TAB# reference_fasta_map_param = {} #LINE# #TAB# else: #LINE# #TAB# #TAB# reference_fasta_map_param = reference_fasta_map_param #LINE# #TAB# if isinstance(reference_fasta_map_param, dict): #LINE# #TAB# #TAB# return reference_fasta_map_param #LINE# #TAB# else: #LINE# #TAB# #TAB# return {assembly_key: reference_fasta_map_param[assembly_key] for #LINE# #TAB# #TAB# #TAB# assembly_key in reference_fasta_map_param}"
def glibc_version_string_ctypes(): #LINE# #TAB# try: #LINE# #TAB# #TAB# gnu_get_libc_version_string = ctypes.c_char_p #LINE# #TAB# #TAB# gnu_get_libc_version_string.restype = ctypes.c_char_p #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# if gnu_get_libc_version_string()!= 0: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# return gnu_get_libc_version_string()[0]
"def get_script_module(script_information, package='pylabcontrol', verbose=False): #LINE# #TAB# try: #LINE# #TAB# #TAB# spec = importlib.util.spec_from_file_location(script_information['name'], #LINE# #TAB# #TAB# #TAB# package) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# module = importlib.util.module_from_spec(spec) #LINE# #TAB# spec.loader.exec_module(module) #LINE# #TAB# return module"
"def flow_perms(user, task): #LINE# #TAB# perm = {} #LINE# #TAB# task_perms = get_task_perms(task, user) #LINE# #TAB# for perm in task_perms: #LINE# #TAB# #TAB# if perm.has_permission('user', user): #LINE# #TAB# #TAB# #TAB# perm['user'] = user #LINE# #TAB# #TAB# elif perm.has_permission('task', task): #LINE# #TAB# #TAB# #TAB# perm['task'] = task"
def create_complete_graph(node_ids): #LINE# #TAB# graph = nx.DiGraph() #LINE# #TAB# graph.add_nodes_from(node_ids) #LINE# #TAB# for nodes in node_ids: #LINE# #TAB# #TAB# for node in nodes: #LINE# #TAB# #TAB# #TAB# graph.add_node(node) #LINE# #TAB# return graph
def check_2d(seq: Sequence) ->Sequence: #LINE# #TAB# if len(seq) < 2: #LINE# #TAB# #TAB# raise ValueError('Input sequence must be two-dimensional.') #LINE# #TAB# elif len(seq) == 1: #LINE# #TAB# #TAB# return seq[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError
"def overridden_method(klass, name): #LINE# #TAB# method = getattr(klass, name) #LINE# #TAB# if method is not None and hasattr(method.im_func, '__func__'): #LINE# #TAB# #TAB# method = method.im_func.__func__ #LINE# #TAB# return method"
"def math_funcdef_handle(tokens): #LINE# #TAB# internal_assert(len(tokens) == 3, ""invalid math funcdef tokens"", tokens) #LINE# #TAB# math_obj = math_funcdef_parser.MathFunctionDef(tokens[1]) #LINE# #TAB# return math_obj.body, tokens[2], tokens[3]"
"def iterable_method(method, params): #LINE# #TAB# if isinstance(params, collections.Mapping): #LINE# #TAB# #TAB# if method in params: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return method in params #LINE# #TAB# else: #LINE# #TAB# #TAB# if isinstance(params, (list, tuple, set)): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False"
"def get_rawvalue_value(data, encoding=None): #LINE# #TAB# if not data: #LINE# #TAB# #TAB# return #LINE# #TAB# if not encoding: #LINE# #TAB# #TAB# encoding = get_encoding() #LINE# #TAB# value = data.getvalue() #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return #LINE# #TAB# return value.decode(encoding) if encoding else value"
"def generate_random_sdr(numSDR, numDims, numActiveInputBits, seed=42): #LINE# sdr = [] #LINE# inputBits = int(numActiveInputBits) #LINE# for _ in range(numDims): #LINE# #TAB# sdr.append(generateRandomSDR(numSDR, numDims, inputBits, seed)) #LINE# for _ in range(numDims): #LINE# #TAB# sdr.append(generateRandomSDR(numSDR, numDims, inputBits, seed)) #LINE# return sdr"
def get_store(cls): #LINE# #TAB# if cls._store is None: #LINE# #TAB# #TAB# cls._store = redis.RedisStore() #LINE# #TAB# return cls._store
def load_csv(location): #LINE# #TAB# with open(location) as csvfile: #LINE# #TAB# #TAB# reader = csv.DictReader(csvfile) #LINE# #TAB# #TAB# rows = [row for row in reader] #LINE# #TAB# #TAB# return rows
"def get_device_int_property(dev_ref, key): #LINE# #TAB# val = _hidapi_get_device_int_property(dev_ref, key) #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(val) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"def get_commuters(unitaries): #LINE# #TAB# commuters = [] #LINE# #TAB# for i in range(len(unitaries) - 1): #LINE# #TAB# #TAB# if unitaries[i + 1][0] == 'CZ': #LINE# #TAB# #TAB# #TAB# commuters.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for j in range(i + 1, len(unitaries) - 1): #LINE# #TAB# #TAB# #TAB# #TAB# if unitaries[j][0] == 'CZ': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# commuters.append(i) #LINE# #TAB# return commuters"
"def describe_chute(ctx, name): #LINE# #TAB# client = ControllerClient() #LINE# #TAB# result = client.get_chute(name) #LINE# #TAB# if result: #LINE# #TAB# #TAB# click.echo(util.format_result(result)) #LINE# #TAB# return result"
"def is_chief(task: backend.Task, run_name: str): #LINE# #TAB# global run_task_dict #LINE# #TAB# if run_name not in run_task_dict: #LINE# #TAB# #TAB# return True #LINE# #TAB# task_list = run_task_dict[run_name] #LINE# #TAB# assert task in task_list, f""Task {task.name} doesn't belong to run {run_name}"" #LINE# #TAB# return task_list[0] == task"
"def build_scaling_factors(S12, model_one, model_two): #LINE# #TAB# model_one_scorer = model_one.scorer(model_one) #LINE# #TAB# model_two_scorer = model_two.scorer(model_two) #LINE# #TAB# scaling_factors = S12.build_scaling_factors(model_one_scorer, #LINE# #TAB# #TAB# model_two_scorer) #LINE# #TAB# return scaling_factors"
"def is_different(old_value, new_value): #LINE# #TAB# if old_value is None: #LINE# #TAB# #TAB# return new_value is None #LINE# #TAB# elif old_value == new_value: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif old_value == '': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return old_value!= new_value"
"def wildcards_overlap(name1, name2): #LINE# #TAB# if name1 is None or name2 is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# pattern1 = re.compile('\\?*') #LINE# #TAB# pattern2 = re.compile('\\?*') #LINE# #TAB# match = pattern1.match(pattern2) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return overlap(match.group(0), match.group(1)) #LINE# #TAB# return False"
"def validate_config(config: Dict) ->Dict: #LINE# #TAB# valid = True #LINE# #TAB# if CONFIG_FILE not in config: #LINE# #TAB# #TAB# log.debug('Config file not found: %s', CONFIG_FILE) #LINE# #TAB# #TAB# valid = False #LINE# #TAB# if CONFIG_LINE_INFO not in config: #LINE# #TAB# #TAB# log.debug('Config file missing: %s', CONFIG_LINE_INFO) #LINE# #TAB# _parse_command_line_arguments(config) #LINE# #TAB# if valid: #LINE# #TAB# #TAB# config['file'] = config['file'] #LINE# #TAB# return config"
"def get_boundary_times(database, table_name, column): #LINE# #TAB# assert isinstance(table_name, str) #LINE# #TAB# assert isinstance(column, Column) #LINE# #TAB# table = database.get_table(table_name) #LINE# #TAB# times = table.get_boundary_times(column) #LINE# #TAB# if not times: #LINE# #TAB# #TAB# return {} #LINE# #TAB# return {column.name: times[column.name] for column in columns}"
"def annotate_with_xmlns(tree, prefix, URI): #LINE# #TAB# try: #LINE# #TAB# #TAB# for node in tree.iter(): #LINE# #TAB# #TAB# #TAB# node.tag = '{%s}%s' % (prefix, URI, node.tag) #LINE# #TAB# #TAB# yield node #LINE# #TAB# except: #LINE# #TAB# #TAB# raise"
"def get_description(cls): #LINE# #TAB# if not cls.DESCRIPTION: #LINE# #TAB# #TAB# raise NotImplementedError #LINE# #TAB# description = getattr(cls, 'DESCRIPTION', None) #LINE# #TAB# if not description: #LINE# #TAB# #TAB# description = cls.__doc__ or '' #LINE# #TAB# return description"
"def get_sourcefile(bytecode_path): #LINE# #TAB# extension = os.path.splitext(bytecode_path)[1].lower() #LINE# #TAB# if extension in ('.pyc', '.pyo', '.pyw'): #LINE# #TAB# #TAB# return bytecode_path #LINE# #TAB# with open(bytecode_path, 'rb') as f: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return f.read() #LINE# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# return f"
"def read_oa_data(): #LINE# #TAB# file_path = get_data_filename() #LINE# #TAB# data = loadmat(file_path) #LINE# #TAB# oa_data = {} #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# oa_data[item[0]] = {'HID': item[1], 'Household': item[2], 'ID': #LINE# #TAB# #TAB# #TAB# item[3], 'Area ID': item[4], 'HID': item[5], 'Household': #LINE# #TAB# #TAB# #TAB# item[6], 'ID': item[7]} #LINE# #TAB# return oa_data"
def is_numline(i): #LINE# #TAB# if i > line_start_pos: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif i == line_start_pos[0]: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def load_objective(config): #LINE# #TAB# file_path = _get_user_config_path(config) #LINE# #TAB# if file_path.endswith('.json'): #LINE# #TAB# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# config = json.load(f) #LINE# #TAB# return config['objective']"
"def ensure_datetime_to_string(maybe_dttm): #LINE# #TAB# if isinstance(maybe_dttm, datetime.datetime): #LINE# #TAB# #TAB# maybe_dttm = _format_datetime(maybe_dttm) #LINE# #TAB# return maybe_dttm"
"def reindex_lst(lst, index) ->List: #LINE# #TAB# new_lst = [] #LINE# #TAB# for i, x in enumerate(lst): #LINE# #TAB# #TAB# if i == index: #LINE# #TAB# #TAB# #TAB# new_lst.append(x) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_lst.append(lst[i]) #LINE# #TAB# return new_lst"
def get_user_email(user_access_token): #LINE# #TAB# try: #LINE# #TAB# #TAB# user = User.objects.get(user_access_token=user_access_token) #LINE# #TAB# except User.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return user.email
"def add_section(obj): #LINE# #TAB# if obj.experiment_type =='multivariate': #LINE# #TAB# #TAB# return {'action': 'add_section', 'title': obj.title} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'action': 'add_section', 'title': obj.title}"
"def null_removal_mode(dataframe, colname): #LINE# #TAB# col = dataframe[colname] #LINE# #TAB# col_numerics = col.loc[col.apply(lambda x: isinstance(x, (int, float)))] #LINE# #TAB# dataframe[colname] = col.fillna(col_numerics.mode().get(0, None)) #LINE# #TAB# return dataframe"
"def get_nonzero_random_bytes(length, rnd=default_crypto_random): #LINE# #TAB# '\n#TAB# Accumulate random bit string and remove \0 bytes until the needed length is obtained.\n#TAB#'#LINE# #TAB# result = [] #LINE# #TAB# while len(result) < length: #LINE# #TAB# #TAB# rnd.shuffle(result) #LINE# #TAB# #TAB# s = rnd.getrandbits(12 * length) #LINE# #TAB# #TAB# i = s.find(b'\0') #LINE# #TAB# #TAB# if i == -1: #LINE# #TAB# #TAB# #TAB# s = b'' #LINE# #TAB# #TAB# result.append(s) #LINE# #TAB# return '\x00' * (length - len(result)) + result"
"def window_mapping(time, col, method='between'): #LINE# #TAB# assert method in ('between', 'between') #LINE# #TAB# n_years = time.shape[0] #LINE# #TAB# window = [] #LINE# #TAB# if method == 'between': #LINE# #TAB# #TAB# for j in range(n_years): #LINE# #TAB# #TAB# #TAB# window.append(time[col][j]) #LINE# #TAB# elif method == 'count': #LINE# #TAB# #TAB# for j in range(n_years): #LINE# #TAB# #TAB# #TAB# window.append(time[col][j + 1]) #LINE# #TAB# #TAB# if not window: #LINE# #TAB# #TAB# #TAB# return time #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# window = window + time[col][j - 1] #LINE# #TAB# return window"
"def format_http_key(url, username): #LINE# #TAB# key = hashlib.sha256(url.encode('utf-8')).hexdigest() #LINE# #TAB# key = base64.urlsafe_b64encode(key).decode('utf-8') #LINE# #TAB# key = base64.urlsafe_b64encode(key.encode('utf-8')).decode('utf-8') #LINE# #TAB# return key"
def get_ip_address(): #LINE# #TAB# ip_address = None #LINE# #TAB# try: #LINE# #TAB# #TAB# ip_address = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ip_address
"def shard_body_tail(num_rows, sbody): #LINE# #TAB# result = [] #LINE# #TAB# row_num = 0 #LINE# #TAB# col_num = 0 #LINE# #TAB# for row in sbody: #LINE# #TAB# #TAB# if row == num_rows: #LINE# #TAB# #TAB# #TAB# row_num += 1 #LINE# #TAB# #TAB# elif row == num_rows - 1: #LINE# #TAB# #TAB# #TAB# col_num += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(row) #LINE# #TAB# #TAB# #TAB# row_num += 1 #LINE# #TAB# return result"
"def flatten_vertex(vertex_json): #LINE# #TAB# flat_vertex_json = {} #LINE# #TAB# for key, value in vertex_json.items(): #LINE# #TAB# #TAB# if type(value) is dict: #LINE# #TAB# #TAB# #TAB# flat_vertex_json.update(flatten_vertex(value)) #LINE# #TAB# #TAB# #TAB# for sub_key, sub_value in flatten_vertex(value).items(): #LINE# #TAB# #TAB# #TAB# #TAB# flat_vertex_json[key + '/' + sub_key] = sub_value #LINE# #TAB# for key, value in vertex_json.items(): #LINE# #TAB# #TAB# flat_vertex_json[key] = flatten_vertex(value) #LINE# #TAB# return flat_vertex_json"
"def dungeon_grid_simple(w, h, room_size=4, delete_chance=0.33): #LINE# #TAB# for x in range(w): #LINE# #TAB# #TAB# for y in range(h): #LINE# #TAB# #TAB# #TAB# x_diff = x - delete_chance #LINE# #TAB# #TAB# #TAB# y_diff = y - delete_chance #LINE# #TAB# #TAB# #TAB# yield x_diff, y_diff"
"def min_n_obs_shrinkage(group_sizes: list, min_n_obs) ->np.ndarray: #LINE# #TAB# res = np.zeros(len(group_sizes)) #LINE# #TAB# for group_size in group_sizes: #LINE# #TAB# #TAB# shrinkage = np.random.randint(0, len(group_sizes) - min_n_obs) #LINE# #TAB# #TAB# if shrinkage >= group_size: #LINE# #TAB# #TAB# #TAB# res = group_sizes[shrinkage] #LINE# #TAB# return res"
"def get_directories(root_dir, empty_only=False): #LINE# #TAB# directories = [] #LINE# #TAB# for base, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# if dirs: #LINE# #TAB# #TAB# #TAB# if not empty_only: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if is_directory(base): #LINE# #TAB# #TAB# #TAB# #TAB# directories.append(base) #LINE# #TAB# #TAB# #TAB# elif is_not_empty(dirs): #LINE# #TAB# #TAB# #TAB# #TAB# if empty_only: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# directories = [] #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# directories.append(base) #LINE# #TAB# return directories"
def shuffle_data(model): #LINE# #TAB# model.data.shuffle() #LINE# #TAB# return model
def is_color_bar(ax): #LINE# #TAB# nb_x = ax.get_nrows() #LINE# #TAB# nb_y = ax.get_ncols() #LINE# #TAB# if nb_x!= nb_y: #LINE# #TAB# #TAB# return False #LINE# #TAB# if nb_x!= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# if nb_y!= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def open_any(filename): #LINE# #TAB# if filename.endswith("".gz""): #LINE# #TAB# #TAB# return gzip.open #LINE# #TAB# if filename.endswith("".bz2""): #LINE# #TAB# #TAB# return bz2.BZ2File #LINE# #TAB# return open"
"def dict_raise_on_duplicates(ordered_pairs): #LINE# #TAB# unique_dict = {} #LINE# #TAB# for key, val in ordered_pairs: #LINE# #TAB# #TAB# if key in unique_dict: #LINE# #TAB# #TAB# #TAB# raise KeyError('Duplicate key: %r' % (key,)) #LINE# #TAB# #TAB# if val is not None: #LINE# #TAB# #TAB# #TAB# raise KeyError('Duplicate value: %r' % (val,)) #LINE# #TAB# #TAB# unique_dict[key] = val"
def create_main_window(): #LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# from PyQt5.QtGui import QMainWindow #LINE# #TAB# #TAB# dlg = QMainWindow() #LINE# #TAB# #TAB# dlg.SetSize(ctypes.c_size_t(75)) #LINE# #TAB# #TAB# dlg.Show() #LINE# #TAB# else: #LINE# #TAB# #TAB# from PyQt5.QtGui import QMainWindow #LINE# #TAB# #TAB# dlg = QMainWindow(None) #LINE# #TAB# dlg.Destroy() #LINE# #TAB# return dlg
"def f_uniform(ftrue, alpha, beta): #LINE# #TAB# popsi = np.shape(ftrue) #LINE# #TAB# fval = _rand(popsi) ** beta * ftrue * np.maximum(1.0, (1000000000.0 / ( #LINE# #TAB# #TAB# ftrue + 1e-99)) ** (alpha * _rand(popsi))) #LINE# #TAB# tol = 1e-08 #LINE# #TAB# fval = fval + 1.01 * tol #LINE# #TAB# idx = ftrue < tol #LINE# #TAB# try: #LINE# #TAB# #TAB# fval[idx] = ftrue[idx] #LINE# #TAB# except (IndexError, TypeError): #LINE# #TAB# #TAB# if idx: #LINE# #TAB# #TAB# #TAB# fval = ftrue #LINE# #TAB# return fval"
"def get_host_info(): #LINE# #TAB# host = socket.gethostname() #LINE# #TAB# try: #LINE# #TAB# #TAB# host = host.split('.')[0] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return {'hostname': host, 'python_version': platform.python_version()}"
def parse_sacct(sacct_stream): #LINE# #TAB# sacct_dicts = [] #LINE# #TAB# for line in sacct_stream: #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line or line[0] == '#': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# entry = SacctEntry(line) #LINE# #TAB# #TAB# sacct_dicts.append(entry) #LINE# #TAB# return sacct_dicts
"def do_or_fake_filter(value, formatter): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# f = formatter(randomInt) #LINE# #TAB# #TAB# return f #LINE# #TAB# if type(value) == dict: #LINE# #TAB# #TAB# return {formatter(value): f} #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
"def compute_pwdr_prof_cw(profList): #LINE# #TAB# for pos, refl, iBeg, iFin in profList: #LINE# #TAB# #TAB# yc[iBeg:iFin] += refl[11 + im] * refl[9 + im] * G2pwd.getEpsVoigt(pos, #LINE# #TAB# #TAB# #TAB# refl[12 + im], refl[13 + im], refl[6 + im], refl[7 + im], x[ #LINE# #TAB# #TAB# #TAB# iBeg:iFin]) / cw[iBeg:iFin] #LINE# #TAB# return yc"
"def get_url_map(): #LINE# #TAB# map = {} #LINE# #TAB# if os.path.exists('pypi/map.txt'): #LINE# #TAB# #TAB# with open('pypi/map.txt') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.rstrip('\n') #LINE# #TAB# #TAB# #TAB# #TAB# if line!= '': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# package, url = line.split(': ') #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# map[package] = url #LINE# #TAB# return map"
def tags_for(doc: List[str]) ->Iterable[List[str]]: #LINE# #TAB# if doc: #LINE# #TAB# #TAB# for tag in doc: #LINE# #TAB# #TAB# #TAB# yield tag
"def direct_to_disk_template(request, template_name): #LINE# #TAB# filename = os.path.basename(template_name) #LINE# #TAB# if '.html' not in filename: #LINE# #TAB# #TAB# template_name += '.html' #LINE# #TAB# if '.htm' not in filename: #LINE# #TAB# #TAB# template_name += '.htm' #LINE# #TAB# t = loader.get_template(filename) #LINE# #TAB# if not t: #LINE# #TAB# #TAB# t = loader.load_template(template_name) #LINE# #TAB# return t"
"def ensure_unicode(s): #LINE# #TAB# if not isinstance(s, six.text_type): #LINE# #TAB# #TAB# s = s.decode('utf-8') #LINE# #TAB# return s"
def theme_text_element_background_color(color=None): #LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# set_options(text_element_background_color=color) #LINE# #TAB# return DEFAULT_TEXT_ELEMENT_BACKGROUND_COLOR
def get_ignore_commits_regex_patterns() ->List[str]: #LINE# #TAB# try: #LINE# #TAB# #TAB# return CONFIG['IgnoreCommits']['regex_patterns'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return []
"def add_ms1_quant_from_top3_mzidtsv(proteins, psms, headerfields, protcol): #LINE# #TAB# if not protcol: #LINE# #TAB# #TAB# protcol = mzidtsvdata.HEADER_MASTER_PROT #LINE# #TAB# top_ms1_psms = generate_top_psms(psms, protcol) #LINE# #TAB# for protein in proteins: #LINE# #TAB# #TAB# prot_acc = protein[prottabledata.HEADER_PROTEIN] #LINE# #TAB# #TAB# prec_area = calculate_protein_precursor_quant(top_ms1_psms, prot_acc) #LINE# #TAB# #TAB# outprotein = {k: v for k, v in protein.items()} #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][prottabledata.HEADER_AREA #LINE# #TAB# #TAB# #TAB# ][None]] = str(prec_area) #LINE# #TAB# #TAB# yield outprotein"
"def sort_lines(text: str, dedupe: bool=True) ->str: #LINE# #TAB# if not dedupe: #LINE# #TAB# #TAB# lines = uncomment(text).splitlines() #LINE# #TAB# else: #LINE# #TAB# #TAB# lines = text.splitlines() #LINE# #TAB# sorted_lines = [] #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.strip() in ['#', '']: #LINE# #TAB# #TAB# #TAB# if line.strip()[0]!= '#': #LINE# #TAB# #TAB# #TAB# #TAB# sorted_lines.append(line) #LINE# #TAB# sorted_lines = list(set(sorted_lines)) #LINE# #TAB# return '\n'.join(sorted_lines) + '\n'"
def find_ranges(iterable): #LINE# #TAB# it = iter(iterable) #LINE# #TAB# for i in it: #LINE# #TAB# #TAB# yield from i #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# yield next(it) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield next(it) #LINE# #TAB# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# #TAB# break
"def scroll_one_line_down(event): #LINE# #TAB# w = find_window_for_buffer_name(event.cli, event.cli.current_buffer_name) #LINE# #TAB# b = event.cli.current_buffer #LINE# #TAB# if w: #LINE# #TAB# #TAB# if w.render_info: #LINE# #TAB# #TAB# #TAB# info = w.render_info #LINE# #TAB# #TAB# #TAB# if w.vertical_scroll < info.content_height - info.window_height: #LINE# #TAB# #TAB# #TAB# #TAB# if info.cursor_position.y <= info.configured_scroll_offsets.top: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# b.cursor_position += b.document.get_cursor_down_position() #LINE# #TAB# #TAB# #TAB# #TAB# w.vertical_scroll += 1"
def get_repository_fullname_from_url(url): #LINE# #TAB# path = url.split(':') #LINE# #TAB# path = path[path.index('/'):] #LINE# #TAB# path = path[path.rindex('/'):] #LINE# #TAB# return path[path.rindex('/'):]
"def find_calc_dtype(a_dtype, b_dtype): #LINE# #TAB# calc_dtype = a_dtype #LINE# #TAB# res_dtype = b_dtype #LINE# #TAB# if calc_dtype == 'float64': #LINE# #TAB# #TAB# calc_dtype = float32 #LINE# #TAB# elif calc_dtype == 'int8': #LINE# #TAB# #TAB# calc_dtype = int8 #LINE# #TAB# elif calc_dtype == 'float32': #LINE# #TAB# #TAB# calc_dtype = float32 #LINE# #TAB# elif calc_dtype == 'float64': #LINE# #TAB# #TAB# calc_dtype = float64 #LINE# #TAB# return calc_dtype, res_dtype"
"def format_content(content): #LINE# #TAB# lines = [] #LINE# #TAB# if isinstance(content, Status): #LINE# #TAB# #TAB# lines.append(content.text) #LINE# #TAB# #TAB# return '\n'.join(lines) #LINE# #TAB# elif isinstance(content, list): #LINE# #TAB# #TAB# lines.extend(format_content(c) for c in content) #LINE# #TAB# #TAB# return '\n'.join(lines) #LINE# #TAB# else: #LINE# #TAB# #TAB# return content"
def yield_code_cells(nb): #LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# for output in cell.outputs: #LINE# #TAB# #TAB# #TAB# #TAB# if'source' in output: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield output['source'] #LINE# #TAB# #TAB# #TAB# elif 'cell_type' in output: #LINE# #TAB# #TAB# #TAB# #TAB# for output in cell.outputs: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield output
def filter_cannonical(df): #LINE# #TAB# new_df = df[df['chrom']!= 'CNA'] #LINE# #TAB# new_df = new_df[~df['chrom'].str.startswith('CNA.')] #LINE# #TAB# return new_df
"def get_frontend_media(request): #LINE# #TAB# media = None #LINE# #TAB# try: #LINE# #TAB# #TAB# if hasattr(request, 'wagtail_frontend_media'): #LINE# #TAB# #TAB# #TAB# media = request.wagtail_frontend_media() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# media = request.wagtail_frontend_media() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# media = None #LINE# #TAB# if media: #LINE# #TAB# #TAB# if request.wagtail_frontend_media: #LINE# #TAB# #TAB# #TAB# request.wagtail_frontend_media = cache_media(media) #LINE# #TAB# return media"
"def get_parallel_regions(batch): #LINE# #TAB# samples = [utils.to_single_data(d) for d in batch] #LINE# #TAB# regions = _get_parallel_regions(samples[0]) #LINE# #TAB# out = [] #LINE# #TAB# n = 10 #LINE# #TAB# for region_key in batch: #LINE# #TAB# #TAB# out.append({ #LINE# #TAB# #TAB# #TAB# ""region_name"": region_key, #LINE# #TAB# #TAB# #TAB# ""size"": n, #LINE# #TAB# #TAB# #TAB# ""min_regions"": regions[region_key][""min_regions""], #LINE# #TAB# #TAB# #TAB# ""max_regions"": regions[region_key][""max_regions""] #LINE# #TAB# #TAB# }) #LINE# #TAB# return out"
def get_all(cls): #LINE# #TAB# db = cls.get_db() #LINE# #TAB# ret = {} #LINE# #TAB# for i in db.all(): #LINE# #TAB# #TAB# ret[i] = [i] #LINE# #TAB# return ret
def generate_seed(seed): #LINE# #TAB# time.sleep(0.01) #LINE# #TAB# random.seed(seed) #LINE# #TAB# return seed
"def conv_ext_to_xy(P): #LINE# #TAB# x, y = P #LINE# #TAB# return x, y"
"def state_with_pickup(state: State, pickup: PickupEntry) ->State: #LINE# #TAB# new_state = copy.deepcopy(state) #LINE# #TAB# new_state.pickup = pickup #LINE# #TAB# new_state.gain = state.gain #LINE# #TAB# return new_state"
"def open_s3(bucket): #LINE# #TAB# conn = boto.connect_s3() #LINE# #TAB# conn.connect_to_bucket(bucket) #LINE# #TAB# key = conn.get_key(Bucket=bucket) #LINE# #TAB# return bucket, key"
"def add_to_grid(x, y, marker, color): #LINE# #TAB# global grid #LINE# #TAB# grid[0] = x #LINE# #TAB# global marker #LINE# #TAB# marker = Marker(color=color) #LINE# #TAB# add_to_grid = True #LINE# #TAB# for i in range(len(grid) - 1): #LINE# #TAB# #TAB# grid[i] = x, y #LINE# #TAB# #TAB# add_to_grid = False #LINE# #TAB# #TAB# if x > grid[0] - 1: #LINE# #TAB# #TAB# #TAB# add_to_grid = False #LINE# #TAB# if y > grid[1]: #LINE# #TAB# #TAB# add_to_grid = False #LINE# #TAB# #TAB# y -= 1 #LINE# #TAB# grid[0] = x #LINE# #TAB# grid[1] = y #LINE# #TAB# return grid"
"def get_path(root, target, pred): #LINE# #TAB# if root == target: #LINE# #TAB# #TAB# return [0] #LINE# #TAB# elif pred == None: #LINE# #TAB# #TAB# return [[], []] #LINE# #TAB# else: #LINE# #TAB# #TAB# p = pred #LINE# #TAB# #TAB# c = root #LINE# #TAB# #TAB# for i, j in enumerate(p): #LINE# #TAB# #TAB# #TAB# if j == target: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# elif p[i] > c: #LINE# #TAB# #TAB# #TAB# #TAB# c = j #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# p.append(i) #LINE# #TAB# #TAB# return p"
"def microphone_transferfactor(sensitivity): #LINE# #TAB# if sensitivity in ['B', 'K']: #LINE# #TAB# #TAB# return 24.0 #LINE# #TAB# if sensitivity == 'R': #LINE# #TAB# #TAB# return 25.0 #LINE# #TAB# result = 0.0028 #LINE# #TAB# if sensitivity == 'M': #LINE# #TAB# #TAB# return 1000.0 #LINE# #TAB# result = (sensitivity * 10 ** 9) / 1000.0 #LINE# #TAB# if sensitivity == 'K': #LINE# #TAB# #TAB# return 1000.0 #LINE# #TAB# result = 10 ** -(8 * np.log10(result)) #LINE# #TAB# return result"
"def process_facet_terms(facet_terms): #LINE# #TAB# if not facets_terms: #LINE# #TAB# #TAB# return [] #LINE# #TAB# facets = [] #LINE# #TAB# for term in facets_terms: #LINE# #TAB# #TAB# term_name = term[0] #LINE# #TAB# #TAB# facet_name = re.sub('\\W+', '_', term_name) #LINE# #TAB# #TAB# if not term_name.endswith('_facet'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# facets.append({'term': term_name, 'facet': facet_name}) #LINE# #TAB# return facets"
"def prepare_configuration_uri(application, configuration): #LINE# #TAB# if configuration: #LINE# #TAB# #TAB# uri = ET.SubElement(configuration, 'uri') #LINE# #TAB# #TAB# uri.text = configuration['uri'].text #LINE# #TAB# #TAB# application.configuration_elements.append(uri) #LINE# #TAB# else: #LINE# #TAB# #TAB# uri = ET.SubElement(configuration, 'uri') #LINE# #TAB# #TAB# application.configuration_elements.append(uri) #LINE# #TAB# return uri"
def get_registry(): #LINE# #TAB# global _registry #LINE# #TAB# if _registry is None: #LINE# #TAB# #TAB# _registry = Registry() #LINE# #TAB# return _registry
"def autodoc_applicationmodel(module): #LINE# #TAB# module_path = module.__file__ #LINE# #TAB# with open(module_path, 'r') as f: #LINE# #TAB# #TAB# text = f.read() #LINE# #TAB# #TAB# for line in text.splitlines(): #LINE# #TAB# #TAB# #TAB# if line.startswith('#') or not line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# models = json.loads(line) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# except json.decoder.JSONDecodeError: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# return modules"
"def bsdi_secret_to_key(secret): #LINE# #TAB# charset = 'utf-8' #LINE# #TAB# key = secret.encode(charset) #LINE# #TAB# if not isinstance(key, bytes): #LINE# #TAB# #TAB# key = key.decode('utf-8') #LINE# #TAB# return key"
"def random_public_quote(request): #LINE# #TAB# id_quote = request.GET.get('quote', None) #LINE# #TAB# if id_quote and request.GET.get('is_public', False): #LINE# #TAB# #TAB# quote_type = request.GET.get('quote_type', None) #LINE# #TAB# #TAB# if quote_type: #LINE# #TAB# #TAB# #TAB# return {'quote_id': id_quote, 'is_public': True} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return {'quote_id': None, 'is_public': False} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'quote_type': None, 'is_public': False}"
def has_isolated_nodes(G): #LINE# #TAB# nodes = list(G.nodes()) #LINE# #TAB# if not nodes: #LINE# #TAB# #TAB# return True #LINE# #TAB# for n in nodes: #LINE# #TAB# #TAB# if not G.has_node(n): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"def get_line_index(line_or_func, lines): #LINE# #TAB# line_idx = None #LINE# #TAB# if callable(line_or_func): #LINE# #TAB# #TAB# lines = lines + [line_or_func] #LINE# #TAB# while lines: #LINE# #TAB# #TAB# line_idx = lines.index(line_or_func) + 1 #LINE# #TAB# #TAB# if line_idx < 0: #LINE# #TAB# #TAB# #TAB# raise ValueError('invalid line_or_func: %r' % line_or_func) #LINE# #TAB# return line_idx"
"def to_string(data): #LINE# #TAB# string_data = {} #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# string_data[key] = str(value) #LINE# #TAB# return string_data"
"def get_md5sum(src_file): #LINE# #TAB# hasher = hashlib.md5() #LINE# #TAB# with open(src_file, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = f.read(1000 * 1000) #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# hasher.update(data) #LINE# #TAB# md5 = hasher.hexdigest() #LINE# #TAB# return md5"
def get_key_func(key_func): #LINE# #TAB# if key_func is not None: #LINE# #TAB# #TAB# if callable(key_func): #LINE# #TAB# #TAB# #TAB# return key_func #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return import_string(key_func) #LINE# #TAB# return default_key_func
"def load_json(file_name, bdd, load_order=False): #LINE# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# if load_order: #LINE# #TAB# #TAB# #TAB# var_order = json.load(f) #LINE# #TAB# #TAB# for var in var_order: #LINE# #TAB# #TAB# #TAB# bdd.add(json.loads(var)) #LINE# #TAB# return bdd"
"def infer_language_pair(path): #LINE# #TAB# lang1, ext = os.path.splitext(path) #LINE# #TAB# if lang1 == '': #LINE# #TAB# #TAB# lang1 = 'python' #LINE# #TAB# elif lang2 == '': #LINE# #TAB# #TAB# lang2 = 'python' #LINE# #TAB# else: #LINE# #TAB# #TAB# lang1, lang2 = lang1.split('-') #LINE# #TAB# if lang2 == '': #LINE# #TAB# #TAB# lang2 = 'python' #LINE# #TAB# return lang1, lang2"
"def match_ids(cls, ids): #LINE# #TAB# modified_ids = [] #LINE# #TAB# for page_id in ids: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# page = cls.get(page_id) #LINE# #TAB# #TAB# except InvalidId: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not page: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# new_id = page.id() if page.id else None #LINE# #TAB# #TAB# if new_id!= modified_ids: #LINE# #TAB# #TAB# #TAB# modified_ids.append(new_id) #LINE# #TAB# return modified_ids"
"def filter_interactions_by_complexes(interactions: pd.DataFrame, complexes: #LINE# #TAB# DataFrame) ->pd.DataFrame: #LINE# #TAB# if not complexes: #LINE# #TAB# #TAB# return interactions #LINE# #TAB# filtered_interactions = interactions[interactions['type']!= 'interaction'] #LINE# #TAB# for i in range(interactions.shape[0]): #LINE# #TAB# #TAB# if not interactions[i].iloc[0] in complexes: #LINE# #TAB# #TAB# #TAB# filtered_interactions = filtered_interactions.loc[filtered_interactions[ #LINE# #TAB# #TAB# #TAB# #TAB# 'type'] == 'interaction'] #LINE# #TAB# return filtered_interactions"
"def compute_all_shapes(v): #LINE# #TAB# index_size = 0 #LINE# #TAB# total_shape = 0 #LINE# #TAB# index_shape = [] #LINE# #TAB# total_size = 0 #LINE# #TAB# if is_list_like(v): #LINE# #TAB# #TAB# for idx in v: #LINE# #TAB# #TAB# #TAB# total_size += 1 #LINE# #TAB# #TAB# #TAB# if idx: #LINE# #TAB# #TAB# #TAB# #TAB# shape.append(idx) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# shape.append(v[idx]) #LINE# #TAB# shape = tuple(shape) #LINE# #TAB# size = sum(shape) #LINE# #TAB# return shape, size, index_shape, index_size, total_shape"
def apply_transformation(inputs): #LINE# #TAB# with mp.Pool() as pool: #LINE# #TAB# #TAB# output = pool.apply_transformation(inputs) #LINE# #TAB# return output
"def get_snpsets_list(): #LINE# #TAB# snp_sets_dict = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# with open('snpsets.py', 'r') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# snp_sets_dict[line[0]] = line[1:].strip() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return snp_sets_dict"
"def cmd_delete(nexus_client, name): #LINE# #TAB# resp = nexus_client.scripts.delete(name) #LINE# #TAB# print(resp) #LINE# #TAB# return exception.CliReturnCode.SUCCESS.value"
"def _cli(cls, opts): #LINE# #TAB# #TAB# logging.basicConfig(level=logging.INFO, format=""%(message)s"") #LINE# #TAB# #TAB# logging.getLogger().setLevel(logging.DEBUG) #LINE# #TAB# #TAB# if opts.quiet: #LINE# #TAB# #TAB# #TAB# logging.basicConfig(level=logging.WARNING) #LINE# #TAB# #TAB# #TAB# logging.getLogger().propagate = False #LINE# #TAB# #TAB# if opts.verbose: #LINE# #TAB# #TAB# #TAB# logging.basicConfig(level=logging.DEBUG) #LINE# #TAB# #TAB# cls._logger = logging.getLogger('pyinotify') #LINE# #TAB# #TAB# if opts.stream: #LINE# #TAB# #TAB# #TAB# cls._stream_handler = logging.StreamHandler(sys.stdout) #LINE# #TAB# #TAB# #TAB# cls._logger.setLevel(logging.DEBUG) #LINE# #TAB# #TAB# return cls"
"def compatibility_from_options(working_dir, options=None, environment=None): #LINE# #TAB# if options is None: #LINE# #TAB# #TAB# options = {} #LINE# #TAB# compatibility = options.get(""compatibility"", None) #LINE# #TAB# if compatibility: #LINE# #TAB# #TAB# project_name = options.get(""project_name"", None) #LINE# #TAB# #TAB# compatibility = os.environ.get(""COMPOSE_COMPATIBILITY"", environment) #LINE# #TAB# #TAB# if project_name: #LINE# #TAB# #TAB# #TAB# return project_name #LINE# #TAB# filename = os.path.join(working_dir, ""COMPOSE_COMPATIBILITY"") #LINE# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# with open(filename, ""r"") as _file: #LINE# #TAB# #TAB# #TAB# content = _file.read() #LINE# #TAB# return compatibility"
"def chunk_sequence(data, size): #LINE# #TAB# data = iter(data) #LINE# #TAB# while size: #LINE# #TAB# #TAB# chunk = data.pop(0) #LINE# #TAB# #TAB# if len(chunk) == size: #LINE# #TAB# #TAB# #TAB# yield chunk #LINE# #TAB# #TAB# #TAB# data = chunk #LINE# #TAB# #TAB# size -= 1"
def validate_get_dbs(connection): #LINE# #TAB# valid = True #LINE# #TAB# try: #LINE# #TAB# #TAB# connection.ping(True) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# valid = False #LINE# #TAB# if not valid: #LINE# #TAB# #TAB# sys.stderr.write( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# sys.stderr.write( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return valid
"def get_server_defaults(): #LINE# #TAB# defaults = dict() #LINE# #TAB# for name, value in os.environ.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# defaults[name] = value #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if 'SERVER_DEFAULTS' not in defaults: #LINE# #TAB# #TAB# defaults['SERVER_DEFAULTS'] = {} #LINE# #TAB# return defaults"
"def do_put(content_path, content): #LINE# #TAB# with safeopen.restore_open(): #LINE# #TAB# #TAB# file_store = Repo(content_path).file #LINE# #TAB# #TAB# blob = Blob.from_string(content) #LINE# #TAB# #TAB# file_store.object_store.put(blob) #LINE# #TAB# #TAB# return blob.id"
"def glob_escape(pathname): #LINE# #TAB# pathname = os.path.abspath(pathname) #LINE# #TAB# pathname = pathname.replace('\\', '\\\\') #LINE# #TAB# pathname = pathname.replace('""', '\\""') #LINE# #TAB# pathname = pathname.replace('*', '\\*') #LINE# #TAB# return pathname"
"def after_request(response): #LINE# #TAB# response = ensure_response(response) #LINE# #TAB# response.headers.add('Access-Control-Allow-Origin', '*') #LINE# #TAB# if response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization' #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# response.headers.add('Access-Control-Allow-Headers', 'X-Authorization') #LINE# #TAB# if request.method == 'OPTIONS': #LINE# #TAB# #TAB# response.headers.add('Access-Control-Allow-Methods', 'GET,POST,DELETE') #LINE# #TAB# return response"
"def quantity_to_hdf5(f, key, q): #LINE# #TAB# if isinstance(q, units.Quantity): #LINE# #TAB# #TAB# q = [q] * len(q.units) #LINE# #TAB# hf = h5py.File(f, 'w') #LINE# #TAB# quantity_to_hdf5Group(f, key, q) #LINE# #TAB# quantity_to_hdf5Group(f, key + '_quantity', q) #LINE# #TAB# return f"
"def add_querystr_to_params(url, params): #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed = urlparse(url) #LINE# #TAB# #TAB# query = parse_qs(parsed.query) #LINE# #TAB# #TAB# if query: #LINE# #TAB# #TAB# #TAB# if isinstance(params, list): #LINE# #TAB# #TAB# #TAB# #TAB# new_params = dict(parse_qsl(params)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# new_params = params #LINE# #TAB# except: #LINE# #TAB# #TAB# new_params = {} #LINE# #TAB# return url, new_params"
"def polygon_area(polygon): #LINE# #TAB# point_area = 0 #LINE# #TAB# for i in range(len(polygon) - 1): #LINE# #TAB# #TAB# point_a = polygon[i] #LINE# #TAB# #TAB# point_b = polygon[i + 1] #LINE# #TAB# #TAB# uv = subtract_vectors(point_a, point_b) #LINE# #TAB# #TAB# point_area += uv * uv #LINE# #TAB# return point_area"
"def grab_config_data_from_data_file(file): #LINE# #TAB# if not os.path.isfile(file): #LINE# #TAB# #TAB# raise ValueError('Config file {} not found'.format(file)) #LINE# #TAB# with io.open(file, 'r') as f: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# #TAB# if data is not None: #LINE# #TAB# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return {}"
def strahler_order(section): #LINE# #TAB# node_order = [] #LINE# #TAB# while section: #LINE# #TAB# #TAB# if section.children: #LINE# #TAB# #TAB# #TAB# node_order.append(section) #LINE# #TAB# #TAB# #TAB# section = section.children[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# node_order.append(section.name) #LINE# #TAB# return node_order
"def et_tostring(elem, pretty_print=False): #LINE# #TAB# html = etree.tostring(elem, encoding='utf-8') #LINE# #TAB# if html.tag == 'html': #LINE# #TAB# #TAB# pass #LINE# #TAB# elif html.tag == 'html/body': #LINE# #TAB# #TAB# f = StringIO() #LINE# #TAB# #TAB# html.write(f.getvalue()) #LINE# #TAB# #TAB# f.close() #LINE# #TAB# #TAB# html = f.getvalue() #LINE# #TAB# elif pretty_print: #LINE# #TAB# #TAB# html = pretty_print(html) #LINE# #TAB# return html"
"def make_key(table_name, objid): #LINE# #TAB# key = str(objid) #LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# md5.update(table_name) #LINE# #TAB# key = md5.hexdigest() #LINE# #TAB# return key"
"def get_centroid(points): #LINE# #TAB# centroid = np.zeros((len(points), 3)) #LINE# #TAB# for p in points: #LINE# #TAB# #TAB# x_coords = p[0].values #LINE# #TAB# #TAB# y_coords = p[1].values #LINE# #TAB# #TAB# x_centroid = np.concatenate([x_coords, np.mean(x_coords)]) #LINE# #TAB# #TAB# centroid[0] = x_centroid / 2 #LINE# #TAB# #TAB# centroid[1] = y_centroid / 2 #LINE# #TAB# return centroid"
def get_name(friendly=False): #LINE# #TAB# if friendly: #LINE# #TAB# #TAB# return 'Alignak live state history' #LINE# #TAB# return 'livesynthesisretention'
"def find_position(string, index, line_offset): #LINE# #TAB# lines = string.count('\n', 0, index) #LINE# #TAB# if lines > 0: #LINE# #TAB# #TAB# first_line = lines - line_offset #LINE# #TAB# #TAB# second_line = string.rfind('\n', first_line + 1, index) #LINE# #TAB# #TAB# if second_line < 0: #LINE# #TAB# #TAB# #TAB# second_line += 1 #LINE# #TAB# #TAB# line = first_line + 1 #LINE# #TAB# column = second_line - line_offset #LINE# #TAB# return line, column"
"def assemble_from_iterable(cls, messages: Sequence): #LINE# #TAB# context = {} #LINE# #TAB# for m in messages: #LINE# #TAB# #TAB# context[m.name] = m #LINE# #TAB# return context"
"def get_resources_by_type(skil, resource_type): #LINE# #TAB# resources = skil.api.get_resources() #LINE# #TAB# for resource in resources: #LINE# #TAB# #TAB# if resource.type == resource_type: #LINE# #TAB# #TAB# #TAB# yield resource"
"def earthquake_contour_preprocessor(impact_function): #LINE# #TAB# from math import sin, cos, tan, sqrt #LINE# #TAB# earthquake_key = impact_function.key #LINE# #TAB# contour = [] #LINE# #TAB# for i in range(len(earthquake_key)): #LINE# #TAB# #TAB# grad = sin(earthquake_key[i]) #LINE# #TAB# #TAB# contour.append((cos(grad), sin(grad))) #LINE# #TAB# return contour"
"def format_to_str(obj): #LINE# #TAB# if type(obj) == list: #LINE# #TAB# #TAB# s = ', '.join(format_to_str(o) for o in obj) #LINE# #TAB# else: #LINE# #TAB# #TAB# s = obj #LINE# #TAB# return s"
def set_enabled(enabled=True): #LINE# #TAB# if enabled == True: #LINE# #TAB# #TAB# plugin_enabled = True #LINE# #TAB# elif enabled is False: #LINE# #TAB# #TAB# plugin_enabled = False #LINE# #TAB# global _enabled #LINE# #TAB# _enabled = enabled
def get_capitalization(word): #LINE# #TAB# word_list = [token.capitalize() for token in word] #LINE# #TAB# if len(word_list) > 0: #LINE# #TAB# #TAB# return word_list[0].capitalize() #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return word[0].capitalize() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# return ''
"def safe_isclass(obj): #LINE# #TAB# try: #LINE# #TAB# #TAB# return isinstance(obj, type) #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False"
def extract_fieldnames(config): #LINE# #TAB# fieldnames = [] #LINE# #TAB# for fname in get_fields(config): #LINE# #TAB# #TAB# if fname not in fieldnames: #LINE# #TAB# #TAB# #TAB# fieldnames.append(fname) #LINE# #TAB# return fieldnames
def get_address_family(table): #LINE# #TAB# try: #LINE# #TAB# #TAB# ip = ipaddress.ip_address(table) #LINE# #TAB# #TAB# if ip.is_IPv4 or ip.is_IPv6: #LINE# #TAB# #TAB# #TAB# return AF_INET #LINE# #TAB# #TAB# elif ip.is_link_local: #LINE# #TAB# #TAB# #TAB# return AF_LINK #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return AF_INET6 #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return AF_INET
def levenshtein_coding(n: int) ->str: #LINE# #TAB# coding = '' #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return coding + 'b' #LINE# #TAB# elif n < 9: #LINE# #TAB# #TAB# if n < 10: #LINE# #TAB# #TAB# #TAB# coding += 'd' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# coding += '+' #LINE# #TAB# elif n < 11: #LINE# #TAB# #TAB# if n < 12: #LINE# #TAB# #TAB# #TAB# coding += '-' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# coding += '+' #LINE# #TAB# return coding
"def datetime_serializer(obj: Any) ->str: #LINE# #TAB# if isinstance(obj, datetime.datetime): #LINE# #TAB# #TAB# return obj.isoformat() #LINE# #TAB# elif isinstance(obj, str): #LINE# #TAB# #TAB# return obj #LINE# #TAB# raise ValueError"
"def first_leaf(nested_dict): #LINE# #TAB# for k in nested_dict.keys(): #LINE# #TAB# #TAB# for v in nested_dict[k]: #LINE# #TAB# #TAB# #TAB# retval = {} #LINE# #TAB# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# retval = first_leaf(v) #LINE# #TAB# #TAB# #TAB# if retval: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return retval #LINE# #TAB# return nested_dict"
def parse_space_world(description): #LINE# #TAB# fields = [] #LINE# #TAB# lines = description.split('\n') #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.strip() == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# field = parse_field(line) #LINE# #TAB# #TAB# if field is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# fields.append(field) #LINE# #TAB# return fields
def get_client(): #LINE# #TAB# client = Client.singleton_instance #LINE# #TAB# if not client: #LINE# #TAB# #TAB# client = Client() #LINE# #TAB# return client
def idl_typecode(i): #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return IDL_BYTES #LINE# #TAB# elif i == 1: #LINE# #TAB# #TAB# return IDL_SHORT #LINE# #TAB# elif i == 2: #LINE# #TAB# #TAB# return IDL_DOUBLE #LINE# #TAB# elif i == 3: #LINE# #TAB# #TAB# return IDL_OBJECT #LINE# #TAB# elif i == 4: #LINE# #TAB# #TAB# return IDL_ARRAY
"def get_task_dir(cls, task_id, create=False): #LINE# #TAB# dir_path = cls.get_task_dir_name(task_id) #LINE# #TAB# if not os.path.exists(dir_path): #LINE# #TAB# #TAB# if create: #LINE# #TAB# #TAB# #TAB# os.makedirs(dir_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise TaskDirExistsError(task_id) #LINE# #TAB# else: #LINE# #TAB# #TAB# return dir_path"
"def get_one_dn_attribute(conn, dn, filtr, attr): #LINE# #TAB# attr_list = conn.search_s(filtr, dn, ldap.SCOPE_SUBTREE, attr) #LINE# #TAB# assert attr_list is not None #LINE# #TAB# for attr in attr_list: #LINE# #TAB# #TAB# attribute_name = attr #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return getattr(conn, attribute_name) #LINE# #TAB# #TAB# except ldap.LDAPError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return None"
"def get_stream_version(tap_stream_id, state): #LINE# #TAB# new_tap_stream = state.tap_streams[tap_stream_id] #LINE# #TAB# if new_tap_stream: #LINE# #TAB# #TAB# new_tap_stream = json.loads(new_tap_stream.decode('utf-8')) #LINE# #TAB# #TAB# return new_tap_stream['version'] #LINE# #TAB# return None"
"def parse_root(raw): #LINE# #TAB# root = ElementTree.fromstring(raw) #LINE# #TAB# qualified_name = root.tag.rsplit('.', 1)[0] #LINE# #TAB# attributes = root.attrib #LINE# #TAB# if attributes is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# attributes = raw.attrib #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# qualified_name = root.tag.rsplit('.', 1)[-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# qualified_name = raw.tag #LINE# #TAB# if len(attributes) == 1: #LINE# #TAB# #TAB# attribute = attributes[0] #LINE# #TAB# return qualified_name, attribute"
def is_numeric(val: str) ->bool: #LINE# #TAB# try: #LINE# #TAB# #TAB# float(val) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False
"def remove_highlight_edges(graph: BELGraph, edges=None): #LINE# #TAB# graph = graph.copy() #LINE# #TAB# if edges is None: #LINE# #TAB# #TAB# edges = graph.edges() #LINE# #TAB# for u, v in edges: #LINE# #TAB# #TAB# with graph.get_edge(u, v) as _: #LINE# #TAB# #TAB# #TAB# with graph.get_edge(v, u) as _: #LINE# #TAB# #TAB# #TAB# #TAB# graph.remove_edge(u, v) #LINE# #TAB# #TAB# #TAB# graph.add_edge(v, u) #LINE# #TAB# for u, v in edges: #LINE# #TAB# #TAB# with graph.get_edge(u, v) as _: #LINE# #TAB# #TAB# #TAB# del graph.get_edge(v, u) #LINE# #TAB# return graph"
"def prepare_file(filename): #LINE# #TAB# new_file = filename.split('(')[0] + '.py')[0] #LINE# #TAB# if not new_file.endswith('.py'): #LINE# #TAB# #TAB# new_file = '%s.py' % new_file #LINE# #TAB# try: #LINE# #TAB# #TAB# os.unlink(filename) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# new_file = os.path.join(filename, '.py') #LINE# #TAB# if not os.path.exists(new_file): #LINE# #TAB# #TAB# parent, file = os.path.split(filename) #LINE# #TAB# #TAB# parent.truncate() #LINE# #TAB# #TAB# os.unlink(file) #LINE# #TAB# return new_file"
def give_color_to_direction_dynamic(dir): #LINE# #TAB# if dir == 'backward': #LINE# #TAB# #TAB# col = BLACK #LINE# #TAB# elif dir == 'forward': #LINE# #TAB# #TAB# col = BLACK #LINE# #TAB# elif dir == 'backward': #LINE# #TAB# #TAB# col = BLACK #LINE# #TAB# else: #LINE# #TAB# #TAB# col = dir #LINE# #TAB# return col
"def try_passwordless_openssh(server, keyfile): #LINE# #TAB# if pexpect is None: #LINE# #TAB# #TAB# raise ImportError(""pexpect unavailable, use paramiko"") #LINE# #TAB# cmd ='ssh -f '+ server #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# cmd +='-i'+ keyfile #LINE# #TAB# cmd +='exit' #LINE# #TAB# p = pexpect.spawn(cmd) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p.expect('[Pp]assword:', timeout=.1) #LINE# #TAB# #TAB# except pexpect.TIMEOUT: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# except pexpect.EOF: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False"
def standard_deviation(numbers): #LINE# #TAB# try: #LINE# #TAB# #TAB# return 1.0 / math.sqrt(sum([(i - 1) ** 2) for i in numbers]) #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# return 0.0
"def vizlib_unary_features(span: SpanMention) ->Iterator[Tuple[str, int]]: #LINE# #TAB# for f in get_visual_aligned_lemmas(span): #LINE# #TAB# #TAB# yield f""ALIGNED_{f}"", 1 #LINE# #TAB# for page in set(span.get_attrib_tokens(""page"")): #LINE# #TAB# #TAB# yield f""PAGE_[{page}]"", 1"
"def positive_int(integer_string, strict=False, cutoff=None): #LINE# #TAB# ret = int(integer_string) #LINE# #TAB# if ret < 0 or ret == 0 and strict: #LINE# #TAB# #TAB# raise ValueError() #LINE# #TAB# if cutoff: #LINE# #TAB# #TAB# return min(ret, cutoff) #LINE# #TAB# return ret"
"def residues_per_turn(p): #LINE# #TAB# n_rot = np.zeros(p.get_residues_per_turn(), dtype=np.int32) #LINE# #TAB# for m in p.monomers: #LINE# #TAB# #TAB# res_turn = len(m.get_residues_per_turn()) #LINE# #TAB# #TAB# n_rot[m.id] = res_turn #LINE# #TAB# return n_rot"
"def move_mouse(x, y): #LINE# #TAB# m_x = _mouse_pos[0] #LINE# #TAB# m_y = _mouse_pos[1] #LINE# #TAB# m_x = int(x) #LINE# #TAB# m_y = int(y) #LINE# #TAB# m_height = _mouse_pos[2] #LINE# #TAB# m_width = _mouse_pos[3] #LINE# #TAB# if m_x > m_y: #LINE# #TAB# #TAB# m_x = m_x #LINE# #TAB# if m_y > m_y: #LINE# #TAB# #TAB# m_y = m_y #LINE# #TAB# mouse_pos[0] = m_x #LINE# #TAB# mouse_pos[1] = m_y #LINE# #TAB# return m_x, mouse_pos[2], mouse_pos[3]"
"def get_genes_from_strain_t(geneT, strainT, genesO): #LINE# #TAB# strainT1 = strainT[0] #LINE# #TAB# strainT2 = strainT[1] #LINE# #TAB# assert len(strainT1) == len(geneT) == len(strainT2) #LINE# #TAB# lGeneT = genesO[geneT[0]].copy() #LINE# #TAB# lGeneT.update(strainT1) #LINE# #TAB# lGeneT.sort() #LINE# #TAB# return lGeneT, strainT2"
"def annotated_references(obj): #LINE# #TAB# result = {} #LINE# #TAB# for name, refobj in _annotated_references_for(obj): #LINE# #TAB# #TAB# if refobj: #LINE# #TAB# #TAB# #TAB# result[name] = refobj #LINE# #TAB# return result"
"def merge_entities(doc): #LINE# #TAB# if not doc.get(""entities""): #LINE# #TAB# #TAB# return doc #LINE# #TAB# new_doc = {} #LINE# #TAB# for entity in doc[""entities""]: #LINE# #TAB# #TAB# if entity[""type""] in ENTITY_TYPES: #LINE# #TAB# #TAB# #TAB# new_doc[entity[""type""]] = entity[""value""] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_doc[entity[""type""]] = entity[""value""] #LINE# #TAB# for entity in doc[""entities""]: #LINE# #TAB# #TAB# if entity[""type""] in ENTITY_TYPES: #LINE# #TAB# #TAB# #TAB# new_doc[entity[""type""]] = entity[""value""] #LINE# #TAB# return new_doc"
"def get_week_start(day=None): #LINE# #TAB# if day is None: #LINE# #TAB# #TAB# day = date.today().isoweekday() #LINE# #TAB# start = date(day.year, day.month, 1) #LINE# #TAB# if day.isoweekday() == 7: #LINE# #TAB# #TAB# month_start = start + timedelta(days=1) #LINE# #TAB# else: #LINE# #TAB# #TAB# month_start = start + timedelta(days=1) #LINE# #TAB# return month_start"
def create_mapping(config): #LINE# #TAB# mapping = {} #LINE# #TAB# for url in config: #LINE# #TAB# #TAB# if url not in mapping: #LINE# #TAB# #TAB# #TAB# mapping[url] = [] #LINE# #TAB# #TAB# mapping[url].append(config[url]) #LINE# #TAB# return mapping
"def get_privkey(address): #LINE# #TAB# DB_PATH = Config.get_db_path() #LINE# #TAB# conn = sqlite3.connect(DB_PATH) #LINE# #TAB# conn.row_factory = sqlite3.Row #LINE# #TAB# key = conn.cursor() #LINE# #TAB# key.execute('SELECT * FROM private_keys WHERE address=?', (address,)) #LINE# #TAB# decrypted_hex = key.fetchone() #LINE# #TAB# conn.close() #LINE# #TAB# if decrypted_hex: #LINE# #TAB# #TAB# return decrypted_hex #LINE# #TAB# return None"
"def split_index(params): #LINE# #TAB# if isinstance(params, dict): #LINE# #TAB# #TAB# if NodeType.INDEX in params.keys(): #LINE# #TAB# #TAB# #TAB# return split_index(params[NodeType.VALUE]) #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# for key in params: #LINE# #TAB# #TAB# #TAB# result[key] = split_index(params[key]) #LINE# #TAB# #TAB# return result #LINE# #TAB# else: #LINE# #TAB# #TAB# return params"
"def random_unit_vector(): #LINE# #TAB# sigma1 = 2.0 * np.random.random() #LINE# #TAB# sigma2 = np.random.random() #LINE# #TAB# phi1 = np.arccos(sigma1) #LINE# #TAB# phi2 = np.sqrt(sigma2) #LINE# #TAB# assert np.linalg.norm(phi1) <= 1e-06 #LINE# #TAB# assert np.linalg.norm(phi2) <= 1e-06 #LINE# #TAB# return phi1, phi2, sigma1"
"def get_param_names(fnode): #LINE# #TAB# assert isinstance(fnode, astroid.FunctionDef) #LINE# #TAB# params = [] #LINE# #TAB# for param in fnode.parameters: #LINE# #TAB# #TAB# param_name = param.name #LINE# #TAB# #TAB# if isinstance(param_name, astroid.Name): #LINE# #TAB# #TAB# #TAB# params.append(param_name) #LINE# #TAB# #TAB# elif isinstance(param_name, astroid.Name): #LINE# #TAB# #TAB# #TAB# params.append(param_name.id) #LINE# #TAB# return params"
def read_d1_letter(fin_txt): #LINE# #TAB# txt = fin_txt.split('\n') #LINE# #TAB# if len(txt) == 0: #LINE# #TAB# #TAB# return {} #LINE# #TAB# l = [] #LINE# #TAB# for l_letter in txt: #LINE# #TAB# #TAB# l_letter = l_letter.strip() #LINE# #TAB# #TAB# if len(l_letter) == 1 and l_letter[0].isalpha(): #LINE# #TAB# #TAB# #TAB# l.append(l_letter[0]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# l.append(int(l_letter)) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return l
def realme_auth_strength(request): #LINE# #TAB# realmme_id = request.session['realme_id'] #LINE# #TAB# try: #LINE# #TAB# #TAB# auth_strength = request.session[realmme_id].auth_strength #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return auth_strength
"def create_from_template(dev_name: str, dev_type: int) ->Input: #LINE# #TAB# dev_template = InputTemplate.get_template(dev_type) #LINE# #TAB# obj = Input(dev_name=dev_name, dev_type=dev_template.type, uses_clk= #LINE# #TAB# #TAB# dev_template.uses_clk, uses_bus=dev_template.uses_bus, parameters={ #LINE# #TAB# #TAB# name: param.copy() for name, param in dev_template.parameters.items()}) #LINE# #TAB# obj.template = dev_template #LINE# #TAB# for name, pin in dev_template.pins.items(): #LINE# #TAB# #TAB# obj.pins[name] = pin.copy(obj) #LINE# #TAB# return obj"
"def camel_resource(obj): #LINE# #TAB# if not isinstance(obj, dict): #LINE# #TAB# #TAB# return obj #LINE# #TAB# for k in list(obj.keys()): #LINE# #TAB# #TAB# v = obj.pop(k) #LINE# #TAB# #TAB# obj[k] = TitleCase(v) #LINE# #TAB# return obj"
"def file_type(path): #LINE# #TAB# signature = {'\x1f\x8b\x08': 'gz', 'BZh': 'bz2', 'PK\x03\x04': 'zip'} #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# for sig, f_type in signature.items(): #LINE# #TAB# #TAB# #TAB# if f.read(4).startswith(sig): #LINE# #TAB# #TAB# #TAB# #TAB# return f_type"
def attach_to(device): #LINE# #TAB# config = get_configuration(device) #LINE# #TAB# device.attach(config) #LINE# #TAB# return True
"def weekday_and_week_to_day(year: int, month: int, week: int, weekday: int): #LINE# #TAB# day_of_week = week - 1 #LINE# #TAB# if weekday < 0: #LINE# #TAB# #TAB# day_of_week = day_of_week + 1 #LINE# #TAB# elif weekday > 30: #LINE# #TAB# #TAB# day_of_week = day_of_week - 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# day_of_week = day_of_week + 1 #LINE# #TAB# return day_of_week"
def is_blacklisted_url(url): #LINE# #TAB# for method in BLACKLIST_URLS: #LINE# #TAB# #TAB# if method(url): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
def is_valid_boolean(val): #LINE# #TAB# val = str(val).lower() #LINE# #TAB# return val in BOOLEAN_TRUE_STRINGS or val in BOOLEAN_FALSE_STRINGS
def single_row(results): #LINE# #TAB# if len(results) > 1: #LINE# #TAB# #TAB# return [r for r in results] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [r for r in results]
"def unpack_scalar(cls, dataset, data): #LINE# #TAB# if len(data)!= 1: #LINE# #TAB# #TAB# return data #LINE# #TAB# key = list(data.keys())[0] #LINE# #TAB# if len(data[key]) == 1 and key in dataset.vdims: #LINE# #TAB# #TAB# return data[key][0]"
"def is_valid_input_array(x, ndim=None): #LINE# #TAB# x = np.asarray(x) #LINE# #TAB# if ndim is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# x = x.reshape(-1) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if x.ndim!= 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if x.shape[1]!= ndim: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def make_directory_if_not_exists(path): #LINE# #TAB# dirname, basename = os.path.split(path) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(dirname) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno!= errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise"
"def preprocess_french(trans, fr_nlp, remove_brackets_content=True): #LINE# #TAB# if remove_brackets_content: #LINE# #TAB# #TAB# trans = pangloss.remove_content_in_brackets(trans, ""[]"") #LINE# #TAB# trans = fr_nlp("" "".join(trans.split()[:])) #LINE# #TAB# trans = "" "".join([token.lower_ for token in trans if not token.is_punct]) #LINE# #TAB# return trans"
def get_suffix(name): #LINE# #TAB# a = name.count('.') #LINE# #TAB# if a: #LINE# #TAB# #TAB# ext = name.split('.')[-1] #LINE# #TAB# #TAB# if ext in LANGS.keys(): #LINE# #TAB# #TAB# #TAB# return ext #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
def compile_markdown(entry): #LINE# #TAB# python_version = entry['python_version'] #LINE# #TAB# if python_version == '0.7': #LINE# #TAB# #TAB# markdown = '```python\n\n' #LINE# #TAB# #TAB# entry['content'] = markdown + entry['description'] + '\n\n' #LINE# #TAB# elif python_version == '2.7': #LINE# #TAB# #TAB# markdown = '```python\n\n' #LINE# #TAB# #TAB# entry['content'] = markdown + entry['description'] + '\n\n' #LINE# #TAB# return python_version
def format_name(value: str): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# value = '__main__' #LINE# #TAB# elif value.startswith('__'): #LINE# #TAB# #TAB# value = value[1:] #LINE# #TAB# elif value.startswith('['): #LINE# #TAB# #TAB# value = value[1:-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value
def translate_vm_state(pvm_state): #LINE# #TAB# if type(pvm_state) == int: #LINE# #TAB# #TAB# pvm_state = pvm_state #LINE# #TAB# power_state = nova.compute.power_state(pvm_state) #LINE# #TAB# return power_state
"def slope_formula(x1, x2, y1, y2): #LINE# #TAB# m = y1 - y2 - y1 / x2 - x1 #LINE# #TAB# return m"
def has_multiple_timesteps(data): #LINE# #TAB# try: #LINE# #TAB# #TAB# if set(data['timestep']) > set(data['timestep']): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False
"def iso_num_weeks(iso_year): #LINE# #TAB# cal_year, cal_ord_days = _get_ordinal(iso_year) #LINE# #TAB# cal_year_next, cal_ord_days_next = _get_ordinal(iso_year + 1) #LINE# #TAB# if cal_ord_days_next == cal_ord_days_next: #LINE# #TAB# #TAB# return cal_year + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return cal_year_next - cal_ord_days + 1"
def t_opt_keyword(t): #LINE# #TAB# t.value = t.value[1:].upper() #LINE# #TAB# return t
"def check_mic_icv(data, mic_key, source, dest): #LINE# #TAB# sa = mac2str(source) #LINE# #TAB# sa += b'\x00' * (4 - len(sa)) #LINE# #TAB# mac = mac2str(dest) #LINE# #TAB# mic = MIC(mic_key, sa, sa + b'\x00' * (4 - len(sa)) + b'\x00' * (4 - len(sa)) + data) #LINE# #TAB# return mic.decrypt(data)[:4]"
"def var_specrad(a: np.ndarray) ->float: #LINE# #TAB# try: #LINE# #TAB# #TAB# return np.sqrt(np.sum(np.square(a), axis=0)) #LINE# #TAB# except np.linalg.LinAlgError: #LINE# #TAB# #TAB# return 0.0"
"def get_output_producers(ssa): #LINE# #TAB# produced = {} #LINE# #TAB# for op_name, ssa_version in ssa.items(): #LINE# #TAB# #TAB# blob_name, version = ssa_version #LINE# #TAB# #TAB# produced[blob_name] = op_index #LINE# #TAB# #TAB# produced[blob_name] = version #LINE# #TAB# return produced"
"def get_point_on_line(x1, y1, x2, y2, n): #LINE# #TAB# p = sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) #LINE# #TAB# if n < 0: #LINE# #TAB# #TAB# return x1, y1 #LINE# #TAB# elif n < 1: #LINE# #TAB# #TAB# return x2, y2 #LINE# #TAB# else: #LINE# #TAB# #TAB# dx = abs(x2 - x1) / n #LINE# #TAB# #TAB# dy = abs(y2 - y1) / n #LINE# #TAB# #TAB# return x1 + dx, y1 + dy"
"def is_tree(pObj): #LINE# #TAB# if isinstance(pObj, ClusterNode): #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(pObj, Hypothesis_Node): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"def xml_records(filename): #LINE# #TAB# rec = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename) as f: #LINE# #TAB# #TAB# #TAB# rec = json.load(f) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print('{}: {}'.format(filename, str(e))) #LINE# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# return rec"
def inject_into_urllib3(): #LINE# #TAB# 'Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.' #LINE# #TAB# _validate_dependencies_met() #LINE# #TAB# util.ssl_.SSLContext = PyOpenSSLContext #LINE# #TAB# util.HAS_SNI = HAS_SNI #LINE# #TAB# util.ssl_.HAS_SNI = HAS_SNI #LINE# #TAB# util.IS_PYOPENSSL = True #LINE# #TAB# util.ssl_.IS_PYOPENSSL = True
"def is_valid_imdb_person_id(value): #LINE# #TAB# if not isinstance(value, str): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not re.match('^[0-9a-fA-F]{8}$', value): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# int(value) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False"
"def catch_keyboard_interrupt(gen, logger=None): #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# if logger: #LINE# #TAB# #TAB# #TAB# logger.info('Keyboard interrupt detected.') #LINE# #TAB# #TAB# raise"
def guess_type(value: str): #LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return float(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return value
def case_1(node): #LINE# #TAB# n = len(node.children) #LINE# #TAB# if n == 2: #LINE# #TAB# #TAB# n2 = node[0] #LINE# #TAB# elif n == 1: #LINE# #TAB# #TAB# n = node[0] #LINE# #TAB# return n2
"def companies_inc5000(size: int=None) ->pd.DataFrame: #LINE# #TAB# _path = Path(AbstractSample._full_path('map_companies_inc5000.csv')) #LINE# #TAB# df = pd.read_csv(_path, encoding='latin1') #LINE# #TAB# return df.iloc[:size]"
"def get_source(source): #LINE# #TAB# if isinstance(source, list): #LINE# #TAB# #TAB# if source[-1] == 'data': #LINE# #TAB# #TAB# #TAB# source = source[0] #LINE# #TAB# #TAB# elif source[-1] == 'file': #LINE# #TAB# #TAB# #TAB# source = source[:-1] #LINE# #TAB# #TAB# elif source[-1] == 'path': #LINE# #TAB# #TAB# #TAB# source = op.abspath(source) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise Exception('Unknown source type {}'.format(source)) #LINE# #TAB# try: #LINE# #TAB# #TAB# return source_registry[source] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None"
"def remove_place(net, place): #LINE# #TAB# if not isinstance(net, PetriNet): #LINE# #TAB# #TAB# raise TypeError('Expected net to be of type PetriNet') #LINE# #TAB# if len(net.places) == 0: #LINE# #TAB# #TAB# return net #LINE# #TAB# tmp = list(net.places) #LINE# #TAB# tmp.remove(place) #LINE# #TAB# net.places = tmp #LINE# #TAB# return net"
"def make_suite_from_cdf(cdf, name=None): #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = cdf.name #LINE# #TAB# s = Suite(name=name) #LINE# #TAB# s.add_rows(cdf.rows) #LINE# #TAB# s.add_edges(cdf.edges) #LINE# #TAB# return s"
"def read_header_marccd2(filename): #LINE# #TAB# f = open(filename, 'rb') #LINE# #TAB# f.seek(2048) #LINE# #TAB# posbyte = 0 #LINE# #TAB# allsentences = '' #LINE# #TAB# for _ in list(range(32)): #LINE# #TAB# #TAB# tt = f.read(32) #LINE# #TAB# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# #TAB# if s1!= '': #LINE# #TAB# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# #TAB# posbyte += 32 #LINE# #TAB# tt = f.read(1024) #LINE# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# if s1!= '': #LINE# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# f.close() #LINE# #TAB# return allsentences"
"def read_json(json_fn): #LINE# #TAB# with open(json_fn) as f: #LINE# #TAB# #TAB# parsed_json = json.load(f) #LINE# #TAB# lines = parsed_json.get('lines', []) #LINE# #TAB# output = [] #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith('{'): #LINE# #TAB# #TAB# #TAB# output.append(line) #LINE# #TAB# return output"
"def get_node_colors_by_attr(G, attr, num_bins=None, cmap='viridis', start=0, stop=1, na_color='none'): #LINE# #TAB# if num_bins is None: #LINE# #TAB# #TAB# num_bins=len(G.nodes()) #LINE# #TAB# bin_labels = range(num_bins) #LINE# #TAB# attr_values = pd.Series([data[attr] for u, v, key, data in G.nodes(data=True)]) #LINE# #TAB# cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels) #LINE# #TAB# colors = get_node_colors(num_bins, cmap, start, stop) #LINE# #TAB# node_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats] #LINE# #TAB# return node_colors"
"def schema_prefix(schema): #LINE# #TAB# if schema == INDEX_DUMMY: #LINE# #TAB# #TAB# return INDEX_DUMMY_PREFIX #LINE# #TAB# if isinstance(schema, dict): #LINE# #TAB# #TAB# return schema #LINE# #TAB# for key in schema.keys(): #LINE# #TAB# #TAB# schema_key = schema[key] #LINE# #TAB# #TAB# if schema_key.startswith(INDEX_PREFIX): #LINE# #TAB# #TAB# #TAB# return schema_key[len(INDEX_PREFIX):] #LINE# #TAB# return INDEX_DUMMY_PREFIX"
"def get_pgroupvolume(module, array): #LINE# #TAB# try: #LINE# #TAB# #TAB# return array.get_pgroupvolume(module.params['name']) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None"
"def contains_sender_names(sender): #LINE# #TAB# for part in sender.split(','): #LINE# #TAB# #TAB# if part.isupper(): #LINE# #TAB# #TAB# #TAB# yield part #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield part"
"def faw_cache_destroy(cache, e): #LINE# #TAB# cache.post_fifo(Event(signal=signals.cache_file_write, payload='')) #LINE# #TAB# return return_status.HANDLED"
"def bc_in_cl_pm73(T, P): #LINE# #TAB# b0 = 0.0215 #LINE# #TAB# b1 = 0.2122 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.00084 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['In'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"def find_element_by_jquery(browser, selector): #LINE# #TAB# elements = find_elements_by_jquery(browser, selector) #LINE# #TAB# return elements[0] if elements else None"
"def unique_value_groups(ar, sort=True): #LINE# #TAB# ar = np.asanyarray(ar) #LINE# #TAB# if sort: #LINE# #TAB# #TAB# ar = ar.sort(key=lambda x: x[1], reverse=True) #LINE# #TAB# seen = set() #LINE# #TAB# result = [] #LINE# #TAB# for val in ar: #LINE# #TAB# #TAB# if val in seen: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# seen.add(val) #LINE# #TAB# #TAB# result.append(val) #LINE# #TAB# return result"
"def force_noupdate(cr, xmlid, noupdate=True, warn=False): #LINE# #TAB# if not noupdate: #LINE# #TAB# #TAB# cr.fetch(xmlid) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del cr.noupdate[xmlid] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# if warn: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# cr.fetch(xmlid) #LINE# #TAB# return cr"
"def str_format(string, width): #LINE# #TAB# new_list = [] #LINE# #TAB# r = len(string) #LINE# #TAB# for i in range(0, r, width): #LINE# #TAB# #TAB# new_list.append(string[i:i + width]) #LINE# #TAB# return new_list"
"def empirical_confidence_interval(sample, interval=0.95): #LINE# #TAB# samples = np.array(sample) #LINE# #TAB# samples.sort() #LINE# #TAB# n = len(samples) #LINE# #TAB# if sample.shape[0] == n: #LINE# #TAB# #TAB# nonzero = sample == 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# nonzero = sample[:, (nonzero)] #LINE# #TAB# #TAB# s_interval = n * interval + s * sample[nonzero, :] #LINE# #TAB# else: #LINE# #TAB# #TAB# s_interval = 0 #LINE# #TAB# print(s_interval) #LINE# #TAB# print('Confidence interval: ', s_interval) #LINE# #TAB# return s_interval"
"def check_nearby_preprocessor(impact_function): #LINE# #TAB# neighbours = impact_function.neighbours #LINE# #TAB# if not neighbours: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not isinstance(neighbours[0], list): #LINE# #TAB# #TAB# return False #LINE# #TAB# if neighbours[0].shape[0] < 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not neighbours[1].shape[0] < 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def is_locked(filepath): #LINE# #TAB# lock = None #LINE# #TAB# pid = os.getpid() #LINE# #TAB# if pid!= 0: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# lock = f.fileno() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if lock == 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"def max_feret(contour): #LINE# #TAB# _, feret = Contours._max_feret_helper(contour) #LINE# #TAB# return feret"
def format_data_dicts_records_for_spreadsheet(data_dicts): #LINE# #TAB# formatted_data_dicts = [] #LINE# #TAB# for data_dict in data_dicts: #LINE# #TAB# #TAB# formatted_data_dict = format_data_dicts_records_for_spreadsheet(data_dict) #LINE# #TAB# #TAB# formatted_data_dicts.append(formatted_data_dict) #LINE# #TAB# return formatted_data_dicts
def get_from_currentdata(key): #LINE# #TAB# res = rundata.get(key) #LINE# #TAB# if res is not None: #LINE# #TAB# #TAB# return res #LINE# #TAB# return ''
"def make_get_text_request(url): #LINE# #TAB# try: #LINE# #TAB# #TAB# result = requests.get(url, headers={'User-Agent': #LINE# #TAB# #TAB# #TAB# 'Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10 (.NET CLR 4.0.20506)'}) #LINE# #TAB# #TAB# result.raise_for_status() #LINE# #TAB# except requests.exceptions.ConnectionError as e: #LINE# #TAB# #TAB# raise ServerError(e) #LINE# #TAB# return result.text"
"def sst_svd(X_test, X_history, n_components): #LINE# #TAB# n_samples, n_features = X_test.shape #LINE# #TAB# X_test_svd = scipy.sparse.linalg.svd(X_test) #LINE# #TAB# sst_train = scipy.sparse.linalg.svd(X_test_svd) #LINE# #TAB# sst_val = np.dot(X_test_svd, sst_train.T) #LINE# #TAB# print('sst_train ='+ str(sst_train)) #LINE# #TAB# print('sst_val ='+ str(sst_val)) #LINE# #TAB# print('n_components ='+ str(n_components)) #LINE# #TAB# return sst_train, sst_val, n_components"
"def generate_sample_order(x_samples, geno_samples): #LINE# #TAB# sample_order = list(sorted(geno_samples)) #LINE# #TAB# for sample_idx in x_samples: #LINE# #TAB# #TAB# if not np.any(sample_idx in geno_samples): #LINE# #TAB# #TAB# #TAB# sample_order.append(sample_idx) #LINE# #TAB# return sample_order"
def remove_dir(dirpath): #LINE# #TAB# try: #LINE# #TAB# #TAB# shutil.rmtree(dirpath) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno!= errno.ENOENT: #LINE# #TAB# #TAB# #TAB# raise
"def init_socks(proxy, timeout): #LINE# #TAB# new_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# new_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# new_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) #LINE# #TAB# proxy.settimeout(timeout) #LINE# #TAB# new_sock.bind(proxy) #LINE# #TAB# return new_sock"
"def boolean_or_dict_field(object_dict, parent_object_dict): #LINE# #TAB# if object_dict.get(parent_object_dict, None)!= 'dict': #LINE# #TAB# #TAB# return False #LINE# #TAB# if not isinstance(object_dict, dict): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def enable_gui(gui, kernel=None): #LINE# #TAB# global gui_enabled, kernel_enabled #LINE# #TAB# gui_enabled = True #LINE# #TAB# if kernel is not None: #LINE# #TAB# #TAB# kernel_enabled = True #LINE# #TAB# try: #LINE# #TAB# #TAB# sys.modules[gui] = kernel #LINE# #TAB# #TAB# gui_enabled = False #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass"
"def eig_leftvec_add_mps(lv, lt1, lt2): #LINE# #TAB# lv = np.tensordot(lv, lt1.conj(), axes=(0, 2)) #LINE# #TAB# lv = np.tensordot(lv, lt2, axes=(0, 2)) #LINE# #TAB# return lv"
"def s_nw_panel(xw, weights, groupidx): #LINE# #TAB# nobs = xw.shape[0] #LINE# #TAB# s = np.zeros((nobs, nobs)) #LINE# #TAB# for grp in groupidx: #LINE# #TAB# #TAB# for ii in range(nobs): #LINE# #TAB# #TAB# #TAB# for kk in range(nobs): #LINE# #TAB# #TAB# #TAB# #TAB# w = xw[grp][ii] #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# w[grp][ii] = weights[grp][ii] #LINE# #TAB# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# s[nobs] = w #LINE# #TAB# return s"
"def max_weight_operator(ops: Iterable[PauliTerm]) -> Union[None, PauliTerm]: #LINE# #TAB# if len(ops) == 1: #LINE# #TAB# #TAB# return ops[0] #LINE# #TAB# elif len(ops) > 1: #LINE# #TAB# #TAB# new_operator = PauliTerm() #LINE# #TAB# #TAB# for op in ops: #LINE# #TAB# #TAB# #TAB# new_operator += max_weight_op(op) #LINE# #TAB# #TAB# return new_operator #LINE# #TAB# return None"
"def dict_to_sequence(d): #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# d[k] = dict_to_sequence(v) #LINE# #TAB# #TAB# #TAB# d[k] = v #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# _dict_to_sequence(v) #LINE# #TAB# #TAB# #TAB# d[k] = [dict_to_sequence(v) for v in v] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d[k] = v #LINE# #TAB# return d"
"def get_latest_block_hash(coin_symbol='btc', api_key=None): #LINE# #TAB# return get_blockchain_overview(coin_symbol=coin_symbol, api_key=api_key)[ #LINE# #TAB# #TAB# 'height']"
def get_hash_from_image(image_file): #LINE# #TAB# hasher = hashlib.sha256() #LINE# #TAB# image_file.seek(0) #LINE# #TAB# while True: #LINE# #TAB# #TAB# data = image_file.read(IMAGE_SIZE) #LINE# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# hasher.update(data) #LINE# #TAB# image_hash = hasher.hexdigest() #LINE# #TAB# return image_hash
"def to_csv(file_path_or_generator): #LINE# #TAB# if isinstance(file_path_or_generator, str): #LINE# #TAB# #TAB# for line in file_path_or_generator: #LINE# #TAB# #TAB# #TAB# line = line.strip('\n') #LINE# #TAB# #TAB# #TAB# cols = line.split(',') #LINE# #TAB# #TAB# #TAB# for a in cols: #LINE# #TAB# #TAB# #TAB# #TAB# yield {'data': a}"
def email_bodies(emails): #LINE# #TAB# result = [] #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# for body in email_body_generator(email): #LINE# #TAB# #TAB# #TAB# result.append(body) #LINE# #TAB# return result
def conv_ev_nm(data): #LINE# #TAB# if data.shape[0] == 3: #LINE# #TAB# #TAB# data = data.copy() #LINE# #TAB# #TAB# for i in range(data.shape[1]): #LINE# #TAB# #TAB# #TAB# data[i] = data[i] / np.sqrt(np.sum(data[i] ** 2)) #LINE# #TAB# conv_ev_nm = data.copy() #LINE# #TAB# for i in range(data.shape[0]): #LINE# #TAB# #TAB# if data[i] < 0: #LINE# #TAB# #TAB# #TAB# conv_ev_nm[i] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# conv_ev_nm[i] = data[i] / np.sqrt(np.sum(data[i] ** 2)) #LINE# #TAB# return conv_ev_nm
"def filter_contaminant(df, config, _filter): #LINE# #TAB# contaminants = df[config['engine'].contaminants == 'contaminant'] #LINE# #TAB# for cell in contaminants: #LINE# #TAB# #TAB# if _filter(cell): #LINE# #TAB# #TAB# #TAB# df = df[df[cell] == _filter(cell)] #LINE# #TAB# return df"
"def get_files_handles(output_dir, barcodes, overwrite, prefix): #LINE# #TAB# file_handles = set() #LINE# #TAB# if not os.path.exists(output_dir): #LINE# #TAB# #TAB# os.makedirs(output_dir) #LINE# #TAB# for filename, barcode in barcodes.items(): #LINE# #TAB# #TAB# file_handles.add(os.path.join(output_dir, filename)) #LINE# #TAB# if overwrite and not os.path.exists(os.path.join(output_dir, filename)): #LINE# #TAB# #TAB# files = sorted(os.listdir(output_dir)) #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# file_handles.add(os.path.join(output_dir, file)) #LINE# #TAB# if not files: #LINE# #TAB# #TAB# os.makedirs(os.path.join(output_dir, filename)) #LINE# #TAB# return file_handles"
"def write_code(): #LINE# #TAB# sz_filename = path.join(path.dirname(__file__),'sz.py') #LINE# #TAB# sh_filename = path.join(path.dirname(path.abspath(__file__)),'sh.py') #LINE# #TAB# with open(sz_filename, 'w') as code_file: #LINE# #TAB# #TAB# code = code_file.read() #LINE# #TAB# sz_sz = open(sz_filename, 'w').readlines() #LINE# #TAB# sz_sz.close() #LINE# #TAB# sh_filename = path.join(path.dirname(path.abspath(__file__)),'sh.py') #LINE# #TAB# with open(sh_filename, 'w') as code_file: # # # #TAB# #TAB# code = code_file.write('\n') #LINE# #TAB# return sz_sz, sh_sz, code"
"def eliminate_consecutive_duplicates(input_list): #LINE# #TAB# no_consecutive_duplicates = [] #LINE# #TAB# for i1, i2 in enumerate(input_list): #LINE# #TAB# #TAB# if i1!= i2: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# no_consecutive_duplicates.append(input_list[i1].tolist()) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# no_consecutive_duplicates.append(input_list[i1]) #LINE# #TAB# return no_consecutive_duplicates"
"def plot_cap_exposures_net(net_exposures, ax=None): #LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# ax = plt.gca() #LINE# #TAB# n_net_exposures = len(net_exposures) #LINE# #TAB# for i, net_exposures_net in enumerate(net_exposures): #LINE# #TAB# #TAB# fig, ax = ax.subplots() #LINE# #TAB# #TAB# ax.scatter(net_exposures_net[i], net_exposures_net[i + 1], c= #LINE# #TAB# #TAB# #TAB# 'compute_cap_exposures_net_{i}'.format(i=i + 1)) #LINE# #TAB# #TAB# ax.set_xlabel('Time [s]', fontsize=12) #LINE# #TAB# #TAB# ax.set_ylabel('Duration [s]', fontsize=10) #LINE# #TAB# return ax"
"def alloc_pinned(num_bytes): #LINE# #TAB# pinned = ctypes.create_string_buffer(num_bytes) #LINE# #TAB# check_call(_LIB.MXAllocPinned(ctypes.byref(pinned))) #LINE# #TAB# if num_bytes > 0: #LINE# #TAB# #TAB# retval = memoryview(pinned) #LINE# #TAB# #TAB# assert retval and isinstance(retval, bytes) #LINE# #TAB# #TAB# return retval #LINE# #TAB# return None"
"def get_title_list(): #LINE# #TAB# title_list = [] #LINE# #TAB# with open('rfc-index.txt', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# split_line = line.split('\t') #LINE# #TAB# #TAB# #TAB# title_list.append(split_line[0]) #LINE# #TAB# return title_list"
"def qfont_from_font(font): #LINE# #TAB# qfont = font.font #LINE# #TAB# if not isinstance(qfont, QFont): #LINE# #TAB# #TAB# raise TypeError('Cannot convert font {} to a QFont'.format(font)) #LINE# #TAB# qfont_dict = {} #LINE# #TAB# for key, value in iteritems(font.properties): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# qfont_dict[key] = value #LINE# #TAB# qfont_dict['face'] = font.face #LINE# #TAB# return qfont"
def sink_wrapper(receiver): #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield receiver.recv() #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# if e.errno == errno.EINTR: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# raise
"def get_person(fake) ->dict: #LINE# #TAB# return {'id': fake.person()['id'], 'context': DOMAIN_CONCEPTS.person, #LINE# #TAB# #TAB# 'title': DOMAIN_CONCEPTS.person}"
def get_module_dir() ->os.PathLike: #LINE# #TAB# module_path = os.path.abspath(__file__) #LINE# #TAB# return module_path
def test_orm_columns(n): #LINE# #TAB# c = sqlalchemy.dialect.create_orm_column(n) #LINE# #TAB# f = select([c.name]).select_from(c.name).order_by(c.name) #LINE# #TAB# for row in f: #LINE# #TAB# #TAB# yield row
"def finger_distance(f1, f2): #LINE# #TAB# d = finger_distance_fast(f1, f2) #LINE# #TAB# if d > 0: #LINE# #TAB# #TAB# return d - 1 #LINE# #TAB# if d < 0: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# return 1"
"def ensure_iterable(item): #LINE# #TAB# if item is None: #LINE# #TAB# #TAB# item = [] #LINE# #TAB# elif is_string(item): #LINE# #TAB# #TAB# item = [item] #LINE# #TAB# elif isinstance(item, collections.Iterable): #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# item = [] #LINE# #TAB# return item"
def get_routing_list(app): #LINE# #TAB# route_list = [] #LINE# #TAB# if app.config.debug: #LINE# #TAB# #TAB# route_list.append('/debug') #LINE# #TAB# routes = [] #LINE# #TAB# for route in app.config.router.routes: #LINE# #TAB# #TAB# if not route.startswith('/'): #LINE# #TAB# #TAB# #TAB# route = '/' + route #LINE# #TAB# #TAB# if route not in route_list: #LINE# #TAB# #TAB# #TAB# route_list.append(route) #LINE# #TAB# return route_list
def chars_to_ranges(s): #LINE# #TAB# char_list = list(s) #LINE# #TAB# char_list.sort() #LINE# #TAB# i = 0 #LINE# #TAB# n = len(char_list) #LINE# #TAB# result = [] #LINE# #TAB# while i < n: #LINE# #TAB# #TAB# code1 = ord(char_list[i]) #LINE# #TAB# #TAB# code2 = code1 + 1 #LINE# #TAB# #TAB# i = i + 1 #LINE# #TAB# #TAB# while i < n and code2 >= ord(char_list[i]): #LINE# #TAB# #TAB# #TAB# code2 = code2 + 1 #LINE# #TAB# #TAB# #TAB# i = i + 1 #LINE# #TAB# #TAB# result.append(code1) #LINE# #TAB# #TAB# result.append(code2) #LINE# #TAB# return result
"def register_django_model(cls: Type[models.Model], type_prefix: str): #LINE# #TAB# if cls in registry.models: #LINE# #TAB# #TAB# return #LINE# #TAB# model_prefix = type_prefix.replace('__', '-') #LINE# #TAB# model = models.Model(type_prefix=type_prefix) #LINE# #TAB# registry.models[cls] = model"
"def array_to_blobproto(arr, diff=None): #LINE# #TAB# arr = np.asarray(arr) #LINE# #TAB# if diff is not None: #LINE# #TAB# #TAB# if arr.ndim == 2: #LINE# #TAB# #TAB# #TAB# diff = tuple([blob_to_blobproto(x) for x in diff]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# diff = blob_to_blobproto(arr) #LINE# #TAB# return arr + diff"
def find_selected(nodes): #LINE# #TAB# selected = None #LINE# #TAB# while selected is None: #LINE# #TAB# #TAB# node = nodes.pop() #LINE# #TAB# #TAB# if node.name == 'nav_extender': #LINE# #TAB# #TAB# #TAB# selected = node #LINE# #TAB# return selected
"def get_incremental_state(module, incremental_state, key): #LINE# #TAB# if incremental_state is not None: #LINE# #TAB# #TAB# full_key = _get_full_incremental_state_key(module, key) #LINE# #TAB# #TAB# if full_key in incremental_state: #LINE# #TAB# #TAB# #TAB# return incremental_state[full_key] #LINE# #TAB# return None"
def config_file_env(): #LINE# #TAB# env_config = os.getenv(CONFIG_FILE_ENV_VAR) #LINE# #TAB# if env_config: #LINE# #TAB# #TAB# return str(env_config) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"def namespace_for(uri: Union[URIRef, Namespace, str]) ->str: #LINE# #TAB# if isinstance(uri, Namespace): #LINE# #TAB# #TAB# return uri.uri #LINE# #TAB# return uri"
"def create_url_identifier(user, password): #LINE# #TAB# if user is not None: #LINE# #TAB# #TAB# user = parse.quote(user.encode('utf-8'), safe=USERINFO_SAFE_CHARS) #LINE# #TAB# #TAB# if password: #LINE# #TAB# #TAB# #TAB# password = parse.quote(password.encode('utf-8'), #LINE# #TAB# #TAB# #TAB# #TAB# safe=USERINFO_SAFE_CHARS) #LINE# #TAB# #TAB# return '{0}:{1}'.format(user, password) #LINE# #TAB# return None"
"def parse_chains(data): #LINE# #TAB# chains = odict() #LINE# #TAB# for line in data.splitlines(True): #LINE# #TAB# #TAB# m = re_chain.match(line) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# policy = None #LINE# #TAB# #TAB# #TAB# if m.group(2)!= '-': #LINE# #TAB# #TAB# #TAB# #TAB# policy = m.group(2) #LINE# #TAB# #TAB# #TAB# chains[m.group(1)] = { #LINE# #TAB# #TAB# #TAB# #TAB# 'policy': policy, #LINE# #TAB# #TAB# #TAB# #TAB# 'packets': int(m.group(3)), #LINE# #TAB# #TAB# #TAB# #TAB# 'bytes': int(m.group(4)), #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# return chains"
def p_raw_dict_chain(t): #LINE# #TAB# if len(t) == 4: #LINE# #TAB# #TAB# t[0] = t[1] + [t[3]] #LINE# #TAB# else: #LINE# #TAB# #TAB# t[0] = t[1]
def file_flags_to_mode(flags): #LINE# #TAB# if flags & os.O_RDONLY: #LINE# #TAB# #TAB# return 'R' #LINE# #TAB# mode = 'w' #LINE# #TAB# if flags & os.O_WRONLY: #LINE# #TAB# #TAB# mode = 'w' #LINE# #TAB# elif flags & os.O_RDWR: #LINE# #TAB# #TAB# mode = 'w' #LINE# #TAB# return mode
"def clean_penn_tree(penn_tree): #LINE# #TAB# penn_tree = re.sub('\\n\\s+','', penn_tree) #LINE# #TAB# penn_tree = re.sub('^\\s+','', penn_tree) #LINE# #TAB# if not penn_tree.strip(): #LINE# #TAB# #TAB# return #LINE# #TAB# lines = penn_tree.split('\n') #LINE# #TAB# if not lines: #LINE# #TAB# #TAB# return #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# clean_line = clean_penn_tree(line) #LINE# #TAB# #TAB# clean_line = re.sub('^\\s+','', clean_line) #LINE# #TAB# #TAB# clean_line = re.sub('\\s+$','', clean_line) #LINE# #TAB# return clean_line"
"def datetime_to_clocktime(index): #LINE# #TAB# index2 = index + pd.to_timedelta(3600, unit='s') #LINE# #TAB# return index2"
"def resolve_configuration_dict(ch, service_name, config): #LINE# #TAB# if ch == 'aws': #LINE# #TAB# #TAB# r = {} #LINE# #TAB# elif ch == 'lambda': #LINE# #TAB# #TAB# r = config #LINE# #TAB# elif not isinstance(config, dict): #LINE# #TAB# #TAB# r = config #LINE# #TAB# resolved = ch #LINE# #TAB# if service_name in config: #LINE# #TAB# #TAB# for key, value in config[service_name].items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# #TAB# value = json.loads(value) #LINE# #TAB# #TAB# #TAB# resolved[service_name] = value #LINE# #TAB# return resolved"
def get_vi_mode(): #LINE# #TAB# if vi_mode is None: #LINE# #TAB# #TAB# return 'vi' #LINE# #TAB# return vi_mode
"def get_config(file=None): #LINE# #TAB# conf_dict = default_config() #LINE# #TAB# if file is None: #LINE# #TAB# #TAB# if os.path.exists(DEFAULT_CONFIG_FILE): #LINE# #TAB# #TAB# #TAB# with open(DEFAULT_CONFIG_FILE, 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# conf_dict = json.load(f) #LINE# #TAB# else: #LINE# #TAB# #TAB# with open(file, 'r') as f: #LINE# #TAB# #TAB# #TAB# conf_dict = json.load(f) #LINE# #TAB# return conf_dict"
"def youtube_dl_available(): #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_output(['youtube-dl', '--help']) #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"def fully_random_weights(n_features, lam_scale, prng): #LINE# #TAB# weights = np.zeros((n_features, n_features)) #LINE# #TAB# n_off_diag = int((n_features ** 2 - n_features) / 2) #LINE# #TAB# weights[np.triu_indices(n_features, k=1)] = 0.1 * lam_scale * prng.randn( #LINE# #TAB# #TAB# n_off_diag) + 0.25 * lam_scale #LINE# #TAB# weights[weights < 0] = 0 #LINE# #TAB# weights = weights + weights.T #LINE# #TAB# return weights"
"def estimate_noiseperbl(data): #LINE# #TAB# datamean = data.mean(axis=2).imag #LINE# #TAB# datameanmin, datameanmax = rtlib.sigma_clip(datamean.flatten()) #LINE# #TAB# good = n.where((datamean > datameanmin) & (datamean < datameanmax)) #LINE# #TAB# noiseperbl = datamean[good].std() #LINE# #TAB# logger.debug('Clipped to %d%% of data (%.3f to %.3f). Noise = %.3f.' % #LINE# #TAB# #TAB# (100.0 * len(good[0]) / len(datamean.flatten()), datameanmin, #LINE# #TAB# #TAB# datameanmax, noiseperbl)) #LINE# #TAB# return noiseperbl"
def get_blocked_reactions(fvaMinmax): #LINE# #TAB# blocked = [] #LINE# #TAB# for rea in fvaMinmax: #LINE# #TAB# #TAB# if rea not in blocked: #LINE# #TAB# #TAB# #TAB# blocked.append(rea) #LINE# #TAB# return blocked
def obd_standards(value): #LINE# #TAB# values = value.split() #LINE# #TAB# standards = [] #LINE# #TAB# for i in range(len(values)): #LINE# #TAB# #TAB# if values[i] == 'obd': #LINE# #TAB# #TAB# #TAB# standards.append(int(values[i])) #LINE# #TAB# #TAB# elif values[i] == 4: #LINE# #TAB# #TAB# #TAB# standards.append(float(values[i])) #LINE# #TAB# for i in range(6): #LINE# #TAB# #TAB# if values[i] == 5: #LINE# #TAB# #TAB# #TAB# standards.append(int(values[i])) #LINE# #TAB# #TAB# elif values[i] == 6: #LINE# #TAB# #TAB# #TAB# standards.append(int(values[i])) #LINE# #TAB# return standards
"def is_block(path): #LINE# #TAB# if not path or not os.path.isfile(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# for name in path.split('/'): #LINE# #TAB# #TAB# file_path = os.path.join(name,'meta.yaml') #LINE# #TAB# #TAB# if not os.path.isfile(file_path): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if _is_tdt_block(file_path): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def trunc_if_integer(n: Any) -> Any: #LINE# #TAB# if cannot_convert_to_integer(n): #LINE# #TAB# #TAB# if isinstance(n, float): #LINE# #TAB# #TAB# #TAB# if n == 0.0: #LINE# #TAB# #TAB# #TAB# #TAB# return 1.0 #LINE# #TAB# #TAB# #TAB# return n #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(n) #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# return n"
"def objects_by_slug(context, slug, limit=5, title=True, text=False): #LINE# #TAB# content_type = ContentType.objects.get_for_model(context) #LINE# #TAB# try: #LINE# #TAB# #TAB# objects = Content.objects.filter(slug=slug, content_type=content_type) #LINE# #TAB# except ObjectDoesNotExist: #LINE# #TAB# #TAB# objects = None #LINE# #TAB# if limit: #LINE# #TAB# #TAB# objects = objects[:limit] #LINE# #TAB# if title: #LINE# #TAB# #TAB# return {'objects': objects, 'title': title} #LINE# #TAB# if text: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'objects': objects, #LINE# #TAB# #TAB# #TAB# 'text': text, #LINE# #TAB# #TAB# }"
"def validate_array(array): #LINE# #TAB# assert_type_or_raise(array, dict, parameter_name='array') #LINE# #TAB# data = Result.validate_array(array) #LINE# #TAB# data['id'] = u(array.get('id')) #LINE# #TAB# data['type'] = u(array.get('type')) #LINE# #TAB# data['name'] = u(array.get('name')) #LINE# #TAB# data['created_at'] = u(array.get('created_at')) if array.get('created_at' #LINE# #TAB# #TAB# ) is not None else None #LINE# #TAB# data['updated_at'] = u(array.get('updated_at')) if array.get('updated_at' #LINE# #TAB# #TAB# ) is not None else None #LINE# #TAB# return data"
"def copyright_years(startyear, sep='-'): #LINE# #TAB# diff = startyear - 1 #LINE# #TAB# if diff < 366: #LINE# #TAB# #TAB# return startyear + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return startyear - 1"
def get_image(image_bytes): #LINE# #TAB# image = Image.open(io.BytesIO(image_bytes)) #LINE# #TAB# return image
"def builtin_set_get(g, rt): #LINE# #TAB# rt._trace('Set get', g) #LINE# #TAB# value = rt._argtypes[g.inx] #LINE# #TAB# g.env[g.inx] = value"
def windows_default_tools_list(): #LINE# #TAB# majorVersion = ctypes.c_int(0) #LINE# #TAB# minorVersion = ctypes.c_int(0) #LINE# #TAB# tools = [] #LINE# #TAB# tools.extend(windows_default_tools_list()) #LINE# #TAB# try: #LINE# #TAB# #TAB# for t in range(majorVersion.value): #LINE# #TAB# #TAB# #TAB# tools.append(windows_default_tools_list[t]) #LINE# #TAB# except WindowsError: #LINE# #TAB# #TAB# tools.append('python') #LINE# #TAB# return tools
"def sr_lookup_extractor(root): #LINE# #TAB# for child in root.iter('softwareReleaseMetadata'): #LINE# #TAB# #TAB# rsr = child.get('softwareReleaseVersion') #LINE# #TAB# #TAB# if rsr: #LINE# #TAB# #TAB# #TAB# LOGGER.debug('Found software release %s', rsr) #LINE# #TAB# #TAB# #TAB# return rsr #LINE# #TAB# return None"
"def _parse_host(cls, host='localhost', port=0): #LINE# #TAB# #TAB# if not host: #LINE# #TAB# #TAB# #TAB# host = 'localhost' #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# port = int(host) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if port < 0: #LINE# #TAB# #TAB# #TAB# port = 443 #LINE# #TAB# #TAB# parsed_host = host.rsplit(':', 1)[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# parsed_host = host #LINE# #TAB# #TAB# #TAB# port = int(parsed_host) #LINE# #TAB# #TAB# return parsed_host, port"
def to_map(value): #LINE# #TAB# result = MapConverter.to_nullable_map(value) #LINE# #TAB# return result if result!= None else {}
"def is_json(content): #LINE# #TAB# if isinstance(content, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# json.loads(content) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if 'type' not in content: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'data' not in content: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"def graph_lasso(X, num_folds): #LINE# #TAB# print('graph_lasso() called') #LINE# #TAB# C = np.zeros((X.shape[1], num_folds)) #LINE# #TAB# for i in range(num_folds): #LINE# #TAB# #TAB# C[:, :, (i)] = scikit_learn.graphlasso_cv(X[:, :, (i)], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# n_folds=num_folds) #LINE# #TAB# return C"
"def preprocess_pca(img, ncomp, normalise_input_image=True, norm_type='perc'): #LINE# #TAB# rchisq = get_rchisq_from_img(img) #LINE# #TAB# out_img = ndimage.preprocessing_image(rchisq, ncomp, norm_type=norm_type) #LINE# #TAB# if norm_type == 'perc': #LINE# #TAB# #TAB# out_img = ndimage.normalization.perc_image(out_img) #LINE# #TAB# return out_img"
"def user_can_edit_snippet_type(user, model): #LINE# #TAB# if user.is_active and user.is_superuser: #LINE# #TAB# #TAB# return True #LINE# #TAB# snippet_models = get_snippet_models(model) #LINE# #TAB# for perm_type in snippet_models: #LINE# #TAB# #TAB# if user_can_edit_snippet_type(user, perm_type): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def get_img_data(toyz_settings, tid, params): #LINE# #TAB# img = session_vars.get('toyz_img') #LINE# #TAB# file_name = session_vars.get('toyz_img_file_%s' % tid) #LINE# #TAB# if file_name: #LINE# #TAB# #TAB# img_data = cv2.imread(file_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# img_data = None #LINE# #TAB# return img_data"
"def pl_inv(P, xm, a): #LINE# #TAB# C = 1 / (-xm / (1 - a) - xm / a + math.exp(a) * xm / a) #LINE# #TAB# return C * P"
"def find_feat_part(artist, albumartist): #LINE# #TAB# matches = [f for f in find_feat_artists(artist, albumartist) if f] #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return None #LINE# #TAB# return matches[0]"
"def create_from_dict(cls, dic): #LINE# #TAB# bars = [] #LINE# #TAB# for key, value in dic.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# bar = cls.create_from_dict(value) #LINE# #TAB# #TAB# #TAB# bars.append(bar) #LINE# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# bars = cls.create_from_list(value) #LINE# #TAB# #TAB# for x in value: #LINE# #TAB# #TAB# #TAB# bars.append(cls.create_from_dict(x)) #LINE# #TAB# #TAB# return bars #LINE# #TAB# return bars"
def format_epilog(parser): #LINE# #TAB# for line in parser.epilog.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if not line or line.startswith('#') or line == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield line
def create_cell(add_synapses=True): #LINE# #TAB# neuron.h.load_file('morphology.hoc') #LINE# #TAB# neuron.h.load_file('biophysics.hoc') #LINE# #TAB# neuron.h.load_file('template.hoc') #LINE# #TAB# print('Loading cell bAC217_L5_BP_d0cc8d7615') #LINE# #TAB# cell = neuron.h.bAC217_L5_BP_d0cc8d7615(1 if add_synapses else 0) #LINE# #TAB# return cell
"def remove_t0_from_wide_db(times_needed, _db): #LINE# #TAB# keys = times_needed.keys() #LINE# #TAB# times_needed_copy = times_needed.copy() #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key not in times_needed_copy: #LINE# #TAB# #TAB# #TAB# times_needed_copy[key] = times_needed_copy[key] - _db[key] #LINE# #TAB# return times_needed_copy"
"def join_prefix(prefix, name): #LINE# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# return name #LINE# #TAB# if prefix.endswith('_'): #LINE# #TAB# #TAB# return name #LINE# #TAB# if name.endswith('__'): #LINE# #TAB# #TAB# return name #LINE# #TAB# return prefix + '.' + name"
"def mkdirs_thread_safe(dst: DirPath) ->None: #LINE# #TAB# if dst.name!= '': #LINE# #TAB# #TAB# mkdirs_thread_safe(dst.parent) #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# while not os.path.exists(dst.parent): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.mkdir(dst.parent, 511) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"def SetUpperTimestamp(cls, timestamp): #LINE# #TAB# if not hasattr(cls, '_upper'): #LINE# #TAB# cls._upper = timestamp #LINE# #TAB# return #LINE# #TAB# if timestamp > cls._upper: #LINE# #TAB# cls._upper = timestamp"
"def load_texture(file_name, resolver): #LINE# #TAB# with open(file_name, 'rb') as f: #LINE# #TAB# #TAB# image = Image.open(f) #LINE# #TAB# #TAB# texture = image.convert('RGB') #LINE# #TAB# #TAB# resolver.resolve(file_name, texture) #LINE# #TAB# return image"
"def attention_layer(cls, attention_input: typing.Any, attention_mask: #LINE# #TAB# typing.Any) ->tf.Tensor: #LINE# #TAB# mask_shape = attention_mask.shape[0] #LINE# #TAB# attention_input_shape = tf.reshape(attention_input, [mask_shape, 1]) #LINE# #TAB# attention_layer_size = tf.shape(attention_input_shape) #LINE# #TAB# output_shape = tf.reshape(attention_input_shape, [attention_layer_size, #LINE# #TAB# #TAB# attention_mask_shape.shape[1]]) #LINE# #TAB# output_attention = tf.reshape(attention_output_shape, [attention_layer_size, 1]) #LINE# #TAB# return output_attention"
"def get_page_and_url(session, url): #LINE# #TAB# r = session.get(url) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# return r.url"
"def _checksum(cls, line): #LINE# #TAB# #TAB# sum = 0 #LINE# #TAB# #TAB# for byte in line: #LINE# #TAB# #TAB# #TAB# sum += ord(byte) #LINE# #TAB# #TAB# return (sum % 256) & 0xffffffff"
def decompose_to_primes(max_prime): #LINE# #TAB# while max_prime!= 2: #LINE# #TAB# #TAB# if max_prime % 2 == 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# max_prime //= 2 #LINE# #TAB# return [2] + [max_prime]
"def find_units(cls, name): #LINE# #TAB# obj = cls._si_unit_objects.get(name, None) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# myokit_unit = cls._si_units.get(name, None) #LINE# #TAB# #TAB# if myokit_unit is None: #LINE# #TAB# #TAB# #TAB# raise CellMLError('Unknown units name ""' + str(name) + '"".') #LINE# #TAB# #TAB# obj = cls(name, myokit_unit, predefined=True) #LINE# #TAB# #TAB# cls._si_unit_objects[name] = obj #LINE# #TAB# return obj"
"def split_list(iterable, condition): #LINE# #TAB# matches, unmatched = [], [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if condition(item): #LINE# #TAB# #TAB# #TAB# matches.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unmatched.append(item) #LINE# #TAB# return matches, unmatched"
def to_gh_query(req): #LINE# #TAB# query = {} #LINE# #TAB# if 'env' in req.env: #LINE# #TAB# #TAB# env = req.env['env'] #LINE# #TAB# #TAB# if env: #LINE# #TAB# #TAB# #TAB# query = dict(env) #LINE# #TAB# #TAB# if 'query' in req.query: #LINE# #TAB# #TAB# #TAB# query['query'] = req.query #LINE# #TAB# return query
"def triangle_area(pt1, pt2, pt3): #LINE# #TAB# s = arccos((pt1[0] - pt2[0]) * (pt1[1] - pt2[1]) + (pt2[0] - pt3[0]) * #LINE# #TAB# #TAB# (pt3[1] - pt1[1])) #LINE# #TAB# return s.area"
"def default_create_thread(callback): #LINE# #TAB# thread = threading.Thread(None, callback) #LINE# #TAB# thread.daemon = True #LINE# #TAB# thread.start() #LINE# #TAB# return thread"
def set_proxy_filter(warningstuple): #LINE# #TAB# if warningstuple[2] is ProxyWarning: #LINE# #TAB# #TAB# return _proxy_map[warningstuple[4]] #LINE# #TAB# else: #LINE# #TAB# #TAB# return warningstuple
def get_sn(): #LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.NoPayload(BaseMsgCode.GetSn)) #LINE# #TAB# return packet
"def gen_topomask(h, lon, lat, dx=1.0, kind=""linear"", plot=False): #LINE# #TAB# if kind == ""linear"": #LINE# #TAB# #TAB# kind = ""linear"" #LINE# #TAB# wind = gen_wind(lon, lat, dx) #LINE# #TAB# mask = wind * h #LINE# #TAB# if plot: #LINE# #TAB# #TAB# plt.close(wind) #LINE# #TAB# topomask = np.zeros(np.shape(mask)) #LINE# #TAB# topomask[mask.astype(int), :] = 0 #LINE# #TAB# return topomask"
"def get_module_task_instance_id(task_instances): #LINE# #TAB# task_instance_ids = [] #LINE# #TAB# for task_instance in task_instances: #LINE# #TAB# #TAB# if isinstance(task_instance, astroid.Module): #LINE# #TAB# #TAB# #TAB# task_instance_ids.append(task_instance.id) #LINE# #TAB# if task_instance_ids: #LINE# #TAB# #TAB# return task_instance_ids[0]"
"def list_regions(service): #LINE# #TAB# command = ['aws','region', 'list', service] #LINE# #TAB# region_cmd =''.join(command) #LINE# #TAB# regions = subprocess.check_output(command).decode('utf-8').split('\n') #LINE# #TAB# region_list = [] #LINE# #TAB# for region in regions: #LINE# #TAB# #TAB# if region: #LINE# #TAB# #TAB# #TAB# region_list.append(region.split(':')[0]) #LINE# #TAB# return region_list"
"def generic_group_update_db(context, group, host, cluster_name): #LINE# #TAB# now = datetime.now() #LINE# #TAB# group.host = host #LINE# #TAB# group.scheduled_at = now #LINE# #TAB# if cluster_name: #LINE# #TAB# #TAB# group.cluster_name = cluster_name #LINE# #TAB# group.scheduled_at = group.scheduled_at.isoformat() #LINE# #TAB# return group"
"def theta_cl_oh_mp98(T, P): #LINE# #TAB# theta = -0.05 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"def instruction_size(op, opc): #LINE# #TAB# res = 0 #LINE# #TAB# while res < opc_size_max: #LINE# #TAB# #TAB# if op in opc_list: #LINE# #TAB# #TAB# #TAB# res += op.size #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res += 1 #LINE# #TAB# return res"
"def all_equal(iterator, test=None): #LINE# #TAB# if test is None: #LINE# #TAB# #TAB# test = lambda x: True #LINE# #TAB# for i in iterator: #LINE# #TAB# #TAB# if not test(i): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield all_equal(i, iterator) #LINE# #TAB# #TAB# except AssertionError: #LINE# #TAB# #TAB# #TAB# pass"
"def transpose_cmd(images, rotate, flip): #LINE# #TAB# if flip: #LINE# #TAB# #TAB# images = [flip] * len(images) #LINE# #TAB# if rotate: #LINE# #TAB# #TAB# images = [rot_cmd(image) for image in images] #LINE# #TAB# else: #LINE# #TAB# #TAB# images = [rotate_cmd(image) for image in images] #LINE# #TAB# return images"
"def split_song(songToSplit, start1, start2): #LINE# #TAB# splitsong = songToSplit.replace(songToSplit[start1:start2], songToSplit[start2:]) #LINE# #TAB# return splitsong, [start1, start2]"
"def load_and_preprocess_imdb_data(n_gram=None): #LINE# #TAB# if n_gram is None: #LINE# #TAB# #TAB# n_gram = get_ngram_features() #LINE# #TAB# X_train, X_test, y_train, y_test = load_imdb_data() #LINE# #TAB# n_gram_hash = hashlib.sha256(n_gram.encode('utf-8')).hexdigest() #LINE# #TAB# X_train = hash_x_features(X_train, n_gram_hash) #LINE# #TAB# y_train = hash_y_features(X_train, n_gram_hash) #LINE# #TAB# return X_train, y_train"
def speed_dial(cls): #LINE# #TAB# if cls._dial is None: #LINE# #TAB# #TAB# from. dial import Speed dial #LINE# #TAB# #TAB# cls._dial = Speed dial() #LINE# #TAB# return cls._dial
"def peer_relation_id(): #LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.must_handle_array = True #LINE# #TAB# function.addParameter('peer_relation_id', dtype='int32', direction= #LINE# #TAB# #TAB# function.OUT) #LINE# #TAB# function.addParameter('index_of_peer', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"def _varslist2axis(cls, fluent: 'TensorFluent', vars_list: List[str]) -> List[int]: #LINE# #TAB# #TAB# axis = [] #LINE# #TAB# #TAB# for var in vars_list: #LINE# #TAB# #TAB# #TAB# if var.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# axis.append(fluent.scope.indices[var].idx) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# axis.append(fluent.scope.indices[var].idx) #LINE# #TAB# #TAB# return axis"
"def ensure_config_exists(config_dir: str, detect_location: bool=True) ->str: #LINE# #TAB# config_path = find_config_file(config_dir) #LINE# #TAB# if config_path is None: #LINE# #TAB# #TAB# print('Unable to find configuration. Creating default one in', #LINE# #TAB# #TAB# #TAB# config_dir) #LINE# #TAB# #TAB# config_path = create_default_config(config_dir, detect_location) #LINE# #TAB# return config_path"
"def connect_su(spec): #LINE# #TAB# return { #LINE# #TAB# #TAB#'method':'su', #LINE# #TAB# #TAB# 'enable_lru': True, #LINE# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# 'username': spec.become_user(), #LINE# #TAB# #TAB# #TAB# 'password': spec.become_pass(), #LINE# #TAB# #TAB# #TAB# 'python_path': spec.python_path(), #LINE# #TAB# #TAB# #TAB#'su_path': spec.become_exe(), #LINE# #TAB# #TAB# #TAB# 'connect_timeout': spec.timeout(), #LINE# #TAB# #TAB# #TAB#'remote_name': get_remote_name(spec), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"def read_long(f): #LINE# #TAB# read_bytes = f.read(4) #LINE# #TAB# return struct.unpack('>l', read_bytes)[0]"
"def by_id(cls, _id, engine_or_session): #LINE# #TAB# ses, auto_close = ensure_session(engine_or_session) #LINE# #TAB# obj = ses.query(cls).get(_id) #LINE# #TAB# if auto_close: #LINE# #TAB# #TAB# ses.close() #LINE# #TAB# return obj"
"def get_ngrams(path): #LINE# #TAB# ngrams = [] #LINE# #TAB# with io.open(path, ""r"", encoding=""utf-8"") as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# word = line.strip() #LINE# #TAB# #TAB# #TAB# ngrams.append(word) #LINE# #TAB# return ngrams"
def task_matcher(details): #LINE# #TAB# if 'task' not in details or 'tasks' not in details['task']: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"def init_group(path): #LINE# #TAB# materials_dict = {} #LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# if filename.endswith('.yaml'): #LINE# #TAB# #TAB# #TAB# materials_dict[filename] = load_yaml(os.path.join(path, filename)) #LINE# #TAB# init_mat = {} #LINE# #TAB# for filename in materials_dict: #LINE# #TAB# #TAB# init_mat[filename] = load_mat(os.path.join(path, filename)) #LINE# #TAB# return init_mat"
def get_rpc_collections(): #LINE# #TAB# global RPC_COLLECTIONS #LINE# #TAB# return RPC_COLLECTIONS
def get_doc_type(doc_type): #LINE# #TAB# doc_type = doc_type.lower() #LINE# #TAB# for declaration in page.DOC_TYPES: #LINE# #TAB# #TAB# if doc_type == declaration.lower(): #LINE# #TAB# #TAB# #TAB# return declaration #LINE# #TAB# return 'UNKNOWN'
"def normals_from_v_f(vertices, faces): #LINE# #TAB# verts = np.array(vertices) #LINE# #TAB# faces = np.array(faces) #LINE# #TAB# is_valid = faces.shape == (3, 3) #LINE# #TAB# if not is_valid: #LINE# #TAB# #TAB# raise ValueError('faces must be (3,3)!') #LINE# #TAB# normals = np.vstack([verts[:, (i)] * np.cos(faces[:, (i)])) for i in #LINE# #TAB# #TAB# range(faces.shape[0])]) #LINE# #TAB# normals = normals / np.linalg.norm(normices) #LINE# #TAB# return normals"
"def quat_from_qx_qy_qz(qx, qy, qz): #LINE# #TAB# w = np.sqrt(qx ** 2 + qy ** 2 + qz ** 2) #LINE# #TAB# if qx.shape[0]!= 2: #LINE# #TAB# #TAB# w = np.real(w) #LINE# #TAB# if qy.shape[0]!= 2: #LINE# #TAB# #TAB# w = np.real(w) #LINE# #TAB# return w"
"def chunk_range(size, chunk_size): #LINE# #TAB# chunk = [] #LINE# #TAB# for x in range(size): #LINE# #TAB# #TAB# while chunk: #LINE# #TAB# #TAB# #TAB# chunk.append(chunk) #LINE# #TAB# #TAB# #TAB# chunk = [] #LINE# #TAB# #TAB# yield chunk"
"def get_direct_dependencies_requirements() ->set: #LINE# #TAB# requirements = set() #LINE# #TAB# with open('Pipfile.lock', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line or line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# requirements.add(line.split(';')[0]) #LINE# #TAB# pipfile_lock_filename = os.path.join('Pipfile', 'lock') #LINE# #TAB# pipfile_lock = PipfileLock.from_filename(pipfile_lock_filename) #LINE# #TAB# for dependency in requirements: #LINE# #TAB# #TAB# if dependency not in pipfile_lock.requires(): #LINE# #TAB# #TAB# #TAB# requirements.add(dependency) #LINE# #TAB# return requirements"
def has_values(l): #LINE# #TAB# if type(l) == list: #LINE# #TAB# #TAB# for i in l: #LINE# #TAB# #TAB# #TAB# if has_values(i): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# elif type(l) == tuple: #LINE# #TAB# #TAB# for i in l: #LINE# #TAB# #TAB# #TAB# if has_values(i): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"def search_file_by_name(name: str, path: Path) ->Optional[Path]: #LINE# #TAB# matches = [f for f in path.rglob('**/*') if f.name == name] #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif len(matches) == 1: #LINE# #TAB# #TAB# return matches[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.debug('found multiple matches for name: %s', name) #LINE# #TAB# #TAB# return None"
"def skip_to_blank(f, spacegroup, setting): #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise SpacegroupNotFoundError( #LINE# #TAB# #TAB# #TAB# #TAB# 'invalid spacegroup %s, setting %i not found in data base' % #LINE# #TAB# #TAB# #TAB# #TAB# ( spacegroup, setting ) ) #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# break"
"def infer_dict(obj): #LINE# #TAB# for ats in (('__len__', 'get', 'has_key', 'items', 'keys', 'values'), ( #LINE# #TAB# #TAB# '__len__', 'get', 'has_key', 'iteritems', 'iterkeys', 'itervalues')): #LINE# #TAB# #TAB# if all(_callable(getattr(obj, a, None)) for a in ats): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"def path_satisfies(predicate, path, value): #LINE# #TAB# try: #LINE# #TAB# #TAB# return predicate(path, value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False"
"def get_activity_type(cls, domain, name, version): #LINE# #TAB# with cls._activity_type_cache_lock: #LINE# #TAB# #TAB# if domain in cls._activity_type_cache: #LINE# #TAB# #TAB# #TAB# return cls._activity_type_cache[domain][name][version] #LINE# #TAB# #TAB# cls._activity_type_cache[domain][name][version] = 1 #LINE# #TAB# #TAB# return 1"
"def number_of_cmds_by_which(cmds_file, which_list): #LINE# #TAB# num_cmds = 0 #LINE# #TAB# with open(cmds_file, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith(which_list): #LINE# #TAB# #TAB# #TAB# #TAB# num_cmds += 1 #LINE# #TAB# return num_cmds"
"def evaluate_resource_requests(time, mem): #LINE# #TAB# time = np.asarray(time) #LINE# #TAB# mem = np.asarray(mem) #LINE# #TAB# requests = [] #LINE# #TAB# for req_id in range(0, len(time)): #LINE# #TAB# #TAB# req_time = time[req_id] / 1000.0 #LINE# #TAB# #TAB# req_mem = mem[req_id] / 1000.0 #LINE# #TAB# #TAB# req = int(req_time) if req_time > 0 else mem[req_id] #LINE# #TAB# #TAB# requests.append(ResourceRequest(req_id, req_mem)) #LINE# #TAB# return requests"
"def codes_get_string_length(handle, key): #LINE# #TAB# size = ffi.new('size_t *') #LINE# #TAB# _codes_get_length(handle, key.encode(ENC), size) #LINE# #TAB# return size[0]"
def parse_mana(mana_div) ->str: #LINE# #TAB# if mana_div == 0: #LINE# #TAB# #TAB# return 'no' #LINE# #TAB# else: #LINE# #TAB# #TAB# mana_str = mana_div.str() #LINE# #TAB# #TAB# if mana_str.endswith('+'): #LINE# #TAB# #TAB# #TAB# return mana_str[:-1] #LINE# #TAB# #TAB# if mana_str.endswith('-'): #LINE# #TAB# #TAB# #TAB# return mana_str[:-1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return mana_str + '-' + str(mana_div) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return mana_str
"def scale_sfs_folded(s, n): #LINE# #TAB# _s = np.empty_like(s) #LINE# #TAB# _s[:n] = s #LINE# #TAB# return _s"
"def pip_kwargs(config_dir: Optional[str]) ->Dict[str, Any]: #LINE# #TAB# kwargs = {'constraints': os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# CONSTRAINT_FILE)} #LINE# #TAB# if config_dir is not None: #LINE# #TAB# #TAB# kwargs['config_dir'] = config_dir #LINE# #TAB# return kwargs"
"def list_vm_snaps(vm_name: str) ->Response: #LINE# #TAB# ps_script = 'Get-VM -VMName ""{}""'.format(vm_name) #LINE# #TAB# rs = run_ps(ps_script) #LINE# #TAB# return rs"
"def load_all(cls, broker): #LINE# #TAB# CleavingContextList = [] #LINE# #TAB# for item in broker.iter_items(): #LINE# #TAB# #TAB# if isinstance(item, CleavingContext): #LINE# #TAB# #TAB# #TAB# cleaving_context = item #LINE# #TAB# #TAB# if isinstance(cleaving_context, CleavingContext): #LINE# #TAB# #TAB# #TAB# cleaving_context.load() #LINE# #TAB# #TAB# #TAB# cleaving_context_list.append((cleaving_context, item.last_modified)) #LINE# #TAB# return cleaving_context_list"
def is_valid(file_path): #LINE# #TAB# try: #LINE# #TAB# #TAB# return not os.path.exists(file_path) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return True
"def get_favicon(url, soup=None): #LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# soup = _get_soup_from_url(url) #LINE# #TAB# favicon = soup.find('link', attrs={'rel': 'icon'}) #LINE# #TAB# if not favicon: #LINE# #TAB# #TAB# return None #LINE# #TAB# favicon = favicon.find('a') #LINE# #TAB# if not favicon: #LINE# #TAB# #TAB# return None #LINE# #TAB# url = url.split('/')[-1] #LINE# #TAB# return url"
"def validate_account_number(number): #LINE# #TAB# match = _account_re.match(number) #LINE# #TAB# if not match: #LINE# #TAB# #TAB# raise ValueError(""Invalid bank account number"") #LINE# #TAB# if not match.group(1).isdigit(): #LINE# #TAB# #TAB# raise ValueError(""Invalid bank account number"") #LINE# #TAB# return True"
"def login_uname_pwd(server, api_key=None): #LINE# #TAB# username = click.prompt('Please enter your One Codex (email)') #LINE# #TAB# if api_key is not None: #LINE# #TAB# #TAB# return username, api_key #LINE# #TAB# password = click.prompt( #LINE# # #TAB# #TAB# 'Please enter your password (typing will be hidden)', hide_input=True) #LINE# #TAB# api_key = fetch_api_key_from_uname(username, password, server) #LINE# #TAB# return username, api_key"
"def find_node(value, node): #LINE# #TAB# while value: #LINE# #TAB# #TAB# value = node.children[0].value #LINE# #TAB# #TAB# if value == node.value: #LINE# #TAB# #TAB# #TAB# return node #LINE# #TAB# return None"
def crc_ccitt(data): #LINE# #TAB# crc = 0 #LINE# #TAB# for b in bytearray(data): #LINE# #TAB# #TAB# crc = crc ^ _CCITT_MASK #LINE# #TAB# #TAB# crc = (crc ^ b) & 0xff #LINE# #TAB# return crc
"def filter_opts(fun): #LINE# #TAB# fun = click.option('--filter-opts', type=click.option('--only', #LINE# #TAB# #TAB# choices=sorted(LIST_FILTER_OPTS, key=lambda k: [k])))(fun) #LINE# #TAB# fun = click.option('--no-filter', type=click.option('--only', #LINE# #TAB# #TAB# choices=sorted(LIST_FILTER_OPTS, key=lambda k: [k])))(fun) #LINE# #TAB# return fun"
"def stretch_signals(source, factor, start_time=None): #LINE# #TAB# if start_time is None: #LINE# #TAB# #TAB# start_time = time.time() #LINE# #TAB# output = pd.DataFrame(source, index=source.index, columns=source.columns #LINE# #TAB# #TAB# ).fillna(start_time) #LINE# #TAB# else: #LINE# #TAB# #TAB# output = source.copy() #LINE# #TAB# #TAB# data = stretched_signals(output, factor, start_time=start_time) #LINE# #TAB# #TAB# output = output.append(data) #LINE# #TAB# return output"
"def validate_master_id(org_client, spec): #LINE# #TAB# master_id = spec.get('master_id') #LINE# #TAB# if not master_id: #LINE# #TAB# #TAB# raise InvalidMasterId(spec) #LINE# #TAB# if org_client.get_org(spec.get('org'), org_client.get_org_name() #LINE# #TAB# #TAB# )!= master_id: #LINE# #TAB# #TAB# raise InvalidMasterId(spec) #LINE# #TAB# return org_client"
def ply_load(fn): #LINE# #TAB# m = Manifold() #LINE# #TAB# m.from_file(fn) #LINE# #TAB# return m
"def get_remote_data_array(backupID, blockNum): #LINE# #TAB# customer_idurl = packetid.CustomerIDURL(backupID) #LINE# #TAB# if backupID not in local_backup_ids(): #LINE# #TAB# #TAB# return [0] * contactsdb.num_suppliers(customer_idurl=customer_idurl) #LINE# #TAB# if blockNum not in local_backup_ids()[backupID]: #LINE# #TAB# #TAB# return [0] * contactsdb.num_suppliers(customer_idurl=customer_idurl) #LINE# #TAB# return local_backup_ids()[backupID][blockNum]['RemoteData']"
"def output_xml(data, code, headers=None): #LINE# #TAB# resp = make_response(dumps({'response': data}), code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
"def get_log_buffer(prefix, buff): #LINE# #TAB# new_buffer = '' #LINE# #TAB# offset = 0 #LINE# #TAB# while offset < len(buff): #LINE# #TAB# #TAB# new_buffer += buff[offset:offset + 4] #LINE# #TAB# #TAB# offset += 4 #LINE# #TAB# #TAB# if buff[offset:offset + 8] == b'\x00': #LINE# #TAB# #TAB# #TAB# new_buffer += b'\x00' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_buffer += b'\x00' #LINE# #TAB# return new_buffer"
"def ok_kwarg(val): #LINE# #TAB# if val is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if not isinstance(val, bool): #LINE# #TAB# #TAB# return False #LINE# #TAB# return val"
def capture_stdout(): #LINE# #TAB# capture_stdout = sys.stdout #LINE# #TAB# try: #LINE# #TAB# #TAB# capture_stdout = StringIO() #LINE# #TAB# #TAB# sys.stdout = capture_stdout #LINE# #TAB# #TAB# yield capture_stdout #LINE# #TAB# finally: #LINE# #TAB# #TAB# sys.stdout = capture_stdout
"def find_scene_textures(): #LINE# #TAB# paths = set() #LINE# #TAB# for f in maya.cmds.ls(long=True, type='file'): #LINE# #TAB# #TAB# if maya.cmds.referenceQuery(f, isNodeReferenced=True): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# texture_path = maya.cmds.getAttr(os.path.normpath('.'.join([f, #LINE# #TAB# #TAB# #TAB# 'fileTextureName']))) #LINE# #TAB# #TAB# if texture_path: #LINE# #TAB# #TAB# #TAB# paths.add(texture_path) #LINE# #TAB# return paths"
"def find_file_format(file_name): #LINE# #TAB# for format in formats: #LINE# #TAB# #TAB# if format.match(file_name): #LINE# #TAB# #TAB# #TAB# return file_name, format #LINE# #TAB# return None, None"
"def initialise_commodity_sources(): #LINE# #TAB# source_df = pd.DataFrame({'commodity_source_id':'source_id', #LINE# #TAB# #TAB# 'commodity_source_short_name':'source_short_name'}) #LINE# #TAB# source_df['commodity_source_id'] ='source_id' #LINE# #TAB# source_df['commodity_source_short_name'] ='source_short_name' #LINE# #TAB# get_results(source_df) #LINE# #TAB# get_results(source_df) #LINE# #TAB# return source_df"
"def sort_response(response: Dict[str, Any]) -> OrderedDict: #LINE# #TAB# sorted_response = OrderedDict() #LINE# #TAB# sorted_response.update(response) #LINE# #TAB# for key, value in response.items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# value = sorted(value) #LINE# #TAB# #TAB# sorted_response[key] = value #LINE# #TAB# return sorted_response"
"def get_sig_string(req, cano_req, scope): #LINE# #TAB# sig = ""%s:%s"" % (cano_req['region'], scope) #LINE# #TAB# if req.get('signature'): #LINE# #TAB# #TAB# sig = ""%s:%s"" % (sig, req['signature']) #LINE# #TAB# return sig"
def cio_tell(cio): #LINE# #TAB# pos = cio.tell() #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# pos = cio.read(1) #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return pos
"def shrink_bb(bb, factor=_EPS): #LINE# #TAB# assert 0 < bb.shape[1] <= 2 #LINE# #TAB# assert 0 < factor <= 1 #LINE# #TAB# for i in range(2): #LINE# #TAB# #TAB# if bb[i] > bb[i + 1] * factor: #LINE# #TAB# #TAB# #TAB# bb[i] -= factor #LINE# #TAB# return bb"
"def get_chr_key(chr_str: str): #LINE# #TAB# chrom_to_key = {int(x): x for x in ['A', 'C', 'G', 'T']} #LINE# #TAB# try: #LINE# #TAB# #TAB# key = chrom_to_key[chr_str] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# key = None #LINE# #TAB# return key"
"def is_valid_path(path, urlconf=None): #LINE# #TAB# if urlconf is None: #LINE# #TAB# #TAB# urlconf = get_default_urlconf() #LINE# #TAB# try: #LINE# #TAB# #TAB# return bool(urlconf.resolve(path)) #LINE# #TAB# except Resolver404: #LINE# #TAB# #TAB# return False"
def is_notebook(): #LINE# #TAB# try: #LINE# #TAB# #TAB# shell = get_ipython().__class__.__name__ #LINE# #TAB# #TAB# if shell == 'ZMQInteractiveShell': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif shell == 'TerminalInteractiveShell': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False
"def check_thickness(s): #LINE# #TAB# s = check_1d(s) #LINE# #TAB# if any(map(lambda d: d <= 0, s)): #LINE# #TAB# #TAB# raise ValueError('thickness must be positive') #LINE# #TAB# if any(map(lambda d: d <= 1, s)): #LINE# #TAB# #TAB# raise ValueError('thickness must be positive') #LINE# #TAB# return s"
"def remove_nexusnve_binding(vni, switch_ip, device_id): #LINE# #TAB# LOG.debug('remove_nexusnve_binding() called') #LINE# #TAB# session = bc.get_writer_session() #LINE# #TAB# binding = session.query(nexus_models_v2.NexusNVEBinding).filter_by(vni #LINE# #TAB# #TAB# =vni, switch_ip=switch_ip, device_id=device_id).one() #LINE# #TAB# if binding: #LINE# #TAB# #TAB# session.delete(binding) #LINE# #TAB# #TAB# session.flush() #LINE# #TAB# #TAB# return binding"
def try_parse_date(value): #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse_date(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None
"def shuffle_reverse(lst, key): #LINE# #TAB# if len(lst) > 1: #LINE# #TAB# #TAB# reverse_list = list(zip(lst[:-1], lst[1:])) #LINE# #TAB# #TAB# for i in range(len(reverse_list)): #LINE# #TAB# #TAB# #TAB# reverse_list[i] = lst[i][key] #LINE# #TAB# #TAB# return reverse_list #LINE# #TAB# else: #LINE# #TAB# #TAB# return lst"
"def get_repositories(request): #LINE# #TAB# return [{'id': repository.id, 'name': repository.name} for repository in #LINE# #TAB# #TAB# Repository.query.all()]"
"def get_latest_operative_config(restore_dir): #LINE# #TAB# operative_configs = [] #LINE# #TAB# for root, _, files in os.walk(restore_dir): #LINE# #TAB# #TAB# for name in files: #LINE# #TAB# #TAB# #TAB# if name == 'operative_config': #LINE# #TAB# #TAB# #TAB# #TAB# operative_configs.append((os.path.basename(root), name)) #LINE# #TAB# if len(operative_configs) == 0: #LINE# #TAB# #TAB# raise Exception('No saved operative_config in'+ restore_dir) #LINE# #TAB# return operative_configs[0]"
"def activity_2_freq(trace): #LINE# #TAB# if not isinstance(trace, tuple): #LINE# #TAB# #TAB# trace = (trace,) #LINE# #TAB# activity_to_freq = {} #LINE# #TAB# for i in range(len(trace)): #LINE# #TAB# #TAB# activity = trace[i] #LINE# #TAB# #TAB# if activity not in activity_to_freq: #LINE# #TAB# #TAB# #TAB# activity_to_freq[activity] = 0 #LINE# #TAB# #TAB# activity_to_freq[activity] += 1 #LINE# #TAB# return activity_to_freq"
"def get_version(version): #LINE# #TAB# main = get_main_version(version) #LINE# #TAB# sub = '' #LINE# #TAB# if version[3]!= 'final': #LINE# #TAB# #TAB# mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'} #LINE# #TAB# #TAB# sub = mapping[version[3]] + str(version[4]) #LINE# #TAB# return main + sub"
"def subcommand_show(api: API) ->int: #LINE# #TAB# version = get_version(api) #LINE# #TAB# if version.major == 0 and 9 > version.minor >= 7: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""Future change warning: command'show' will be renamed 'purge' in version 0.9.0, with an 'autoremove' alias."" #LINE# #TAB# #TAB# #TAB#, file=sys.stderr) #LINE# #TAB# return 0"
"def pkg_config_header_strings(pkg_libraries): #LINE# #TAB# _, _, header_dirs = pkg_config(pkg_libraries) #LINE# #TAB# header_strings = [] #LINE# #TAB# for header_dir in header_dirs: #LINE# #TAB# #TAB# header_strings.append(""-I"" + header_dir) #LINE# #TAB# return header_strings"
"def parse_rgb_txt_file(path): #LINE# #TAB# obj = {} #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# parts = line.rstrip('\n').split() #LINE# #TAB# #TAB# #TAB# assert len(parts) == 3 #LINE# #TAB# #TAB# #TAB# if parts[0] == '1': #LINE# #TAB# #TAB# #TAB# #TAB# obj[parts[0]] = int(parts[1]) #LINE# #TAB# #TAB# #TAB# if parts[2] == '0': #LINE# #TAB# #TAB# #TAB# #TAB# obj[parts[2]] = 0 #LINE# #TAB# #TAB# #TAB# if parts[3] == '1': #LINE# #TAB# #TAB# #TAB# #TAB# obj[parts[3] = int(parts[4]) #LINE# #TAB# return obj"
"def to_rest_rels(model, props): #LINE# #TAB# for prop_name, prop in model.relationships.items(): #LINE# #TAB# #TAB# rel = props[prop_name] #LINE# #TAB# #TAB# if isinstance(rel, Relationship): #LINE# #TAB# #TAB# #TAB# props[prop_name] = { #LINE# #TAB# #TAB# #TAB# #TAB# 'target': rel.related_model, #LINE# #TAB# #TAB# #TAB# #TAB#'source': rel.source, #LINE# #TAB# #TAB# #TAB# #TAB# 'type': rel.related_model.__class__.__name__, #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# #TAB# elif isinstance(rel, models.ManyToManyRel): #LINE# #TAB# #TAB# #TAB# props[prop_name] = { #LINE# #TAB# #TAB# #TAB# #TAB#'source': rel.source, #LINE# #TAB# #TAB# #TAB# }"
"def list_escher_maps(): #LINE# #TAB# import os #LINE# #TAB# map_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),'maps')) #LINE# #TAB# maps = [] #LINE# #TAB# files = os.listdir(map_dir) #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# if f.endswith('.pyc'): #LINE# #TAB# #TAB# #TAB# map_file = os.path.join(map_dir, f) #LINE# #TAB# #TAB# #TAB# if os.path.isfile(map_file): #LINE# #TAB# #TAB# #TAB# #TAB# maps.append(f) #LINE# #TAB# return maps"
"def wallet_config(wallet_name): #LINE# #TAB# return {'name': wallet_name, 'roles': [{'name':'master', 'type': 'int', #LINE# #TAB# #TAB# 'value': '0'}, {'name': 'private', 'type': 'int', 'value': #LINE# #TAB# #TAB# '1'}, {'name': 'private', 'type': 'bool', 'value': True}, {'name': #LINE# #TAB# #TAB# 'public_key', 'value': '0.0.0.0', 'type': 'bool', 'value': #LINE# #TAB# #TAB# '0.0.0.0'}}, {'name': 'private', 'type': 'int', 'value': #LINE# #TAB# #TAB# '1'}]}"
def is_number(s): #LINE# #TAB# try: #LINE# #TAB# #TAB# int(s) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"def get_flux_cmd(flux_path, no_errors=False): #LINE# #TAB# cmd = 'flux -f {flux_path}'.format(flux_path=flux_path) #LINE# #TAB# if no_errors: #LINE# #TAB# #TAB# cmd +='-v' #LINE# #TAB# return cmd"
"def server_group_list(request): #LINE# #TAB# try: #LINE# #TAB# #TAB# return api.nova.server_group_list(request) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# exceptions.handle(request, _('Unable to retrieve Nova server groups.')) #LINE# #TAB# #TAB# return []"
"def json_add_collection_dir(file_name, force=True): #LINE# #TAB# current_dir = os.getcwd() #LINE# #TAB# json_file_path = os.path.join(current_dir, file_name) #LINE# #TAB# if force or not os.path.isfile(json_file_path): #LINE# #TAB# #TAB# with open(json_file_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# json_collection_dir = os.path.join(current_dir, json_file_path) #LINE# #TAB# #TAB# with open(json_collection_dir, 'w') as f: #LINE# #TAB# #TAB# #TAB# json.dump(data, f, indent=2, sort_keys=True) #LINE# #TAB# else: #LINE# #TAB# #TAB# json_collection_dir = current_dir #LINE# #TAB# return json_collection_dir"
"def override_dict(a, b, path=None): #LINE# #TAB# if not path: #LINE# #TAB# #TAB# path = [] #LINE# #TAB# for key in b: #LINE# #TAB# #TAB# if key in a: #LINE# #TAB# #TAB# #TAB# if isinstance(a[key], dict) and isinstance(b[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# override_dict(a[key], b[key]) #LINE# #TAB# #TAB# #TAB# elif a[key]!= b[key]: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# a[key] = b[key] #LINE# #TAB# return a"
"def check_delta_unique_ids(delta): #LINE# #TAB# for item in delta: #LINE# #TAB# #TAB# file_id = item[3] #LINE# #TAB# #TAB# if file_id is not None: #LINE# #TAB# #TAB# #TAB# yield item #LINE# #TAB# #TAB# elif item[2] is None: #LINE# #TAB# #TAB# #TAB# raise errors.InconsistentDelta(file_id, item[2]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield item"
"def get_units(obs, filter_fn=None, owner=None, unit_type=None, tag=None): #LINE# #TAB# filter_fn = filter_fn or _default_filter_fn #LINE# #TAB# filter_fn = _unit_filter_fn(filter_fn, owner, unit_type, tag) #LINE# #TAB# units = {} #LINE# #TAB# for unit in obs.units: #LINE# #TAB# #TAB# if filter_fn(unit): #LINE# #TAB# #TAB# #TAB# units[unit.name] = unit #LINE# #TAB# return units"
"def is_physical_entity(pe): #LINE# #TAB# val = isinstance(pe, _bp('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('Entity')) #LINE# #TAB# return val"
"def introspect_table(table): #LINE# #TAB# table_desc = table.describe() #LINE# #TAB# shelf = {'name': table.name, 'description': table_desc.description, 'columns': #LINE# #TAB# #TAB# list(table.columns.keys())} #LINE# #TAB# shelf['type'] = table_desc.type #LINE# #TAB# if isinstance(table.primary_key, Column): #LINE# #TAB# #TAB# shelf['primary_key'] = table.primary_key.name #LINE# #TAB# if isinstance(table.unique_key, Integer): #LINE# #TAB# #TAB# shelf['unique_key'] = table.unique_key.name #LINE# #TAB# if isinstance(table.options, ColumnOptions): #LINE# #TAB# #TAB# shelf['options'] = table.options.to_dict() #LINE# #TAB# return shelf"
def assert_secure_file(file): #LINE# #TAB# secure = os.path.isfile(file) #LINE# #TAB# if secure: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def educate_dashes_old_school(text): #LINE# #TAB# text = re.sub('---\\d+', '---', text) #LINE# #TAB# text = re.sub('---\\d+', '---', text) #LINE# #TAB# return text"
"def _is_expired_token_response(cls, response): #LINE# #TAB# #TAB# EXPIRED_MESSAGE = 'Expired oauth2 access token' #LINE# #TAB# #TAB# INVALID_MESSAGE = 'Invalid oauth2 access token' #LINE# #TAB# #TAB# if response.status_code == 400: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# body = response.json() #LINE# #TAB# #TAB# #TAB# #TAB# if len(body) > EXPIRED_MESSAGE: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return False"
"def encode_path(s): #LINE# #TAB# if isinstance(s, bytes): #LINE# #TAB# #TAB# return s.encode('utf-8') #LINE# #TAB# return s"
"def remove_regex(urls, regex): #LINE# #TAB# if not regex: #LINE# #TAB# #TAB# return urls #LINE# #TAB# if not isinstance(urls, list): #LINE# #TAB# #TAB# urls = [urls] #LINE# #TAB# elif isinstance(urls, str): #LINE# #TAB# #TAB# urls = [urls] #LINE# #TAB# try: #LINE# #TAB# #TAB# compiled = re.compile(regex) #LINE# #TAB# except re.error: #LINE# #TAB# #TAB# return urls #LINE# #TAB# for url in urls: #LINE# #TAB# #TAB# if compiled.search(url): #LINE# #TAB# #TAB# #TAB# urls.remove(url) #LINE# #TAB# return urls"
"def is_likely_benign(bs_terms, bp_terms): #LINE# #TAB# if not bs_terms and not bp_terms: #LINE# #TAB# #TAB# return False #LINE# #TAB# if bs_terms[0] == bs_terms[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not bp_terms[0]: #LINE# #TAB# #TAB# return False #LINE# #TAB# for bs_term, bp_term in zip(bs_terms, bp_terms): #LINE# #TAB# #TAB# if not is_likely_benign(bs_term, bp_term): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"def set_live(cls): #LINE# #TAB# for key, value in vars(cls).items(): #LINE# #TAB# #TAB# setattr(cls, key, value) #LINE# #TAB# return cls"
"def emitter_9(): #LINE# #TAB# e = arcade.Emitter(center_xy=CENTER_POS, emit_controller=arcade. #LINE# #TAB# #TAB# EmitBurst(BURST_PARTICLE_COUNT // 4), particle_factory=lambda #LINE# #TAB# #TAB# emitter: arcade.LifetimeParticle(filename_or_texture=TEXTURE, #LINE# #TAB# #TAB# change_xy=arcade.rand_vec_magnitude(45, 1.0, 4.0), lifetime= #LINE# #TAB# #TAB# DEFAULT_PARTICLE_LIFETIME, scale=DEFAULT_SCALE, alpha=DEFAULT_ALPHA)) #LINE# #TAB# return emitter_9.__doc__, e"
"def get_last_modified_timestamp(path, ignore=None): #LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# return None #LINE# #TAB# ignore = ignore or [] #LINE# #TAB# entries = os.listdir(path) #LINE# #TAB# entries.sort(key=os.path.getmtime) #LINE# #TAB# try: #LINE# #TAB# #TAB# return max([(get_last_modified_timestamp(e, ignore) if ignore is None else #LINE# #TAB# #TAB# #TAB# get_last_modified_timestamp(e, ignore)) for e in entries]) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
def clean_toml_text(input_toml: str) ->str: #LINE# #TAB# cleaned_input_toml = toml.loads(input_toml) #LINE# #TAB# return cleaned_input_toml
def index_to_nice_name(index): #LINE# #TAB# if index: #LINE# #TAB# #TAB# return '.'.join(index_to_nice_name(i) for i in range(index + 1)) #LINE# #TAB# return 'index'
"def create_magnitude_spectrum(image): #LINE# #TAB# if len(image.shape) == 2: #LINE# #TAB# #TAB# mag_spectrum = image.reshape(image.shape[0], image.shape[1], 1) #LINE# #TAB# elif len(image.shape) == 3: #LINE# #TAB# #TAB# mag_spectrum = image.reshape(image.shape[0], image.shape[1], 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('The input image should have 2 or 3 dimensions') #LINE# #TAB# return mag_spectrum"
"def is_history_file(path): #LINE# #TAB# is_history = False #LINE# #TAB# if path: #LINE# #TAB# #TAB# if isfile(path): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# mode = open(path, 'r').readline().strip() #LINE# #TAB# #TAB# #TAB# #TAB# if mode == 'history': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# is_history = True #LINE# #TAB# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return is_history"
"def translate_exception(exp: Exception) ->Exception: #LINE# #TAB# if isinstance(exp, CommException): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# message = exp.message #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# message = exp.__class__.__name__ #LINE# #TAB# else: #LINE# #TAB# #TAB# message = str(exp) #LINE# #TAB# return message"
def users_unlock(login_or_id): #LINE# #TAB# try: #LINE# #TAB# #TAB# lock = UserLock.objects.get(login_or_id=login_or_id) #LINE# #TAB# #TAB# if lock.locked: #LINE# #TAB# #TAB# #TAB# session.delete(lock.token) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# session.delete(lock) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# except UserLock.DoesNotExist: #LINE# #TAB# #TAB# return False
"def extract_derivative_feature(feature): #LINE# #TAB# derivative_feature = {} #LINE# #TAB# derivatives = ['L', 'R', 'T'] #LINE# #TAB# for derivative in derivatives: #LINE# #TAB# #TAB# if isinstance(derivative[0], str): #LINE# #TAB# #TAB# #TAB# derivative_feature[derivative[0]] = derivative[1] #LINE# #TAB# #TAB# elif isinstance(derivative[1], str): #LINE# #TAB# #TAB# #TAB# derivative_feature[derivative[1]] = derivative[0] #LINE# #TAB# return derivative_feature"
"def get_bounding_box(in_file): #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# line = next(in_handle) #LINE# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# m = re.search('([0-9]+)x([0-9]+)', line) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# x = m.group(1) #LINE# #TAB# #TAB# #TAB# y = m.group(2) #LINE# #TAB# #TAB# #TAB# w = m.group(3) #LINE# #TAB# #TAB# #TAB# h = m.group(4) #LINE# #TAB# #TAB# #TAB# box = np.array(w.flatten(), np.int32) #LINE# #TAB# #TAB# #TAB# return box"
def convert_yaml_to_bool(_yaml_bool_value): #LINE# #TAB# if type(_yaml_bool_value) is bool: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"def end_request(req, collector_addr='tcp://127.0.0.2:2345', prefix='my_app'): #LINE# #TAB# req.environ.setdefault('REMOTE_ADDR', collector_addr) #LINE# #TAB# req.environ['REMOTE_ADDR'] = collector_addr #LINE# #TAB# req.environ['SERVER_PORT'] = collector_addr #LINE# #TAB# _end_requests.append(req) #LINE# #TAB# return req"
"def get_min_isr(zk, topic): #LINE# #TAB# try: #LINE# #TAB# #TAB# return zk.get_min_isr(topic) #LINE# #TAB# except kazoo.client.NoNodeError: #LINE# #TAB# #TAB# return None"
"def safe_minmax(values): #LINE# #TAB# mins = [] #LINE# #TAB# maxes = [] #LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if val!= 0.0: #LINE# #TAB# #TAB# #TAB# if val < 0.0: #LINE# #TAB# #TAB# #TAB# #TAB# mins.append(val) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# maxes.append(val) #LINE# #TAB# #TAB# elif val > np.inf: #LINE# #TAB# #TAB# #TAB# maxes.append(val) #LINE# #TAB# if not mins: #LINE# #TAB# #TAB# mins = np.nan #LINE# #TAB# if not maxes: #LINE# #TAB# #TAB# maxes = np.nan #LINE# #TAB# return mins, maxes"
