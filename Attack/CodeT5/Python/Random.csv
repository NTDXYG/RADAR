#LINE# #TAB# if c == '\n': #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# i = int(c) #LINE# #TAB# #TAB# if i < 10: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return True
"#LINE# #TAB# minimum = ctypes.c_float() #LINE# #TAB# maximum = ctypes.c_float() #LINE# #TAB# _Get_Exception_From_thrift_result(ctypes.c_void_p(p_state), #LINE# #TAB# #TAB# ctypes.c_int(idx_image), ctypes.c_int(idx_chain)) #LINE# #TAB# return minimum, maximum"
"#LINE# #TAB# if not argv: #LINE# #TAB# #TAB# return None, {} #LINE# #TAB# command_name, args = argv[1:] #LINE# #TAB# if command_name.startswith('connect'): #LINE# #TAB# #TAB# command_name = 'connect' #LINE# #TAB# if command_name == 'list': #LINE# #TAB# #TAB# command_name = 'list' #LINE# #TAB# elif command_name =='set': #LINE# #TAB# #TAB# command_name ='set' #LINE# #TAB# args = argv[1:] #LINE# #TAB# return command_name, args"
#LINE# #TAB# raw.byteorder = Byteorder(raw.read(1)) #LINE# #TAB# raw.read(3) #LINE# #TAB# marker = raw.read(1) #LINE# #TAB# if marker == b'\x03\x00\x00\x00': #LINE# #TAB# #TAB# return #LINE# #TAB# marker_len = 2 #LINE# #TAB# if marker_len <= len(raw): #LINE# #TAB# #TAB# return #LINE# #TAB# reference_cluster = raw[marker_len:] #LINE# #TAB# if reference_cluster == b'': #LINE# #TAB# #TAB# reference_cluster = raw[marker_len + 1:] #LINE# #TAB# return reference_cluster
#LINE# #TAB# if gain > 1: #LINE# #TAB# #TAB# return 'Gain' #LINE# #TAB# elif gain < 0: #LINE# #TAB# #TAB# return 'Pain' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Peak'
"#LINE# #TAB# if isinstance(e, socket.error) and e.errno in ('503', '408', '500'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(e, BadStatusLine): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# c_type = page.headers.get('content-type') #LINE# #TAB# if not c_type: #LINE# #TAB# #TAB# return #LINE# #TAB# pos = c_type.find(';') #LINE# #TAB# if pos == -1: #LINE# #TAB# #TAB# return #LINE# #TAB# c_type = c_type[:pos] #LINE# #TAB# i = c_type.find(';') #LINE# #TAB# if i >= 0: #LINE# #TAB# #TAB# pos = c_type.find(';') #LINE# #TAB# else: #LINE# #TAB# #TAB# i = c_type.rfind(';') #LINE# #TAB# return { #LINE# #TAB# #TAB# 'Content-Type': c_type[:i], #LINE# #TAB# #TAB# 'Accept-Encoding': 'gzip' #LINE# #TAB# }"
"#LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fvalue = float(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if minimum is not None and fvalue < minimum: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if maximum is not None and fvalue > maximum: #LINE# #TAB# #TAB# return False #LINE# #TAB# if cut: #LINE# #TAB# #TAB# return True #LINE# #TAB# if pad: #LINE# #TAB# #TAB# return round(fvalue, 4) #LINE# #TAB# else: #LINE# #TAB# #TAB# return fvalue"
"#LINE# #TAB# for path, _, files in os.walk(dist): #LINE# #TAB# #TAB# for file in files: #LINE# #TAB# #TAB# #TAB# if file.endswith('.py'): #LINE# #TAB# #TAB# #TAB# #TAB# with open(os.path.join(path, file)) as f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield path, line"
"#LINE# #TAB# n = len(signal) #LINE# #TAB# center = signal[0] #LINE# #TAB# width = signal[0] - center #LINE# #TAB# half_width = signal[1] - center #LINE# #TAB# return center, width, half_width"
#LINE# #TAB# sp = ctx.command['module'].split(' ') #LINE# #TAB# if len(sp) == 1: #LINE# #TAB# #TAB# return sp[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# m = pkg_resources.get_distribution(sp[0]) #LINE# #TAB# #TAB# out_obj = m.out_object #LINE# #TAB# #TAB# return out_obj
"#LINE# #TAB# auth_user = os.environ.get('AUTH_USER') #LINE# #TAB# auth_pass = os.environ.get('AUTH_PASSWORD') #LINE# #TAB# client = keystonev2.Client(auth_user=auth_user, auth_pass=auth_pass) #LINE# #TAB# try: #LINE# #TAB# #TAB# client.trust(trust_id) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# LOGGER.error('Unable to get client for trust %s', trust_id) #LINE# #TAB# #TAB# raise #LINE# #TAB# return client.id"
"#LINE# #TAB# delim1, delim2, delim3 = delims #LINE# #TAB# items = [] #LINE# #TAB# start = 0 #LINE# #TAB# for delim in delim1: #LINE# #TAB# #TAB# end = start + len(delim) #LINE# #TAB# #TAB# items.append(txt[start:end]) #LINE# #TAB# #TAB# start = end #LINE# #TAB# return items"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# bool_str = str_2_bool(raw_input(prompt)) #LINE# #TAB# except PipelineError: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""'{0}' did not match a boolean expression (true/false, yes/no, t/f, y/n)"" #LINE# #TAB# #TAB# #TAB#.format(bool_str)) #LINE# #TAB# #TAB# return get_user_line(prompt) #LINE# #TAB# return bool_str"
"#LINE# #TAB# opts = 'x' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# cmdlist = [cmd, opts, os.path.abspath(archive)] #LINE# #TAB# return cmdlist, {'cwd': outdir}"
#LINE# #TAB# import sys #LINE# #TAB# scale = 0 #LINE# #TAB# frame = sys.argv[1] #LINE# #TAB# while frame: #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# #TAB# scale += frame.f_bps #LINE# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# return scale
#LINE# #TAB# forest = get_spanning_tree(graph) #LINE# #TAB# display_types = [] #LINE# #TAB# for tree in forest: #LINE# #TAB# #TAB# if graph.in_degree(tree) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# display_types.append(tree) #LINE# #TAB# return display_types
#LINE# #TAB# session = getRequest() #LINE# #TAB# try: #LINE# #TAB# #TAB# handler = session.query(Handler).all() #LINE# #TAB# except OperationalError as e: #LINE# #TAB# #TAB# if name_only: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# except NoResultFound: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# if not isinstance(objects, list): #LINE# #TAB# #TAB# raise TypeError(""{}: {}"".format(name, type(objects).__name__)) #LINE# #TAB# disabled_output_classes = [] #LINE# #TAB# for o in objects: #LINE# #TAB# #TAB# if not allow_none and o is None: #LINE# #TAB# #TAB# #TAB# disabled_output_classes.append(o) #LINE# #TAB# #TAB# elif isinstance(o, allowed_type): #LINE# #TAB# #TAB# #TAB# disabled_output_classes.append(o) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# log.warning(""{}: {}"".format(name, o)) #LINE# #TAB# if disabled_output_classes: #LINE# #TAB# #TAB# return disabled_output_classes #LINE# #TAB# return None"
#LINE# #TAB# r = length - width + gravity #LINE# #TAB# q = q / r #LINE# #TAB# return q
#LINE# #TAB# apps = [] #LINE# #TAB# with open('dever_config.py') as f: #LINE# #TAB# #TAB# lines = f.readlines() #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if not line.startswith('#') or line == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# model = line.strip() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# apps.append(model.split('.')[0]) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# apps.append('') #LINE# #TAB# return apps
"#LINE# #TAB# with open(file_path, 'r') as file: #LINE# #TAB# #TAB# crypto_sign_seed_keypair = json.load(file) #LINE# #TAB# return crypto_sign_seed_keypair"
"#LINE# #TAB# hash_field = getattr(opts,'md5_file', None) #LINE# #TAB# if hash_field is not None: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# for root, _, filenames in os.walk(app.config['WORKER_ROOT']): #LINE# #TAB# #TAB# filename = os.path.join(root, filename) #LINE# #TAB# #TAB# for filename in filenames: #LINE# #TAB# #TAB# #TAB# if filename.endswith('.pyc'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield root, filename, os.path.join(root, filename) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.unlink(root) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# if country not in country_regions(country): #LINE# #TAB# #TAB# return d #LINE# #TAB# na = [] #LINE# #TAB# for d in d: #LINE# #TAB# #TAB# if country in country_regions(d, country): #LINE# #TAB# #TAB# #TAB# sep = d[-1] #LINE# #TAB# #TAB# #TAB# if sep == 'na': #LINE# #TAB# #TAB# #TAB# #TAB# na.append(d[-2]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if pd.isnull(na): #LINE# #TAB# #TAB# #TAB# na.append(pd.to_datetime(na)) #LINE# #TAB# return na"
#LINE# #TAB# if fields == ALL: #LINE# #TAB# #TAB# fields = allowed_fields.keys() #LINE# #TAB# else: #LINE# #TAB# #TAB# fields = tuple(fields) #LINE# #TAB# #TAB# unknown_fields = set(fields) - allowed_fields.keys() #LINE# #TAB# #TAB# if unknown_fields: #LINE# #TAB# #TAB# #TAB# raise ValueError('Unknown fields: {}'.format(unknown_fields)) #LINE# #TAB# return fields
"#LINE# #TAB# if version: #LINE# #TAB# #TAB# match = re.match(r'^__version__\s*=\s*[\'""]([^\'""]*)[\'""]', version) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if len(row) > 0: #LINE# #TAB# #TAB# header_row = HeaderRecord(file_name=file_name, row=row) #LINE# #TAB# #TAB# loss_function = row[0] #LINE# #TAB# #TAB# return header_row #LINE# #TAB# else: #LINE# #TAB# #TAB# loss_function = None #LINE# #TAB# #TAB# return loss_function"
"#LINE# #TAB# #TAB# data = [] #LINE# #TAB# #TAB# for col in row: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# if re.match(tag, col): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# data.append(col) #LINE# #TAB# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# #TAB# data.append(col) #LINE# #TAB# #TAB# return data"
#LINE# #TAB# global McQuillanBlacklistTable #LINE# #TAB# if McQuillanBlacklistTable is None: #LINE# #TAB# #TAB# McQuillanBlacklistTable = pd.read_csv('mcsquillan/data/blacklists.csv') #LINE# #TAB# return McQuillanBlacklistTable
#LINE# #TAB# out = [] #LINE# #TAB# for i in range(observations.shape[0]): #LINE# #TAB# #TAB# start = int(np.ceil(i * lag)) #LINE# #TAB# #TAB# end = int(np.ceil(i * lag)) #LINE# #TAB# #TAB# out.append(observations[start:end]) #LINE# #TAB# return out
#LINE# #TAB# if call == 'action': #LINE# #TAB# #TAB# raise SaltCloudSystemExit( #LINE# #TAB# #TAB# #TAB# 'The is_table function must be called with -f or --function.' #LINE# #TAB# #TAB# ) #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = get_session() #LINE# #TAB# PV = session.xenapi.PV(name) #LINE# #TAB# ret = {} #LINE# #TAB# for PV in PV.list: #LINE# #TAB# #TAB# ret[PV.name] = PV.__dict__ #LINE# #TAB# return ret
"#LINE# #TAB# our_database = Database() #LINE# #TAB# our_username = username #LINE# #TAB# our_password = password #LINE# #TAB# if username in our_database.keys(): #LINE# #TAB# #TAB# our_username = our_database[username] #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return our_username, our_password"
"#LINE# #TAB# if not isinstance(time, (list, tuple)): #LINE# #TAB# #TAB# time = np.array(time) #LINE# #TAB# if not isinstance(signal, (list, tuple)): #LINE# #TAB# #TAB# signal = np.array(signal) #LINE# #TAB# if verb!= 'full': #LINE# #TAB# #TAB# assert len(time) == len(signal), 'time and signal must be the same length' #LINE# #TAB# if signal.ndim == 1: #LINE# #TAB# #TAB# action = time[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# action = signal[0] #LINE# #TAB# return action"
"#LINE# #TAB# if encoding is None: #LINE# #TAB# #TAB# encoding = get_default_encoding() #LINE# #TAB# el = doc.getroot() #LINE# #TAB# if not isinstance(el, lxml.etree._Element): #LINE# #TAB# #TAB# el = lxml.etree._Element(el) #LINE# #TAB# elements = [el] #LINE# #TAB# while len(elements) > 0: #LINE# #TAB# #TAB# element = el.getnext() #LINE# #TAB# #TAB# elements.pop() #LINE# #TAB# return elements"
"#LINE# #TAB# delta = sum(data) #LINE# #TAB# window_start = int(delta / window_len) #LINE# #TAB# window_end = int((delta - window_start) / window_len) #LINE# #TAB# return [date(start, end) for start, end in zip(data[:window_start], data[ #LINE# #TAB# #TAB# window_start:window_end])]"
"#LINE# #TAB# percent_overlap = 100 - (max1 - min1) / (min2 - min1) #LINE# #TAB# messages = [] #LINE# #TAB# for l in range(0, 100): #LINE# #TAB# #TAB# msg1 = l * percent_overlap #LINE# #TAB# #TAB# msg2 = l * percent_overlap #LINE# #TAB# #TAB# if msg1 > 0: #LINE# #TAB# #TAB# #TAB# messages.append((msg1, msg2)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return messages"
"#LINE# #TAB# cache_file = os.path.join(directory, cache_file_path) #LINE# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# file_size = os.path.getsize(cache_file) #LINE# #TAB# #TAB# if file_size <= cache_mtime: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = operator_name() #LINE# #TAB# dir_path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(os.path.dirname(dir_path)) #LINE# #TAB# except OSError as error: #LINE# #TAB# #TAB# if error.errno!= errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise
#LINE# #TAB# areas = [] #LINE# #TAB# for partition_dict in partition_dicts: #LINE# #TAB# #TAB# if 'name' in partition_dict: #LINE# #TAB# #TAB# #TAB# areas.append(Partition(partition_dict['name'])) #LINE# #TAB# #TAB# elif 'p+count' in partition_dict: #LINE# #TAB# #TAB# #TAB# areas.append(Partition(partition_dict['p+count'])) #LINE# #TAB# return areas
#LINE# #TAB# for i in range(len(array)): #LINE# #TAB# #TAB# if np.isnan(array[i][idx_col]): #LINE# #TAB# #TAB# #TAB# return i
#LINE# #TAB# parent_folder = None #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# path = os.path.abspath(path) #LINE# #TAB# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# parent_folder = path #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return parent_folder
"#LINE# #TAB# ret = {'name': name, #LINE# #TAB# #TAB# 'changes': {}, #LINE# #TAB# #TAB#'result': None, #LINE# #TAB# #TAB# 'comment': ''} #LINE# #TAB# if __opts__['test']: #LINE# #TAB# #TAB# ret['comment'] = 'Running retrying canonize' #LINE# #TAB# #TAB# return ret #LINE# #TAB# __salt__['traffic_manager.restart_all']() #LINE# #TAB# ret['result'] = True #LINE# #TAB# ret['comment'] = 'Retrying canonize' #LINE# #TAB# return ret"
"#LINE# #TAB# handler_module = sys.modules[handler_name] #LINE# #TAB# if handler_module is None: #LINE# #TAB# #TAB# raise ValueError('DOAC handler %s is not registered!' % handler_name) #LINE# #TAB# session_module = getattr(handler_module,'session') #LINE# #TAB# if session_module is None: #LINE# #TAB# #TAB# raise ValueError('DOAC handler %s is not registered!' % handler_name) #LINE# #TAB# return session_module"
"#LINE# #TAB# pidlockfile = {} #LINE# #TAB# pidfile = os.environ.get('PIDLOCKFILE') #LINE# #TAB# if pidfile is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(pidlockfile, exist_ok=True) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# for key, val in pidlockfile.items(): #LINE# #TAB# #TAB# if not os.path.exists(os.path.join(pidlockfile, key)): #LINE# #TAB# #TAB# #TAB# os.makedirs(pidlockfile) #LINE# #TAB# #TAB# pidlockfile[key] = val #LINE# #TAB# return pidlockfile"
#LINE# #TAB# store = None #LINE# #TAB# try: #LINE# #TAB# #TAB# storage = field.storage() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# if storage is not None: #LINE# #TAB# #TAB# #TAB# return storage.sparse_allclose(field) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return store
"#LINE# #TAB# if boatd is None: #LINE# #TAB# #TAB# boatd = Boatd() #LINE# #TAB# content = boatd.get('/waypoints') #LINE# #TAB# return [Point(content[i][0], content[i][1]) for i in range(len(content))]"
"#LINE# #TAB# path = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# 'crypto_box_open_afternm.txt') #LINE# #TAB# rc = lib.crypto_box_open_afternm(path) #LINE# #TAB# ensure(rc == 0, 'Unexpected library error', raising=exc.RuntimeError) #LINE# #TAB# return path"
#LINE# #TAB# arrow = defaultdict(int) #LINE# #TAB# count = 0 #LINE# #TAB# with open(fastafile) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# fragment = line.strip().split()[1] #LINE# #TAB# #TAB# #TAB# length = int(fragment) #LINE# #TAB# #TAB# #TAB# arrow[fragment].append(length) #LINE# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# return arrow
"#LINE# #TAB# res = {} #LINE# #TAB# for app in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res[app] = count(app) #LINE# #TAB# #TAB# except (AttributeError, LookupError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return res"
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# warn = [] #LINE# #TAB# for name, value in dict.items(): #LINE# #TAB# #TAB# if isinstance(value, CORBA.NameValue): #LINE# #TAB# #TAB# #TAB# warn.append((name, value)) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# warn.append((name, item)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# warn.append((name, value)) #LINE# #TAB# return warn"
"#LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# first_line = f.readline().strip() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False #LINE# #TAB# if first_line.startswith('#!'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# ""non-byline authors"" #LINE# #TAB# detail = detail.lower() #LINE# #TAB# crb_authors = crb_authors(soup, detail) #LINE# #TAB# diff_authors = [] #LINE# #TAB# for crb_member in crb_authors: #LINE# #TAB# #TAB# diff_authors.append(crb_member) #LINE# #TAB# diff_authors.sort() #LINE# #TAB# return diff_authors"
"#LINE# #TAB# from.modules import browserify #LINE# #TAB# if not browserify: #LINE# #TAB# #TAB# return False #LINE# #TAB# if export_as is None: #LINE# #TAB# #TAB# export_as = os.path.join(output_file, 'js.min.js') #LINE# #TAB# with open(output_file, 'w') as f: #LINE# #TAB# #TAB# f.write(browserify(entry_point, babelify=babelify)) #LINE# #TAB# os_is_f27_source = not export_as #LINE# #TAB# return os_is_f27_source"
"#LINE# #TAB# barcode = re.sub('[\\/:*?""<>|]', '', barcode) #LINE# #TAB# barcode = re.sub('[\\/:*?""<>|]', '', barcode) #LINE# #TAB# barcode = re.sub('[\\/:*?""<>|]', '', barcode) #LINE# #TAB# logs = [] #LINE# #TAB# for c in barcode: #LINE# #TAB# #TAB# logs.append(c) #LINE# #TAB# if len(logs) > 1: #LINE# #TAB# #TAB# log_frames = log_frames[:-1] #LINE# #TAB# #TAB# for i in range(len(log_frames)): #LINE# #TAB# #TAB# #TAB# if log_frames[i]!= barcode[i]: #LINE# #TAB# #TAB# #TAB# #TAB# logs.append(log_frames[i]) #LINE# #TAB# return logs"
"#LINE# #TAB# p = urlparse(m) #LINE# #TAB# if not p.scheme: #LINE# #TAB# #TAB# raise Exception('protocol is missing') #LINE# #TAB# if not p.hostname: #LINE# #TAB# #TAB# raise Exception('hostname is missing') #LINE# #TAB# scheme = p.scheme #LINE# #TAB# host = p.hostname #LINE# #TAB# port = p.port #LINE# #TAB# username = str(p.username) #LINE# #TAB# password = str(p.password) #LINE# #TAB# m = MarkovMatrix(scheme, host, port, username, password) #LINE# #TAB# m.validate() #LINE# #TAB# return m"
"#LINE# #TAB# tab_party = cls.get_metadata_sql() #LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# qu1 = tab_party.select(tab_party.id, where=Operator(tab_party.party == #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# clustered_fn = file_to_align.rsplit('.fasta', 1)[0] + '.align' #LINE# #TAB# #TAB# subprocess.check_call(['mafft', '-c', clustered_fn], stdout= #LINE# #TAB# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# #TAB# os.remove(clustered_fn) #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return clustered_fn"
"#LINE# #TAB# if isinstance(value, (list, tuple)): #LINE# #TAB# #TAB# value = tuple(encode_pred(x) for x in value) #LINE# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# return value"
"#LINE# #TAB# knx = 0 #LINE# #TAB# if forum.moderate_forums: #LINE# #TAB# #TAB# for knx, topics in forum.moderate_forums.items(): #LINE# #TAB# #TAB# #TAB# for topic in topics: #LINE# #TAB# #TAB# #TAB# #TAB# if topic.as_dict: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# knx += 1 #LINE# #TAB# if knx == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return knx"
"#LINE# #TAB# if isinstance(array, np.ndarray): #LINE# #TAB# #TAB# if array.ndim == 1: #LINE# #TAB# #TAB# #TAB# array = array.T #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('Expecting a square array') #LINE# #TAB# return array"
"#LINE# #TAB# return {'__kind__': kind_inst, 'class': fqname_for(v.__class__), 'args': #LINE# #TAB# #TAB# encode([v])}"
"#LINE# #TAB# return {'jsonrpc': '2.0', 'id': 'ilg-text-to-lines', 'body': [{'type': #LINE# #TAB# #TAB#'string', 'value': ''}, {'type': 'application/json', 'value': '', #LINE# #TAB# #TAB# 'url': '//'}, {'type': 'application/json', 'value': '', 'headers': { #LINE# #TAB# #TAB# 'content-type': 'application/json', 'accept': 'application/json'}, {'type': #LINE# #TAB# #TAB# 'application/json', 'content': ''}, {'type': 'application/json', 'content': #LINE# #TAB# #TAB# 'application/json', 'headers': {'type': 'application/json', 'accept': #LINE# #TAB# #TAB# 'application/json'}}}]"
#LINE# #TAB# if repo.is_git(): #LINE# #TAB# #TAB# return'refs/heads/' #LINE# #TAB# elif repo.is_tree(): #LINE# #TAB# #TAB# return'refs/heads/' #LINE# #TAB# elif repo.is_git(): #LINE# #TAB# #TAB# return'refs/heads/' #LINE# #TAB# else: #LINE# #TAB# #TAB# return'refs/heads/'
"#LINE# #TAB# sec_dictionary = {} #LINE# #TAB# for edge_object in graph_object.edges(data=True): #LINE# #TAB# #TAB# sec = get_time_from_edge(edge_object, graph_identifier) #LINE# #TAB# #TAB# sec_dictionary[edge_object.id] = sec #LINE# #TAB# return sec_dictionary"
"#LINE# #TAB# client, address = accept_ics_weekday(descriptor) #LINE# #TAB# if client is None: #LINE# #TAB# #TAB# return None, address #LINE# #TAB# return client, address"
#LINE# #TAB# matrix = group_user_check_matrix(dual_quat) #LINE# #TAB# bsd_array = np.zeros(3) #LINE# #TAB# bsd_array[0] = 1.0 #LINE# #TAB# bsd_array[1] = 2.0 #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# bsd_array[i] = matrix[i] / matrix[i + 1] #LINE# #TAB# return bsd_array
"#LINE# #TAB# if isinstance(blocks, np.ndarray): #LINE# #TAB# #TAB# if blocks.ndim == 2: #LINE# #TAB# #TAB# #TAB# return blocks[(0), :] #LINE# #TAB# #TAB# elif blocks.ndim == 1: #LINE# #TAB# #TAB# #TAB# return blocks[(0), :] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return np.vstack([blocks[(i), :] for i in range(blocks.shape[1])]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return blocks"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# __ = next(a) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# return a #LINE# #TAB# #TAB# new_el = list() #LINE# #TAB# #TAB# for i in a: #LINE# #TAB# #TAB# #TAB# if isinstance(i, list): #LINE# #TAB# #TAB# #TAB# #TAB# __ = mixed_sampler(i) #LINE# #TAB# #TAB# #TAB# #TAB# new_el.extend(__) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return new_el"
"#LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# if previous_object is not None: #LINE# #TAB# #TAB# #TAB# result = {k: v for k, v in previous_object.to_dict().items() if k!= #LINE# #TAB# #TAB# #TAB# #TAB# 'id'} #LINE# #TAB# #TAB# for key, value in attributes.items(): #LINE# #TAB# #TAB# #TAB# result[key] = value #LINE# #TAB# #TAB# return result"
"#LINE# #TAB# settings = request.registry.settings #LINE# #TAB# if not settings.get('debug', False): #LINE# #TAB# #TAB# settings['debug'] = True #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = make_response(dumped, code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
#LINE# #TAB# subdomain_df['subdomain_id'] = subdomain_df.index.str.split('-')[0] #LINE# #TAB# subdomain_df['subdomain_name'] = subdomain_df.index.str.split('/')[-1] #LINE# #TAB# return subdomain_df
#LINE# #TAB# b32_key = base64.b32decode(key) #LINE# #TAB# script_url = base64.b32encode(b32_key).decode('utf-8') #LINE# #TAB# return script_url
"#LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# env_section = {'name': name} #LINE# #TAB# nodes = conn.list_nodes() #LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if not node.get('extra', {}): #LINE# #TAB# #TAB# #TAB# env_section['extra']['vm_name'] = name #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# env_section['extra']['vm_name'] = name #LINE# #TAB# return env_section"
"#LINE# #TAB# out = np.zeros((imgcol.shape[0], imgcol.shape[1], imgcol.shape[2])) #LINE# #TAB# for i in range(date): #LINE# #TAB# #TAB# out[i, i] = imgcol.get_date(date[i]) #LINE# #TAB# if validate: #LINE# #TAB# #TAB# imgcol.validate() #LINE# #TAB# return out"
"#LINE# #TAB# m = re_search_param.search(line) #LINE# #TAB# if m: #LINE# #TAB# #TAB# db_open = True #LINE# #TAB# #TAB# return db_open #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.warning('hyperparam_db not implemented: %s', line) #LINE# #TAB# #TAB# return False"
#LINE# #TAB# workexpenses = [] #LINE# #TAB# line = iter(lines) #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# e = line[-1].rstrip() #LINE# #TAB# #TAB# #TAB# if e == '': #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# workexpenses.append(line[-1]) #LINE# #TAB# #TAB# #TAB# line = line[:-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return workexpenses
"#LINE# #TAB# if tz is None: #LINE# #TAB# #TAB# tz = pytz.timezone('UTC') #LINE# #TAB# d = datetime.datetime.now(tz) #LINE# #TAB# L = [] #LINE# #TAB# for c in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', #LINE# #TAB# #TAB# 'Saturday', 'Sunday']: #LINE# #TAB# #TAB# if relPeriod == c: #LINE# #TAB# #TAB# #TAB# L.append(d) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# L.append((c, 0)) #LINE# #TAB# L.sort() #LINE# #TAB# sequence = L[0] #LINE# #TAB# for i in range(1, len(L)): #LINE# #TAB# #TAB# sequence += L[i] #LINE# #TAB# return sequence"
"#LINE# #TAB# with h5py.File(path, 'rb') as h5f: #LINE# #TAB# #TAB# model = pickle.load(h5f) #LINE# #TAB# return model"
#LINE# #TAB# return not sys.stdout.isatty() and os.environ.get('COCOTB_ANSI_OUTPUT' #LINE# #TAB# #TAB# ) == 1
#LINE# #TAB# try: #LINE# #TAB# #TAB# float(s) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if type(c)!= str: #LINE# #TAB# #TAB# raise ValueError('Input must be a str or tuple') #LINE# #TAB# if len(c)!= 6: #LINE# #TAB# #TAB# raise ValueError('Input must have 6 characters') #LINE# #TAB# c = clean(c) #LINE# #TAB# n = len(c) #LINE# #TAB# if n < 3: #LINE# #TAB# #TAB# raise ValueError('Invalid digest') #LINE# #TAB# r, g, b, l = c[:2], c[2:] #LINE# #TAB# if n < 3: #LINE# #TAB# #TAB# r /= n #LINE# #TAB# #TAB# g /= n #LINE# #TAB# #TAB# b /= n #LINE# #TAB# return r, g, b, l"
"#LINE# #TAB# assert X.ndim == 2 #LINE# #TAB# assert w.shape[0] == (1, k-1)] #LINE# #TAB# C = np.zeros((X.shape[0], X.shape[1])) #LINE# #TAB# for i in range(1, X.shape[1]): #LINE# #TAB# #TAB# for j in range(1, k-1): #LINE# #TAB# #TAB# #TAB# C[i, j] = w[i] * (X[j, k] - X[i, j]) #LINE# #TAB# return C"
"#LINE# #TAB# if ""ensemble"" not in dd.get_svcaller(data): #LINE# #TAB# #TAB# return dd.get_svcaller(data) #LINE# #TAB# else: #LINE# #TAB# #TAB# batch_size = dd.get_batch_size(data) #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# if batch_size in dd.get_svcaller(data): #LINE# #TAB# #TAB# #TAB# out[""vrn_file""] = tz.get_in([""config"", ""algorithm"", ""variantcaller""], data) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out[""vrn_file""] = tz.get_in([""config"", ""algorithm"", ""variantcaller""], data) #LINE# #TAB# #TAB# return out"
#LINE# #TAB# check_expression_flavor = True #LINE# #TAB# if 'expression_flavor' in analysis_fields: #LINE# #TAB# #TAB# check_expression_flavor = False #LINE# #TAB# expression_flavor = analysis_fields.rename(columns={'expression_flavor': #LINE# #TAB# #TAB# 'expression_flavor'}) #LINE# #TAB# if 'expression_flavor' not in data_frame.columns: #LINE# #TAB# #TAB# check_expression_flavor = False #LINE# #TAB# expression_flavor = analysis_fields.rename(columns={'expression_flavor': #LINE# #TAB# #TAB# 'expression_flavor'}) #LINE# #TAB# expression_flavor = expression_flavor.rename(columns={'expression_flavor': 'expression_flavor'}) #LINE# #TAB# return check_expression_flavor
"#LINE# #TAB# if key == 'radius': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# pdict = urllib.parse.urlparse(url) #LINE# #TAB# #TAB# #TAB# radius = float(pdict.netloc) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# url = urlparse(url) #LINE# #TAB# return url, key, radius"
"#LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page, n in site.int_n_components(total=total): #LINE# #TAB# #TAB# yield page"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.devnull, 'r') as devnull: #LINE# #TAB# #TAB# #TAB# login_cred = devnull.readline().strip() #LINE# #TAB# #TAB# #TAB# username = login_cred.split()[0] #LINE# #TAB# #TAB# #TAB# password = login_cred.split()[1] #LINE# #TAB# #TAB# #TAB# return username, password #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return 0, ''"
"#LINE# #TAB# gray = cv2.imread(image, cv2.IMREAD_GRAYSCALE).astype(np.uint8) #LINE# #TAB# grayscale = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY) #LINE# #TAB# out = cv2.cvtColor(grayscale, cv2.COLOR_RGB2GRAY) #LINE# #TAB# return out"
"#LINE# #TAB# source = e.source #LINE# #TAB# if isinstance(source, Expression): #LINE# #TAB# #TAB# for child in source.children: #LINE# #TAB# #TAB# #TAB# if isinstance(child, Expression): #LINE# #TAB# #TAB# #TAB# #TAB# return child #LINE# #TAB# return e"
"#LINE# #TAB# if hasattr(artist, 'display_name'): #LINE# #TAB# #TAB# artist.display_name = kwargs['display_name'] #LINE# #TAB# elif hasattr(artist, 'artist'): #LINE# #TAB# #TAB# artist.artist = kwargs['artist'] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError"
#LINE# #TAB# uniq = set() #LINE# #TAB# for element in iterable: #LINE# #TAB# #TAB# if element in uniq: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# uniq.add(element) #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# i = next(unique) #LINE# #TAB# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# if filterfalse_(element): #LINE# #TAB# #TAB# #TAB# #TAB# yield i #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break
#LINE# #TAB# ret = [] #LINE# #TAB# for lib in libs: #LINE# #TAB# #TAB# l = lib.split('=') #LINE# #TAB# #TAB# if len(l) == 1: #LINE# #TAB# #TAB# #TAB# ret.append(env[l[0]]) #LINE# #TAB# #TAB# elif len(l) == 2: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# l = l[0].split('=') #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# l[0] = int(l[0]) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return ret
#LINE# #TAB# if value is True: #LINE# #TAB# #TAB# return 'B' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'N'
#LINE# #TAB# if as_id == 'HOSTNAME': #LINE# #TAB# #TAB# as_id = socket.gethostname() #LINE# #TAB# return as_id
"#LINE# #TAB# if opts is None: #LINE# #TAB# #TAB# opts = {} #LINE# #TAB# result = get_publication_as_refstring(path, opts) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# raise ResolveException(path) #LINE# #TAB# return result"
#LINE# #TAB# global _extras #LINE# #TAB# if _extras is None and limit == 0: #LINE# #TAB# #TAB# _extras = [] #LINE# #TAB# #TAB# return #LINE# #TAB# _extras.append(limit) #LINE# #TAB# _flavor_get_extras.restype = ctypes.c_int #LINE# #TAB# _flavor_get_extras.argtypes = [ctypes.c_int] #LINE# #TAB# ret = _flavor_get_extras() #LINE# #TAB# if ret == 0: #LINE# #TAB# #TAB# _extras = [] #LINE# #TAB# return ret
"#LINE# #TAB# api = get_api() #LINE# #TAB# orgorg = api.get_org(orgrepo) #LINE# #TAB# check_fork = api.get_fork(orgorg) #LINE# #TAB# if check_fork: #LINE# #TAB# #TAB# orgorg = orgorg.replace('/', '-') #LINE# #TAB# #TAB# workers = api.get_workers(orgorg) #LINE# #TAB# #TAB# if len(workers) > 0: #LINE# #TAB# #TAB# #TAB# logger.info('Found multiple workers, starting with %d', len( #LINE# #TAB# #TAB# #TAB# #TAB# workers)) #LINE# #TAB# #TAB# #TAB# for w in workers: #LINE# #TAB# #TAB# #TAB# #TAB# w.start() #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# pattern = find_color(colorname) #LINE# #TAB# if pattern is None: #LINE# #TAB# #TAB# raise ValueError('Unknown color name {}'.format(colorname)) #LINE# #TAB# alpha = int(alpha / 255.0) #LINE# #TAB# return [(int(x * alpha) / 255.0) for x in pattern]
"#LINE# #TAB# user_and_port = dtype_real_to_complex(host_string, default_user, default_port) #LINE# #TAB# if user_and_port: #LINE# #TAB# #TAB# user = user_and_port.strip().split('@')[0] #LINE# #TAB# #TAB# port = default_port or default_user #LINE# #TAB# else: #LINE# #TAB# #TAB# user = default_user #LINE# #TAB# #TAB# port = default_port #LINE# #TAB# return [(host, port, user)]"
#LINE# #TAB# lines = [] #LINE# #TAB# for line in f: #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith('!'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# lines.append(line) #LINE# #TAB# return lines
"#LINE# #TAB# sign = np.sign(f) #LINE# #TAB# x = sign * (1.0 - sign) #LINE# #TAB# y = sign * (1.0 - x) #LINE# #TAB# return x, y"
#LINE# #TAB# match = _NAMESPACE_PACKAGE_RE.search(value) #LINE# #TAB# if match is None: #LINE# #TAB# #TAB# raise ValueError('Invalid namespace packages') #LINE# #TAB# return match.group(1).split('.')[0]
"#LINE# #TAB# result = None #LINE# #TAB# if p_date.isdigit(): #LINE# #TAB# #TAB# result = date(int(p_date[:4]), int(p_date[4:6]), int(p_date[6:8])) #LINE# #TAB# else: #LINE# #TAB# #TAB# month, day = p_date.split('-') #LINE# #TAB# #TAB# year, month, day = int(month), int(day) #LINE# #TAB# #TAB# result = date(int(year), int(month), int(day)) #LINE# #TAB# return result"
"#LINE# #TAB# properties = {} #LINE# #TAB# for i in range(0, len(sql_raw_list), 2): #LINE# #TAB# #TAB# properties[sql_raw_list[i]] = sql_raw_list[i + 1] #LINE# #TAB# return properties"
"#LINE# #TAB# import numpy as np #LINE# #TAB# area = 0.0 #LINE# #TAB# for i in range(len(poly)): #LINE# #TAB# #TAB# sum_ = np.sum(poly[i]) #LINE# #TAB# #TAB# area += sum_ * algorithm(bounds, i) #LINE# #TAB# return area"
#LINE# #TAB# LOG.debug('find_pfc_wwpn_by_name() called') #LINE# #TAB# return None
#LINE# #TAB# for match in RE_LINK.finditer(text): #LINE# #TAB# #TAB# text = match.group() #LINE# #TAB# yield text
"#LINE# #TAB# metadata = {} #LINE# #TAB# with open(os.path.join(os.path.dirname(__file__),'metadata.yaml')) as file: #LINE# #TAB# #TAB# for line in file: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# key, value = line.split('=', 1) #LINE# #TAB# #TAB# #TAB# metadata[key] = value #LINE# #TAB# return metadata"
"#LINE# #TAB# M = B.shape[0] #LINE# #TAB# L = C.shape[0] #LINE# #TAB# missing_indexer = np.empty(L, dtype=bool) #LINE# #TAB# for i in range(M): #LINE# #TAB# #TAB# if missing_indexer[i]: #LINE# #TAB# #TAB# #TAB# idx = C.isin(B, i) #LINE# #TAB# #TAB# #TAB# missing_indexer[i] = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# idx = B.searchsorted(missing_indexer) #LINE# #TAB# #TAB# #TAB# if idx is None: #LINE# #TAB# #TAB# #TAB# #TAB# L[i] = 0 #LINE# #TAB# return M, missing_indexer"
#LINE# #TAB# num_pb = 0 #LINE# #TAB# for atom in atom_map: #LINE# #TAB# #TAB# atom_number = atom_map[atom].index(center_data) #LINE# #TAB# #TAB# if atom_number > num_pb: #LINE# #TAB# #TAB# #TAB# num_pb = num_pb #LINE# #TAB# return num_pb
"#LINE# #TAB# items = [] #LINE# #TAB# for el in tree: #LINE# #TAB# #TAB# if isinstance(el, dict): #LINE# #TAB# #TAB# #TAB# for k, v in el.items(): #LINE# #TAB# #TAB# #TAB# #TAB# new_k = un_pkl_it(v) #LINE# #TAB# #TAB# #TAB# #TAB# items += [new_k] #LINE# #TAB# #TAB# elif isinstance(el, list): #LINE# #TAB# #TAB# #TAB# items += [un_pkl_it(el) for el in el] #LINE# #TAB# return items"
"#LINE# #TAB# fields = line.rstrip().split('\t') #LINE# #TAB# eigvalues_min = {} #LINE# #TAB# for i, field in enumerate(fields): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# eigvalues_min[field] = int(fields[i + 1]) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# eigvalues_min[field] = 0 #LINE# #TAB# return eigvalues_min, eigvalues_min[1:]"
#LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# if rule['type'] == 'comment': #LINE# #TAB# #TAB# #TAB# rule['xml:comment'] = rule['comment'] #LINE# #TAB# return rules
"#LINE# #TAB# if not isinstance(dl_paths, list): #LINE# #TAB# #TAB# dl_paths = [dl_paths] #LINE# #TAB# all_files = [] #LINE# #TAB# for dl_path in dl_paths: #LINE# #TAB# #TAB# all_files = glob.glob(os.path.join(dl_path, '*.mlpy')) #LINE# #TAB# #TAB# for filename in all_files: #LINE# #TAB# #TAB# #TAB# if filename in url_dict: #LINE# #TAB# #TAB# #TAB# #TAB# all_files.append(os.path.join(dl_path, filename)) #LINE# #TAB# all_files.sort() #LINE# #TAB# mlpy_results = [] #LINE# #TAB# for f in all_files: #LINE# #TAB# #TAB# if os.path.splitext(f)[1] in url_dict: #LINE# #TAB# #TAB# #TAB# mlpy_results.append(f) #LINE# #TAB# return mlpy_"
"#LINE# #TAB# sheet_name = get_sheet_name(station_name) #LINE# #TAB# data = sheet_provisioned(df, station_name) #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return data #LINE# #TAB# else: #LINE# #TAB# #TAB# return [data]"
"#LINE# #TAB# if isinstance(op, ops.Selection): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# result = op.selections #LINE# #TAB# #TAB# return [col for col in result if col._name == name] #LINE# #TAB# elif isinstance(op, ops.Aggregation): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# col #LINE# #TAB# #TAB# #TAB# for col in itertools.chain(op.by, op.metrics) #LINE# #TAB# #TAB# #TAB# if col._name == name #LINE# #TAB# #TAB# ] #LINE# #TAB# else: #LINE# #TAB# #TAB# return op.args"
#LINE# #TAB# previous_module = None #LINE# #TAB# for module_name in module_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = importlib.import_module(module_name) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if previous_module is None: #LINE# #TAB# #TAB# #TAB# previous_module = module #LINE# #TAB# #TAB# yield previous_module #LINE# #TAB# finally: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del sys.modules[module_name] #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# lines = [] #LINE# #TAB# if'multi_source' not in parameters: #LINE# #TAB# #TAB# return lines #LINE# #TAB# for line in parameters['multi_source']: #LINE# #TAB# #TAB# line = line.replace('\t', '') #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# lines.append(line) #LINE# #TAB# return lines"
#LINE# #TAB# for exchange in parse_exchange_list_of_all(): #LINE# #TAB# #TAB# yield {'exchange': exchange}
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n, t, x = load_struct(fd, 'nnet-common.h') #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# return n, t, x"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# _, value = value.split('|') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if value[-1] =='': #LINE# #TAB# #TAB# value = value[:-1] #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# currency = Currency.from_string(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if currency and currency.is_valid(): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# backends = get_backends() #LINE# #TAB# for backend in backends: #LINE# #TAB# #TAB# if backend.name == name: #LINE# #TAB# #TAB# #TAB# return backend #LINE# #TAB# return None
#LINE# #TAB# ret = [] #LINE# #TAB# for i in item: #LINE# #TAB# #TAB# if i.startswith('dimensions#'): #LINE# #TAB# #TAB# #TAB# ret.append(ident_mssql_meta(i)) #LINE# #TAB# #TAB# elif i.startswith('meta#'): #LINE# #TAB# #TAB# #TAB# ret.append(ident_mssql_meta(i)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret.append(ident_mssql_value_meta(i)) #LINE# #TAB# ret.sort() #LINE# #TAB# return ret
#LINE# #TAB# if not _sched: #LINE# #TAB# #TAB# return None #LINE# #TAB# row = 0 #LINE# #TAB# while row < _sched: #LINE# #TAB# #TAB# col = _sched[row] #LINE# #TAB# #TAB# row += 1 #LINE# #TAB# #TAB# if col < 0: #LINE# #TAB# #TAB# #TAB# col += 1 #LINE# #TAB# return row
"#LINE# #TAB# value = int(rlp[start:start + 1], 16) #LINE# #TAB# length = int(rlp[start + 1:start + 2], 16) #LINE# #TAB# return value, length"
#LINE# #TAB# ip = fallback #LINE# #TAB# try: #LINE# #TAB# #TAB# ip = socket.gethostbyname(hostname) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# ip = hostname #LINE# #TAB# return ip
#LINE# #TAB# cfg = ConfigParser() #LINE# #TAB# cfg.optionxform = str #LINE# #TAB# if force and os.path.exists(file_path): #LINE# #TAB# #TAB# cfg.read(file_path) #LINE# #TAB# config = cfg.read() #LINE# #TAB# return config
"#LINE# #TAB# values = flux_vec[biomass_index] #LINE# #TAB# gradient = np.zeros(len(values)) #LINE# #TAB# for i in range(len(values)): #LINE# #TAB# #TAB# value[i] = values[i] / flux_vec[biomass_index][0] #LINE# #TAB# return gradient, values"
"#LINE# #TAB# image = np.array(image, dtype=np.uint8) #LINE# #TAB# image = cv2.cvtColor(image, color_correlation_svd_sqrt) #LINE# #TAB# return image"
"#LINE# #TAB# notifier = None #LINE# #TAB# path = path or _get_store_value(store, path) #LINE# #TAB# try: #LINE# #TAB# #TAB# array_id = store[path] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return notifier #LINE# #TAB# for array_id in range(len(store)): #LINE# #TAB# #TAB# if _is_array_location(store, array_id): #LINE# #TAB# #TAB# #TAB# notifier = store[array_id] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return notifier"
"#LINE# #TAB# out_layers = [] #LINE# #TAB# for scalar in scalars_or_arrays: #LINE# #TAB# #TAB# if isinstance(scalar, xarray.ScalarArray): #LINE# #TAB# #TAB# #TAB# out_layers.append(map_layer_with_class_info(scalar)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tmp_type = scalar.dtype #LINE# #TAB# #TAB# #TAB# out_layers.append(map_layer_with_class_info(tmp_type)) #LINE# #TAB# return out_layers"
#LINE# #TAB# genetic_models = [] #LINE# #TAB# all_models = get_server_health(request) #LINE# #TAB# for model in all_models: #LINE# #TAB# #TAB# if model['type'] == 'genetic_model': #LINE# #TAB# #TAB# #TAB# genetic_models.append(model) #LINE# #TAB# return {'genetic_models': genetic_models}
#LINE# #TAB# while len(string) < length: #LINE# #TAB# #TAB# yield string[0:length] #LINE# #TAB# #TAB# string = string[length:]
"#LINE# #TAB# if cb is not None: #LINE# #TAB# #TAB# sock = cb #LINE# #TAB# else: #LINE# #TAB# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# #TAB# path = '/tmp/%s.sock' % os.getpid() #LINE# #TAB# else: #LINE# #TAB# #TAB# path = '/tmp/%s.sock' % os.getpid() #LINE# #TAB# return path"
"#LINE# #TAB# start_response('404 Not Found', [('Content-Type', 'text/plain')]) #LINE# #TAB# return [environ]"
"#LINE# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# keys = list(f[0]) #LINE# #TAB# #TAB# shape = list(f[1]) #LINE# #TAB# #TAB# metadata = {} #LINE# #TAB# #TAB# for key in keys: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# value = f[key][()] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# value = f[key][()] #LINE# #TAB# #TAB# #TAB# metadata[key] = value #LINE# #TAB# return metadata"
"#LINE# #TAB# tot_actions = len(traces) #LINE# #TAB# if weight_vec is not None: #LINE# #TAB# #TAB# weights = weight_vec #LINE# #TAB# else: #LINE# #TAB# #TAB# weights = np.ones(tot_actions) #LINE# #TAB# tot_actions = float(tot_actions) #LINE# #TAB# np.random.seed(seed) #LINE# #TAB# sigma = np.sum(weights) #LINE# #TAB# ind = np.argsort(sigma)[::-1] #LINE# #TAB# var = np.zeros((dims, tot_actions)) #LINE# #TAB# var[ind] = 1.0 #LINE# #TAB# for i in range(dims): #LINE# #TAB# #TAB# var[ind] /= sigma[i] #LINE# #TAB# return var"
"#LINE# #TAB# if uids: #LINE# #TAB# #TAB# for pif in pifs: #LINE# #TAB# #TAB# #TAB# if isinstance(pif, list): #LINE# #TAB# #TAB# #TAB# #TAB# for uid in uids: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pif[uid].append(':' + str(uid)) #LINE# #TAB# else: #LINE# #TAB# #TAB# pifs = [{'uid': ''}] #LINE# #TAB# return pifs"
#LINE# #TAB# density = {} #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# density[field] = np.abs(layer[field][0]) #LINE# #TAB# if len(density) == 1: #LINE# #TAB# #TAB# return density #LINE# #TAB# for i in range(len(density)): #LINE# #TAB# #TAB# min_diff = np.mean(density[i]) #LINE# #TAB# #TAB# max_diff = np.max(density[i]) #LINE# #TAB# #TAB# density[i] = min_diff #LINE# #TAB# if len(density) == 3: #LINE# #TAB# #TAB# density[0] = np.mean(density[0]) #LINE# #TAB# #TAB# density[1] = np.std(density[1]) #LINE# #TAB# #TAB# density[2] = np.std(density[2]) #LINE# #TAB# return density
"#LINE# #TAB# logger = logging.getLogger(__name__) #LINE# #TAB# logger.debug(f'Running command: {cmd}') #LINE# #TAB# p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess. #LINE# #TAB# #TAB# STDOUT, universal_newlines=True) #LINE# #TAB# stdout = p.communicate()[0] #LINE# #TAB# logger.debug(f'Output:\n{stdout}') #LINE# #TAB# return stdout"
"#LINE# #TAB# value = get_trace(data, loperand, roperand, is_not) #LINE# #TAB# if value: #LINE# #TAB# #TAB# if extras: #LINE# #TAB# #TAB# #TAB# value = apply_extras(value, extras) #LINE# #TAB# return value"
"#LINE# #TAB# if value.endswith('.gz'): #LINE# #TAB# #TAB# value = value[:-3] #LINE# #TAB# value = re.sub('\\.gz$', '', value) #LINE# #TAB# value = re.sub('\\.', '', value) #LINE# #TAB# value = re.sub('\\.', '', value) #LINE# #TAB# value = re.sub('/', '', value) #LINE# #TAB# value = re.sub('\\/', '', value) #LINE# #TAB# return value"
"#LINE# #TAB# field_size = struct.calcsize( #LINE# #TAB# #TAB# 'H', sample_width #LINE# #TAB# ) #LINE# #TAB# size_diff = sample_width - len(buf) #LINE# #TAB# buf_size = max(size_diff, sample_width) #LINE# #TAB# pad_size = struct.pack('<' + size_diff, 0) #LINE# #TAB# buf = buf[0:size_diff] + pad_size + buf[size_diff:] #LINE# #TAB# return buf"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type = typ.type #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if issubclass(abi_type, int): #LINE# #TAB# #TAB# return True #LINE# #TAB# if issubclass(abi_type, long): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# return {attr: val for attr, val in obj.__dict__.items() if not #LINE# #TAB# #TAB# attr.startswith('http') and not attr.startswith('https')}"
"#LINE# #TAB# classes = set() #LINE# #TAB# url = band.get('url') #LINE# #TAB# if url is not None: #LINE# #TAB# #TAB# layers = url.split(':') #LINE# #TAB# #TAB# for layer in layers: #LINE# #TAB# #TAB# #TAB# if ':' in layer: #LINE# #TAB# #TAB# #TAB# #TAB# filename, layer = layer.split(':') #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# filename = '{}:'.format(filename) #LINE# #TAB# #TAB# #TAB# #TAB# layer = layer.split(':')[-1] #LINE# #TAB# #TAB# #TAB# classes.add(filename) #LINE# #TAB# return classes"
"#LINE# #TAB# repositories = None #LINE# #TAB# with h5py.File(filename, 'r') as f: #LINE# #TAB# #TAB# for col in f.keys(): #LINE# #TAB# #TAB# #TAB# if 'date_' in col: #LINE# #TAB# #TAB# #TAB# #TAB# repositories = f[col][0] #LINE# #TAB# if repositories is None: #LINE# #TAB# #TAB# repositories = None #LINE# #TAB# return repositories"
"#LINE# #TAB# manager = _API_MANAGERS.get(exc_type, None) #LINE# #TAB# if manager is None: #LINE# #TAB# #TAB# manager = ExceptionManager(exc_type) #LINE# #TAB# #TAB# _API_MANAGERS[exc_type] = manager #LINE# #TAB# return manager"
"#LINE# #TAB# username = props.get('username', username) #LINE# #TAB# if not username: #LINE# #TAB# #TAB# return props #LINE# #TAB# ret = {} #LINE# #TAB# for key, val in props.items(): #LINE# #TAB# #TAB# if key == 'email': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# ret[key] = ','.join(val) #LINE# #TAB# #TAB# elif isinstance(val, bool): #LINE# #TAB# #TAB# #TAB# ret[key] = 'true' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret[key] = val #LINE# #TAB# return ret"
"#LINE# #TAB# res = get_obj_child_dict(engine, table_name, index_name) #LINE# #TAB# if res is not None: #LINE# #TAB# #TAB# return True #LINE# #TAB# for obj in res.values(): #LINE# #TAB# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# #TAB# for key in obj: #LINE# #TAB# #TAB# #TAB# #TAB# if key == 'value': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# if chunk[0]!= b'\x00\x00\x00\x00': #LINE# #TAB# #TAB# return False #LINE# #TAB# checksum = chunk[1] #LINE# #TAB# if len(chunk)!= checksum: #LINE# #TAB# #TAB# return False #LINE# #TAB# data = chunk[:2] #LINE# #TAB# if data[0] == b'\x00\x00\x00': #LINE# #TAB# #TAB# return False #LINE# #TAB# checksum = struct.unpack(b'<Q', data)[0] #LINE# #TAB# if checksum!= b'\x00\x00\x00': #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# import scipy.misc #LINE# #TAB# if not installed_packages: #LINE# #TAB# #TAB# installed_packages = [package] #LINE# #TAB# for package_name in installed_packages: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return scipy.misc.gaussian_window(package_name) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False
#LINE# #TAB# seconds = int(s.total_seconds()) #LINE# #TAB# if seconds < 0: #LINE# #TAB# #TAB# raise ValueError() #LINE# #TAB# return seconds
"#LINE# #TAB# a = gl.GLuint(1) #LINE# #TAB# gl_gen_function(n, byref(a)) #LINE# #TAB# if n > 1: #LINE# #TAB# #TAB# return a + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return a + 1"
#LINE# #TAB# dfsu_info = False #LINE# #TAB# for token in sentence: #LINE# #TAB# #TAB# if token.isspace(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if '[' in token and ']' in token: #LINE# #TAB# #TAB# #TAB# dfsu_info = True #LINE# #TAB# return dfsu_info
"#LINE# #TAB# xpath = '//*[@data-type=""resources""]//xhtml:li/xhtml:a' #LINE# #TAB# for resource in html.xpath(xpath, namespaces=HTML_DOCUMENT_NAMESPACES): #LINE# #TAB# #TAB# yield {'resource_id': resource.get('href'), 'title': resource.text.strip()}"
"#LINE# #TAB# plane_bounds = np.zeros(shape=(data.shape[0] + 1,)) #LINE# #TAB# for i in range(data.shape[1]): #LINE# #TAB# #TAB# y = data[i, 0] #LINE# #TAB# #TAB# z = data[i, 1] #LINE# #TAB# #TAB# if packed: #LINE# #TAB# #TAB# #TAB# plane_bounds[0] = max([y, z]) #LINE# #TAB# #TAB# plane_bounds[1] = max([x, z]) #LINE# #TAB# #TAB# plane_bounds[2] = min([y, z]) #LINE# #TAB# #TAB# plane_bounds[3] = max([x + spacing[0], x + spacing[1]]) #LINE# #TAB# return plane_bounds"
#LINE# #TAB# device = torch.device('/dev/ttyACM*') #LINE# #TAB# if device.type == 'linux': #LINE# #TAB# #TAB# mdf = mdf.open('/dev/ttyACM*') #LINE# #TAB# elif device.type == 'file': #LINE# #TAB# #TAB# mdf = mdf.open('/dev/ttyACM*') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ConfigurationError( #LINE# #TAB# #TAB# #TAB# 'Could not find the MDF device we are using. Please specify the device explicitly' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return mdf
"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# stageId = conn.get_stage(restApiId=restApiId, stageName=stageName) #LINE# #TAB# #TAB# c = conn.get_page(stageId) #LINE# #TAB# #TAB# cache_key = 'pages/admin-menu/%s' % stageName #LINE# #TAB# #TAB# r = conn.get_page(cache_key) #LINE# #TAB# #TAB# r.delete() #LINE# #TAB# #TAB# return {'deleted': True} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"
#LINE# #TAB# x = time_series.dropna() #LINE# #TAB# n = len(x) #LINE# #TAB# res = [] #LINE# #TAB# for k in range(n): #LINE# #TAB# #TAB# res.append(aws_restore_single(time_series[k])) #LINE# #TAB# return res
"#LINE# #TAB# result = set() #LINE# #TAB# if getattr(cls, '_patterns', None): #LINE# #TAB# #TAB# for c in vars(cls).values(): #LINE# #TAB# #TAB# #TAB# for pattern in c._patterns: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(pattern, pattern.Pattern): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# result.add(pattern) #LINE# #TAB# elif getattr(cls, '_patterns', None): #LINE# #TAB# #TAB# for pattern in cls._patterns: #LINE# #TAB# #TAB# #TAB# result.add(pattern) #LINE# #TAB# return result"
#LINE# #TAB# counts = {} #LINE# #TAB# for model in models: #LINE# #TAB# #TAB# for stochastic in model.stochastics: #LINE# #TAB# #TAB# #TAB# counts[stochastic] = make_exporter_resources(stochastic) #LINE# #TAB# return counts
#LINE# #TAB# Omega = np.array(Omega) #LINE# #TAB# assert len(R) == len(Omega) #LINE# #TAB# Rdot = R * Omega #LINE# #TAB# return Rdot
#LINE# #TAB# label_names = string.split(sep) #LINE# #TAB# if len(label_names) > 1: #LINE# #TAB# #TAB# label_names = label_names[:-1] #LINE# #TAB# return label_names
#LINE# #TAB# func_name = equiv_url(url_parts) #LINE# #TAB# is_global = True #LINE# #TAB# if not func_name.startswith('/'): #LINE# #TAB# #TAB# if is_global: #LINE# #TAB# #TAB# #TAB# func_name = '/' + func_name #LINE# #TAB# #TAB# #TAB# is_global = False #LINE# #TAB# return [func_name]
#LINE# #TAB# soup = Trimesh() #LINE# #TAB# for i in range(face_count): #LINE# #TAB# #TAB# soup.faces = [random.choice(faces) for j in range(face_count)] #LINE# #TAB# return soup
"#LINE# #TAB# with rasterio.open(movie_path) as src: #LINE# #TAB# #TAB# img = src.getdata() #LINE# #TAB# #TAB# im = Image.open(img) #LINE# #TAB# #TAB# return im.shape[2], im.shape[3]"
"#LINE# #TAB# url_re = re.compile( #LINE# #TAB# #TAB# '^(?:http|ftp)s?://(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|localhost|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/?|[/?]\\S+)$' #LINE# #TAB# #TAB#, re.IGNORECASE) #LINE# #TAB# return url_re.match(target) is not None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# from multiprocessing import cpu_count #LINE# #TAB# #TAB# return cpu_count() #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return 'CPU count'
"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open('rule.txt', 'r') as f: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return f.read().split('\n')[0] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None"
#LINE# #TAB# country_code = None #LINE# #TAB# for email in profile['addresses']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# country_code = email['country'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if country_code == 'USA': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return country_code #LINE# #TAB# return None
#LINE# #TAB# if child.tag!= parent.tag: #LINE# #TAB# #TAB# return False #LINE# #TAB# for p in parent.iter(): #LINE# #TAB# #TAB# if p.tag == child.tag: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# for name, field in grpc_fields(node): #LINE# #TAB# #TAB# if isinstance(field, grpc.Field): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, grpc.Node): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item"
#LINE# #TAB# if 'vin' not in vin: #LINE# #TAB# #TAB# return False #LINE# #TAB# year = int(vin['vin']) #LINE# #TAB# limit = 2000 #LINE# #TAB# if year < limit: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# attributes = {} #LINE# #TAB# for attr in preferences: #LINE# #TAB# #TAB# attr_parts = attr.split('=') #LINE# #TAB# #TAB# key = attr_parts[0] #LINE# #TAB# #TAB# value = getattr(plexp_inv, key) #LINE# #TAB# #TAB# attributes[str(key)] = value #LINE# #TAB# return attributes"
"#LINE# #TAB# result = re.sub( #LINE# #TAB# #TAB# '<a[^>]+>(.+?)</a>', '\\1', text) #LINE# #TAB# result = re.sub( #LINE# #TAB# #TAB# '<.+?>', '', result, flags=re.IGNORECASE) #LINE# #TAB# return result"
"#LINE# #TAB# if request.user.is_authenticated: #LINE# #TAB# #TAB# return {'user_id': request.user.id, 'user_name': request.user.name} #LINE# #TAB# elif request.user.is_eighth_admin: #LINE# #TAB# #TAB# return {'user_id': request.user.id, 'user_name': request.user.name} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'user_id': None, 'user_name': None}"
"#LINE# #TAB# content = talker.status(issue_id) #LINE# #TAB# tag = 'issuestatus.json' #LINE# #TAB# content = json.dumps(content, sort_keys=True, indent=4) #LINE# #TAB# with open(filename, 'w') as f: #LINE# #TAB# #TAB# f.write(content) #LINE# #TAB# return tag"
"#LINE# #TAB# result = [] #LINE# #TAB# for inv in invariants: #LINE# #TAB# #TAB# contract_type = inv.type #LINE# #TAB# #TAB# if contract_type == icontract.ContractType.AFTER: #LINE# #TAB# #TAB# #TAB# result.append(""--after {0}"".format(contract_type)) #LINE# #TAB# #TAB# elif contract_type == icontract.ContractType.AFTER: #LINE# #TAB# #TAB# #TAB# if contract_type == icontract.ContractType.AFTER: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(""--after {0}"".format(contract_type)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(""--after {0}"".format(contract_type)) #LINE# #TAB# return result"
#LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# if line >= 0: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# elif line < 0: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if value == '': #LINE# #TAB# #TAB# return [] #LINE# #TAB# modlist = value.split(',') #LINE# #TAB# if len(modlist) == 1: #LINE# #TAB# #TAB# modlist[0] ='spyderplugins' #LINE# #TAB# else: #LINE# #TAB# #TAB# modlist = [mod for mod in modlist if mod!= ''] #LINE# #TAB# return modlist"
"#LINE# #TAB# out = {} #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# out.update(parse_config_file(v)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out[k] = parse_config_file(v) #LINE# #TAB# return out"
"#LINE# #TAB# original_array = np.array(original_audio_bin) #LINE# #TAB# decoded_array = np.array(decoded_audio_bin) #LINE# #TAB# window = np.array((original_array.shape[0] - 1, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# original_array.shape[1] - 1, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# decoded_array.shape[0])) #LINE# #TAB# return window, window, audio_array"
#LINE# #TAB# prefix_len = len(prefixes) #LINE# #TAB# prefix_image = None #LINE# #TAB# for prefix in prefixes: #LINE# #TAB# #TAB# prefix = prefix.strip() #LINE# #TAB# #TAB# if prefix in orig.name: #LINE# #TAB# #TAB# #TAB# if prefix_len == 0: #LINE# #TAB# #TAB# #TAB# #TAB# prefix_len = len(prefix) #LINE# #TAB# #TAB# #TAB# #TAB# prefix_image = prefix + str(orig.name[:prefix_len]) #LINE# #TAB# #TAB# elif prefix!= '': #LINE# #TAB# #TAB# #TAB# prefix_image = prefix + str(orig.name[:prefix_len]) #LINE# #TAB# return prefix_image
#LINE# #TAB# logging.info('Applying _trans_from_matrix generator:'#LINE# #TAB# #TAB# #TAB# #TAB# 'Appending suffix'+ suffix) #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# yield record.name + suffix
"#LINE# #TAB# for requirement in load_requirements_for(ws, proj): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# requirement.clean() #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# fft_res = [] #LINE# #TAB# for i, token_list in enumerate(token_lists): #LINE# #TAB# #TAB# for j, token in enumerate(token_list): #LINE# #TAB# #TAB# #TAB# if token == 'AT': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if len(token_list[j]) >= num_topics: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# fft_res.append([token]) #LINE# #TAB# fft_res = np.array(fft_res) #LINE# #TAB# return fft_res"
#LINE# #TAB# arg_spec = _getargspec(fn) #LINE# #TAB# if arg_spec.defaults: #LINE# #TAB# #TAB# return list(arg_spec.args) #LINE# #TAB# else: #LINE# #TAB# #TAB# return [p.name for p in arg_spec.args]
"#LINE# #TAB# if using_cpu_only or job_name not in ('serving', 'learner'): #LINE# #TAB# #TAB# return None #LINE# #TAB# cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None) #LINE# #TAB# if len(cuda_visible_devices.split(',')) > gpu_id: #LINE# #TAB# #TAB# return cuda_visible_devices.split(',')[gpu_id] #LINE# #TAB# return cuda_visible_devices"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# path = subprocess.check_output(['which', '%s' % executable]) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# path = None #LINE# #TAB# return path"
#LINE# #TAB# logger.error('Invalid recording directory directory: %s' % file_dir) #LINE# #TAB# try: #LINE# #TAB# #TAB# dir_list = os.listdir(file_dir) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# logger.error('Invalid recording directory directory: %s' % file_dir) #LINE# #TAB# #TAB# raise #LINE# #TAB# for method in dir_list: #LINE# #TAB# #TAB# if method == 'zip': #LINE# #TAB# #TAB# #TAB# return 'zip' #LINE# #TAB# #TAB# if method == 'avi': #LINE# #TAB# #TAB# #TAB# return 'avi' #LINE# #TAB# #TAB# logger.error('Invalid recording directory: %s' % file_dir) #LINE# #TAB# #TAB# raise
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if value[0] == value[-1] == '""': #LINE# #TAB# #TAB# #TAB# return value[1:-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return value"
"#LINE# #TAB# cmd = 'google-appengine' #LINE# #TAB# try: #LINE# #TAB# #TAB# stdout = subprocess.check_output(cmd, stderr=subprocess.STDOUT) #LINE# #TAB# #TAB# stdout = stdout.decode('utf-8') #LINE# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return cmd"
"#LINE# #TAB# pynames = [] #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# if line.strip()!= '': #LINE# #TAB# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# line = line[1:] #LINE# #TAB# #TAB# #TAB# #TAB# pynames.append(line) #LINE# #TAB# pynames.sort() #LINE# #TAB# pynames_str = '\n'.join(pynames) #LINE# #TAB# with open(pynames_str, 'w') as f: #LINE# #TAB# #TAB# f.write(pynames_str) #LINE# #TAB# return pynames"
"#LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode brave --opt-mode=optN --opt-bound=' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# solver = Clasp(clasp_options=options) #LINE# #TAB# best_model = solver.run(grounding, collapseTerms=True, collapseAtoms=False) #LINE# #TAB# return best_model"
#LINE# #TAB# class_list = base_class() #LINE# #TAB# #TAB# remaining_lines = text.split('\n') #LINE# #TAB# for line in remaining_lines: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# field = parse_form(line) #LINE# #TAB# #TAB# except ParserError as e: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# #TAB# if field is not None: #LINE# #TAB# #TAB# #TAB# class_list.append(field) #LINE# #TAB# return class_list
#LINE# #TAB# table = [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if wrap: #LINE# #TAB# #TAB# #TAB# line = list(item) #LINE# #TAB# #TAB# #TAB# if len(line) >= size: #LINE# #TAB# #TAB# #TAB# #TAB# yield line #LINE# #TAB# #TAB# #TAB# table.append(line) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for line in item: #LINE# #TAB# #TAB# #TAB# #TAB# line = list(line) #LINE# #TAB# #TAB# #TAB# #TAB# table.append(line) #LINE# #TAB# if wrap: #LINE# #TAB# #TAB# return table #LINE# #TAB# return table
"#LINE# #TAB# ifupdater is None: #LINE# #TAB# #TAB# updater = UPDATEER #LINE# #TAB# setup_py = None #LINE# #TAB# if not hasattr(updater,'setup_py'): #LINE# #TAB# #TAB# setattr(updater,'setup_py', setup_py) #LINE# #TAB# daemon = threading.Thread(target=_setup_py) #LINE# #TAB# daemon.start() #LINE# #TAB# return setup_py"
"#LINE# #TAB# D = {} #LINE# #TAB# for i, d in enumerate(data): #LINE# #TAB# #TAB# new_d = D[mask] #LINE# #TAB# #TAB# D[i] = new_d #LINE# #TAB# return D"
"#LINE# #TAB# if filepath is None: #LINE# #TAB# #TAB# filepath = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# #TAB# 'fydarc') #LINE# #TAB# with open(filepath, 'r') as port_file: #LINE# #TAB# #TAB# data = yaml.safe_load(port_file) #LINE# #TAB# data['fydarc'] = data #LINE# #TAB# return data"
#LINE# #TAB# string = string[0].upper() + string[1:] #LINE# #TAB# return string
"#LINE# #TAB# for f in os.listdir(dirname): #LINE# #TAB# #TAB# full_path = os.path.join(dirname, f) #LINE# #TAB# #TAB# if os.path.isdir(full_path): #LINE# #TAB# #TAB# #TAB# for cell in compare_cell_moderate(full_path): #LINE# #TAB# #TAB# #TAB# #TAB# yield cell"
"#LINE# #TAB# if not blocks: #LINE# #TAB# #TAB# return '-' #LINE# #TAB# else: #LINE# #TAB# #TAB# etag = '%s-%s' % (blocks[0][0], blocks[0][1]) #LINE# #TAB# #TAB# return etag"
"#LINE# #TAB# if isinstance(data, Mapping): #LINE# #TAB# #TAB# yield data #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# for leftover in cls.greatest_references(v): #LINE# #TAB# #TAB# #TAB# #TAB# yield leftover #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# for item in v: #LINE# #TAB# #TAB# #TAB# #TAB# yield item"
"#LINE# #TAB# thread_levels = [] #LINE# #TAB# for result in B: #LINE# #TAB# #TAB# if 'application/json' in result['Content-Type']: #LINE# #TAB# #TAB# #TAB# thread_levels.append(('application/json', 'yes')) #LINE# #TAB# #TAB# elif 'application/json' in result['Content-Type']: #LINE# #TAB# #TAB# #TAB# return ('application/json', 'yes') #LINE# #TAB# return thread_levels"
"#LINE# #TAB# slot_names = [] #LINE# #TAB# superclasses = [c for c in classlist(class_)[1:]] #LINE# #TAB# for c in superclasses: #LINE# #TAB# #TAB# if hasattr(c, '__slots__'): #LINE# #TAB# #TAB# #TAB# slot_names += get_sub_contours_and_ordering_by_frame(c) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# slot_names.append(c.__name__) #LINE# #TAB# return slot_names"
"#LINE# #TAB# for i, score2 in enumerate(score): #LINE# #TAB# #TAB# if i == 5: #LINE# #TAB# #TAB# #TAB# score2[i] = colors.GREEN #LINE# #TAB# #TAB# if i == 6: #LINE# #TAB# #TAB# #TAB# score2[i] = colors.RED #LINE# #TAB# return score2"
"#LINE# #TAB# things = {} #LINE# #TAB# for param in args: #LINE# #TAB# #TAB# if param in path: #LINE# #TAB# #TAB# #TAB# things[param] = _rest_filter(path, *args[param]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# things[param] = _rest_get(path, *args) #LINE# #TAB# return things"
"#LINE# #TAB# ds_path = os.path.join(base_dir, ddf_id + '.ddf') #LINE# #TAB# if os.path.exists(ds_path): #LINE# #TAB# #TAB# with open(ds_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# ds = DDF(json.load(f)) #LINE# #TAB# return ds"
#LINE# #TAB# if fd.fileno() > 0: #LINE# #TAB# #TAB# handles = fd.fileno() #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# data = read_file(fd) #LINE# #TAB# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# handles.extend(data) #LINE# #TAB# #TAB# fd.close() #LINE# #TAB# else: #LINE# #TAB# #TAB# return
"#LINE# #TAB# if variable not in obj: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# for key in obj.keys(): #LINE# #TAB# #TAB# #TAB# if key == variable: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# fields = cls.get_fields() #LINE# #TAB# fullname = fields[0] #LINE# #TAB# if'schema' in fields[1]: #LINE# #TAB# #TAB# schema = fields[1].get('schema') #LINE# #TAB# #TAB# if schema: #LINE# #TAB# #TAB# #TAB# fullname = schema.table_name #LINE# #TAB# return fullname
"#LINE# #TAB# code3 = max(code1, ord('a')) #LINE# #TAB# code4 = min(code2, ord('z') + 1) #LINE# #TAB# if code3 < code4: #LINE# #TAB# #TAB# g1 = ord('A') #LINE# #TAB# #TAB# g2 = ord('Z') #LINE# #TAB# elif code3 < code4: #LINE# #TAB# #TAB# g1 = ord('A') #LINE# #TAB# #TAB# g2 = ord('Z') #LINE# #TAB# else: #LINE# #TAB# #TAB# g1 = '' #LINE# #TAB# #TAB# g2 = '' #LINE# #TAB# return g1, g2"
"#LINE# #TAB# if type(list_) is tuple: #LINE# #TAB# #TAB# list_ = list_, #LINE# #TAB# else: #LINE# #TAB# #TAB# list_ = [list_] #LINE# #TAB# return list_"
"#LINE# #TAB# parsed = sqlparse.parse(statement) #LINE# #TAB# netns = [] #LINE# #TAB# last_token = None #LINE# #TAB# for token in parsed: #LINE# #TAB# #TAB# if token.is_whitespace(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if last_token: #LINE# #TAB# #TAB# #TAB# netns.append(token.replace('\t','')) #LINE# #TAB# #TAB# #TAB# last_token = token.strip() #LINE# #TAB# if last_token: #LINE# #TAB# #TAB# netns.append(last_token) #LINE# #TAB# return netns"
"#LINE# #TAB# return [['year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], [ #LINE# #TAB# #TAB# 'year','month', 'day', 'hour','minute','second'], []]]"
"#LINE# #TAB# if name in _YAML_EXTENSIONS: #LINE# #TAB# #TAB# extension = _YAML_EXTENSIONS[name] #LINE# #TAB# #TAB# if extension in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# #TAB# with open(os.path.join(os.getcwd(), '%s.yml' % extension), 'r') as f: #LINE# #TAB# #TAB# #TAB# #TAB# out = yaml.safe_load(f) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out = yaml.safe_load(f) #LINE# #TAB# else: #LINE# #TAB# #TAB# out = None #LINE# #TAB# return out"
"#LINE# #TAB# address = address.lower() #LINE# #TAB# for family in [socket.AF_INET, socket.AF_INET6]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# socket.inet_pton(family, address) #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return family"
"#LINE# #TAB# if args is None: #LINE# #TAB# #TAB# args = () #LINE# #TAB# try: #LINE# #TAB# #TAB# obj_size = len(method(args)) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj_size = int(hash(method(args))) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# for key, value in args.items(): #LINE# #TAB# #TAB# #TAB# #TAB# obj_size *= len(key) #LINE# #TAB# #TAB# return obj_size #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj_size"
"#LINE# #TAB# if infile.endswith('.docx'): #LINE# #TAB# #TAB# docid = 'docx' #LINE# #TAB# else: #LINE# #TAB# #TAB# docid = 'odf' #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(infile, 'r') as f: #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# line = f.readline().strip() #LINE# #TAB# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# #TAB# docid = line.split('\t')[0] #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return docid"
"#LINE# #TAB# import json #LINE# #TAB# config = json.loads((Path(__file__).parent / 'feh.json').read_text()) #LINE# #TAB# home = os.environ.get('HOME', os.path.expanduser('~') + '/.config') #LINE# #TAB# if home is None: #LINE# #TAB# #TAB# os.makedirs(home) #LINE# #TAB# config_zip = os.path.join(home, 'feh.zip') #LINE# #TAB# zip_path = os.path.join(home, 'feh.zip') #LINE# #TAB# if not os.path.exists(zip_path): #LINE# #TAB# #TAB# os.makedirs(zip_path) #LINE# #TAB# with open(config_zip, 'w') as f: #LINE# #TAB# #TAB# f.write(json.dumps(config)) #LINE# #TAB# return config_zip"
#LINE# #TAB# t.value = float(t.value) #LINE# #TAB# return t
"#LINE# #TAB# if _py2 and isinstance(s, str): #LINE# #TAB# #TAB# return s.decode('utf-8') #LINE# #TAB# if _py3 and isinstance(s, str): #LINE# #TAB# #TAB# return s #LINE# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(num) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# return num"
"#LINE# #TAB# ret = 0 #LINE# #TAB# for n, t in enumerate(agents_in_list): #LINE# #TAB# #TAB# if t == val: #LINE# #TAB# #TAB# #TAB# ret += 1 #LINE# #TAB# return ret"
"#LINE# #TAB# print('Generate the SVG file {:s}'.format(ai_filename)) #LINE# #TAB# o_accounts = [] #LINE# #TAB# for i_ol in ai_figure: #LINE# #TAB# #TAB# o_acc = generate_account(i_ol, ai_filename) #LINE# #TAB# #TAB# for o_acc in o_acc: #LINE# #TAB# #TAB# #TAB# o_accounts.append(o_acc) #LINE# #TAB# return o_accounts"
"#LINE# #TAB# _check_template_string(template) #LINE# #TAB# return template.find('{') >= 0 and template.find('}', 0, _check_template_string #LINE# #TAB# #TAB# ) >= 0"
"#LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# _, src = os.path.split(filename) #LINE# #TAB# #TAB# _, dst = os.path.split(dst) #LINE# #TAB# #TAB# os.symlink(src, dst) #LINE# #TAB# return dst"
"#LINE# #TAB# d = {'a': r[0], 'b': r[1]} #LINE# #TAB# return {'type': 'float', 'data': {'min': r[0],'max': r[1],'step': r[2], #LINE# #TAB# #TAB# 'angle': r[3]}"
"#LINE# #TAB# cwd = Path.cwd() #LINE# #TAB# repo_path = str(cwd) #LINE# #TAB# col_mean_prob_path = repo_path / 'data' / 'col_mean.prob' #LINE# #TAB# if col_mean_prob_path.exists(): #LINE# #TAB# #TAB# return col_mean_prob_path #LINE# #TAB# else: #LINE# #TAB# #TAB# with open(repo_path) as f: #LINE# #TAB# #TAB# #TAB# data = f.readlines() #LINE# #TAB# #TAB# #TAB# for row in data: #LINE# #TAB# #TAB# #TAB# #TAB# col_mean_prob_path.insert(0, row) #LINE# #TAB# #TAB# return col_mean_prob_path"
"#LINE# #TAB# if low_freq_cutoff < 0: #LINE# #TAB# #TAB# raise ValueError(""The low_freq_cutoff can't be negative."") #LINE# #TAB# broadcast_length = length * delta_f #LINE# #TAB# broadcast_freq = FrequencySeries(broadcast_length, index=broadcast_length) #LINE# #TAB# for col in range(1, broadcast_length + 1): #LINE# #TAB# #TAB# broadcast_freq[col] = broadcast_freq[col] / float(length) #LINE# #TAB# return broadcast_freq"
#LINE# #TAB# rate = ET.Element('rate') #LINE# #TAB# rate.text = fps #LINE# #TAB# return rate
"#LINE# #TAB# if not hasattr(this, 'group_fields'): #LINE# #TAB# #TAB# return #LINE# #TAB# this.group_fields = [this] #LINE# #TAB# for field in this.group_fields: #LINE# #TAB# #TAB# if field.name == group_name: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not hasattr(field, 'fields'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# field.fields.append(this) #LINE# #TAB# return"
"#LINE# #TAB# string = re.sub('^[^_A-Za-z0-9]+$', '', string) #LINE# #TAB# string = re.sub('_([a-zA-Z])', lambda m: m.group(1).upper(), string) #LINE# #TAB# return string"
"#LINE# #TAB# if maya.cmds.objExists(geometry): #LINE# #TAB# #TAB# geo_obj = maya.cmds.listRelatives(geometry, s=True, pa=True) #LINE# #TAB# #TAB# if not geo_obj: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# pairs = maya.cmds.listRelatives(geo_obj, pa=True) #LINE# #TAB# #TAB# if len(pairs) == 1: #LINE# #TAB# #TAB# #TAB# return pairs[0] #LINE# #TAB# return None"
"#LINE# #TAB# if data.chunks: #LINE# #TAB# #TAB# chunks = list(range(data.shape[1])) #LINE# #TAB# else: #LINE# #TAB# #TAB# chunks = range(data.shape[1]) #LINE# #TAB# for i in range(num_epochs): #LINE# #TAB# #TAB# chunk = data[i * batch_size:(i + 1) * batch_size] #LINE# #TAB# #TAB# train_data = np.array(chunk).reshape((batch_size,) + data.shape[1:]) #LINE# #TAB# #TAB# test_data = np.array(chunk).reshape((batch_size,)) #LINE# #TAB# #TAB# train_data = train_data.reshape((batch_size,) + data.shape[1:]) #LINE# #TAB# #TAB# test_data = train_data.reshape((batch_size,) + data.shape[2:]) #LINE# #TAB# #TAB# yield train_data, test_data"
"#LINE# #TAB# values = [s] #LINE# #TAB# count = int(gl_get_info_log(s, n)) #LINE# #TAB# if count > 0: #LINE# #TAB# #TAB# for i in range(count): #LINE# #TAB# #TAB# #TAB# values.append(values[i] ) #LINE# #TAB# #TAB# _simple.gl_get_info_log(s, count, i) #LINE# #TAB# else: #LINE# #TAB# #TAB# _simple.gl_get_info_log_arb(s, n, None, values) #LINE# #TAB# return values"
"#LINE# #TAB# config = json.loads(open(file_name, 'r').read()) #LINE# #TAB# return {'configs': config['configs'], 'args': config['args'], #LINE# #TAB# #TAB# 'params': config['params']}"
#LINE# #TAB# if len(letter)!= 1: #LINE# #TAB# #TAB# raise ValueError('The target string must be one letter.') #LINE# #TAB# if letter[0] == 'u': #LINE# #TAB# #TAB# return True #LINE# #TAB# elif letter[0] == 'v': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if session is None: #LINE# #TAB# #TAB# session = flask.session #LINE# #TAB# obj = session.query(obj).filter_by(pk=obj.pk).first() #LINE# #TAB# if skip!= {}: #LINE# #TAB# #TAB# obj.pk = skip #LINE# #TAB# return obj
"#LINE# #TAB# B = nx.DiGraph() #LINE# #TAB# for v in G.nodes(data=True): #LINE# #TAB# #TAB# for u, v2 in G.edges(v): #LINE# #TAB# #TAB# #TAB# B.add_edge(u, v2) #LINE# #TAB# return B"
"#LINE# #TAB# res = [] #LINE# #TAB# for start, quantity in d.items(): #LINE# #TAB# #TAB# if start == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# end = min(start + width, len(quantity) - 1) #LINE# #TAB# #TAB# res.append((start, end, quantity)) #LINE# #TAB# return res"
"#LINE# #TAB# count = None #LINE# #TAB# for repo in [reponame, reposave]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# count = bam.get_read_count(path) #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# print('Could not find'+ repo) #LINE# #TAB# if not count: #LINE# #TAB# #TAB# return 'Could not find'+ path #LINE# #TAB# return count"
#LINE# #TAB# if len(value) == 8: #LINE# #TAB# #TAB# return cache_property_accessor_8(value) #LINE# #TAB# try: #LINE# #TAB# #TAB# return cache_property_accessor_16(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if not 'quaternion' in holder: #LINE# #TAB# #TAB# holder['quaternion'] = Quaternion() #LINE# #TAB# return holder['quaternion']
"#LINE# #TAB# for field in node.fields: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield field, getattr(node, field) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass"
#LINE# #TAB# term_info = get_term_info() #LINE# #TAB# if 'truecolor' in term_info: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif '24bit' in term_info: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# _module_name = 'ds_connectors.handlers.aws_s3_handlers' #LINE# #TAB# _handler = 'AwsS3PersistHandler' #LINE# #TAB# return _module_name, _handler"
"#LINE# #TAB# if mod_name is None: #LINE# #TAB# #TAB# mod_name = filepath #LINE# #TAB# mod_path = os.path.dirname(filepath) #LINE# #TAB# sys.path.insert(0, mod_path) #LINE# #TAB# with open(filepath) as f: #LINE# #TAB# #TAB# code = compile(f.read(), filepath, 'exec') #LINE# #TAB# mod = sys.modules[mod_name] #LINE# #TAB# return mod.epsilon_effective"
"#LINE# #TAB# if isinstance(src, dict): #LINE# #TAB# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# value = str(value) #LINE# #TAB# #TAB# #TAB# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# value = str(value) #LINE# #TAB# #TAB# #TAB# src[key] = value #LINE# #TAB# elif isinstance(src, list): #LINE# #TAB# #TAB# for item in src: #LINE# #TAB# #TAB# #TAB# resolve_tree(item) #LINE# #TAB# return src"
"#LINE# #TAB# subparser.add_argument('-d', '--debug', action='store_true', default=False, #LINE# #TAB# #TAB# help='Be verbose.') #LINE# #TAB# return download_audio"
#LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# properties = [] #LINE# #TAB# property_list = soup.find_all('property') #LINE# #TAB# for prop in property_list: #LINE# #TAB# #TAB# if prop.get('type') == 'article-id': #LINE# #TAB# #TAB# #TAB# article_id = prop.get('content') #LINE# #TAB# #TAB# #TAB# properties.append(article_id) #LINE# #TAB# return properties
"#LINE# #TAB# max_lang = wx.Config.Get().Get(wx.CONF_LANG) #LINE# #TAB# if max_lang: #LINE# #TAB# #TAB# wx.Config.Set('ToolBox', 'ToolBox height: {}'.format(max_lang)) #LINE# #TAB# else: #LINE# #TAB# #TAB# wx.Config.Set('ToolBox', 'ToolBox height: {}'.format(max_lang)) #LINE# #TAB# return"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# winreg.OpenKey(key, HKEY_LOCAL_MACHINE) #LINE# #TAB# except WindowsError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return request.query_params.get('jni.access_token', u'') #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if 'evidence' not in json: #LINE# #TAB# #TAB# return json #LINE# #TAB# for k, v in json['evidence'].items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# json['evidence'][k] = fix_evidence_text(v) #LINE# #TAB# #TAB# elif 'text' in v and 'link' in v: #LINE# #TAB# #TAB# #TAB# json['evidence'][k]['link'] = Link(v['text']) #LINE# #TAB# return json"
#LINE# #TAB# if header in IGNORE_HEADERS: #LINE# #TAB# #TAB# return True #LINE# #TAB# if header.startswith('HTTP_') or header == 'CONTENT_TYPE': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# if not isinstance(inp, numbers.Number): #LINE# #TAB# #TAB# check_value(inp) #LINE# #TAB# #TAB# raise error.BASICError(err) #LINE# #TAB# return inp"
"#LINE# #TAB# url = c_urls.compose_reset_state_url.format(cid) #LINE# #TAB# headers = {'Accept': 'application/json'} #LINE# #TAB# r = requests.post(url, headers=headers) #LINE# #TAB# if r.status_code!= 200: #LINE# #TAB# #TAB# return '' #LINE# #TAB# try: #LINE# #TAB# #TAB# return r.json() #LINE# #TAB# except JSONDecodeError: #LINE# #TAB# #TAB# return ''"
"#LINE# #TAB# bigdataset_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), #LINE# #TAB# #TAB# envkey) #LINE# #TAB# nearest_big_datasets = [] #LINE# #TAB# big_dataset_paths = get_big_data_paths(bigdataset_path) #LINE# #TAB# for path in big_dataset_paths: #LINE# #TAB# #TAB# relative_path = os.path.relpath(path, start=bigdataset_path) #LINE# #TAB# #TAB# if relative_path.count('/') == 1: #LINE# #TAB# #TAB# #TAB# nearest_big_datasets.append(relative_path) #LINE# #TAB# if len(nearest_big_datasets) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return nearest_big_datasets[0]"
"#LINE# #TAB# ner = cfg.specs.get('ner') #LINE# #TAB# seq_label_config = cfg.specs.get('seq_label') #LINE# #TAB# return { #LINE# #TAB# #TAB# 'ner': ner, #LINE# #TAB# #TAB#'seq_label': seq_label_config, #LINE# #TAB# }"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# c = compile(source, name, 'eval') #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# c = compile(source, name, 'exec') #LINE# #TAB# return c"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# spellcheck_output = subprocess.check_output(['spellcheck', #LINE# #TAB# #TAB# #TAB# '--disk-partition-usage'], cwd=build_dir) #LINE# #TAB# #TAB# return len(spellcheck_output.decode('utf-8').splitlines()) #LINE# #TAB# except (OSError, subprocess.CalledProcessError): #LINE# #TAB# #TAB# return 0"
"#LINE# #TAB# s = s.replace('&', '&amp;') #LINE# #TAB# s = s.replace('<', '&lt;') #LINE# #TAB# s = s.replace('>', '&gt;') #LINE# #TAB# s = s.replace('""', '&quot;') #LINE# #TAB# return s"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return next(w for w, _ in enumerate(yw)) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return 0, 0"
"#LINE# #TAB# if registry is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# key = registry.CreateKey(registry.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Valve\\Steam') #LINE# #TAB# return registry.QueryValueEx(key, 'SteamPath')[0]"
"#LINE# #TAB# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# #TAB# el.text = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# el.text = el.text.strip() #LINE# #TAB# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# for node in program_ast.body: #LINE# #TAB# #TAB# if isinstance(node, ast.Name): #LINE# #TAB# #TAB# #TAB# name = node.id #LINE# #TAB# #TAB# #TAB# value = getattr(node, 'value', None) #LINE# #TAB# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# #TAB# return value #LINE# #TAB# return None"
#LINE# #TAB# consensus_snapshot = cls.generate_consensus_snapshot_from_ops_hash( record_root_hash ) #LINE# #TAB# for prev_hash in prev_consensus_hashes: #LINE# #TAB# #TAB# consensus_snapshot = cls.generate_consensus_snapshot_from_ops_hash( #LINE# #TAB# #TAB# #TAB# prev_hash ) #LINE# #TAB# return consensus_snapshot
"#LINE# #TAB# a_names = set(a.keys()) #LINE# #TAB# b_names = set(b.keys()) #LINE# #TAB# a_def = dict(name=a['name'], description=a['description']) #LINE# #TAB# b_def = dict(name=b['name'], description=b['description']) #LINE# #TAB# return a_names == b_names"
"#LINE# #TAB# if not _handler_loaded: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(handler_path, 'r') as fp: #LINE# #TAB# #TAB# #TAB# #TAB# data = json.load(fp) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# data = {} #LINE# #TAB# #TAB# _handler_loaded = True #LINE# #TAB# return data"
"#LINE# #TAB# return {'data': {'action_button': button.data, 'action_button_label': #LINE# #TAB# #TAB# button.label, 'action_button_color': button.color}, 'actions': [ #LINE# #TAB# #TAB# combined_action_to_dict(action) for action in button.actions]}"
#LINE# #TAB# global _open_brackets #LINE# #TAB# _open_brackets[name] = app
"#LINE# #TAB# family = socket.AF_INET #LINE# #TAB# sock = socket.socket(family, socket.SOCK_DGRAM) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1) #LINE# #TAB# return sock"
"#LINE# #TAB# alt_genome = tz.get_in([""config"", ""algorithm"", ""genotype""], data) #LINE# #TAB# genotype = tz.get_in([""config"", ""algorithm"", ""genotype""], data) #LINE# #TAB# query_string = ""{0} | {1}"".format( #LINE# #TAB# #TAB# genotype, #LINE# #TAB# #TAB# alt_genome #LINE# #TAB# ) #LINE# #TAB# return query_string"
#LINE# #TAB# if sys.version_info[0] >= 3: #LINE# #TAB# #TAB# encoding = 'utf-8-sig' #LINE# #TAB# elif sys.version_info[0] == 2: #LINE# #TAB# #TAB# if encoding == 'utf-8-sig': #LINE# #TAB# #TAB# #TAB# encoding = 'utf-8-sig' #LINE# #TAB# #TAB# elif encoding == 'cp1252': #LINE# #TAB# #TAB# #TAB# encoding = 'cp1252' #LINE# #TAB# return encoding
#LINE# #TAB# repo = Repo(str(git_folder)) #LINE# #TAB# neighbour_hashes = list(repo.neighbours.keys()) #LINE# #TAB# if len(neighbour_hashes) == 1: #LINE# #TAB# #TAB# return neighbour_hashes[0].hexsha #LINE# #TAB# elif len(neighbour_hashes) > 1: #LINE# #TAB# #TAB# return neighbour_hashes[-1].hexsha #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# x = int(x) #LINE# #TAB# if x < 0: #LINE# #TAB# #TAB# raise ValueError('Cannot be smaller than 0') #LINE# #TAB# if x >= 1: #LINE# #TAB# #TAB# return x - 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return x
"#LINE# #TAB# tab = [] #LINE# #TAB# total = 0 #LINE# #TAB# for line in fileList.split('\n'): #LINE# #TAB# #TAB# temp = line.split() #LINE# #TAB# #TAB# if len(temp) < 2: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# channel = temp[0] #LINE# #TAB# #TAB# length = float(temp[1]) #LINE# #TAB# #TAB# low = float(temp[2]) #LINE# #TAB# #TAB# high = float(temp[3]) #LINE# #TAB# #TAB# tab.append([channel, int(low), int(high)]) #LINE# #TAB# #TAB# total += length #LINE# #TAB# assert total > 0 #LINE# #TAB# return tab"
"#LINE# #TAB# import json #LINE# #TAB# loc_file = os.path.join(os.path.dirname(__file__), 'data','refund_address.json') #LINE# #TAB# with open(loc_file) as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# refund_address = {'service_id': data['service_id'],'refund_address': #LINE# #TAB# #TAB# #TAB# data['refund_address']} #LINE# #TAB# return refund_address"
"#LINE# #TAB# leushort = getattr(module, class_string, None) #LINE# #TAB# if leushort is not None: #LINE# #TAB# #TAB# return leushort #LINE# #TAB# parts = class_string.split('.') #LINE# #TAB# for index in range(len(parts) - 1): #LINE# #TAB# #TAB# leushort = getattr(leushort, parts[index + 1]) #LINE# #TAB# #TAB# if inspect.isclass(leushort): #LINE# #TAB# #TAB# #TAB# return leushort"
"#LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# if grayscale: #LINE# #TAB# #TAB# #TAB# img = cv2.imread(path, 0) #LINE# #TAB# #TAB# #TAB# if target_size: #LINE# #TAB# #TAB# #TAB# #TAB# img = cv2.resize(img, target_size) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# img = cv2.imread(path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# img = None #LINE# #TAB# else: #LINE# #TAB# #TAB# img = path #LINE# #TAB# return img, grayscale"
"#LINE# #TAB# form_yaml = None #LINE# #TAB# with open(input_file, 'r') as form_file: #LINE# #TAB# #TAB# form_yaml = yaml.load(form_file) #LINE# #TAB# return form_yaml"
#LINE# #TAB# languages = [] #LINE# #TAB# for lang in get_available_locales(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# language = Language.objects.get(**request.GET) #LINE# #TAB# #TAB# except Language.DoesNotExist: #LINE# #TAB# #TAB# #TAB# language = None #LINE# #TAB# #TAB# if not language: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# languages.append(language) #LINE# #TAB# return {'language_code': language.code}
"#LINE# #TAB# try: #LINE# #TAB# #TAB# acl_url = urljoin(_acl_url(), 'users/{}'.format(uid)) #LINE# #TAB# #TAB# r = http.put(acl_url) #LINE# #TAB# #TAB# assert r.status_code == 201 #LINE# #TAB# except DCOSHTTPException as e: #LINE# #TAB# #TAB# if e.response.status_code!= 409: #LINE# #TAB# #TAB# #TAB# raise"
#LINE# #TAB# if word.islower(): #LINE# #TAB# #TAB# return word #LINE# #TAB# if correction in manchester_map: #LINE# #TAB# #TAB# return manchester_map[correction] #LINE# #TAB# if word.isupper(): #LINE# #TAB# #TAB# return word.lower() #LINE# #TAB# if word == '0' and correction not in manchester_map: #LINE# #TAB# #TAB# return '0' #LINE# #TAB# for c in correction: #LINE# #TAB# #TAB# if word.islower(): #LINE# #TAB# #TAB# #TAB# return '0' + c #LINE# #TAB# return word
"#LINE# #TAB# names = list(names) #LINE# #TAB# while names: #LINE# #TAB# #TAB# name = names.pop() #LINE# #TAB# #TAB# parent = os.path.dirname(name) #LINE# #TAB# #TAB# while parent and os.path.isfile(os.path.join(parent, name)): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.unlink(name) #LINE# #TAB# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# names.remove(name) #LINE# #TAB# #TAB# parent = os.path.dirname(name) #LINE# #TAB# return names"
"#LINE# #TAB# msg_dict = {} #LINE# #TAB# for k, v in dict_of_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# msg_dict[k] = check_and_report_errors(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# msg_dict[k] = v #LINE# #TAB# return msg_dict"
"#LINE# #TAB# radar = sorted(radar, key=lambda x: (x[1], x[0])) #LINE# #TAB# for sweep in radar: #LINE# #TAB# #TAB# for i in range(2): #LINE# #TAB# #TAB# #TAB# if sweep not in radar[i]: #LINE# #TAB# #TAB# #TAB# #TAB# yield sweep"
"#LINE# #TAB# arr = None #LINE# #TAB# if isinstance(mylist, list): #LINE# #TAB# #TAB# mylist = np.asarray(mylist) #LINE# #TAB# for i, v in enumerate(mylist): #LINE# #TAB# #TAB# arr[i] = dcmdottoang_vel(v) #LINE# #TAB# return arr"
#LINE# #TAB# if not os.path.isfile(folder): #LINE# #TAB# #TAB# return False #LINE# #TAB# if '.py' in folder or '.pyc' in folder: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# escseq = '\nInvalid request.' #LINE# #TAB# return escseq
"#LINE# #TAB# dim = len(s) #LINE# #TAB# x = np.arange(dim) #LINE# #TAB# is_valid = False #LINE# #TAB# while not is_valid: #LINE# #TAB# #TAB# if s[dim] == 0: #LINE# #TAB# #TAB# #TAB# is_valid = True #LINE# #TAB# #TAB# #TAB# x, y = s[dim], s[dim] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# x, y = np.abs(s[dim]), np.abs(s[dim]) #LINE# #TAB# #TAB# x, y = np.meshgrid(x, y) #LINE# #TAB# #TAB# is_valid = True #LINE# #TAB# row = [int(x[0]), int(x[1])] #LINE# #TAB# col = [int(x[2])] #LINE# #TAB# return [row, col]"
"#LINE# #TAB# hasher = sha256() #LINE# #TAB# with open(file, 'rb') as f: #LINE# #TAB# #TAB# buf = f.read() #LINE# #TAB# #TAB# while len(buf) > 0: #LINE# #TAB# #TAB# #TAB# hasher.update(buf) #LINE# #TAB# #TAB# #TAB# buf = f.read(0x1000000) #LINE# #TAB# return hasher.hexdigest()[0:16]"
#LINE# #TAB# #TAB# obj = cls(name) #LINE# #TAB# #TAB# obj.exporter = 'get_output_table_name' #LINE# #TAB# #TAB# obj.output_name = output_name #LINE# #TAB# #TAB# return obj
"#LINE# #TAB# mapper = SimpleFastaMapper() #LINE# #TAB# with open(fasta_name, 'r') as f: #LINE# #TAB# #TAB# while 1: #LINE# #TAB# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# mapper.add_record(line) #LINE# #TAB# #TAB# #TAB# fasta_name = line[1:] #LINE# #TAB# return mapper"
"#LINE# #TAB# if not isinstance(identifier, URIRef): #LINE# #TAB# #TAB# identifier = URIRef(identifier) #LINE# #TAB# store = DjangoStore(store_id) #LINE# #TAB# graph = Graph(store, identifier=identifier) #LINE# #TAB# graph.open() #LINE# #TAB# return graph"
"#LINE# #TAB# b1 = set(g1[0]) #LINE# #TAB# b2 = set(g2[0]) #LINE# #TAB# a = set(g1[1]) #LINE# #TAB# b = set(g2[1]) #LINE# #TAB# c = set(g1[2]) #LINE# #TAB# if a!= b: #LINE# #TAB# #TAB# return False, False #LINE# #TAB# for a, b in zip(a, b): #LINE# #TAB# #TAB# if b!= a: #LINE# #TAB# #TAB# #TAB# return False, False #LINE# #TAB# return True, True"
#LINE# #TAB# merge_props = {} #LINE# #TAB# if previous_props: #LINE# #TAB# #TAB# for key in list(previous_props.keys()): #LINE# #TAB# #TAB# #TAB# if key in next_props.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# merge_props[key] = next_props[key] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# merge_props[key] = previous_props[key] #LINE# #TAB# if merge_props: #LINE# #TAB# #TAB# return {filename_to_hex(x) for x in merge_props.keys()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}
#LINE# #TAB# ret = None #LINE# #TAB# array_type = construct_array(message)[0] #LINE# #TAB# if array_type == 'N': #LINE# #TAB# #TAB# ret = construct_array(message[2:]) #LINE# #TAB# if array_type == 'A': #LINE# #TAB# #TAB# ret = construct_array(message[3:]) #LINE# #TAB# if array_type == 'C': #LINE# #TAB# #TAB# ret = construct_array(message[4:]) #LINE# #TAB# return ret
#LINE# #TAB# all_languages = _load_all_languages() #LINE# #TAB# for language in all_languages: #LINE# #TAB# #TAB# if search_text in language.search_text: #LINE# #TAB# #TAB# #TAB# yield language
"#LINE# #TAB# value = value.replace('\\', '\\\\') #LINE# #TAB# value = value.replace('+', '\\+') #LINE# #TAB# value = value.replace('-', '\\-') #LINE# #TAB# value = value.replace('_', '\\_') #LINE# #TAB# return value"
"#LINE# #TAB# text = re.sub('\\[(.*?)\\]\\]', '\\1', text) #LINE# #TAB# text = re.sub('\\[(.*?)\\]', '\\1', text) #LINE# #TAB# return text"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# c = getattr(m, 'config') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# c = m #LINE# #TAB# #TAB# setattr(m, 'config', c) #LINE# #TAB# return c"
#LINE# #TAB# try: #LINE# #TAB# #TAB# import slug_strip #LINE# #TAB# #TAB# return slug_strip.slug #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if igs is None: #LINE# #TAB# #TAB# igs = range(len(conns)) #LINE# #TAB# mesh = Mesh(name) #LINE# #TAB# mesh._set_data(coors=coors, ngroups=ngroups, conns=[conns[ig] for ig in #LINE# #TAB# #TAB# igs], mat_ids=[mat_ids[ig] for ig in igs], descs=[descs[ig] for ig in #LINE# #TAB# #TAB# igs]) #LINE# #TAB# mesh._set_shape_info() #LINE# #TAB# return mesh"
"#LINE# #TAB# for obj in builder.get_objects(): #LINE# #TAB# #TAB# if isinstance(obj, gtk.Window): #LINE# #TAB# #TAB# #TAB# return obj"
#LINE# #TAB# key = '' #LINE# #TAB# for obj in objects: #LINE# #TAB# #TAB# key += str(obj.__class__) #LINE# #TAB# #TAB# if key not in summary: #LINE# #TAB# #TAB# #TAB# summary[key] = type(obj) #LINE# #TAB# if summary[key] == 'all': #LINE# #TAB# #TAB# return 'all' #LINE# #TAB# else: #LINE# #TAB# #TAB# return summary[key]
"#LINE# #TAB# path = os.path.join(os.path.dirname(os.path.abspath(__file__)), #LINE# #TAB# #TAB# 'data', 'bilby.json') #LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# return json.load(f) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if not _is_pil_image(img): #LINE# #TAB# #TAB# raise TypeError('img should be PIL Image. Got {}'.format(type(img))) #LINE# #TAB# if bgcolor is not None: #LINE# #TAB# #TAB# img = img.convert('RGBA') #LINE# #TAB# else: #LINE# #TAB# #TAB# img = img.convert(bgcolor) #LINE# #TAB# return img
"#LINE# #TAB# fret_score = 0 #LINE# #TAB# for u, v in df.iteritems(): #LINE# #TAB# #TAB# if u[fld] > fret_score: #LINE# #TAB# #TAB# #TAB# fret_score = v #LINE# #TAB# if fret_score > 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# _logger = logging.getLogger('reduce') #LINE# #TAB# formatter = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(name)s - %(levelname)s - %(message)s') #LINE# #TAB# handler = logging.StreamHandler(sys.stderr) #LINE# #TAB# handler.setFormatter(formatter) #LINE# #TAB# _logger.addHandler(handler) #LINE# #TAB# _logger.setLevel(logging.INFO) #LINE# #TAB# return _logger
"#LINE# #TAB# from tpDcc.libs.python import path #LINE# #TAB# if name.lower() == 'qtconsole': #LINE# #TAB# #TAB# return path.dirname(path.abspath(__file__)) + '/init.py' #LINE# #TAB# else: #LINE# #TAB# #TAB# from os import path #LINE# #TAB# #TAB# path = path.dirname(path.abspath(__file__)) + '/init.py' #LINE# #TAB# #TAB# if os.path.isfile(path.join(path, '%s.py' % name)): #LINE# #TAB# #TAB# #TAB# return path.abspath(path.join(path, '%s.py' % name)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return path"
"#LINE# #TAB# pop_no_diff_fields(latest_config, current_config) #LINE# #TAB# diff = DeepDiff( #LINE# #TAB# #TAB# latest_config, #LINE# #TAB# #TAB# current_config, #LINE# #TAB# #TAB# ignore_order=True #LINE# #TAB# ) #LINE# #TAB# return diff"
"#LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB# 'In requests 3.0, get_dependencies will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)' #LINE# #TAB# #TAB#, DeprecationWarning) #LINE# #TAB# tried_encodings = [] #LINE# #TAB# encoding = get_encoding(r) #LINE# #TAB# if encoding: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return str(r.content, encoding) #LINE# #TAB# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# #TAB# tried_encodings.append(encoding) #LINE# #TAB# try: #LINE# #TAB# #TAB# return str(r.content, encoding, errors='replace') #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return r.content"
#LINE# #TAB# amount = item.amount #LINE# #TAB# if amount > max_amount: #LINE# #TAB# #TAB# amount = 0 #LINE# #TAB# return amount + max_amount
"#LINE# #TAB# with pysam.AlignmentFile(bam_file, 'r') as f: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f.fetch() #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return f"
#LINE# #TAB# encoding = None #LINE# #TAB# if name is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# encoding = name.lower() #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# if sys.version_info[0] >= 3: #LINE# #TAB# #TAB# #TAB# #TAB# encoding = 'ascii' #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# encoding = 'ascii' #LINE# #TAB# if not encoding: #LINE# #TAB# #TAB# return None #LINE# #TAB# return encoding
"#LINE# #TAB# del value, frame #LINE# #TAB# return []"
#LINE# #TAB# val = None #LINE# #TAB# if 'incrementalMFD' in node.attributes.keys(): #LINE# #TAB# #TAB# val = float(node.attributes['incrementalMFD']) #LINE# #TAB# elif 'truncGutenbergRichterMFD' in node.attributes.keys(): #LINE# #TAB# #TAB# val = float(node.attributes['truncGutenbergRichterMFD']) #LINE# #TAB# elif'maxGutenbergRichterMFD' in node.attributes.keys(): #LINE# #TAB# #TAB# val = float(node.attributes['maxGutenbergRichterMFD']) #LINE# #TAB# elif'maxMagneticMFD' in node.attributes.keys(): #LINE# #TAB# #TAB# val = float(node.attributes['maxMagneticMFD']) #LINE# #TAB# return val
#LINE# #TAB# params = set() #LINE# #TAB# while len(string) > 0 and string[0].isdigit(): #LINE# #TAB# #TAB# params.add(int(string)) #LINE# #TAB# return params
#LINE# #TAB# combined_headers = set() #LINE# #TAB# for name in fnames: #LINE# #TAB# #TAB# header = _header_from_instrument(name) #LINE# #TAB# #TAB# if header is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if blend and header not in combined_headers: #LINE# #TAB# #TAB# #TAB# combined_headers.add(header) #LINE# #TAB# #TAB# combined_headers |= header #LINE# #TAB# return combined_headers
#LINE# #TAB# signal = [] #LINE# #TAB# for i in range(4): #LINE# #TAB# #TAB# signal.append(chr(i)) #LINE# #TAB# return signal
#LINE# #TAB# if model_mae is None or naive_mae is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# model_wwns = np.sqrt(float(model_mae) / float(naive_mae)) #LINE# #TAB# return model_wwns
#LINE# #TAB# for filename in os.listdir(directory): #LINE# #TAB# #TAB# if filename.endswith('.xml'): #LINE# #TAB# #TAB# #TAB# yield filename
"#LINE# #TAB# width, height = entity.width, entity.height #LINE# #TAB# if width and height: #LINE# #TAB# #TAB# x_min = xy[0] - width #LINE# #TAB# #TAB# y_min = xy[1] - height #LINE# #TAB# #TAB# x_max = xy[2] - width #LINE# #TAB# #TAB# y_max = xy[3] - height #LINE# #TAB# else: #LINE# #TAB# #TAB# x_min = xy[0] #LINE# #TAB# #TAB# y_min = xy[1] - width #LINE# #TAB# #TAB# y_max = xy[2] - height #LINE# #TAB# return x_min, y_min, x_max, y_max"
"#LINE# #TAB# if isinstance(obj, str): #LINE# #TAB# #TAB# return json.loads(obj) #LINE# #TAB# return obj"
"#LINE# #TAB#Forwarding = models.UwEmailForwarding() #LINE# #TAB#Forwarding.register_from_string(netid, 'callout_bg') #LINE# #TAB# returnForwarding"
#LINE# #TAB# clazz = [] #LINE# #TAB# for item in series: #LINE# #TAB# #TAB# if item in df.columns: #LINE# #TAB# #TAB# #TAB# clazz.append(item) #LINE# #TAB# if len(clazz) > 0: #LINE# #TAB# #TAB# return df[clazz].iloc[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return df.iloc[0]
#LINE# #TAB# tree = ET.parse(string) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# changed = False #LINE# #TAB# if root!= string: #LINE# #TAB# #TAB# changed = True #LINE# #TAB# return changed
#LINE# #TAB# message = [] #LINE# #TAB# for i in range(len(polygon)): #LINE# #TAB# #TAB# message.append(polygon[i * scale + i]) #LINE# #TAB# return message
"#LINE# #TAB# if isinstance(data, h5py.highlevel.Dataset): #LINE# #TAB# #TAB# data = data.values #LINE# #TAB# elif isinstance(data, np.ndarray): #LINE# #TAB# #TAB# data = np.array(data) #LINE# #TAB# for var in data.attrs: #LINE# #TAB# #TAB# if 'tolerance' in var: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# var = var.attrs['tolerance'] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError('{} is not a valid integer tolerance.'. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# format(var)) #LINE# #TAB# return var"
#LINE# #TAB# paths = {} #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# path = record.path #LINE# #TAB# #TAB# if path not in paths: #LINE# #TAB# #TAB# #TAB# paths[path] = AAIndexObject() #LINE# #TAB# #TAB# paths[path].path = path #LINE# #TAB# return paths
"#LINE# #TAB# for i in range(len(u) - 1, -1, -1): #LINE# #TAB# #TAB# if u[i] not in _kth_map: #LINE# #TAB# #TAB# #TAB# u = u.replace(i, _kth_map[u[i]]) #LINE# #TAB# return u"
#LINE# #TAB# velocity = {} #LINE# #TAB# if'velocity' in object: #LINE# #TAB# #TAB# velocity['velocity'] = object['velocity'] #LINE# #TAB# if'velocity' in object: #LINE# #TAB# #TAB# velocity['velocity'] = object['velocity'] #LINE# #TAB# return velocity
#LINE# #TAB# min_val = numpy.min(matrix) #LINE# #TAB# max_val = numpy.max(matrix) #LINE# #TAB# matrix[shift:shift + scale] = min_val #LINE# #TAB# return matrix
#LINE# #TAB# if 'image/png' not in mimetype or 'image/gif' not in mimetype: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# result = None #LINE# #TAB# filenames = glob.glob(mypath) #LINE# #TAB# for filename in filenames: #LINE# #TAB# #TAB# if filename.endswith('.vtk'): #LINE# #TAB# #TAB# #TAB# result = vtk.vtkFileSystem #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# raise FileNotFoundError('File not found.') #LINE# #TAB# return result
"#LINE# #TAB# response_data = response.json() #LINE# #TAB# indexer = response_data['indexer'] #LINE# #TAB# return {'indexer': indexer,'mask': indexer}"
#LINE# #TAB# for item in data: #LINE# #TAB# #TAB# if 'team' in item: #LINE# #TAB# #TAB# #TAB# return item['team'] #LINE# #TAB# return 'unknown'
"#LINE# #TAB# if exn is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if hasattr(exn, 'args'): #LINE# #TAB# #TAB# return any(is_pendant(arg) for arg in exn.args) #LINE# #TAB# return False"
#LINE# #TAB# obj = cls(name) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = obj.discrete_hmm() #LINE# #TAB# #TAB# except NotImplementedError: #LINE# #TAB# #TAB# #TAB# raise NotImplementedError('No curve named %s' % name) #LINE# #TAB# return obj
#LINE# #TAB# conn = None #LINE# #TAB# try: #LINE# #TAB# #TAB# conn = stream.open() #LINE# #TAB# except StreamError as err: #LINE# #TAB# #TAB# if err.errno!= errno.ECONNRESET: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# if not conn: #LINE# #TAB# #TAB# #TAB# raise IOError('Could not open connection: {0}'.format(err)) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# conn.recv(8192) #LINE# #TAB# #TAB# except IOError as err: #LINE# #TAB# #TAB# #TAB# log.error('Could not read 8192 bytes: {0}'.format(err)) #LINE# #TAB# return conn
"#LINE# #TAB# binding_list = [] #LINE# #TAB# for binding in model.bindings: #LINE# #TAB# #TAB# valid = predicate(binding) #LINE# #TAB# #TAB# if valid: #LINE# #TAB# #TAB# #TAB# binding_list.append((binding, valid)) #LINE# #TAB# return binding_list"
#LINE# #TAB# alpha = np.log(np.sum(A * B)) #LINE# #TAB# beta = np.log(np.sum(A ** 2)) #LINE# #TAB# return alpha + beta
"#LINE# #TAB# try: #LINE# #TAB# #TAB# out = structure.copy() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# out = structure.copy() #LINE# #TAB# if hasattr(out,'start_timer'): #LINE# #TAB# #TAB# out.start_timer() #LINE# #TAB# return out"
"#LINE# #TAB# return {'id': obj.id,'sectie': obj.sectie, 'capakey': obj.capakey, #LINE# #TAB# #TAB# 'file': obj.file}"
"#LINE# #TAB# stat = os.stat(path) #LINE# #TAB# if not stat.S_ISLNK(stat.st_mode): #LINE# #TAB# #TAB# return None #LINE# #TAB# time_objects = [] #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# code_name = f.readline() #LINE# #TAB# #TAB# #TAB# if not code_name: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# code = int(code_name) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# time_objects.append(code) #LINE# #TAB# return time_objects"
"#LINE# #TAB# days = int(duration_in_seconds / 86400) #LINE# #TAB# hours = int(duration_in_seconds / 3600) #LINE# #TAB# minutes = int(duration_in_seconds / 60) #LINE# #TAB# seconds = int(duration_in_seconds) % 60 #LINE# #TAB# if hours > 0 and minutes > 0: #LINE# #TAB# #TAB# return f'{days} days, {hours:d}:{minutes:d}' #LINE# #TAB# if hours < 0 and minutes > 0: #LINE# #TAB# #TAB# return f'{hours:d} minutes, {minutes:d}' #LINE# #TAB# if seconds > 0: #LINE# #TAB# #TAB# return f'{minutes:d} seconds, {seconds:d}' #LINE# #TAB# return f'{days:d}'"
#LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440-pre' #LINE# #TAB# cfg.tag_prefix = 'None' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source = 'gg_si_o3.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"#LINE# #TAB# for row in searchList: #LINE# #TAB# #TAB# cols = list(row.keys()) #LINE# #TAB# #TAB# lon = cols[0] #LINE# #TAB# #TAB# lat = cols[1] #LINE# #TAB# #TAB# crs = cols[2] #LINE# #TAB# #TAB# if crs!= cols[0]: #LINE# #TAB# #TAB# #TAB# crs = None #LINE# #TAB# #TAB# if lon == cols[0]: #LINE# #TAB# #TAB# #TAB# lon = None #LINE# #TAB# #TAB# if lat == cols[1]: #LINE# #TAB# #TAB# #TAB# lat = None #LINE# #TAB# #TAB# if crs!= cols[2]: #LINE# #TAB# #TAB# #TAB# crs = None #LINE# #TAB# return lon, lat, crs"
#LINE# #TAB# g = GitClient() #LINE# #TAB# tag = annotated_tag(tag_name) #LINE# #TAB# g.add_repo(tag) #LINE# #TAB# if push: #LINE# #TAB# #TAB# g.git_push() #LINE# #TAB# return g
#LINE# #TAB# keymaps = Keymap() #LINE# #TAB# for kmap in raw_keymaps: #LINE# #TAB# #TAB# kmap.set_lower_timestamp(time.time()) #LINE# #TAB# return keymaps
#LINE# #TAB# tokens = file_tokenize(fp) #LINE# #TAB# data = [] #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.isdigit(): #LINE# #TAB# #TAB# #TAB# data.append(char) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data.append(token.strip()) #LINE# #TAB# return data
#LINE# #TAB# global _node_not_found_hook #LINE# #TAB# if _node_not_found_hook is not None: #LINE# #TAB# #TAB# del _node_not_found_hook
"#LINE# #TAB# import re #LINE# #TAB# parts = string.split('/') #LINE# #TAB# author = parts[0] #LINE# #TAB# name = parts[1] #LINE# #TAB# return author, name"
"#LINE# #TAB# if len(regexps) == 0: #LINE# #TAB# #TAB# return content #LINE# #TAB# assert isinstance(regexps, (list, tuple)) #LINE# #TAB# for regex, replace in regexps: #LINE# #TAB# #TAB# re_obj = re.compile(regex) #LINE# #TAB# #TAB# for replace in replace: #LINE# #TAB# #TAB# #TAB# content = re_obj.sub(replace, content) #LINE# #TAB# return content"
"#LINE# #TAB# if id_cols is None: #LINE# #TAB# #TAB# id_cols = ['plugin_id','registered_plugin_name'] #LINE# #TAB# valid_plugin_uids = df[df[id_cols].isin(['0-9', 'plugin_name', #LINE# #TAB# #TAB#'registered_plugin_version']).any(axis=1)] #LINE# #TAB# if len(valid_plugin_uids) > 0: #LINE# #TAB# #TAB# logger.warning('{}: {} registered plugin(s) with invalid uid(s)'. #LINE# #TAB# #TAB# #TAB# format(len(valid_plugin_uids), valid_plugin_uids)) #LINE# #TAB# return valid_plugin_uids"
#LINE# #TAB# type_str = str(args) #LINE# #TAB# if type_str.startswith('http'): #LINE# #TAB# #TAB# return configparser.SectionProxy(type_str) #LINE# #TAB# return type_str
"#LINE# #TAB# filename_list = [f for f in os.listdir(directory) if f.endswith('.txt')] #LINE# #TAB# filename_list.sort() #LINE# #TAB# distances = np.array([np.loadtxt(os.path.join(directory, f)) for f in #LINE# #TAB# #TAB# filename_list]) #LINE# #TAB# return distances"
"#LINE# #TAB# table_name = 'cache_clear_%s' % field #LINE# #TAB# if expiration_ms is not None: #LINE# #TAB# #TAB# table_name += '_' + str(expiration_ms) #LINE# #TAB# else: #LINE# #TAB# #TAB# table_name = 'cache_clear_%s' % field #LINE# #TAB# tables = get_tables(table_name) #LINE# #TAB# for i in range(0, len(tables)): #LINE# #TAB# #TAB# cache_key = table_name + '_' + str(i) #LINE# #TAB# #TAB# cache = tables[cache_key] #LINE# #TAB# #TAB# if config.has_option('cache', 'clear'): #LINE# #TAB# #TAB# #TAB# cache['clear'] = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cache['clear'] = False #LINE# #TAB# return cache"
"#LINE# #TAB# previous = None #LINE# #TAB# for href, calendar in calendars: #LINE# #TAB# #TAB# if previous is not None: #LINE# #TAB# #TAB# #TAB# previous = calendar #LINE# #TAB# #TAB# elif calendar is not None: #LINE# #TAB# #TAB# #TAB# if previous.href == href: #LINE# #TAB# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield previous #LINE# #TAB# if previous is not None: #LINE# #TAB# #TAB# yield previous"
"#LINE# #TAB# begidx = get_begidx(text, start) #LINE# #TAB# endidx = get_endidx(text, end) #LINE# #TAB# return [text[begidx:endidx] for begidx in range(start, endidx + 1)]"
#LINE# #TAB# log('Please enter your API key ( from'+ read_webapp_url() +'):') #LINE# #TAB# api_key = input('API key: ') #LINE# #TAB# if api_key == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# return api_key
"#LINE# #TAB# assert theta_units in ['radians', 'degrees'],\ #LINE# #TAB# #TAB# ""kwarg theta_units must specified in radians or degrees"" #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.pi / 2 * x #LINE# #TAB# elif theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = np.pi / 2 * (x * y) #LINE# #TAB# return theta"
"#LINE# #TAB# #TAB# attributes = resource.attributes #LINE# #TAB# #TAB# for attr in request.GET.keys(): #LINE# #TAB# #TAB# #TAB# parse_attribute = parse_attribute(attr) #LINE# #TAB# #TAB# #TAB# if parse_attribute: #LINE# #TAB# #TAB# #TAB# #TAB# setattr(attributes, attr, parse_attribute(attr, parent_resource)) #LINE# #TAB# #TAB# return attributes"
"#LINE# #TAB# if renderer is None: #LINE# #TAB# #TAB# renderer = 'agg' #LINE# #TAB# import matplotlib.pyplot as plt #LINE# #TAB# if renderer == 'agg': #LINE# #TAB# #TAB# from matplotlib.colors import FuncFormatter #LINE# #TAB# #TAB# renderer = 'agg' #LINE# #TAB# return plt.pyplot, renderer"
"#LINE# #TAB# x = x[(...), ::-1] #LINE# #TAB# x[:, :, 0] -= x[:, :, 1] #LINE# #TAB# x[:, :, 2] -= x[:, :, :-1] #LINE# #TAB# return x"
"#LINE# #TAB# for e in G.es: #LINE# #TAB# #TAB# for f in G.es[e]: #LINE# #TAB# #TAB# #TAB# if f not in PYTHON_ATTRIBUTES: #LINE# #TAB# #TAB# #TAB# #TAB# PYTHON_ATTRIBUTES[e] = set() #LINE# #TAB# #TAB# #TAB# if f not in PYTHON_ATTRIBUTES[e]: #LINE# #TAB# #TAB# #TAB# #TAB# PYTHON_ATTRIBUTES[e] |= set(['x', 'y']) #LINE# #TAB# for e in G.es: #LINE# #TAB# #TAB# if e not in PYTHON_ATTRIBUTES: #LINE# #TAB# #TAB# #TAB# del G.es[e]"
#LINE# #TAB# with open(fn) as f: #LINE# #TAB# #TAB# token = float(f.readline().split()[0]) / 1000 #LINE# #TAB# return token
"#LINE# #TAB# tuple_args = tuple(args) #LINE# #TAB# default_value = defaults[0] #LINE# #TAB# tensor = tuple_args[0] if len(args) > 1 else None #LINE# #TAB# if tensor is None: #LINE# #TAB# #TAB# tensor = tuple_args[1] #LINE# #TAB# return tensor, default_value"
"#LINE# #TAB# entity = widget.data.get('entity', None) #LINE# #TAB# groups = [] #LINE# #TAB# if entity is None: #LINE# #TAB# #TAB# return groups #LINE# #TAB# if isinstance(entity, Compound): #LINE# #TAB# #TAB# groups.append(entity) #LINE# #TAB# for value in data.get('values', []): #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(value, (list, tuple)): #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# groups.append(Group(item)) #LINE# #TAB# return groups"
"#LINE# #TAB# #TAB# if not pretty: #LINE# #TAB# #TAB# #TAB# s = repr(obj) #LINE# #TAB# #TAB# #TAB# if isinstance(obj, string_types): #LINE# #TAB# #TAB# #TAB# #TAB# s = s.replace('\n','' * indent) #LINE# #TAB# #TAB# #TAB# return s #LINE# #TAB# #TAB# elif isinstance(obj, tuple): #LINE# #TAB# #TAB# #TAB# s = ', '.join([ #LINE# #TAB# #TAB# #TAB# #TAB# get_repr(item, pretty=pretty, indent=indent) #LINE# #TAB# #TAB# #TAB# #TAB# for item in obj #LINE# #TAB# #TAB# #TAB# ]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# s = repr(obj) #LINE# #TAB# #TAB# return s"
#LINE# #TAB# failures = [] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result = elastic_client.indices.create(index=index) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# failures.append(index) #LINE# #TAB# return failures
"#LINE# #TAB# mod = types.ModuleType(name) #LINE# #TAB# _stack.append(mod) #LINE# #TAB# mod.__dict__.update(sort_recursively(name, code=code)) #LINE# #TAB# return mod"
"#LINE# #TAB# data = [] #LINE# #TAB# for obj in object_list: #LINE# #TAB# #TAB# app = obj.app_label #LINE# #TAB# #TAB# for key in app.config['DOMAIN']: #LINE# #TAB# #TAB# #TAB# if key in data: #LINE# #TAB# #TAB# #TAB# #TAB# del data[key] #LINE# #TAB# #TAB# data.append({'app_label': app.config['DOMAIN'][key], 'name': str( #LINE# #TAB# #TAB# #TAB# obj.name)}) #LINE# #TAB# return data"
"#LINE# #TAB# t = angles.copy() #LINE# #TAB# t[:, (0)] = t[:, (1)] #LINE# #TAB# t[:, (1)] = t[:, (0)] * np.cos(t) #LINE# #TAB# t[:, (2)] = t[:, (1)] * np.sin(t) #LINE# #TAB# t[:, (3)] = t[:, (1)] * np.cos(t) #LINE# #TAB# return t"
#LINE# #TAB# params['logging.order'] = 0 #LINE# #TAB# return params
#LINE# #TAB# command = [] #LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# command.append(col) #LINE# #TAB# return command
"#LINE# #TAB# edit_text = {'lr': lr, 'epoch': epoch,'steps': steps, 'factor': #LINE# #TAB# #TAB# factor} #LINE# #TAB# return edit_text"
"#LINE# #TAB# Tpm = np.array(tpm) #LINE# #TAB# N = Tpm.shape[0] #LINE# #TAB# connectivity = np.zeros((N, N)) #LINE# #TAB# state = np.zeros((N, N)) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# for j in range(N): #LINE# #TAB# #TAB# #TAB# state[i, j] = 1 #LINE# #TAB# #TAB# connectivity[i, j] = 1 #LINE# #TAB# return connectivity"
"#LINE# #TAB# for key in network.keys(): #LINE# #TAB# #TAB# if type(network[key]) is str: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# elif type(network[key]) is dict: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# point = {} #LINE# #TAB# #TAB# #TAB# for key, value in network[key].items(): #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# point[key] = create_point(network[key]) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# point[key] = value #LINE# #TAB# return point"
#LINE# #TAB# psi_Na = {} #LINE# #TAB# if file_dict.get(' psi_Na'): #LINE# #TAB# #TAB# psi_Na['psi_Na'] = file_dict['psi_Na'] #LINE# #TAB# if file_dict.get('CO3_Cl_HMW84'): #LINE# #TAB# #TAB# psi_Na['CO3_Cl_HMW84'] = file_dict['CO3_Cl_HMW84'] #LINE# #TAB# return psi_Na
"#LINE# #TAB# if isinstance(sql, six.string_types): #LINE# #TAB# #TAB# args = sql, #LINE# #TAB# else: #LINE# #TAB# #TAB# args = sql, #LINE# #TAB# metrics = Metrics() #LINE# #TAB# for line in args: #LINE# #TAB# #TAB# values = line.split() #LINE# #TAB# #TAB# if len(values) == 2: #LINE# #TAB# #TAB# #TAB# metrics.add_metric(values[0], values[1]) #LINE# #TAB# #TAB# elif len(values) == 4: #LINE# #TAB# #TAB# #TAB# metrics.add_metric(values[0], values[1]) #LINE# #TAB# return metrics"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = ContentType.objects.get(slug=ct_name) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# msg = _( #LINE# #TAB# #TAB# #TAB# 'The content type {} was not found. Please check your settings file.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# warnings.warn(msg.format(ct_name), UserWarning) #LINE# #TAB# #TAB# return None #LINE# #TAB# if content_type.pk: #LINE# #TAB# #TAB# msg = _( #LINE# #TAB# #TAB# #TAB# 'The content type {} was not found. Please check your settings file.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# warnings.warn(msg.format(ct_name), UserWarning) #LINE# #TAB# #TAB# return None #LINE# #TAB# return content_type"
#LINE# #TAB# if transform == 'tanh': #LINE# #TAB# #TAB# return 'tanh' #LINE# #TAB# elif transform == 'exp': #LINE# #TAB# #TAB# return 'exp' #LINE# #TAB# elif transform == 'logit': #LINE# #TAB# #TAB# return 'logit' #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if isinstance(obj, Promise): #LINE# #TAB# #TAB# return obj.result() #LINE# #TAB# return obj"
#LINE# #TAB# return [os.path.dirname(filename) for filename in os.listdir(os.path. #LINE# #TAB# #TAB# dirname(module_name)) if not filename.endswith('.py')]
"#LINE# #TAB# return {key: val for key, val in annotations.items() if key!= '__annotations__'}"
#LINE# #TAB# res = False #LINE# #TAB# for ext in form.get_extensions(): #LINE# #TAB# #TAB# if ext.startswith('.'): #LINE# #TAB# #TAB# #TAB# if key in ext: #LINE# #TAB# #TAB# #TAB# #TAB# res = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res = False #LINE# #TAB# return res
"#LINE# #TAB# cmd_dict = {} #LINE# #TAB# for prop, value in element.items(): #LINE# #TAB# #TAB# if prop in cmd_dict: #LINE# #TAB# #TAB# #TAB# cmd_dict[prop] = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cmd_dict[prop] =''.join(cmd_dict[prop]) #LINE# #TAB# return cmd_dict"
"#LINE# #TAB# out = [] #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# ssms = line.split(""\t"") #LINE# #TAB# #TAB# #TAB# chrom, start, end = ssms[0], ssms[-1] #LINE# #TAB# #TAB# #TAB# out.append(""{} {}:{}"".format(chrom, start, end)) #LINE# #TAB# return out"
#LINE# #TAB# l = [] #LINE# #TAB# with open(MACHINE_ID_FILE) as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line.startswith('duplicate '): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# l.append(line) #LINE# #TAB# l.sort() #LINE# #TAB# machine_id = ''.join(l) #LINE# #TAB# return machine_id
"#LINE# #TAB# if not context: #LINE# #TAB# #TAB# return {} #LINE# #TAB# if context.is_os_admin: #LINE# #TAB# #TAB# return {v: k for k, v in context.items() if k!='session'} #LINE# #TAB# if not context.user: #LINE# #TAB# #TAB# return {} #LINE# #TAB# return {k: v for k, v in context.user.items() if k!='session'}"
#LINE# #TAB# attributes = {} #LINE# #TAB# for key in profile.keys(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# attributes[key] = float(profile[key]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return attributes
"#LINE# #TAB# threshold = 25 #LINE# #TAB# a = data.mean(axis=0) > threshold #LINE# #TAB# b = data.mean(axis=1) > threshold #LINE# #TAB# a = a.drop(a.index[0], axis=1) #LINE# #TAB# b = b.drop(b.index[0], axis=1) #LINE# #TAB# return a"
"#LINE# #TAB# if not operand_name_list: #LINE# #TAB# #TAB# operand_name_list = [] #LINE# #TAB# value_type = None #LINE# #TAB# for operand in operand_list: #LINE# #TAB# #TAB# if isinstance(operand, Operator): #LINE# #TAB# #TAB# #TAB# if is_list_like(operand): #LINE# #TAB# #TAB# #TAB# #TAB# value_type = _infer_value_type(operand, operand_name_list) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# elif isinstance(operand, Array): #LINE# #TAB# #TAB# #TAB# value_type = _infer_value_type(operand, operand_name_list) #LINE# #TAB# return value_type"
"#LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# try: #LINE# #TAB# #TAB# s.connect() #LINE# #TAB# #TAB# s.close() #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# with open(filename, 'r') as file_: #LINE# #TAB# #TAB# lc_input_data = pickle.load(file_) #LINE# #TAB# return lc_input_data"
"#LINE# #TAB# m = re.match('\\[(.*)\]', signature) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return f'{m.group(1)}{m.group(2)}' #LINE# #TAB# return f'{signature}{m.group(3)}'"
#LINE# #TAB# ckan_error = None #LINE# #TAB# for blueprint in blueprints: #LINE# #TAB# #TAB# if blueprint.static_folder: #LINE# #TAB# #TAB# #TAB# if not os.path.isdir(blueprint.static_folder): #LINE# #TAB# #TAB# #TAB# #TAB# ckan_error = blueprint #LINE# #TAB# return ckan_error
"#LINE# #TAB# client = DatapipelineClient() #LINE# #TAB# table = client.get(table_name) #LINE# #TAB# char_ids = [] #LINE# #TAB# for char_id in table.char_ids: #LINE# #TAB# #TAB# char = char_id #LINE# #TAB# #TAB# while char['startTime'] is None: #LINE# #TAB# #TAB# #TAB# char_ids.append(char_id) #LINE# #TAB# #TAB# now = datetime.now() #LINE# #TAB# #TAB# char_start = now.strftime('%H:%M:%S') #LINE# #TAB# #TAB# char_end = now + timedelta(hours=char_start - utc_hour) #LINE# #TAB# #TAB# char_ids.append(char_end) #LINE# #TAB# return { #LINE# #TAB# #TAB# 'pipeline': [{ #LINE# #TAB# #TAB# #TAB#'startTime': char_start, #LINE# #TAB# #TAB# #TAB# 'endTime': char_"
"#LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in fromDic: #LINE# #TAB# #TAB# #TAB# if not isinstance(toDic[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# toDic[key] = fromDic[key].copy() #LINE# #TAB# #TAB# #TAB# del fromDic[key]"
#LINE# #TAB# data = [] #LINE# #TAB# for line in sourcelist: #LINE# #TAB# #TAB# data.append(Source.from_string(line)) #LINE# #TAB# return data
"#LINE# #TAB# dirs = [] #LINE# #TAB# files = os.listdir(package) #LINE# #TAB# for name in files: #LINE# #TAB# #TAB# full_name = package_name(name, top) #LINE# #TAB# #TAB# if full_name == package: #LINE# #TAB# #TAB# #TAB# dirs.append(name) #LINE# #TAB# #TAB# elif full_name == top: #LINE# #TAB# #TAB# #TAB# dirs.append(name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield full_name, name"
"#LINE# #TAB# p = None #LINE# #TAB# for proc in model.processors: #LINE# #TAB# #TAB# if proc.tag.startswith(""STAT""): #LINE# #TAB# #TAB# #TAB# proc.tag = proc.tag[4:] #LINE# #TAB# #TAB# if proc.tag.startswith(""STATUS""): #LINE# #TAB# #TAB# #TAB# status = proc.get_stat(model) #LINE# #TAB# #TAB# #TAB# yield proc #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if status: #LINE# #TAB# #TAB# #TAB# #TAB# p.append(proc) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield proc"
#LINE# #TAB# opener = urllib.request.build_opener(*get_handlers()) #LINE# #TAB# urllib.request.install_opener(opener) #LINE# #TAB# return opener
"#LINE# #TAB# scores = {} #LINE# #TAB# exemplar_followers = set() #LINE# #TAB# for followers in exemplars.values(): #LINE# #TAB# #TAB# exemplar_followers |= followers #LINE# #TAB# for brand, followers in brands: #LINE# #TAB# #TAB# scores[brand] = _get_jaccard(followers, exemplar_followers) #LINE# #TAB# return scores"
"#LINE# #TAB# y = year - 1 + EPOCH_GREGORIAN_YEAR #LINE# #TAB# if month!= 20: #LINE# #TAB# #TAB# m = 0 #LINE# #TAB# elif gregorian.leap(year): #LINE# #TAB# #TAB# m = -14 #LINE# #TAB# elif gregorian.leap(year): #LINE# #TAB# #TAB# m = -15 #LINE# #TAB# else: #LINE# #TAB# #TAB# m = -15 #LINE# #TAB# return y, m, day"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# data = response.json() #LINE# #TAB# #TAB# if ""error_description"" in data: #LINE# #TAB# #TAB# #TAB# return data['error_description'] #LINE# #TAB# #TAB# if ""error"" in data: #LINE# #TAB# #TAB# #TAB# return data['error'] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return ""Unknown error"""
"#LINE# #TAB# lexer, extension = os.path.splitext(file_name) #LINE# #TAB# if extension == '': #LINE# #TAB# #TAB# return #LINE# #TAB# lexer = _load_loggers(extension) #LINE# #TAB# if lexer: #LINE# #TAB# #TAB# return [lexer(text)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [text]"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return import_module(module) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
#LINE# #TAB# index = str(index) #LINE# #TAB# count = int(count) #LINE# #TAB# if index == '': #LINE# #TAB# #TAB# return '0' * count #LINE# #TAB# return f'Tetrahedral: {index}{count}'
#LINE# #TAB# privkey = rsa.load_private_key(file_contents=privkeystring) #LINE# #TAB# result = aes_cbc.decrypt_private_key(privkey) #LINE# #TAB# if result == b'': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# match = re.match(""^(?P<major>[0-9]*)\\.(?P<minor>[0-9]*)(?P<change>[0-9]*)$"", version_string) #LINE# #TAB# if not match: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""Version string '{}' does not appear to be a valid release version."" #LINE# #TAB# #TAB# #TAB#.format(version_string)) #LINE# #TAB# major = match.group(""major"") #LINE# #TAB# minor = match.group(""minor"") #LINE# #TAB# return major, minor, version_string"
"#LINE# #TAB# if cores == 1: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# import pprocess #LINE# #TAB# queue_maker = partial(pprocess.Queue, cores=cores) #LINE# #TAB# try: #LINE# #TAB# #TAB# _ = queue_maker.make() #LINE# #TAB# except: #LINE# #TAB# #TAB# cores = 1 #LINE# #TAB# except: #LINE# #TAB# #TAB# cores = 1 #LINE# #TAB# return cores"
"#LINE# #TAB# config_file = args['config_file'] #LINE# #TAB# if config_file.endswith('.csv'): #LINE# #TAB# #TAB# with open(config_file, 'r') as f: #LINE# #TAB# #TAB# #TAB# reader = csv.reader(f) #LINE# #TAB# #TAB# #TAB# for i, row in enumerate(reader): #LINE# #TAB# #TAB# #TAB# #TAB# yield int(row[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield 0"
"#LINE# #TAB# update_clause = '' #LINE# #TAB# for key, value in dictionary.items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# update_clause +='{0} = {1}'.format(key, value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# update_clause +='{0} = {1}'.format(key, value) #LINE# #TAB# return update_clause"
#LINE# #TAB# if ';' not in line: #LINE# #TAB# #TAB# return line #LINE# #TAB# cmd_split = line.split(';') #LINE# #TAB# competitions = {} #LINE# #TAB# for cmd in cmd_split: #LINE# #TAB# #TAB# if cmd == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# competition = cmd.split('=') #LINE# #TAB# #TAB# if len(competition) > 1: #LINE# #TAB# #TAB# #TAB# competitions[competition[0]] = {'competition': competition[1]} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# competitions[competition] = {'competition': competition} #LINE# #TAB# return competitions
"#LINE# #TAB# if isinstance(nmea_degrees, str): #LINE# #TAB# #TAB# nmea_degrees = nmea_degrees.split(':') #LINE# #TAB# #TAB# nmea_degrees = [float(d) for d in nmea_degrees] #LINE# #TAB# lat_map = {} #LINE# #TAB# lon_map = {} #LINE# #TAB# for nmea_degrees[0], nmea_degrees[1]: #LINE# #TAB# #TAB# lat_map[nmea_degrees[0]] = float(nmea_degrees[1]) #LINE# #TAB# #TAB# lon_map[nmea_degrees[2]] = float(nmea_degrees[3]) #LINE# #TAB# lat_map[str(nmea_degrees[0])] = round(lat_map[str(nmea_degrees[1])], 2) #LINE# #TAB# return lat_map, lon_map"
"#LINE# #TAB# dataclass = _dataclass(game_id, gameweek) #LINE# #TAB# picks = dataclass.picks(gameweek) #LINE# #TAB# return dataclass, picks"
"#LINE# #TAB# string = '' #LINE# #TAB# if hasattr(obj,'signupsheet') and obj.signupsheet: #LINE# #TAB# #TAB# string = f'{obj.signupsheet} {event.name}' #LINE# #TAB# return string"
"#LINE# #TAB# if 'Authorization' not in request.headers: #LINE# #TAB# #TAB# return None #LINE# #TAB# auth = request.headers['Authorization'] #LINE# #TAB# if not auth: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(auth, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# auth = base64.b64decode(auth.strip()).decode('utf-8') #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# if auth: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return 'username:%s' % auth #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return auth"
#LINE# #TAB# vehicle_journey_status = None #LINE# #TAB# for elem in vehicle_journey_element: #LINE# #TAB# #TAB# if elem.tag == 'TransXChange': #LINE# #TAB# #TAB# #TAB# weekday = elem.attrib.get('weekday') #LINE# #TAB# #TAB# #TAB# if weekday!= 'No': #LINE# #TAB# #TAB# #TAB# #TAB# vehicle_journey_status = elem #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if vehicle_journey_status is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return vehicle_journey_status
"#LINE# #TAB# errorCode = c_uint() #LINE# #TAB# try: #LINE# #TAB# #TAB# cdb = conf.lib.clang_CompilationDatabase_get_mime_widget_types(buildDir, #LINE# #TAB# #TAB# #TAB# byref(errorCode)) #LINE# #TAB# except CompilationDatabaseError as e: #LINE# #TAB# #TAB# raise CompilationDatabaseError(int(errorCode.value), #LINE# #TAB# #TAB# #TAB# 'CompilationDatabase loading failed') #LINE# #TAB# return cdb"
"#LINE# #TAB# global _g2_point #LINE# #TAB# if _g2_point is None: #LINE# #TAB# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# #TAB# _g2_point = os.path.join(os.path.expanduser('~'), '.steam') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _g2_point = os.path.join(os.path.expanduser('~'), '.g2') #LINE# #TAB# return _g2_point"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# apps_data = get_app_config(tag) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None #LINE# #TAB# return [get_name_from_app_config(tag, app_config) for app_config in apps_data]"
#LINE# #TAB# global _packages #LINE# #TAB# _packages[prefix] = namespace
#LINE# #TAB# if is_cold_start(config): #LINE# #TAB# #TAB# return 'cold' #LINE# #TAB# return 'bool'
#LINE# #TAB# git_files = [] #LINE# #TAB# for f in gl.iter_files(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# git_files.append(gb + f) #LINE# #TAB# #TAB# #TAB# git_files[-1] = f + '.git' #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return git_files
#LINE# #TAB# old_logger = logger #LINE# #TAB# logger.setLevel(level) #LINE# #TAB# yield #LINE# #TAB# logger = old_logger
#LINE# #TAB# follow_out_edges = [] #LINE# #TAB# start_activities = [] #LINE# #TAB# for node in dfg: #LINE# #TAB# #TAB# if node['type'] =='start': #LINE# #TAB# #TAB# #TAB# start_activities.append(node['object_id']) #LINE# #TAB# #TAB# elif node['type'] == 'follow': #LINE# #TAB# #TAB# #TAB# follow_out_edges.append(node['object_id']) #LINE# #TAB# #TAB# elif node['type'] == 'out': #LINE# #TAB# #TAB# #TAB# start_activities.append(node['object_id']) #LINE# #TAB# return start_activities
"#LINE# #TAB# i = 0 #LINE# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# chunk = f.read(1024) #LINE# #TAB# #TAB# #TAB# if len(chunk) > 10: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# f_split = file_path.split('.') #LINE# #TAB# for i in range(len(f_split)): #LINE# #TAB# #TAB# if f_split[i] == 'batch': #LINE# #TAB# #TAB# #TAB# i -= 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return f_split[i - 1]"
"#LINE# #TAB# ret = [] #LINE# #TAB# for name, val in inspect.signature(func).parameters.items(): #LINE# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(val, bytes): #LINE# #TAB# #TAB# #TAB# val = val.decode('utf-8') #LINE# #TAB# #TAB# ret.append((name, val)) #LINE# #TAB# return ret"
#LINE# #TAB# coarse_grains = coarse_grains_for_blackbox(request) #LINE# #TAB# for coarse_group in coarse_grains: #LINE# #TAB# #TAB# if str(coarse_group['id']) == str(coarse_group['id']): #LINE# #TAB# #TAB# #TAB# coarse_grains[coarse_group['id']] = coarse_group #LINE# #TAB# return coarse_grains
#LINE# #TAB# if is_forward_ref(typ): #LINE# #TAB# #TAB# return typ[0:-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return typ
#LINE# #TAB# if not value: #LINE# #TAB# #TAB# return None #LINE# #TAB# m = re_error_data.match(value) #LINE# #TAB# if not m: #LINE# #TAB# #TAB# return None #LINE# #TAB# return m.group(1) if m.group(2) else None
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if t.value.count('.') == 0: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = int(t.value) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('[%d]: Number %s is not valid!' % (t.lineno, t.value)) #LINE# #TAB# #TAB# t.value = 0 #LINE# #TAB# return t"
#LINE# #TAB# if len(_bytes) == 0: #LINE# #TAB# #TAB# return [] #LINE# #TAB# mode = 0 #LINE# #TAB# for _byte in _bytes: #LINE# #TAB# #TAB# if _byte & 128!= 0: #LINE# #TAB# #TAB# #TAB# mode |= 1 #LINE# #TAB# #TAB# if _byte & 192!= 0: #LINE# #TAB# #TAB# #TAB# mode |= 2 #LINE# #TAB# #TAB# if _byte & 255!= 0: #LINE# #TAB# #TAB# #TAB# mode |= 3 #LINE# #TAB# #TAB# if _byte & 255!= 0: #LINE# #TAB# #TAB# #TAB# mode |= 4 #LINE# #TAB# return [mode]
#LINE# #TAB# Tr = 298.15 #LINE# #TAB# return abc[0] + abc[1] * (T - Tr) + abc[2] * (T - Tr) ** 2
"#LINE# #TAB# code = str(code) #LINE# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# crs = osr.SpatialReference() #LINE# #TAB# crs.ImportFromProj4(proj4) #LINE# #TAB# rows, cols = crs.get_rows_cols() #LINE# #TAB# return rows, cols"
"#LINE# #TAB# v1_length = np.linalg.norm(v1) #LINE# #TAB# v2_length = np.linalg.norm(v2) #LINE# #TAB# angle = np.arccos(np.dot(v1, v2_length)) #LINE# #TAB# return angle"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return module.models #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# modules = _get_subpage_modules(module) #LINE# #TAB# #TAB# for module in modules: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# import_module('%s.models' % module.__name__) #LINE# #TAB# #TAB# #TAB# #TAB# return module.models #LINE# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# from bs4 import BeautifulSoup #LINE# #TAB# from mezzanine.core.request import current_request #LINE# #TAB# request = current_request() #LINE# #TAB# if request is not None: #LINE# #TAB# #TAB# dom = BeautifulSoup(html, 'html.parser') #LINE# #TAB# #TAB# for tag, attr in ABSOLUTE_URL_TAGS.items(): #LINE# #TAB# #TAB# #TAB# for node in dom.findAll(tag): #LINE# #TAB# #TAB# #TAB# #TAB# url = node.get(attr, '') #LINE# #TAB# #TAB# #TAB# #TAB# if url: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# node[attr] = request.build_absolute_uri(url) #LINE# #TAB# #TAB# html = str(dom) #LINE# #TAB# return html"
#LINE# #TAB# try: #LINE# #TAB# #TAB# print('eigenvalues for %s' % class_name) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# if keys is None: #LINE# #TAB# #TAB# return orig_dict #LINE# #TAB# return {k: (encode_observation(v, default) if isinstance(v, dict) else v) for #LINE# #TAB# #TAB# k, v in orig_dict.items()}"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# spike_idx = int(min_c * winlen) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return concept #LINE# #TAB# res = [] #LINE# #TAB# for i, k in enumerate(concept): #LINE# #TAB# #TAB# if k in min_neu: #LINE# #TAB# #TAB# #TAB# res.append(concept[i]) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if k == max_c: #LINE# #TAB# #TAB# #TAB# res.append(concept[i]) #LINE# #TAB# #TAB# if k == min_z: #LINE# #TAB# #TAB# #TAB# res.append(concept[i]) #LINE# #TAB# #TAB# elif k == max_neu: #LINE# #TAB# #TAB# #TAB# res.append(concept[i]) #LINE# #TAB# return res"
"#LINE# #TAB# bins = Bins() #LINE# #TAB# for i, p in enumerate(probs): #LINE# #TAB# #TAB# p = float(p) #LINE# #TAB# #TAB# if np.isnan(p): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# bins.ins[i] = p / float(num_bins) #LINE# #TAB# return bins"
#LINE# #TAB# hashes = [files[0]] #LINE# #TAB# for f in files[1:]: #LINE# #TAB# #TAB# h = md5(f.read()).hexdigest() #LINE# #TAB# #TAB# if len(hashes) == psize: #LINE# #TAB# #TAB# #TAB# return h #LINE# #TAB# #TAB# hashes.append(h) #LINE# #TAB# return hashes[-1]
#LINE# #TAB# if string is True: #LINE# #TAB# #TAB# return True #LINE# #TAB# if string == 'False': #LINE# #TAB# #TAB# return False #LINE# #TAB# if string == 'None': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# locale = request.accept_language.best_match(CE_LANGUAGES) #LINE# #TAB# if not locale: #LINE# #TAB# #TAB# return [] #LINE# #TAB# locale_code = locale.code.split(';')[0] #LINE# #TAB# locale_code = locale_code.split('-')[0] #LINE# #TAB# if locale_code[0] == 'en': #LINE# #TAB# #TAB# return [ce_ratios[locale_code[1]] for locale_code in #LINE# #TAB# #TAB# #TAB# locale_code.split(',')] #LINE# #TAB# return [ce_ratios[locale_code] for locale_code in locale_code]"
"#LINE# #TAB# diff_one = np.sum(array_one, axis=1) #LINE# #TAB# diff_two = np.sum(array_two, axis=1) #LINE# #TAB# if diff_one > diff_two: #LINE# #TAB# #TAB# exp_diff = 1.0 - diff_one #LINE# #TAB# else: #LINE# #TAB# #TAB# exp_diff = 0.0 #LINE# #TAB# return exp_diff"
"#LINE# #TAB# data = {} #LINE# #TAB# r = rectangle[0] #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# for j in range(3): #LINE# #TAB# #TAB# #TAB# temp = {} #LINE# #TAB# #TAB# #TAB# for k, v in grid[i].items(): #LINE# #TAB# #TAB# #TAB# #TAB# temp[k] = np.median(v) #LINE# #TAB# #TAB# #TAB# data[i] = temp #LINE# #TAB# return data"
"#LINE# #TAB# rst = '' #LINE# #TAB# if format: #LINE# #TAB# #TAB# format = format.lower() #LINE# #TAB# #TAB# json_file = open(filename) #LINE# #TAB# #TAB# data = json.load(json_file) #LINE# #TAB# #TAB# rst = '\n::\n' + rst + '\n\n' #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# rst += '%s = %s\n' % (key, data[key]) #LINE# #TAB# rst += '\n' #LINE# #TAB# return rst"
"#LINE# #TAB# return get_origin(annotation) is dict and getattr(annotation,'module', None #LINE# #TAB# #TAB# ) == 'blog.settings'"
"#LINE# #TAB# assert event['source_key'] in CRASHED_SCORE_PREFIXES #LINE# #TAB# assert event['source_key'] == CRASHED_SCORE_PREFIXES[secret] #LINE# #TAB# extra = {} #LINE# #TAB# msg = event.copy() #LINE# #TAB# extra['source_key'] = CRASHED_SCORE_PREFIXES[secret] #LINE# #TAB# msg['action'] = event.get('action') #LINE# #TAB# msg['detail'] = event.get('detail') #LINE# #TAB# if event.get('project_id') and event.get('project_id'): #LINE# #TAB# #TAB# obj_id = event['project_id'] #LINE# #TAB# else: #LINE# #TAB# #TAB# obj_id = str(uuid.uuid4()) #LINE# #TAB# msg['action'] = 'criticality_score' #LINE# #TAB# return json.dumps(msg, cls=JSONEncoder) + extra"
#LINE# #TAB# skipUserInput = str(skipUserInput) #LINE# #TAB# while True: #LINE# #TAB# #TAB# leadItem = input('Enter the lead item: ') #LINE# #TAB# #TAB# if not leadItem: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if leadItem.upper().startswith('N') or leadItem.lower().startswith( #LINE# #TAB# #TAB# #TAB# 'E:'): #LINE# #TAB# #TAB# #TAB# leadItem = 'E:' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return leadItem
#LINE# #TAB# if error_message is None: #LINE# #TAB# #TAB# error_message = 'Missing value in row[name]' #LINE# #TAB# val = row[name] #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# return val #LINE# #TAB# elif len(row) > 0 and error_message is not None: #LINE# #TAB# #TAB# return error_message #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# command_split = split(args) #LINE# #TAB# command_list = [] #LINE# #TAB# if not command_split[0]: #LINE# #TAB# #TAB# return command_list #LINE# #TAB# for i, arg in enumerate(command_split): #LINE# #TAB# #TAB# if isinstance(arg, str): #LINE# #TAB# #TAB# #TAB# command_list.append(arg) #LINE# #TAB# #TAB# elif arg[0]!= '@': #LINE# #TAB# #TAB# #TAB# command_list.append(arg) #LINE# #TAB# return command_list"
#LINE# #TAB# args = [] #LINE# #TAB# for arg in sys.argv: #LINE# #TAB# #TAB# if arg.startswith('os_'): #LINE# #TAB# #TAB# #TAB# host = arg[2:] #LINE# #TAB# #TAB# #TAB# if host.find('.') > -1: #LINE# #TAB# #TAB# #TAB# #TAB# args.append(host) #LINE# #TAB# #TAB# elif arg.startswith('win32'): #LINE# #TAB# #TAB# #TAB# args.append('win') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# args.append(arg) #LINE# #TAB# return args
"#LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# s.connect(('8.8.8.8', 80)) #LINE# #TAB# local_timezone = s.getsockname()[0] #LINE# #TAB# s.close() #LINE# #TAB# return local_timezone"
"#LINE# #TAB# path = os.path.dirname(geo) #LINE# #TAB# bed_file_path = os.path.join(path,'regional_bed.h5') #LINE# #TAB# if not os.path.exists(bed_file_path): #LINE# #TAB# #TAB# raise FileNotFoundError('No such bed file ""{}""'.format(bed_file_path)) #LINE# #TAB# with open(bed_file_path) as fh: #LINE# #TAB# #TAB# return [shp for shp in fh.readlines() if not shp.startswith('//' #LINE# #TAB# #TAB# #TAB# ) and shp.endswith('.h5')]"
"#LINE# #TAB# if isinstance(input, Node): #LINE# #TAB# #TAB# return input.content_type #LINE# #TAB# elif isinstance(input, str): #LINE# #TAB# #TAB# return input #LINE# #TAB# elif isinstance(input, bytes): #LINE# #TAB# #TAB# return input.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if text == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return DateTime(text) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None
#LINE# #TAB# app = Flask(__name__) #LINE# #TAB# app.register_blueprint(routes.blueprint) #LINE# #TAB# return app
#LINE# #TAB# if url.startswith('http'): #LINE# #TAB# #TAB# return url.rstrip('/') #LINE# #TAB# return url
"#LINE# #TAB# if isinstance(solution, str): #LINE# #TAB# #TAB# return store[solution] #LINE# #TAB# columns = [store.select(chromosome_plot_info['subwindow']) for chromosome_plot_info in #LINE# #TAB# #TAB# chromosome_plot_info['breakpoints']] #LINE# #TAB# else: #LINE# #TAB# #TAB# columns = [store.select(chromosome_plot_info['subwindow']) for chromosome_plot_info in #LINE# #TAB# #TAB# #TAB# chromosome_plot_info['breakpoints']] #LINE# #TAB# return columns"
#LINE# #TAB# if prameter_type == 'book': #LINE# #TAB# #TAB# context = {} #LINE# #TAB# #TAB# context['path'] = path #LINE# #TAB# #TAB# context['param'] = param #LINE# #TAB# #TAB# return context #LINE# #TAB# if param in ddf_admin_actions: #LINE# #TAB# #TAB# context['query'] = ddf_admin_actions[param] #LINE# #TAB# #TAB# return context #LINE# #TAB# context['where'] = kwargs['where'] #LINE# #TAB# return context
#LINE# #TAB# #TAB# url = quote(url) #LINE# #TAB# #TAB# if postfix: #LINE# #TAB# #TAB# #TAB# url = url + '?' + postfix #LINE# #TAB# #TAB# return url
"#LINE# #TAB# if entry: #LINE# #TAB# #TAB# return entry.get('path', '') #LINE# #TAB# elif username: #LINE# #TAB# #TAB# return getpass.getuser() #LINE# #TAB# elif prompt: #LINE# #TAB# #TAB# return prompt #LINE# #TAB# if always_ask: #LINE# #TAB# #TAB# return os.path.abspath(os.path.expanduser(prompt)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# fid_array = real_fid.GetData() #LINE# #TAB# re_val = unpack('i', fid_array) #LINE# #TAB# out_val = unpack('i', re_val[0]) #LINE# #TAB# out_fid = unpack('I', out_val[0]) #LINE# #TAB# return out_fid"
"#LINE# #TAB# if six.PY2: #LINE# #TAB# #TAB# if isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# value = value.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
"#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.Ack(BaseMsgCode.GetUserName, AckCode.OK)) #LINE# #TAB# return packet"
#LINE# #TAB# from scipy.sparse import Matrix #LINE# #TAB# repo = None #LINE# #TAB# for row in key: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# repo = Matrix(row) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return repo
"#LINE# #TAB# size = len(text) #LINE# #TAB# stripped_text = text #LINE# #TAB# if remove_url: #LINE# #TAB# #TAB# text = re.sub(r'<(.*?)>', r'\1', text, flags=re.MULTILINE) #LINE# #TAB# stripped_text = re.sub(r'<(.*?)>', r'\1', stripped_text, flags=re.MULTILINE) #LINE# #TAB# stripped_text = re.sub(r'<(.*?)>', '', stripped_text, flags=re.MULTILINE) #LINE# #TAB# return stripped_text"
"#LINE# #TAB# if isinstance(x, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# x = json.loads(x) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# ret = salt.utils.mac_utils.execute_return_result( #LINE# #TAB# #TAB#'systemsetup -h' #LINE# #TAB# #TAB# ) #LINE# #TAB# return salt.utils.mac_utils.validate_enabled( #LINE# #TAB# #TAB# salt.utils.mac_utils.parse_return(ret)) == 'on'
#LINE# #TAB# try: #LINE# #TAB# #TAB# v = socket.gethostbyname(hostname) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# ip_address = None #LINE# #TAB# else: #LINE# #TAB# #TAB# ip_address = v.getsockname()[0] #LINE# #TAB# return ip_address
"#LINE# #TAB# result = bytearray(m) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# temp = [] #LINE# #TAB# #TAB# while len(temp) < n and temp[-1] == b'\x00': #LINE# #TAB# #TAB# #TAB# temp.append(b'\x00') #LINE# #TAB# #TAB# for j in range(0, n): #LINE# #TAB# #TAB# #TAB# if j >= g: #LINE# #TAB# #TAB# #TAB# #TAB# temp.append(b'\x00') #LINE# #TAB# #TAB# #TAB# result.extend(temp) #LINE# #TAB# #TAB# result.append(b'\x00') #LINE# #TAB# return result"
#LINE# #TAB# Tdim = T - 273.15 #LINE# #TAB# if Tdim < 0: #LINE# #TAB# #TAB# R = 0 #LINE# #TAB# elif Tdim == 1: #LINE# #TAB# #TAB# R = 1000 #LINE# #TAB# elif Tdim == 2: #LINE# #TAB# #TAB# R = 298.15 #LINE# #TAB# else: #LINE# #TAB# #TAB# R = 1000 #LINE# #TAB# return R
"#LINE# #TAB# x2ys = np.array(x2ys) #LINE# #TAB# scores = np.zeros((len(x2ys), n_trans)) #LINE# #TAB# for i in range(n_trans): #LINE# #TAB# #TAB# tmp = 0 #LINE# #TAB# #TAB# for j in range(i, len(x2ys)): #LINE# #TAB# #TAB# #TAB# scores[j] = co_occurrences(x2ys[i], x2ys[j]) #LINE# #TAB# #TAB# #TAB# tmp += 1 #LINE# #TAB# #TAB# x2ys[:, (i)] = tmp #LINE# #TAB# return x2ys, scores"
"#LINE# #TAB# y_pos = np.sum(sample_weight[y == 1]) #LINE# #TAB# y_neg = np.sum(sample_weight[y == -1]) #LINE# #TAB# intercept = np.mean(y_pos) #LINE# #TAB# d_beta = safe_sparse_dot(X.T, intercept - y_pos) / X.shape[0] #LINE# #TAB# p = 1 / np.max([np.linalg.norm(d_beta[np.where(group_index == g)[0]], #LINE# #TAB# #TAB# 2) for g in range(0, len(group_index))]) #LINE# #TAB# cpc = 1 / np.max([np.linalg.norm(d_beta[np.where(group_index == g)[0]], #LINE# #TAB# #TAB# 2) for g in range(0, len(group_index))]) #LINE# #TAB# return intercept, d_beta, cpc"
#LINE# #TAB# global wxagg_online #LINE# #TAB# global wxagg_standby #LINE# #TAB# if not wxagg_online: #LINE# #TAB# #TAB# return False #LINE# #TAB# if peer.get_state() == 'online': #LINE# #TAB# #TAB# wxagg_online = True #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del wxagg_standby #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if not wxagg_standby: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# records = {} #LINE# #TAB# colours = [] #LINE# #TAB# for i, col in enumerate(colour.colours): #LINE# #TAB# #TAB# record = { #LINE# #TAB# #TAB# #TAB#'s': col.s, #LINE# #TAB# #TAB# #TAB# 'n': col.n, #LINE# #TAB# #TAB# #TAB# 'x': col.x, #LINE# #TAB# #TAB# #TAB# 'y': col.y, #LINE# #TAB# #TAB# #TAB# 'color': col.color #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# colours.append(record) #LINE# #TAB# return {k: colours[v] for k, v in colours.items()}"
#LINE# #TAB# if not item_pid_exists(item_pid): #LINE# #TAB# #TAB# return False #LINE# #TAB# profile_info = get_profile_info(item_pid) #LINE# #TAB# try: #LINE# #TAB# #TAB# profile = profile_info['value'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False #LINE# #TAB# profile['is_available'] = '1' in profile #LINE# #TAB# return True
"#LINE# #TAB# base, ext = os.path.splitext(filename) #LINE# #TAB# if ext in ['.pyc', '.pyo']: #LINE# #TAB# #TAB# return os.path.getmtime(base) #LINE# #TAB# if ext == '.pyc': #LINE# #TAB# #TAB# return os.path.getmtime(base) #LINE# #TAB# return None"
"#LINE# #TAB# curr_dir = os.path.dirname(nested_path) #LINE# #TAB# while curr_dir!= os.path.dirname(os.path.dirname(curr_dir)): #LINE# #TAB# #TAB# nested_path = os.path.join(curr_dir, nested_path) #LINE# #TAB# #TAB# if os.path.exists(nested_path): #LINE# #TAB# #TAB# #TAB# return get_ubv_bandpasses(nested_path) #LINE# #TAB# #TAB# curr_dir = os.path.dirname(os.path.abspath(curr_dir)) #LINE# #TAB# return []"
"#LINE# #TAB# if entry.get('path', '') == '/': #LINE# #TAB# #TAB# return get_first_formatted_dir_in_dir(remote, entry) #LINE# #TAB# else: #LINE# #TAB# #TAB# return entry"
#LINE# #TAB# cc = np.nonzero(abs(m))[0] #LINE# #TAB# cc[cc!= masking_value] = 0 #LINE# #TAB# return cc
#LINE# #TAB# return p[0] <= rect[0] and p[1] <= rect[1] and p[2] >= rect[2 #LINE# #TAB# #TAB# ] and p[3] <= rect[3]
"#LINE# #TAB# for v in vertices: #LINE# #TAB# #TAB# if np.abs(np.dot(v, pole)) > np.abs(np.dot(v, pole)): #LINE# #TAB# #TAB# #TAB# return 'quadratic' #LINE# #TAB# if np.abs(np.cos(np.pi * width / 180)) > np.abs(np.sin(np.pi * width / #LINE# #TAB# #TAB# 180)): #LINE# #TAB# #TAB# return 'quadratic' #LINE# #TAB# return None"
#LINE# #TAB# if word.endswith('.'): #LINE# #TAB# #TAB# last_word = word[:-1] #LINE# #TAB# elif is_module: #LINE# #TAB# #TAB# last_word = word #LINE# #TAB# else: #LINE# #TAB# #TAB# last_word = word #LINE# #TAB# index = len(word) - 1 #LINE# #TAB# while index >= 0: #LINE# #TAB# #TAB# if word[index] in 'aeiouy': #LINE# #TAB# #TAB# #TAB# last_word = word[:index] + 'aeiouy' #LINE# #TAB# #TAB# #TAB# index -= 1 #LINE# #TAB# return last_word
#LINE# #TAB# path = set() #LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if node.type == 'path': #LINE# #TAB# #TAB# #TAB# for name in names: #LINE# #TAB# #TAB# #TAB# #TAB# if name not in node.name: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# path.add(name) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# path.add(node) #LINE# #TAB# return path
#LINE# #TAB# global _RotatedFieldFrame #LINE# #TAB# _RotatedFieldFrame = port #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# _RotatedFieldFrame = None
#LINE# #TAB# result = set() #LINE# #TAB# try: #LINE# #TAB# #TAB# ip = dns.resolver.query(dns_name) #LINE# #TAB# #TAB# for ip in ip: #LINE# #TAB# #TAB# #TAB# result.add(ip) #LINE# #TAB# except dns.resolver.NXDOMAIN: #LINE# #TAB# #TAB# pass #LINE# #TAB# return result
#LINE# #TAB# in_compatibility = options.in_compatibility or options.out_compatibility #LINE# #TAB# if not in_compatibility: #LINE# #TAB# #TAB# if os.path.exists(options.target): #LINE# #TAB# #TAB# #TAB# in_compatibility = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# in_compatibility = True #LINE# #TAB# if not os.path.isdir(in_compatibility): #LINE# #TAB# #TAB# in_compatibility = False #LINE# #TAB# return in_compatibility
"#LINE# #TAB# seeds = list(seeds) #LINE# #TAB# checks = [] #LINE# #TAB# for seed in seeds: #LINE# #TAB# #TAB# if '|' in seed: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# user, url = seed.split('|') #LINE# #TAB# #TAB# #TAB# #TAB# if user: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# checks.append((user, url)) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# checks.append((seed, None)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# checks.append((seed, url)) #LINE# #TAB# return checks"
"#LINE# #TAB# cs_output = {} #LINE# #TAB# if os.path.exists(directory) and os.path.isfile(directory): #LINE# #TAB# #TAB# files = [os.path.join(directory, path) for path in os.listdir( #LINE# #TAB# #TAB# #TAB# directory) if os.path.isfile(os.path.join(directory, path))] #LINE# #TAB# #TAB# for file_hash in hashlib.sha256(files).hexdigest(): #LINE# #TAB# #TAB# #TAB# cs_output[file_hash.hexdigest()] = file_hash #LINE# #TAB# return cs_output"
"#LINE# #TAB# if image.pixeltype!= 'unsigned char': #LINE# #TAB# #TAB# image = image.clone('unsigned char') #LINE# #TAB# h, w, d = image.shape #LINE# #TAB# pixel_array = np.empty((h, w, d)) #LINE# #TAB# for i in range(pixel_array.shape[0]): #LINE# #TAB# #TAB# pixel_array[i] = image.getpixel(i) #LINE# #TAB# new_img = iio.ANTsImage(pixel_array.shape, color_array.dtype, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# color_array.height, color_array.width, color_array.opacity) #LINE# #TAB# return new_img"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# credential = context.client.batch_get_credential(name) #LINE# #TAB# except google.api_core.exceptions.NotFound: #LINE# #TAB# #TAB# LOG.error(_LE('no credential found with name: %s'), name) #LINE# #TAB# #TAB# return None #LINE# #TAB# return credential.bytes"
#LINE# #TAB# connection_cls = base_dict.copy() #LINE# #TAB# connection_cls.update(extra_dict) #LINE# #TAB# return connection_cls
"#LINE# #TAB# tmp_share_dir = tempfile.mkdtemp() #LINE# #TAB# vault_client.share(path=tmp_share_dir, opt=opt) #LINE# #TAB# return tmp_share_dir"
"#LINE# #TAB# id, version = get_id_n_version(ident_hash) #LINE# #TAB# if id == None: #LINE# #TAB# #TAB# return None #LINE# #TAB# conn = cnx.connect() #LINE# #TAB# cursor = conn.cursor() #LINE# #TAB# timestamp_init = {} #LINE# #TAB# cursor.execute('SELECT * FROM timestamp_init WHERE id=%s', (id,)) #LINE# #TAB# for row in cursor: #LINE# #TAB# #TAB# timestamp_init[row['id']] = int(row['timestamp']) #LINE# #TAB# cursor.close() #LINE# #TAB# return timestamp_init"
"#LINE# #TAB# raw = get_raw_blast(pdb_id, chain_id, output_form) #LINE# #TAB# raw_blast = parse_blast(raw, output_form) #LINE# #TAB# return raw_blast"
"#LINE# #TAB# if guess == True: #LINE# #TAB# #TAB# node_name = 'default' #LINE# #TAB# #TAB# tags_str = 'default' #LINE# #TAB# else: #LINE# #TAB# #TAB# node_name = guess #LINE# #TAB# #TAB# tags_str = 'default' #LINE# #TAB# return node_name, tags_str"
"#LINE# #TAB# high = 0 #LINE# #TAB# for _ in range(8): #LINE# #TAB# #TAB# next_byte = data_stream[start] #LINE# #TAB# #TAB# if next_byte!= b'\x00': #LINE# #TAB# #TAB# #TAB# yield next_byte #LINE# #TAB# #TAB# #TAB# high += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# score = 0.0 #LINE# #TAB# for _ in range(16): #LINE# #TAB# #TAB# score += _compute_agreement_score(data_stream, start, high) #LINE# #TAB# #TAB# start += 2 #LINE# #TAB# return score"
"#LINE# #TAB# events_d = {'from': batch.sender, 'to': sorted(batch.recipients)} #LINE# #TAB# if batch.delivery_report: #LINE# #TAB# #TAB# events_d['delivery_report'] = batch.delivery_report #LINE# #TAB# if batch.send_at: #LINE# #TAB# #TAB# events_d['send_at'] = _write_datetime(batch.send_at) #LINE# #TAB# if batch.expire_at: #LINE# #TAB# #TAB# events_d['expire_at'] = _write_datetime(batch.expire_at) #LINE# #TAB# if batch.tags: #LINE# #TAB# #TAB# events_d['tags'] = sorted(batch.tags) #LINE# #TAB# return events_d"
"#LINE# #TAB# if power not in wg: #LINE# #TAB# #TAB# p1, p2 = power #LINE# #TAB# #TAB# if p1 == 0: #LINE# #TAB# #TAB# #TAB# yy = wg[0, -1] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(yy, p2 / 2).sum() / len(yy) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# xx = wg[-1, 0] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(xx, p1 / 2).sum() / len(xx) #LINE# #TAB# return wg"
"#LINE# #TAB# response_json = _get_indices_cache.get(url) #LINE# #TAB# if response_json is None: #LINE# #TAB# #TAB# response_json = _get_indices_from_cache(url) #LINE# #TAB# #TAB# _set_indices_cache(url, response_json) #LINE# #TAB# return response_json"
"#LINE# #TAB# list_attr = '' #LINE# #TAB# for a in dic.keys(): #LINE# #TAB# #TAB# list_attr += str(dic[a]) + ','#LINE# #TAB# list_attr += str(dic[a]) + ','#LINE# #TAB# return list_attr"
"#LINE# #TAB# if module is None: #LINE# #TAB# #TAB# module = loadConfig() #LINE# #TAB# slot = findSlot(name, module) #LINE# #TAB# if not slot: #LINE# #TAB# #TAB# return [] #LINE# #TAB# authors = [] #LINE# #TAB# for slot in slot.authors: #LINE# #TAB# #TAB# if slot.name == name: #LINE# #TAB# #TAB# #TAB# authors.append(slot) #LINE# #TAB# return authors"
"#LINE# #TAB# body_backbone_names = [f.name for f in body_backbone_CNN] #LINE# #TAB# image_backbone_names = [f.name for f in image_backbone_CNN] #LINE# #TAB# weights_filenames = ['{}-weights.csv'.format(body_backbone_names[0]) for #LINE# #TAB# #TAB# body_backbone_name in body_backbone_names] #LINE# #TAB# csvlogger_filenames = ['{}-image-backbone-{}'.format(body_backbone_names[1], #LINE# #TAB# #TAB# image_backbone_names[0]) for image_backbone_name in image_backbone_names] #LINE# #TAB# return weights_filenames, csvlogger_filenames"
"#LINE# #TAB# payload = sock.recv(struct.unpack('>I', payload)) #LINE# #TAB# return int(payload[0]) if payload else 0"
"#LINE# #TAB# n, r = covmatrix.shape #LINE# #TAB# corrmatrix = np.empty((n, r)) #LINE# #TAB# for i in range(r): #LINE# #TAB# #TAB# for j in range(r): #LINE# #TAB# #TAB# #TAB# corrmatrix[i, j] = covmatrix[i, j] / np.sqrt(np.outer(covmatrix[i, j], #LINE# #TAB# #TAB# #TAB# #TAB# covmatrix[i, j])) #LINE# #TAB# return corrmatrix"
#LINE# #TAB# formatted_pr_list = [] #LINE# #TAB# for pr in pr_list: #LINE# #TAB# #TAB# if 'api-gateway' in pr['name']: #LINE# #TAB# #TAB# #TAB# formatted_pr_list.append(pr['name']) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# formatted_pr_list.append(pr['name']) #LINE# #TAB# return formatted_pr_list
#LINE# #TAB# value = entry.get(prop) #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return default #LINE# #TAB# if raw: #LINE# #TAB# #TAB# return value #LINE# #TAB# return value[0]
"#LINE# #TAB# if datatype in (INT8, UINT8): #LINE# #TAB# #TAB# dbrcode = 1 #LINE# #TAB# elif datatype in (INT16, UINT16): #LINE# #TAB# #TAB# dbrcode = 2 #LINE# #TAB# elif datatype in (INT32, UINT32): #LINE# #TAB# #TAB# dbrcode = 4 #LINE# #TAB# elif datatype in (INT64, UINT64): #LINE# #TAB# #TAB# dbrcode = 8 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception('Unknown datatype:'+ str(datatype)) #LINE# #TAB# return dbrcode, dtype"
"#LINE# #TAB# if mono: #LINE# #TAB# #TAB# window = np.hanning(n) + 1e-05 #LINE# #TAB# else: #LINE# #TAB# #TAB# window = np.array([np.hanning(n) + 1e-05, np.hanning(n) + 1e-05]) #LINE# #TAB# functions = np.array([np.sin(window) for _ in range(n)]) #LINE# #TAB# classes = np.array([np.cos(function) for function in functions]) #LINE# #TAB# return functions, classes"
"#LINE# #TAB# default_permission = getattr(cls, 'default_permission', None) #LINE# #TAB# if default_permission is None: #LINE# #TAB# #TAB# logger.warning('Glean default permissions are disabled.') #LINE# #TAB# #TAB# return False #LINE# #TAB# if cls.default_user is None: #LINE# #TAB# #TAB# logger.warning('Glean default user is not logged in.') #LINE# #TAB# #TAB# return False #LINE# #TAB# if cls.default_user.is_staff or cls.default_user.is_superuser: #LINE# #TAB# #TAB# logger.warning('Glean user is already logged in.') #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, from_date=True) #LINE# #TAB# parser.add_argument('-f', '--fill-value', dest='fill_value', help= #LINE# #TAB# #TAB# 'The NNTP output file.') #LINE# #TAB# return parser.fill_value"
#LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# yield sequence[index]
#LINE# #TAB# global _MOLGENIS_EDGE_NODES #LINE# #TAB# if _MOLGENIS_EDGE_NODES is None: #LINE# #TAB# #TAB# _MOLGENIS_EDGE_NODES = mcp._compute_edge_nodes() #LINE# #TAB# return _MOLGENIS_EDGE_NODES
"#LINE# #TAB# departures = np.zeros((len(events), len(slots))) #LINE# #TAB# for row, event in enumerate(events): #LINE# #TAB# #TAB# for col, slot in enumerate(slots): #LINE# #TAB# #TAB# #TAB# departures[row, col] = event.departure #LINE# #TAB# return departures"
"#LINE# #TAB# for key in dir(color_marker): #LINE# #TAB# #TAB# if key.startswith(name): #LINE# #TAB# #TAB# #TAB# func = getattr(color_marker, key) #LINE# #TAB# #TAB# #TAB# if callable(func): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# func(color_marker) #LINE# #TAB# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return func"
#LINE# #TAB# for profile in cfg['profiles']: #LINE# #TAB# #TAB# auth = profile.get('auth') #LINE# #TAB# #TAB# key = auth.get('key') #LINE# #TAB# #TAB# if key is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if key == msg['user']: #LINE# #TAB# #TAB# #TAB# cfg['profiles'][profile]['auth'] = auth #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# cfg['profiles'][profile]['auth'] = '' #LINE# #TAB# return cfg
"#LINE# #TAB# path = os.path.join(_ROOT, 'items.json') #LINE# #TAB# items = read_json_file(path) #LINE# #TAB# for item_id in items: #LINE# #TAB# #TAB# if item_name == item_id: #LINE# #TAB# #TAB# #TAB# return item_id #LINE# #TAB# return None"
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if value <= lower: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if round_ is None: #LINE# #TAB# #TAB# round_ = _layer_sort_round #LINE# #TAB# used, total = list(zip(used, total)) #LINE# #TAB# ret = round(used / total * 100, 2) #LINE# #TAB# if ret == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# return 1 - ret"
"#LINE# #TAB# if enabled: #LINE# #TAB# #TAB# return enabled #LINE# #TAB# if __version__ < (3, 5): #LINE# #TAB# #TAB# return str(enabled) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return next(modules) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# found_values = [] #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# if which_key in k: #LINE# #TAB# #TAB# #TAB# #TAB# found_values.append(process_init_data(v, which_key)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# found_values.append(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# found_values.append(k) #LINE# #TAB# return found_values"
"#LINE# #TAB# #TAB# results = {} #LINE# #TAB# #TAB# for layer_id, line in f.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line or line[0] == '#': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if layer_id not in results: #LINE# #TAB# #TAB# #TAB# #TAB# results[layer_id] = [] #LINE# #TAB# #TAB# #TAB# results[layer_id].append(line) #LINE# #TAB# #TAB# return results"
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# the parameter was retrieved #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# could not retrieve parameter #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
#LINE# #TAB# order = len(waves) #LINE# #TAB# l = len(waves[0]) #LINE# #TAB# gap = [0] * l #LINE# #TAB# for i in range(l): #LINE# #TAB# #TAB# if waves[i]!= waves[i + 1]: #LINE# #TAB# #TAB# #TAB# gap[i] = waves[i + 1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# gap[i] = 0 #LINE# #TAB# for i in range(l): #LINE# #TAB# #TAB# if waves[i]!= waves[i + 1]: #LINE# #TAB# #TAB# #TAB# gap[i] = waves[i - 1] #LINE# #TAB# return gap
#LINE# #TAB# field_list = [] #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# if item in depends: #LINE# #TAB# #TAB# #TAB# field_list.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# field_list.append(item) #LINE# #TAB# return field_list
"#LINE# #TAB# if ot is None: #LINE# #TAB# #TAB# ot = get_ot() #LINE# #TAB# text = reverse_complement_sequences(search, ot) #LINE# #TAB# if resultformat == 'brief': #LINE# #TAB# #TAB# return text #LINE# #TAB# else: #LINE# #TAB# #TAB# return text"
"#LINE# #TAB# #TAB# return [goea.enrichment, #LINE# #TAB# #TAB# #TAB# #TAB# goea.namespace, #LINE# #TAB# #TAB# #TAB# #TAB# goea.depth, #LINE# #TAB# #TAB# #TAB# #TAB# goea.namespace_name, #LINE# #TAB# #TAB# #TAB# #TAB# goea.GO]"
#LINE# #TAB# try: #LINE# #TAB# #TAB# client.get_bucket(bucket_name) #LINE# #TAB# #TAB# return True #LINE# #TAB# except google.api_core.exceptions.NotFound: #LINE# #TAB# #TAB# return False
#LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# df['pc_metric_' + col] = df[col] * 10000 #LINE# #TAB# return df
#LINE# #TAB# sdm = getsdm(sdmname) #LINE# #TAB# sourcedict = {} #LINE# #TAB# for s in sdm['sources']: #LINE# #TAB# #TAB# srcdict = get_source(s) #LINE# #TAB# #TAB# decdict = get_dec(s) #LINE# #TAB# #TAB# sourcedict[s['name']] = srcdict #LINE# #TAB# return sourcedict
"#LINE# #TAB# for k, v in duration.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# if k in ('seconds','minutes','seconds'): #LINE# #TAB# #TAB# #TAB# #TAB# return str(v) #LINE# #TAB# #TAB# #TAB# elif isinstance(v, int): #LINE# #TAB# #TAB# #TAB# #TAB# return '%d:%02d' % (v * 60, k) #LINE# #TAB# #TAB# #TAB# elif isinstance(v, float): #LINE# #TAB# #TAB# #TAB# #TAB# return '%d:%02d' % (v * 60, k) #LINE# #TAB# return ''"
#LINE# #TAB# if'sqlite_version' in settings.DATABASES: #LINE# #TAB# #TAB# property_type ='sqlite_version' #LINE# #TAB# elif 'postgresql_version' in settings.DATABASES: #LINE# #TAB# #TAB# property_type = 'postgresql_version' #LINE# #TAB# elif 'psycopg2' in settings.DATABASES: #LINE# #TAB# #TAB# property_type = 'psycopg2' #LINE# #TAB# else: #LINE# #TAB# #TAB# property_type = 'local' #LINE# #TAB# return property_type
#LINE# #TAB# if not s: #LINE# #TAB# #TAB# return '' #LINE# #TAB# lines = s.split('-') #LINE# #TAB# if len(lines) == 1: #LINE# #TAB# #TAB# return lines[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return lines[0] + '-' + lines[1]
#LINE# #TAB# accepted_additions = set() #LINE# #TAB# if not requested_additions: #LINE# #TAB# #TAB# return accepted_additions #LINE# #TAB# for request in requested_additions: #LINE# #TAB# #TAB# if request.addition_uuid == tea_slug: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# accepted_additions.addition_uuid = request.addition_uuid #LINE# #TAB# return accepted_additions
#LINE# #TAB# if is_runtime(): #LINE# #TAB# #TAB# return 'runtime' #LINE# #TAB# elif is_frozen(): #LINE# #TAB# #TAB# return 'frozen' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'original'
#LINE# #TAB# tnow = time.time() #LINE# #TAB# for ws in nb.worksheets: #LINE# #TAB# #TAB# for cell in ws.cells: #LINE# #TAB# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# cell.outputs = [[]] #LINE# #TAB# #TAB# #TAB# #TAB# for output in cell.outputs: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# output.append(numpy.array(output)) #LINE# #TAB# #TAB# #TAB# #TAB# tnow += time.time() - tnow #LINE# #TAB# return nb.cells
#LINE# #TAB# if not os.path.isdir(dir_name): #LINE# #TAB# #TAB# msg = '{0} is not a directory'.format(dir_name) #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(msg) #LINE# #TAB# return dir_name
"#LINE# #TAB# from..forms.converter import decode_4chan_2bit_fanout2field #LINE# #TAB# return [decode_4chan_2bit_fanout2field(field_type, name) for name, field_type in #LINE# #TAB# #TAB# filterset_class.fields.items()]"
#LINE# #TAB# default_value = [] #LINE# #TAB# for i in range(len(initialparameters)): #LINE# #TAB# #TAB# default_value.append(initialparameters[i]) #LINE# #TAB# return default_value
#LINE# #TAB# assert tree is not None #LINE# #TAB# if 'wave_flux' in tree.attrib: #LINE# #TAB# #TAB# wave_flux = tree.attrib['wave_flux'] #LINE# #TAB# elif 'wave_flux_0' in tree.attrib: #LINE# #TAB# #TAB# wave_flux = tree.attrib['wave_flux_0'] #LINE# #TAB# elif 'wave_flux' in tree.attrib: #LINE# #TAB# #TAB# wave_flux = tree.attrib['wave_flux'] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception('Unknown wave flux format') #LINE# #TAB# return wave_flux
"#LINE# #TAB# for sLine in oLine.lines: #LINE# #TAB# #TAB# if sLine.comment: #LINE# #TAB# #TAB# #TAB# oLine.set_attr('comment', None) #LINE# #TAB# return"
#LINE# #TAB# interiors = set() #LINE# #TAB# for paradigm in paradigms: #LINE# #TAB# #TAB# for symbol in paradigm.exterior.symbols: #LINE# #TAB# #TAB# #TAB# interiors.add(symbol) #LINE# #TAB# return interiors
#LINE# #TAB# my_fd.close() #LINE# #TAB# global _eta #LINE# #TAB# _eta = None #LINE# #TAB# if _eta is not None: #LINE# #TAB# #TAB# _eta.join() #LINE# #TAB# return _eta
#LINE# #TAB# result = [] #LINE# #TAB# for _ in range(steps_per_epoch): #LINE# #TAB# #TAB# next_step = current_step + steps_per_loop #LINE# #TAB# #TAB# if next_step not in result: #LINE# #TAB# #TAB# #TAB# result.append(next_step) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# step = next_step * steps_per_loop #LINE# #TAB# #TAB# #TAB# result.append(step) #LINE# #TAB# return result
"#LINE# #TAB# scraperclasses = [] #LINE# #TAB# for token in reserved_tokens: #LINE# #TAB# #TAB# if token == '@': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tokens = token.split() #LINE# #TAB# #TAB# #TAB# scraperclass = tuple(re.match(r'[A-Z]', tokens)) #LINE# #TAB# #TAB# #TAB# if scraperclass: #LINE# #TAB# #TAB# #TAB# #TAB# scraperclasses.append(scraperclass) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# return scraperclasses"
"#LINE# #TAB# path = os.path.normpath(path) #LINE# #TAB# current_path = os.path.dirname(path) #LINE# #TAB# path = os.path.join(current_path, path) #LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# new_path = os.path.join(path, filename) #LINE# #TAB# #TAB# if os.path.isdir(new_path): #LINE# #TAB# #TAB# #TAB# delete_header(new_path) #LINE# #TAB# #TAB# elif os.path.isfile(new_path): #LINE# #TAB# #TAB# #TAB# delete_header(new_path) #LINE# #TAB# return path"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls._response_queue.popleft() #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# normvectpart = np.sqrt(quat[0] ** 2 + quat[1] ** 2 + quat[2] ** 2 + #LINE# #TAB# #TAB# quat[3] ** 2) #LINE# #TAB# angle = np.arccos(quat[3] / normvectpart) * 2.0 #LINE# #TAB# unitvec = np.array(quat[:3]) / np.sin(angle / 2) / normvectpart #LINE# #TAB# return unitvec, angle"
#LINE# #TAB# osvif_subnet = [] #LINE# #TAB# with open(filepath) as handle: #LINE# #TAB# #TAB# for line in handle: #LINE# #TAB# #TAB# #TAB# tokens = line.strip().split('\t') #LINE# #TAB# #TAB# #TAB# terms = terms[0:3] #LINE# #TAB# #TAB# #TAB# osvif_subnet.append(tokens) #LINE# #TAB# return osvif_subnet
#LINE# #TAB# orient_by_sl = [] #LINE# #TAB# for tag in ds.tags: #LINE# #TAB# #TAB# if tag[0] == 'P': #LINE# #TAB# #TAB# #TAB# orient_by_sl.append(tag) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield tag #LINE# #TAB# if orient_by_sl: #LINE# #TAB# #TAB# for sl in ds.tags: #LINE# #TAB# #TAB# #TAB# if sl[0]!= 'P': #LINE# #TAB# #TAB# #TAB# #TAB# orient_by_sl.append(sl) #LINE# #TAB# return orient_by_sl
"#LINE# #TAB# for i, entry in enumerate(contents): #LINE# #TAB# #TAB# if i > 0: #LINE# #TAB# #TAB# #TAB# s = oligo_dict[entry[3]] #LINE# #TAB# #TAB# #TAB# if s.get('status') == 'composite': #LINE# #TAB# #TAB# #TAB# #TAB# distance = round(s.get('distance', 0), 4) #LINE# #TAB# #TAB# #TAB# #TAB# oligo_dict[entry[0]][i] = distance #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return oligo_dict"
"#LINE# #TAB# s = {} #LINE# #TAB# for k, v in nested_dict.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if isinstance(v, HTTPException): #LINE# #TAB# #TAB# #TAB# #TAB# s[k] = v #LINE# #TAB# #TAB# #TAB# elif isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# for k1, v2 in v.items(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# s[k1] =.map_http_status_to_exception(v2) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# s[k] = v #LINE# #TAB# #TAB# except (KeyError, ValueError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return s"
#LINE# #TAB# events = {} #LINE# #TAB# for event in all_events: #LINE# #TAB# #TAB# event = event['event'] #LINE# #TAB# #TAB# if event['game_id'] == game_id: #LINE# #TAB# #TAB# #TAB# event['full_name'] = event['name'] #LINE# #TAB# #TAB# #TAB# event['intensity'] = event['intensity'] #LINE# #TAB# #TAB# events[event['game_id']] = event #LINE# #TAB# return events
#LINE# #TAB# status = data['headers']['status'] #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# Memory.leave_room = data['room']
"#LINE# #TAB# parser.add_argument('--os-path-contained-by', metavar= #LINE# #TAB# #TAB# '<path-contained-by>', default=utils.env('OS_PATH_CONTAINED_BY', #LINE# #TAB# #TAB# default=DEFAULT_PATH_CONTAINED_BY), help= #LINE# #TAB# #TAB# 'Pathcontained by command line interface, default=' + #LINE# #TAB# #TAB# DEFAULT_PATH_CONTAINED_BY +'(Env: OS_PATH_CONTAINED_BY)') #LINE# #TAB# return parser"
"#LINE# #TAB# with open(path, 'rt') as f: #LINE# #TAB# #TAB# box_list = f.read().split('\n') #LINE# #TAB# #TAB# num_rows = len(box_list) #LINE# #TAB# #TAB# num_cols = len(box_list[0]) #LINE# #TAB# #TAB# box_dict = {} #LINE# #TAB# #TAB# for row in box_list: #LINE# #TAB# #TAB# #TAB# col = int(row[0]) #LINE# #TAB# #TAB# #TAB# row[0] = float(row[1]) #LINE# #TAB# #TAB# #TAB# col[1] = float(row[2]) #LINE# #TAB# #TAB# #TAB# box_dict[col[0]][row[1]] = float(row[3]) #LINE# #TAB# #TAB# box_list.append(box_dict) #LINE# #TAB# return box_list"
#LINE# #TAB# if value is not None and value!= '': #LINE# #TAB# #TAB# return value
"#LINE# #TAB# with open(model_path, 'rb') as f: #LINE# #TAB# #TAB# model_dict = json.load(f) #LINE# #TAB# metrics = model_dict['metrics'] #LINE# #TAB# metrics = _convert_metrics_to_diagonal(metrics) #LINE# #TAB# metrics = _convert_metrics_to_diagonal(metrics) #LINE# #TAB# retinanet_model = Kerasretinanet() #LINE# #TAB# retinanet_model.load_diagonal_model_from_JSON(model_dict, metrics) #LINE# #TAB# return retinanet_model"
"#LINE# #TAB# definition = [ #LINE# #TAB# #TAB# sa.asc(getattr(archive_table, col_name)) for col_name in archive_table._version_col_names #LINE# #TAB# ] #LINE# #TAB# definition.append(sa.asc(archive_table.version_id)) #LINE# #TAB# return definition"
#LINE# #TAB# queryset = cls.objects.filter(name__in=cls.get_all_names()) #LINE# #TAB# queryset = queryset.filter(category__in=cls.get_log_categories()) #LINE# #TAB# return queryset
"#LINE# mem_ma = {} #LINE# fname = pkg_resources.resource_filename(__name__, 'gdaldem.csv') #LINE# with open(fname, 'rb') as csvfile: #LINE# #TAB# reader = csv.reader(csvfile, delimiter = ',') #LINE# #TAB# for row in reader: #LINE# #TAB# mem_ma[row[0]] = row[1] #LINE# return mem_ma"
"#LINE# #TAB# d = {} #LINE# #TAB# for key, value in p.items(): #LINE# #TAB# #TAB# d[key] = value #LINE# #TAB# return d"
"#LINE# #TAB# r = requests.get(url='{}scenarios/{}'.format(LIZARD_URL, scenario_uuid), #LINE# #TAB# #TAB# headers=get_headers()) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# if r.status_code == 200: #LINE# #TAB# #TAB# return 'OK' #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# pool_name = params['pool_name'] #LINE# #TAB# doc = '' #LINE# #TAB# if pool_name!= '': #LINE# #TAB# #TAB# doc +='pool_name = {0}'.format(pool_name) #LINE# #TAB# if params['update_index'] is True: #LINE# #TAB# #TAB# doc +='update_index = {0}'.format(params['update_index']) #LINE# #TAB# if params['delete_index'] is True: #LINE# #TAB# #TAB# doc +='delete_index = {0}'.format(params['delete_index']) #LINE# #TAB# doc += '\n' #LINE# #TAB# return doc
"#LINE# #TAB# config = {} #LINE# #TAB# for path in sys.path: #LINE# #TAB# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# config[path] = {} #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# f = open(path, 'r') #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key = next(f.readlines())[0] #LINE# #TAB# #TAB# value = next(f.readlines())[0] #LINE# #TAB# #TAB# config[path][key] = value #LINE# #TAB# return config"
"#LINE# #TAB# output = subprocess.check_output([git_path, 'log', '--pretty=%H'], #LINE# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# match = re.search('^HEAD', output.decode('utf-8')) #LINE# #TAB# return match.group(1) if match else None"
"#LINE# #TAB# bone_name, line_number = name.split('.') #LINE# #TAB# line_number = '{0}_{1}'.format(bone_name, line_number) #LINE# #TAB# if topping: #LINE# #TAB# #TAB# return '{0}_{1}'.format(bone_name, line_number) #LINE# #TAB# else: #LINE# #TAB# #TAB# return line_number"
"#LINE# #TAB# return cls.build_send_payload('list_installed_genomes', {k: v for k, v in #LINE# #TAB# #TAB# cls.__dict__.items() if k.endswith('_cmd')})[0]"
"#LINE# #TAB# for text in attribute_set: #LINE# #TAB# #TAB# text = text.replace('&amp;', '&') #LINE# #TAB# #TAB# text = text.replace('<', '&lt;') #LINE# #TAB# #TAB# text = text.replace('>', '&gt;') #LINE# #TAB# return text"
"#LINE# #TAB# if hasattr(request, '_kinto_core_resource'): #LINE# #TAB# #TAB# return request._kinto_core_resource #LINE# #TAB# return 'float'"
#LINE# #TAB# p = request.json['action']['detailParameters'] #LINE# #TAB# return p
"#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"#LINE# #TAB# assert isinstance(key, Key) #LINE# #TAB# out = eval_metrics_impl(key, use, kid) #LINE# #TAB# if isinstance(out, list): #LINE# #TAB# #TAB# for i in out: #LINE# #TAB# #TAB# #TAB# out[i] = [eval_metrics(kid, i)] #LINE# #TAB# elif isinstance(out, dict): #LINE# #TAB# #TAB# for k, v in out.items(): #LINE# #TAB# #TAB# #TAB# out[k] = eval_metrics(v, use, kid) #LINE# #TAB# else: #LINE# #TAB# #TAB# out = eval_metrics_impl(out, use, kid) #LINE# #TAB# return out"
"#LINE# #TAB# size = len(list1) #LINE# #TAB# sum1 = sum(list1) #LINE# #TAB# sum2 = sum(list2) #LINE# #TAB# sum_sq1 = sum([pow(l, 2) for l in list1]) #LINE# #TAB# sum_sq2 = sum([pow(l, 2) for l in list2]) #LINE# #TAB# score = sum1 * sum2 / (sum1 + sum2) #LINE# #TAB# return score"
#LINE# #TAB# headHeaders = copy.copy(allHeadersContent) #LINE# #TAB# radon = {} #LINE# #TAB# for headHeader in headHeaders: #LINE# #TAB# #TAB# radon[headHeader] = [] #LINE# #TAB# for x in range(len(headHeaders)): #LINE# #TAB# #TAB# if x in headHeaders: #LINE# #TAB# #TAB# #TAB# headHeaders.remove(x) #LINE# #TAB# for x in range(len(headHeaders)): #LINE# #TAB# #TAB# if x not in headHeaders: #LINE# #TAB# #TAB# #TAB# radon[x] = [allHeadersContent[x][x] for x in range(len( #LINE# #TAB# #TAB# #TAB# #TAB# headHeaders))] #LINE# #TAB# return radon
"#LINE# #TAB# if cause_string: #LINE# #TAB# #TAB# cause_list = ','.join(cause_string.split(',')) #LINE# #TAB# #TAB# for cause in cause_list: #LINE# #TAB# #TAB# #TAB# queryset = queryset.filter( #LINE# #TAB# #TAB# #TAB# #TAB# contracts_version__matches_version=cause, #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return queryset"
#LINE# #TAB# all_ref_counts = [] #LINE# #TAB# ref_bar = [] #LINE# #TAB# for c in row_content: #LINE# #TAB# #TAB# if c.strip(): #LINE# #TAB# #TAB# #TAB# all_ref_counts.append(len(c.strip())) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ref_bar.append(len(c.strip())) #LINE# #TAB# return all_ref_counts
"#LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# for path in os.listdir(md5): #LINE# #TAB# #TAB# path = os.path.join(md5, path) #LINE# #TAB# #TAB# if os.path.islink(path): #LINE# #TAB# #TAB# #TAB# md5.update(path) #LINE# #TAB# md5.hexdigest() #LINE# #TAB# return md5"
#LINE# #TAB# with cls._batcher_lock: #LINE# #TAB# #TAB# if not cls._batcher: #LINE# #TAB# #TAB# #TAB# cls._batcher = cls._default_batcher() #LINE# #TAB# return cls._batcher
"#LINE# #TAB# b0 = 0.0641 * 3 / 4 #LINE# #TAB# b1 = 2.255 * 3 / 4 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.0 * 3 / 2 ** (5 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Na'] * i2c['S2O3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# if not isinstance(intfspec, list): #LINE# #TAB# #TAB# intfspec = [intfspec] #LINE# #TAB# h2o = {'create': 'h2o {}'.format(intfspec)} #LINE# #TAB# try: #LINE# #TAB# #TAB# return h2o[intfspec[0]] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return h2o"
#LINE# #TAB# path = private_seed_file_path(model_id) #LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# return path
"#LINE# #TAB# name = field_type.name #LINE# #TAB# value = buffer.get(name, _byte_order) #LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# if is_string: #LINE# #TAB# #TAB# #TAB# return value.encode('utf-8') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# return value"
"#LINE# #TAB# parser = argparse.ArgumentParser(add_help=False) #LINE# #TAB# parser.add_argument('-f', '--inputfile', type=argparse.FileType('r'), #LINE# #TAB# #TAB# default=sys.stdout, help='The input file') #LINE# #TAB# parser.add_argument('-o', '--output', type=argparse.FileType('w'), #LINE# #TAB# #TAB# default=sys.stdout, help='The output file') #LINE# #TAB# args = parser.parse_args() #LINE# #TAB# return args"
"#LINE# #TAB# #TAB# checkpoint = os.path.join(checkpointDir,'model.checkpoint') #LINE# #TAB# #TAB# model = cls.readFromCheckpoint(checkpoint) #LINE# #TAB# #TAB# model.checkpoint = checkpoint #LINE# #TAB# #TAB# return model"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return os.kill(int(pid), 0) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# data_length = len(data) #LINE# #TAB# #TAB# payload = data[data_length - 6:] #LINE# #TAB# #TAB# assert len(payload) == 4 #LINE# #TAB# #TAB# return payload[0:data_length + 6] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# logger.warning('invalid UUID length %s', len(payload)) #LINE# #TAB# #TAB# raise"
#LINE# #TAB# if include_start: #LINE# #TAB# #TAB# return _datetime_to_djd_first_line(code) #LINE# #TAB# else: #LINE# #TAB# #TAB# return _datetime_to_djd_second(code) - _datetime_to_djd_first_line(code #LINE# #TAB# #TAB# #TAB# ).djd
#LINE# #TAB# if repo is None: #LINE# #TAB# #TAB# repo = fe.buildtimetrend.settings.REPO_NAME #LINE# #TAB# return 'git-repos/%s' % repo
"#LINE# #TAB# sig = inspect.signature(func) #LINE# #TAB# num_inputs = 1 if is_list_like(func) else len(func.inputs) #LINE# #TAB# if num_inputs is None: #LINE# #TAB# #TAB# num_inputs = 1 #LINE# #TAB# for type_ in input_type: #LINE# #TAB# #TAB# if type_ in sig.parameters: #LINE# #TAB# #TAB# #TAB# num_inputs += 1 #LINE# #TAB# return sig, num_inputs"
"#LINE# #TAB# feed_tags = html.find_all('feed', class_='rss') #LINE# #TAB# fixed_class_lookup = [] #LINE# #TAB# if not feed_tags: #LINE# #TAB# #TAB# return fixed_class_lookup #LINE# #TAB# for link in feed_tags: #LINE# #TAB# #TAB# if link.get('href').startswith(url): #LINE# #TAB# #TAB# #TAB# fixed_class_lookup.append(link) #LINE# #TAB# return fixed_class_lookup"
"#LINE# #TAB# global _frozen #LINE# #TAB# _frozen = True #LINE# #TAB# words = {} #LINE# #TAB# for k, v in six.iteritems(kwargs): #LINE# #TAB# #TAB# if not k.startswith('_'): #LINE# #TAB# #TAB# #TAB# word = k[1:] #LINE# #TAB# #TAB# #TAB# words[word] = v #LINE# #TAB# return words"
"#LINE# #TAB# expr = expression.Expression('v{} {}'.format(vid, expr)) #LINE# #TAB# cols = dtable.columns(vid) #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# col = col.name #LINE# #TAB# #TAB# mask = expr.evaluate(dtable, {vid: col}) #LINE# #TAB# #TAB# dtable[mask, col] = np.nan"
"#LINE# #TAB# t = a / (np.sqrt(k1 * (x - x1) + np.sqrt(k2 * (x - x2)) + #LINE# #TAB# #TAB# np.sqrt(k1 * (x - x2)))) #LINE# #TAB# topics = [] #LINE# #TAB# for t1, t2 in zip(t, t): #LINE# #TAB# #TAB# if np.abs(t1) >= k1 and np.abs(t2) >= k2: #LINE# #TAB# #TAB# #TAB# topic = t1 / (k1 * (x - x2) + np.exp(k2 * (x - x1)))) #LINE# #TAB# #TAB# #TAB# topics.append(topic) #LINE# #TAB# return topics"
"#LINE# #TAB# s = _socket.socket(_socket.AF_INET, _socket.SOCK_DGRAM) #LINE# #TAB# try: #LINE# #TAB# #TAB# s.connect_ex((addr[0], addr[1])) #LINE# #TAB# #TAB# s.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, True) #LINE# #TAB# except (OSError, socket.error): #LINE# #TAB# #TAB# pass"
#LINE# #TAB# unpacked_data = {} #LINE# #TAB# unpacked_data['_id'] = row['_id'] #LINE# #TAB# unpacked_data['type'] = row['type'] #LINE# #TAB# if collection.__name__.startswith('collection'): #LINE# #TAB# #TAB# unpacked_data['_id'] = collection.id #LINE# #TAB# else: #LINE# #TAB# #TAB# unpacked_data['_id'] = row['id'] #LINE# #TAB# return unpacked_data
#LINE# #TAB# try: #LINE# #TAB# #TAB# py_module = importlib.import_module(module) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if'scipy' in py_module.__name__: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'numpy' in py_module.__name__: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# path = os.path.join(os.path.dirname(__file__), 'thresh', name) #LINE# #TAB# return path"
"#LINE# #TAB# j1_total_angular_momenta = j1['angular_momenta'] #LINE# #TAB# j2_total_angular_momenta = j2['angular_momenta'] #LINE# #TAB# for f in [j1_total_angular_momenta, j2_total_angular_momenta]: #LINE# #TAB# #TAB# file_name = os.path.join(j1, f) #LINE# #TAB# #TAB# file_name = os.path.replace(file_name, f) #LINE# #TAB# return j1_total_angular_momenta, j2_total_angular_momenta"
"#LINE# #TAB# assert tp.is_type_class #LINE# #TAB# result = [] #LINE# #TAB# for member in tp.array: #LINE# #TAB# #TAB# if not getattr(member, 'is_function', False): #LINE# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# #TAB# if isinstance(member, TypeInfo): #LINE# #TAB# #TAB# #TAB# if member.is_array: #LINE# #TAB# #TAB# #TAB# #TAB# result.extend(transform_velocity_array(member)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# return result"
"#LINE# #TAB# with open(path, 'r', encoding='utf-8') as f: #LINE# #TAB# #TAB# contents = f.read() #LINE# #TAB# return contents"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return math.log(value) >= 0.5 #LINE# #TAB# except: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# sock.bind(('', port)) #LINE# #TAB# #TAB# #TAB# active_rule_set = sock #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# LOG.error(e) #LINE# #TAB# #TAB# #TAB# active_rule_set = None #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# return active_rule_set"
"#LINE# #TAB# inchi_vectors = np.concatenate((vectors[:, (0)] * np.cos(np.radians(vectors[1] #LINE# #TAB# #TAB# ), vectors[:, (1)])) #LINE# #TAB# return inchi_vectors"
"#LINE# #TAB# parser = cls(desc=' HyperKitty argument parser') #LINE# #TAB# parser.add_argument('--sleep-time', dest='sleep_time', default= #LINE# #TAB# #TAB# DEFAULT_SLEEP_TIME, help='Sleep time in case of connection lost') #LINE# #TAB# return parser"
"#LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# itksize = vnl_vector.size() #LINE# #TAB# shape = [itksize] #LINE# #TAB# pixelType = 'SS' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlSS._get_memview(vnl_vector) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_vector) #LINE# #TAB# return itk_view"
#LINE# #TAB# psi = '' #LINE# #TAB# if log_level == logging.INFO: #LINE# #TAB# #TAB# psi = 'INFO' #LINE# #TAB# elif log_level == logging.WARNING: #LINE# #TAB# #TAB# psi = 'WARNING' #LINE# #TAB# elif log_level == logging.INFO: #LINE# #TAB# #TAB# psi = 'INFO' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unsupported log level: {}'.format(log_level)) #LINE# #TAB# return psi
"#LINE# #TAB# parsedString = messageString.split('\n') #LINE# #TAB# kwargs = {} #LINE# #TAB# if len(parsedString) > 2: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# params = {'message': parsedString[1]} #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# params = {} #LINE# #TAB# #TAB# for param in parsedString[2:]: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = param.split('=') #LINE# #TAB# #TAB# #TAB# #TAB# kwargs[key] = value #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return kwargs"
"#LINE# #TAB# path = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# with open(os.path.join(path, 'VERSION')) as f: #LINE# #TAB# #TAB# return f.read() #LINE# #TAB# return None"
"#LINE# #TAB# if target.position is None: #LINE# #TAB# #TAB# func = sa.sql.func #LINE# #TAB# #TAB# stmt = sa.select([func.coalesce(func.max(mapper.mapped_table.c. #LINE# #TAB# #TAB# #TAB# position), -1)]) #LINE# #TAB# #TAB# target.position = connection.execute(stmt).scalar() + 1 #LINE# #TAB# return target"
#LINE# #TAB# retv = None #LINE# #TAB# for ksym in conn.keysymlist(): #LINE# #TAB# #TAB# if kstr.lower() in ksym.lower(): #LINE# #TAB# #TAB# #TAB# retv = conn.find_keycode(kstr) #LINE# #TAB# #TAB# #TAB# if retv is not None: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# elif kstr.lower() == ksym.lower(): #LINE# #TAB# #TAB# #TAB# retv = conn.find_keycode(kstr) #LINE# #TAB# if retv is not None: #LINE# #TAB# #TAB# return retv #LINE# #TAB# return None
#LINE# #TAB# logger = [] #LINE# #TAB# for line in txt.split('\n'): #LINE# #TAB# #TAB# logger.append(line) #LINE# #TAB# return logger
"#LINE# #TAB# levels = [] #LINE# #TAB# if isinstance(prefix, six.string_types): #LINE# #TAB# #TAB# levels.append(prefix) #LINE# #TAB# elif isinstance(prefix, bytes): #LINE# #TAB# #TAB# levels.append(bytes2str(prefix)) #LINE# #TAB# for i in range(len(suffix)): #LINE# #TAB# #TAB# levels.append(suffix[i]) #LINE# #TAB# return levels"
#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(model[for_node]) #LINE# #TAB# #TAB# except OSError as exc: #LINE# #TAB# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(model[for_node]): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise
"#LINE# #TAB# ra_deg, dec_deg = _get_ra_dec(body, location, date) #LINE# #TAB# if ra_deg is None or dec_deg < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# body = body.replace(ra_deg, dec_deg) #LINE# #TAB# lat_rad = _get_lat_rad(body) #LINE# #TAB# lon_rad = _get_lon_rad(body) #LINE# #TAB# alt_rad = _get_altitude(body) #LINE# #TAB# return ra_deg, dec_deg, lon_rad, alt_rad"
"#LINE# #TAB# if name[0] == '/': #LINE# #TAB# #TAB# return os.path.split(os.path.dirname(name))[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# lst = name.split('/') #LINE# #TAB# #TAB# for i in range(1, len(lst)): #LINE# #TAB# #TAB# #TAB# if lst[i] == '/': #LINE# #TAB# #TAB# #TAB# #TAB# return '/' + lst[i] #LINE# #TAB# #TAB# return name"
"#LINE# #TAB# return cls.build_send_payload('is_number', {'cacheDisabled': cacheDisabled} #LINE# #TAB# #TAB# ), None"
"#LINE# #TAB# psi = 0.0 #LINE# #TAB# for f in os.listdir(dir): #LINE# #TAB# #TAB# if f.endswith('.lr'): #LINE# #TAB# #TAB# #TAB# psi += 1 #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# f = os.path.join(dir, f) #LINE# #TAB# #TAB# if os.path.isfile(f): #LINE# #TAB# #TAB# #TAB# f = open(f, 'r') #LINE# #TAB# #TAB# f.close() #LINE# #TAB# return psi, psi"
#LINE# #TAB# words = name.split('_') #LINE# #TAB# while len(words) < 3: #LINE# #TAB# #TAB# name = words.pop() #LINE# #TAB# return name
"#LINE# #TAB# canonical, https, httpswww = (domain.canonical, domain.https, domain. #LINE# #TAB# #TAB# httpswww) #LINE# #TAB# if canonical.host == 'www': #LINE# #TAB# #TAB# canonical_https = httpswww #LINE# #TAB# else: #LINE# #TAB# #TAB# canonical_https = https #LINE# #TAB# return canonical_https, httpswww"
#LINE# #TAB# content = [] #LINE# #TAB# for _ in range(n): #LINE# #TAB# #TAB# content.append(source_iter.next()) #LINE# #TAB# return content
"#LINE# #TAB# frame = inspect.currentframe().f_back.f_back #LINE# #TAB# while frame is not None: #LINE# #TAB# #TAB# module = frame.f_globals.get('__name__', None) #LINE# #TAB# #TAB# if module is None: #LINE# #TAB# #TAB# #TAB# frame = frame.f_back #LINE# #TAB# #TAB# elif hasattr(module, name): #LINE# #TAB# #TAB# #TAB# frame = module.__name__ #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise RuntimeError('Unknown local variable %r' % (name,)) #LINE# #TAB# #TAB# idx = name.find('.') #LINE# #TAB# #TAB# if idx == -1: #LINE# #TAB# #TAB# #TAB# frame = None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# frame = frame[:idx] #LINE# #TAB# return name, value"
"#LINE# #TAB# if pat in video: #LINE# #TAB# #TAB# return video.keys() #LINE# #TAB# keys = sorted(_video.keys()) #LINE# #TAB# if pat in keys: #LINE# #TAB# #TAB# return [k for k in keys if re.search(pat, k, re.I)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
#LINE# #TAB# ta_to_map = [] #LINE# #TAB# for app in apps_list: #LINE# #TAB# #TAB# if app not in ta_to_map: #LINE# #TAB# #TAB# #TAB# ta_to_map.append(app) #LINE# #TAB# return ta_to_map
#LINE# #TAB# print(''.join(console_wait_for_keypress_lines())) #LINE# #TAB# kb.refresh() #LINE# #TAB# for line in console_wait_for_keypress_lines(): #LINE# #TAB# #TAB# if line == '\n': #LINE# #TAB# #TAB# #TAB# sys.stdout.write(''.join(console_wait_for_keypress_lines())) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sys.stdout.write('\n'.join(console_wait_for_keypress_lines())) #LINE# #TAB# return ''
"#LINE# #TAB# intersections = OrderedDict() #LINE# #TAB# intersections['email'] = user.email #LINE# #TAB# intersections['first_name'] = user.first_name #LINE# #TAB# intersections['last_name'] = user.last_name #LINE# #TAB# user.email = user.email #LINE# #TAB# user.profile = user.profile #LINE# #TAB# for k, v in user.profile.items(): #LINE# #TAB# #TAB# if hasattr(v, 'get'): #LINE# #TAB# #TAB# #TAB# intersections[k] = v #LINE# #TAB# for k, v in intersections.items(): #LINE# #TAB# #TAB# if hasattr(v, 'get'): #LINE# #TAB# #TAB# #TAB# intersections[k] = v #LINE# #TAB# return intersections"
"#LINE# #TAB# thread = threading.Thread(parent=parent) #LINE# #TAB# if not hasattr(worker, '_in_order'): #LINE# #TAB# #TAB# worker._in_order = True #LINE# #TAB# if deleteWorkerLater: #LINE# #TAB# #TAB# worker._in_order = False #LINE# #TAB# worker._thread = thread #LINE# #TAB# return thread"
"#LINE# #TAB# template_dir = get_template_dir() #LINE# #TAB# font_size = get_font_size(platform, command) #LINE# #TAB# textfsm_obj = clitable.CliTable(template_dir) #LINE# #TAB# attrs = {""Command"": command, ""Platform"": platform} #LINE# #TAB# try: #LINE# #TAB# #TAB# textfsm_obj.ParseCmd(raw_output, attrs) #LINE# #TAB# #TAB# del attrs[""FontSize""] #LINE# #TAB# #TAB# textfsm_obj.Destroy() #LINE# #TAB# #TAB# return True #LINE# #TAB# except CliTableError: #LINE# #TAB# #TAB# return raw_output"
#LINE# #TAB# if 'depth' not in src: #LINE# #TAB# #TAB# return tgt #LINE# #TAB# if 'depth' not in tgt: #LINE# #TAB# #TAB# tgt['depth'] = src['depth'] #LINE# #TAB# for k in src: #LINE# #TAB# #TAB# if k in tgt: #LINE# #TAB# #TAB# #TAB# tgt[k] = src[k] #LINE# #TAB# return tgt
#LINE# #TAB# rv = np.zeros_like(mass1) #LINE# #TAB# for a in range(3): #LINE# #TAB# #TAB# mass1 = mass1 * a #LINE# #TAB# #TAB# mass2 = mass2 * a #LINE# #TAB# #TAB# xi1 = xi2 * mass2 #LINE# #TAB# #TAB# rv[a] = np.sin(xi1 / (mass2 + mass1)) #LINE# #TAB# #TAB# rv[a] = np.cos(xi1 / (mass2 + mass1)) #LINE# #TAB# return rv
#LINE# #TAB# for i in range(alpha.size): #LINE# #TAB# #TAB# if alpha[i] < eps: #LINE# #TAB# #TAB# #TAB# yield i + 1 #LINE# #TAB# #TAB# elif alpha[i] > eps: #LINE# #TAB# #TAB# #TAB# yield i - 1
#LINE# #TAB# ConversionMap): #LINE# #TAB# object_parser = MultifileObjectParser(parser_finder) #LINE# #TAB# object_parser.finder = conversion_finder #LINE# #TAB# return object_parser
#LINE# #TAB# result = e.getResult() #LINE# #TAB# result.status = 'ERROR' #LINE# #TAB# result.error = True #LINE# #TAB# result.reason = e.getDescription() #LINE# #TAB# return result
"#LINE# #TAB# with tkinter.Tk() as tkinter: #LINE# #TAB# #TAB# root = tkinter.Tk() #LINE# #TAB# #TAB# root.withdraw() #LINE# #TAB# #TAB# root.update(title=title, message=message) #LINE# #TAB# #TAB# tkinter.destroy() #LINE# #TAB# #TAB# return root"
"#LINE# #TAB# dataframes = [] #LINE# #TAB# if number is True: #LINE# #TAB# #TAB# dataframes.append('+') #LINE# #TAB# else: #LINE# #TAB# #TAB# dataframes.append(str(number)) #LINE# #TAB# dataframes = sorted(dataframes, key=itemgetter(1)) #LINE# #TAB# for frame in dataframes: #LINE# #TAB# #TAB# yield frame #LINE# #TAB# #TAB# if round(number, precision): #LINE# #TAB# #TAB# #TAB# yield '-' + str(frame) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield frame"
#LINE# #TAB# for s in sets: #LINE# #TAB# #TAB# if not s: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if '/' not in relative_scope_name: #LINE# #TAB# #TAB# return relative_scope_name #LINE# #TAB# return relative_scope_name + '/'
"#LINE# #TAB# #TAB# for rec in cls.search([('name', '=', uri), ('website', '=', request. #LINE# #TAB# #TAB# #TAB# nereid_website.id)], limit=1): #LINE# #TAB# #TAB# #TAB# if not silent: #LINE# #TAB# #TAB# #TAB# #TAB# return rec #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# win_groups = sta_win.get('windows', []) #LINE# #TAB# else: #LINE# #TAB# #TAB# win_groups = sta_win.get('windows', []) #LINE# #TAB# ret = {} #LINE# #TAB# for group in win_groups: #LINE# #TAB# #TAB# ret[group[0]] = {'group': group[1], 'channel': int(group[2]), #LINE# #TAB# #TAB# #TAB# 'count': len(group[3])} #LINE# #TAB# return ret"
#LINE# #TAB# assert n <= 8 #LINE# #TAB# if a & 1 == 0: #LINE# #TAB# #TAB# a = a >> 1 #LINE# #TAB# b = a & 255 #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# if b & 1 == 1: #LINE# #TAB# #TAB# #TAB# b = b >> 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# b = b >> 1 #LINE# #TAB# return b
"#LINE# #TAB# global _INSTANCE_STATE #LINE# #TAB# _INSTANCE_STATE = { #LINE# #TAB# #TAB# 0: 'running', #LINE# #TAB# #TAB# 1: 'running', #LINE# #TAB# #TAB# 2: 'running', #LINE# #TAB# #TAB# 3: 'running', #LINE# #TAB# }"
"#LINE# #TAB# if password is None or not is_password_usable(encoded): #LINE# #TAB# #TAB# return False #LINE# #TAB# preferred = get_hasher(preferred) #LINE# #TAB# hasher = identify_hasher(encoded) #LINE# #TAB# must_update = hasher.algorithm!= preferred.algorithm #LINE# #TAB# is_correct = hasher.verify(password, encoded) #LINE# #TAB# if setter and is_correct and must_update: #LINE# #TAB# #TAB# setter(password) #LINE# #TAB# return is_correct"
"#LINE# #TAB# if request.authorization and request.authorization[0] in 'Basic': #LINE# #TAB# #TAB# return {'status':'success', 'token': request.authorization[0]} #LINE# #TAB# return {'status': 'error', 'token': None}"
#LINE# #TAB# global _mol_dist #LINE# #TAB# global _find_mol_dist_path #LINE# #TAB# _mol_dist = None
#LINE# #TAB# block = file.split('-')[0] #LINE# #TAB# block = block.split('-')[1] #LINE# #TAB# return block
"#LINE# #TAB# result = {} #LINE# #TAB# with open('/proc/net/route', 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# parts = line.split() #LINE# #TAB# #TAB# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if parts[0] == '127': #LINE# #TAB# #TAB# #TAB# #TAB# result[parts[0]] = int(parts[1]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result[parts[0]] = float(parts[1]) #LINE# #TAB# return result"
"#LINE# #TAB# configuration: Dict[str, str] = {} #LINE# #TAB# for comment in configuration_for_kind(ing): #LINE# #TAB# #TAB# if comment.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# comment_parts = comment.split('#') #LINE# #TAB# #TAB# if len(comment_parts) == 2: #LINE# #TAB# #TAB# #TAB# configuration[comment_parts[0]] = '\n'.join( #LINE# #TAB# #TAB# #TAB# #TAB# comment_parts[1:]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# configuration[comment_parts[0]] = '\n'.join( #LINE# #TAB# #TAB# #TAB# #TAB# comment_parts[1:]) #LINE# #TAB# return configuration, comment_parts[1]"
"#LINE# #TAB# with open(filepath, 'r') as json_file: #LINE# #TAB# #TAB# json_data = json.load(json_file) #LINE# #TAB# fill_value = json_data[property] #LINE# #TAB# return fill_value"
"#LINE# #TAB# tmpdir = tempfile.mkdtemp() #LINE# #TAB# os.makedirs(tmpdir, exist_ok=True) #LINE# #TAB# return tmpdir"
"#LINE# #TAB# pretty_element = {'db_url': db_url, 'db_name': db_name} #LINE# #TAB# del pretty_element['database'] #LINE# #TAB# return pretty_element"
#LINE# #TAB# with db.session.begin_nested(): #LINE# #TAB# #TAB# package_eups_version = insert_single_package_eups( #LINE# #TAB# #TAB# #TAB# sample_id) #LINE# #TAB# #TAB# package_eups_version = session.query(PackageEupsVersion).filter( #LINE# #TAB# #TAB# #TAB# PackageEupsVersion.id == package_eups_version).one() #LINE# #TAB# return package_eups_version
"#LINE# #TAB# return get_block_overview(block_representation=block_representation, #LINE# #TAB# #TAB# coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['prev_block']"
#LINE# #TAB# dF = Ft / Fo #LINE# #TAB# return dF
"#LINE# #TAB# user = request.user #LINE# #TAB# if not user.is_authenticated(): #LINE# #TAB# #TAB# return False #LINE# #TAB# instances = [] #LINE# #TAB# while user.is_authenticated(): #LINE# #TAB# #TAB# username = user.get_username() #LINE# #TAB# #TAB# if username in ['instagram', 'instagram.com']: #LINE# #TAB# #TAB# #TAB# if password == user.get_password(): #LINE# #TAB# #TAB# #TAB# #TAB# instances.append(cfservice_instance(request, username, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'instagram.com')) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# password = user.get_password() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return instances"
"#LINE# #TAB# key_type = _get_key_type(field) #LINE# #TAB# if isinstance(key_type, (list, tuple)) and len(key_type) == 1: #LINE# #TAB# #TAB# key_name = list(field)[0] #LINE# #TAB# #TAB# yield key_type, key_name #LINE# #TAB# elif isinstance(key_type, dict): #LINE# #TAB# #TAB# key_name = _get_key_name(field) #LINE# #TAB# #TAB# yield key_type, key_name #LINE# #TAB# else: #LINE# #TAB# #TAB# for idx, _ in enumerate(field): #LINE# #TAB# #TAB# #TAB# yield idx"
"#LINE# #TAB# for series in seriesList: #LINE# #TAB# #TAB# series.name = ""get_oe_flag(%s)"" % (series.name) #LINE# #TAB# #TAB# val = safeMin(series) #LINE# #TAB# #TAB# if val is not None and val >= n: #LINE# #TAB# #TAB# #TAB# series.name = series.name #LINE# #TAB# return seriesList"
#LINE# #TAB# effective = [] #LINE# #TAB# stack = [job] #LINE# #TAB# while stack: #LINE# #TAB# #TAB# new_job = None #LINE# #TAB# #TAB# for cmdset in cmdsets: #LINE# #TAB# #TAB# #TAB# if cmdset.depth!= job.depth: #LINE# #TAB# #TAB# #TAB# #TAB# new_job = cmdset.job #LINE# #TAB# #TAB# #TAB# #TAB# stack.append(new_job) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# effective.append(new_job) #LINE# #TAB# #TAB# #TAB# #TAB# job = new_job #LINE# #TAB# for job in effective: #LINE# #TAB# #TAB# if job not in stack: #LINE# #TAB# #TAB# #TAB# stack.append(job) #LINE# #TAB# return effective
"#LINE# #TAB# h1, w1 = img1.shape[:2] #LINE# #TAB# h2, w2 = img2.shape[:2] #LINE# #TAB# if h1 > h2: #LINE# #TAB# #TAB# return ""padded"" #LINE# #TAB# elif h1 < w2: #LINE# #TAB# #TAB# return ""padded"" #LINE# #TAB# elif h2 > w1: #LINE# #TAB# #TAB# return ""padded"" #LINE# #TAB# else: #LINE# #TAB# #TAB# return ""padded"""
"#LINE# #TAB# resname = '' #LINE# #TAB# pTM_atom = False #LINE# #TAB# try: #LINE# #TAB# #TAB# node1.tag = node2.tag #LINE# #TAB# #TAB# resname = node1.resname #LINE# #TAB# #TAB# pTM_atom = node2.ptm_atom #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# pTM_atom = node2.ptm_atom #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return resname, pTM_atom, pTM_resname, pTM_atom"
"#LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if isinstance(val, six.string_types): #LINE# #TAB# #TAB# #TAB# if val.lower() == 'true': #LINE# #TAB# #TAB# #TAB# #TAB# value = True #LINE# #TAB# #TAB# #TAB# elif val.lower() == 'false': #LINE# #TAB# #TAB# #TAB# #TAB# value = False #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# value = float(val) #LINE# #TAB# return value"
"#LINE# #TAB# session = generic_session(session) #LINE# #TAB# ok = False #LINE# #TAB# for path in base_makedirs(session): #LINE# #TAB# #TAB# if not ok: #LINE# #TAB# #TAB# #TAB# ok = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# for dir in base_makedirs(path): #LINE# #TAB# #TAB# #TAB# if not ok: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# os.mkdir(dir) #LINE# #TAB# #TAB# #TAB# except FileExistsError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return ok, dirs"
"#LINE# #TAB# if a.base is not None: #LINE# #TAB# #TAB# return a.copy() #LINE# #TAB# elif np.issubsctype(a, np.float32): #LINE# #TAB# #TAB# return np.array(a, dtype=a.dtype) #LINE# #TAB# else: #LINE# #TAB# #TAB# return a"
"#LINE# #TAB# add_rewards_to_saved_game_proto(schema, path) #LINE# #TAB# if isinstance(schema, list): #LINE# #TAB# #TAB# return [add_rewards_to_saved_game_proto(item, path + [str(i) for i in #LINE# #TAB# #TAB# #TAB# schema]) for item in schema] #LINE# #TAB# if isinstance(schema, dict): #LINE# #TAB# #TAB# for key in schema.keys(): #LINE# #TAB# #TAB# #TAB# add_rewards_to_saved_game_proto(schema[key], path + [str(key) for #LINE# #TAB# #TAB# #TAB# #TAB# key in schema.keys()]) #LINE# #TAB# return schema"
"#LINE# #TAB# in_int = int(in_int) #LINE# #TAB# out_struct = _get_ethercat_interface_info_struct(in_int, max_bits, new_dim_length) #LINE# #TAB# return out_struct"
"#LINE# #TAB# s = """""" #LINE# #TAB# { #LINE# #TAB# #TAB# SELECT * FROM pg_catalog.pg_class #LINE# #TAB# #TAB# WHERE #LINE# #TAB# #TAB# ( #LINE# #TAB# #TAB# #TAB# ( #LINE# #TAB# #TAB# #TAB# #TAB# class_name.endswith('{') #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# } #LINE# #TAB# ) #LINE# #TAB# """""" #LINE# #TAB# list_ = class_name.split(';') #LINE# #TAB# for i in range(len(list_)): #LINE# #TAB# #TAB# if list_[i[-1] == 'Template': #LINE# #TAB# #TAB# #TAB# i = i[:-1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# i = i[-1] #LINE# #TAB# return list_[0]"
"#LINE# #TAB# if np.isscalar(scale): #LINE# #TAB# #TAB# scale = 1.0 #LINE# #TAB# path = os.path.join(settings.DATA_ROOT,'manuf', scale + '.npy') #LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# if copy: #LINE# #TAB# #TAB# #TAB# os.remove(path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# os.remove(path) #LINE# #TAB# return path"
"#LINE# #TAB# results = [] #LINE# #TAB# f = open(csvFile, 'rb') #LINE# #TAB# for line in f: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# values = json.loads(line) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# results.append(values) #LINE# #TAB# f.close() #LINE# #TAB# return results"
#LINE# #TAB# if description: #LINE# #TAB# #TAB# swagger_spec = {'description': description} #LINE# #TAB# if resource: #LINE# #TAB# #TAB# swagger_spec['resource'] = resource #LINE# #TAB# if options: #LINE# #TAB# #TAB# swagger_spec['options'] = options #LINE# #TAB# return swagger_spec
#LINE# #TAB# if method in bg_type_map: #LINE# #TAB# #TAB# return bg_type_map[method] #LINE# #TAB# return None
"#LINE# #TAB# cmd = cmd.strip() #LINE# #TAB# if not cmd: #LINE# #TAB# #TAB# return None #LINE# #TAB# log_dir = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# log_file = '{0}.log'.format(log_dir) #LINE# #TAB# log_file = os.path.join(log_dir, log_file) #LINE# #TAB# log_file = os.path.abspath(log_file) #LINE# #TAB# log_file = os.path.join(log_dir, log_file) #LINE# #TAB# log_file = os.path.abspath(log_file) #LINE# #TAB# with open(log_file, 'w') as f: #LINE# #TAB# #TAB# f.write(cmd) #LINE# #TAB# return log_file"
"#LINE# #TAB# ca = numpy.zeros((len(chimerics), len(msa))) #LINE# #TAB# c = numpy.zeros((len(chimerics), len(msa))) #LINE# #TAB# for i, c in enumerate(chimerics): #LINE# #TAB# #TAB# a, b, c = c #LINE# #TAB# #TAB# for j, k in enumerate(a): #LINE# #TAB# #TAB# #TAB# if k == j: #LINE# #TAB# #TAB# #TAB# #TAB# c[i] = np.nan #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# c[j] = np.nan #LINE# #TAB# return ca, c"
"#LINE# #TAB# while len(string) > 0: #LINE# #TAB# #TAB# letter = random.choice(letters) #LINE# #TAB# #TAB# if letter not in string: #LINE# #TAB# #TAB# #TAB# string.remove(letter) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# idx = random.randint(0, len(letters) - 1) #LINE# #TAB# #TAB# #TAB# string = string[:idx] + string[idx + 1:] #LINE# #TAB# return string"
#LINE# #TAB# frag = cls() #LINE# #TAB# frag.content = pods['content'] #LINE# #TAB# frag._resources = [FragmentResource(**d) for d in pods['resources']] #LINE# #TAB# frag.js_init_fn = pods['js_init_fn'] #LINE# #TAB# frag.js_init_version = pods['js_init_version'] #LINE# #TAB# frag.json_init_args = pods['json_init_args'] #LINE# #TAB# return frag
"#LINE# #TAB# extras = [] #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# warnings.warn('get_links_in_html_page is deprecated.', DeprecationWarning) #LINE# #TAB# packages = get_outdated_packages() #LINE# #TAB# if pip_cmd and verbose: #LINE# #TAB# #TAB# output = subprocess.check_output([pip_cmd, '--help']) #LINE# #TAB# else: #LINE# #TAB# #TAB# output = subprocess.check_output([pip_cmd, '--list-packages'], stderr= #LINE# #TAB# #TAB# #TAB# subprocess.STDOUT) #LINE# #TAB# for pkg in packages: #LINE# #TAB# #TAB# extras.append('%s: %s' % (pkg, pkg)) #LINE# #TAB# return extras"
"#LINE# #TAB# meaning = [] #LINE# #TAB# for group_name in groups: #LINE# #TAB# #TAB# matching_files=[] #LINE# #TAB# #TAB# for fname in groups[group_name]: #LINE# #TAB# #TAB# #TAB# matching_files.append(glob.glob(os.path.join(folder,fname))) #LINE# #TAB# #TAB# meaning.append(matching_files) #LINE# #TAB# return meaning"
"#LINE# #TAB# for i, data in enumerate(data_iterator): #LINE# #TAB# #TAB# if data is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if net is None: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# f = data.read() #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# patterns = {} #LINE# #TAB# for key, val in PARAMS.items(): #LINE# #TAB# #TAB# patterns[key] = re.compile(val) #LINE# #TAB# return patterns"
"#LINE# #TAB# for host in get_hosts(tree): #LINE# #TAB# #TAB# for prop in get_properties(host, cve): #LINE# #TAB# #TAB# #TAB# yield prop"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# emails = get_email_list(homedir) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if re.match('^[0-9]+@[0-9]+\\.[0-9]+$', email): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
"#LINE# #TAB# components = name.split('.') #LINE# #TAB# mod = importlib.import_module('.'.join(components[:-1])) #LINE# #TAB# cls = getattr(mod, components[-1]) #LINE# #TAB# return cls"
"#LINE# #TAB# _, s = split_s(s) #LINE# #TAB# if os.path.isfile(s): #LINE# #TAB# #TAB# with open(s, 'r') as f: #LINE# #TAB# #TAB# #TAB# return f.read() #LINE# #TAB# return s"
#LINE# #TAB# tree = ET.parse(stats_xml) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# total_tokens = int(root.find('size/total/tokens').text) #LINE# #TAB# blinded_texts = int(root.find('blinded_texts').text) #LINE# #TAB# connection_state = 1 if total_tokens > blinded_texts else 0 #LINE# #TAB# return connection_state
#LINE# #TAB# p = osr.SpatialReference() #LINE# #TAB# p.ImportFromEPSG(4326) #LINE# #TAB# result = p.CanDeleteOwn() #LINE# #TAB# return result
#LINE# #TAB# if actor.id in actor_to_velocity: #LINE# #TAB# #TAB# return actor_to_velocity[actor.id] #LINE# #TAB# if actor.root is None: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# flag = 0 #LINE# #TAB# for child in actor.children: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# tag = child.tag #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if tag == 'v': #LINE# #TAB# #TAB# #TAB# #TAB# flag -= 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return flag
#LINE# #TAB# symbol = None #LINE# #TAB# try: #LINE# #TAB# #TAB# symbol = RequestInfo.objects.get(request=request) #LINE# #TAB# except RequestInfo.DoesNotExist: #LINE# #TAB# #TAB# pass #LINE# #TAB# return symbol
"#LINE# #TAB# result = list() #LINE# #TAB# stack = [None] * 6 #LINE# #TAB# while stack: #LINE# #TAB# #TAB# obj_id = np.random.randint(0, 6) #LINE# #TAB# #TAB# stack.append(obj_id) #LINE# #TAB# #TAB# obj_id = np.random.randint(0, 6) #LINE# #TAB# return stack"
#LINE# #TAB# #TAB# resolved_packages = [] #LINE# #TAB# #TAB# for package in include_packages: #LINE# #TAB# #TAB# #TAB# resolved_packages.append(package) #LINE# #TAB# #TAB# packages_to_resolve = set(resolved_packages) #LINE# #TAB# #TAB# del resolved_packages #LINE# #TAB# #TAB# return packages_to_resolve
#LINE# #TAB# summary = '' #LINE# #TAB# for command_summary in command_summaries: #LINE# #TAB# #TAB# summary += command_summary.get('meta_xwing') #LINE# #TAB# return summary
"#LINE# #TAB# arr = [] #LINE# #TAB# content_type = 'application/json' #LINE# #TAB# if response and response.headers: #LINE# #TAB# #TAB# for elem in response.headers: #LINE# #TAB# #TAB# #TAB# if isinstance(elem, list): #LINE# #TAB# #TAB# #TAB# #TAB# if elem[0] =='matrix': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# arr.append(par_matrix_to_gates(elem)) #LINE# #TAB# #TAB# #TAB# #TAB# elif elem[0] == 'array': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# arr.append(par_matrix_to_gates(elem)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# content_type = 'application/json' #LINE# #TAB# return content_type"
"#LINE# #TAB# return cls.build_send_payload('convert_units', {'eventName': eventName} #LINE# #TAB# #TAB# ), None"
"#LINE# #TAB# perm_module = import_module(perm_model) #LINE# #TAB# cachevault = getattr(perm_module, 'cachevault', None) #LINE# #TAB# if cachevault is None: #LINE# #TAB# #TAB# cachevault = create_cachevault(model, perm_model) #LINE# #TAB# #TAB# return cachevault #LINE# #TAB# roles = get_all_roles(model) #LINE# #TAB# return roles"
#LINE# #TAB# if seq[0] == 'N': #LINE# #TAB# #TAB# return 'N' #LINE# #TAB# elif seq[0] == 'O': #LINE# #TAB# #TAB# return 'A' #LINE# #TAB# elif seq[0] == 'R': #LINE# #TAB# #TAB# return 'R'
#LINE# #TAB# e = [] #LINE# #TAB# e += [copy_func(arg) for arg in arg1] #LINE# #TAB# e += [copy_func(arg) for arg in arg2] #LINE# #TAB# return e
"#LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# result = subprocess.run(['docker', 'ps', '--quiet', node], stdout= #LINE# #TAB# #TAB# #TAB# subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# if result.returncode == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# df = pd.DataFrame({'id': 'country', 'title': 'Country'}) #LINE# #TAB# df['select_country'] = df.loc[df.countries.isna() | df.countries.isnull() | df. #LINE# #TAB# #TAB# countries.isnull() | df.countries.isnull()] #LINE# #TAB# df = df.sort_values(by=['alpha_2'], ascending=False) #LINE# #TAB# if len(df.columns) == 1: #LINE# #TAB# #TAB# return df.loc[df.columns[0]] #LINE# #TAB# elif len(df.columns) > 1: #LINE# #TAB# #TAB# return df.loc[df.columns[0]] #LINE# #TAB# else: #LINE# #TAB# #TAB# return df"
"#LINE# #TAB# image = imageClass() #LINE# #TAB# for a in stretchDim: #LINE# #TAB# #TAB# xstretch = image.attrs.get(a, 0) #LINE# #TAB# #TAB# ystretch = image.attrs.get(a, 0) #LINE# #TAB# #TAB# shape = a.shape #LINE# #TAB# #TAB# if len(shape) == 1: #LINE# #TAB# #TAB# #TAB# shape = [shape[0], int(shape[1]), int(shape[2])] #LINE# #TAB# #TAB# image.setImageData(xstretch, ystretch, shape) #LINE# #TAB# return image"
#LINE# #TAB# if success is True: #LINE# #TAB# #TAB# return 'SUCCESS' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'FAILED'
"#LINE# #TAB# file_data = '' #LINE# #TAB# if not file_name: #LINE# #TAB# #TAB# file_name = file_ext #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# if type(value) == str: #LINE# #TAB# #TAB# #TAB# file_data += '{0}{1}'.format(key, value) #LINE# #TAB# #TAB# elif type(value) == bytes: #LINE# #TAB# #TAB# #TAB# file_data += '{0}{1}'.format(key, value) #LINE# #TAB# file_data = '{0}{1}'.format(file_name, file_data) #LINE# #TAB# return file_data"
"#LINE# #TAB# if rand_name_at_end(logical_line): #LINE# #TAB# #TAB# pos = logical_line.find('-') #LINE# #TAB# #TAB# if pos > -1: #LINE# #TAB# #TAB# #TAB# yield pos, ""T108: Cannot use numeric event attribute value with hyphen: {}"".format( #LINE# #TAB# #TAB# #TAB# #TAB# rand_name_at_end(logical_line)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield pos, ""T108: Cannot use numeric event attribute value with hyphen: {}"".format( #LINE# #TAB# #TAB# #TAB# #TAB# rand_name_at_end(logical_line))"
"#LINE# #TAB# text = hparams.get(""text"") #LINE# #TAB# if hparams.add_position_timing_signal: #LINE# #TAB# #TAB# text += "" --position-timing-signal="" + str(hparams.add_position_timing_signal) #LINE# #TAB# if hparams.add_output_timing_signal: #LINE# #TAB# #TAB# text += "":output-timing-signal="" + str(hparams.add_output_timing_signal) #LINE# #TAB# if hparams.add_sru: #LINE# #TAB# #TAB# text += "":sru="" + str(hparams.sru) #LINE# #TAB# return text"
#LINE# #TAB# r = requests.get(course_url) #LINE# #TAB# if r.status_code > 400: #LINE# #TAB# #TAB# return r.text #LINE# #TAB# elif r.status_code == 200: #LINE# #TAB# #TAB# return set_colors(r.text) #LINE# #TAB# else: #LINE# #TAB# #TAB# return course_url
"#LINE# #TAB# #TAB# nt_filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# os.pardir, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# cls.NAME) #LINE# #TAB# #TAB# return nt_filename"
#LINE# #TAB# if len(pn1.public_numbers)!= len(pn2.public_numbers): #LINE# #TAB# #TAB# return None #LINE# #TAB# same = False #LINE# #TAB# for n in pn1.public_numbers: #LINE# #TAB# #TAB# if n!= pn2.public_numbers[n]: #LINE# #TAB# #TAB# #TAB# same = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if not same: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return pn2.public_numbers
#LINE# #TAB# global unbound_dimensions_cache #LINE# #TAB# if unbound_dimensions_cache is None: #LINE# #TAB# #TAB# unbound_dimensions_cache = {} #LINE# #TAB# #TAB# _generate_device_id() #LINE# #TAB# #TAB# unbound_dimensions_cache = {} #LINE# #TAB# ret = unbound_dimensions_cache.get(seed) #LINE# #TAB# if ret is not None: #LINE# #TAB# #TAB# return ret #LINE# #TAB# return None
"#LINE# #TAB# if transaction is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# cls._xml_entity_escape = False #LINE# #TAB# if isinstance(transaction, dict): #LINE# #TAB# #TAB# for child_transaction in transaction.values(): #LINE# #TAB# #TAB# #TAB# cls._xml_entity_escape = cls.xml_entity_escape( #LINE# #TAB# #TAB# #TAB# #TAB# child_transaction) #LINE# #TAB# elif isinstance(transaction, list): #LINE# #TAB# #TAB# for child_transaction in transaction: #LINE# #TAB# #TAB# #TAB# cls._xml_entity_escape = cls.xml_entity_escape( #LINE# #TAB# #TAB# #TAB# #TAB# child_transaction) #LINE# #TAB# return cls._xml_entity_escape"
"#LINE# #TAB# fin = open(base_name + '.bai', 'r') #LINE# #TAB# fin.seek(start_at) #LINE# #TAB# names = [] #LINE# #TAB# for _ in range(num_seqs): #LINE# #TAB# #TAB# name = fin.read() #LINE# #TAB# #TAB# name = re.sub('[^\\w\\s]', '_', name) #LINE# #TAB# #TAB# name = name.replace('.', '_', 1) #LINE# #TAB# #TAB# names.append(name) #LINE# #TAB# fin.close() #LINE# #TAB# return names"
"#LINE# #TAB# assert a and b #LINE# #TAB# a, b = a, b #LINE# #TAB# if a == b: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# logger = logging.getLogger(classnames[0]) #LINE# #TAB# for k, v in element_map.items(): #LINE# #TAB# #TAB# if k in logger.keys(): #LINE# #TAB# #TAB# #TAB# logger.pop(k) #LINE# #TAB# return logger"
#LINE# #TAB# #TAB# game_version_list = cls.get_all() #LINE# #TAB# #TAB# if not game_version_list: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if update: #LINE# #TAB# #TAB# #TAB# game_version_list.update(game_version_list) #LINE# #TAB# #TAB# return game_version_list
"#LINE# #TAB# defined_levels = [] #LINE# #TAB# for element in orifs: #LINE# #TAB# #TAB# if isinstance(element, CodeUnit): #LINE# #TAB# #TAB# #TAB# defined_levels.append(element) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unit = find_defined_level(element, file_locator) #LINE# #TAB# #TAB# #TAB# if unit is not None: #LINE# #TAB# #TAB# #TAB# #TAB# defined_levels.append(unit) #LINE# #TAB# return defined_levels"
#LINE# #TAB# sys.stdout.write(payload) #LINE# #TAB# sys.stdout.flush() #LINE# #TAB# return payload
"#LINE# #TAB# title_sort = [] #LINE# #TAB# with open(filename, 'r') as fp: #LINE# #TAB# #TAB# reader = csv.reader(fp, delimiter='\t') #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# if row[0]: #LINE# #TAB# #TAB# #TAB# #TAB# title_sort.append(row[0]) #LINE# #TAB# #TAB# #TAB# if row[1]: #LINE# #TAB# #TAB# #TAB# #TAB# title_sort.append(row[1]) #LINE# #TAB# return title_sort"
#LINE# #TAB# global _observed_features_to_observed_range #LINE# #TAB# old_maxblock = _observed_features_to_observed_range #LINE# #TAB# _observed_features_to_observed_range = maxblock #LINE# #TAB# yield #LINE# #TAB# _observed_features_to_observed_range = old_maxblock
"#LINE# #TAB# if context is None: #LINE# #TAB# #TAB# context = get_global_context() #LINE# #TAB# llvmir = _encode_string(llvmir) #LINE# #TAB# strbuf = cufflinks.encode(llvmir) #LINE# #TAB# with ffi.OutputString() as errmsg: #LINE# #TAB# #TAB# mod = ModuleRef(ffi.lib.LLVMPY_CreateModule(context, strbuf, #LINE# #TAB# #TAB# #TAB# errmsg), context) #LINE# #TAB# #TAB# if errmsg: #LINE# #TAB# #TAB# #TAB# mod.close() #LINE# #TAB# #TAB# #TAB# raise RuntimeError('LLVM IR parsing error\n{0}'.format(errmsg)) #LINE# #TAB# return mod"
"#LINE# #TAB# v = np.zeros((len(y),len(yr))) #LINE# #TAB# for i in range(len(y)): #LINE# #TAB# #TAB# x = y[i]*psd[i] #LINE# #TAB# #TAB# for j in range(i): #LINE# #TAB# #TAB# #TAB# v[j] = v[j] / np.sum(psd[i])**2 #LINE# #TAB# address = v[0]+v[1]#TAB# #TAB# #LINE# #TAB# return address"
#LINE# #TAB# total = 0 #LINE# #TAB# edges = nxg.edges(data=True) #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if 'label' in edge[2]: #LINE# #TAB# #TAB# #TAB# total += edge[2]['label'] #LINE# #TAB# return total
"#LINE# #TAB# src = ( #LINE# #TAB# #TAB# join(os.path.dirname(__file__),'settings.py'), #LINE# #TAB# #TAB# join(os.path.dirname(os.path.abspath(__file__)), #LINE# #TAB# #TAB#'settings.py'), #LINE# #TAB# ) #LINE# #TAB# for path in src: #LINE# #TAB# #TAB# is_pcl_module = path.endswith('pcl') #LINE# #TAB# #TAB# if is_pcl_module: #LINE# #TAB# #TAB# #TAB# src = os.path.join(path[:-1], src) #LINE# #TAB# return src"
"#LINE# #TAB# if include_expired: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM snapshots WHERE timestamp =?;"" #LINE# #TAB# else: #LINE# #TAB# #TAB# query = ""SELECT COUNT(*) FROM snapshots WHERE timestamp =?;"" #LINE# #TAB# args = (current_block,) if include_expired else () #LINE# #TAB# count, _ = run_sql( cur, query, args ) #LINE# #TAB# return count"
"#LINE# #TAB# res_datetime = None #LINE# #TAB# if residue.get_resname() == 'N/A': #LINE# #TAB# #TAB# res_datetime = datetime.strptime(residue.get_resname(), '%Y-%m-%dT%H:%M:%S.%fZ') #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# res_datetime = datetime.strptime(residue.get_resname(), '%Y-%m-%dT%H:%M:%S' #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# res_datetime = datetime.strptime(residue.get_resname(), '%Y-%m-%dT%H:%M:%S' #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# return res_datetime"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# info = obj.plugin_info() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# info = {} #LINE# #TAB# columns = info.get('columns', []) #LINE# #TAB# return columns"
#LINE# #TAB# url_patterns = [] #LINE# #TAB# for route in dbh24.routes: #LINE# #TAB# #TAB# url_patterns.extend(route.url_patterns) #LINE# #TAB# return url_patterns
"#LINE# #TAB# with open(os.path.join(dirname, 'README.rst')) as f: #LINE# #TAB# #TAB# rst = f.read() #LINE# #TAB# return rst"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# query = event.context['request']['url']['query'] #LINE# #TAB# #TAB# event.context['request']['url']['query'] = varmap(_sanitize, query) #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# return False #LINE# #TAB# return event.context['request']['url']['query'] == query"
#LINE# #TAB# opcodes = [] #LINE# #TAB# for i in range(len(data)): #LINE# #TAB# #TAB# if data[i][0] == OP_DUP: #LINE# #TAB# #TAB# #TAB# opcodes.append(cls.extract_reward(data[i])) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opcodes.append(cls.extract_reward(data[i])) #LINE# #TAB# return opcodes
#LINE# #TAB# if not SharedInstance.instance: #LINE# #TAB# #TAB# clear_cache() #LINE# #TAB# #TAB# SharedInstance.instance = geojson.loads(SharedInstance.ip_address) #LINE# #TAB# return SharedInstance.instance
#LINE# #TAB# doc = etree.fromstring(xmlcontent) #LINE# #TAB# root = doc.getroot() #LINE# #TAB# document_markups = [] #LINE# #TAB# for el in root: #LINE# #TAB# #TAB# if el.tag == _name('{{{w}}}p'): #LINE# #TAB# #TAB# #TAB# document_markups.append(document_markup) #LINE# #TAB# #TAB# if el.tag == _name('{{{w}}}tbl'): #LINE# #TAB# #TAB# #TAB# document_markups.append(document_markup_tbl(el)) #LINE# #TAB# #TAB# if el.tag == _name('{{{w}}}sdt'): #LINE# #TAB# #TAB# #TAB# document_markups.append(document_markup_sdt) #LINE# #TAB# return document_markups
"#LINE# #TAB# with h5py.File(h5, 'r') as h: #LINE# #TAB# #TAB# output = dict() #LINE# #TAB# #TAB# for key in h.keys(): #LINE# #TAB# #TAB# #TAB# if key == 'target': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if readH5pyDataset: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = h.attrs[key] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = h.attrs[key] #LINE# #TAB# return output"
#LINE# #TAB# out = pd.DataFrame() #LINE# #TAB# out['X'] = row['X'] + dem_adjustment #LINE# #TAB# out['Y'] = row['Y'] + dem_adjustment #LINE# #TAB# return out
"#LINE# #TAB# sorted_signal = [] #LINE# #TAB# for i, value in enumerate(signal): #LINE# #TAB# #TAB# if np.isnan(value): #LINE# #TAB# #TAB# #TAB# sorted_signal.append(i) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sorted_signal.append((i, value)) #LINE# #TAB# if sorted_signal == []: #LINE# #TAB# #TAB# return signal #LINE# #TAB# else: #LINE# #TAB# #TAB# filtered_signal = [] #LINE# #TAB# #TAB# for i, j in enumerate(sorted_signal): #LINE# #TAB# #TAB# #TAB# if np.isnan(j) or np.isinf(j): #LINE# #TAB# #TAB# #TAB# #TAB# filtered_signal.append((i, j)) #LINE# #TAB# #TAB# return filtered_signal"
"#LINE# #TAB# schema_files = glob.glob(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# DIRPATH_SCHEMAS + '*')) #LINE# #TAB# return schema_files"
"#LINE# #TAB# energy_matrix = np.zeros((size, size)) #LINE# #TAB# smooth_factor = float(smooth_factor) #LINE# #TAB# for i in range(0, 2 * size): #LINE# #TAB# #TAB# energy_matrix[i, i] = response_tuple_to_repo_format(size[i], #LINE# #TAB# #TAB# #TAB# smooth_factor) #LINE# #TAB# return energy_matrix"
#LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# yield from encode_lines(line) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
"#LINE# #TAB# return [pseudo_named_tuple_row(colname, row) for colname, row in zip( #LINE# #TAB# #TAB# colnames, rows)]"
"#LINE# #TAB# keys = list(attributes_set) #LINE# #TAB# for attr in keys: #LINE# #TAB# #TAB# if not re.search('\\[', attr): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not re.search('\\[', attr): #LINE# #TAB# #TAB# #TAB# attributes_set.remove(attr) #LINE# #TAB# return attributes_set"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(subprocess.check_output(['pm', '-n']).decode().strip()) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# msg = 'Unable to determine the atomic number of {0}: {1}'.format(name, e) #LINE# #TAB# #TAB# logger.error(msg) #LINE# #TAB# #TAB# raise e #LINE# #TAB# if residue: #LINE# #TAB# #TAB# return residue #LINE# #TAB# else: #LINE# #TAB# #TAB# return num"
"#LINE# #TAB# account = User.objects.get(pk=accountid) #LINE# #TAB# if not request.user.can_access(account): #LINE# #TAB# #TAB# raise PermDeniedException() #LINE# #TAB# filename = request.params.get('filename', None) #LINE# #TAB# if not filename: #LINE# #TAB# #TAB# raise ModoboaException(_('No document available for this user')) #LINE# #TAB# content = request.user.download_file(filename) #LINE# #TAB# if content: #LINE# #TAB# #TAB# resp = HttpResponse(content) #LINE# #TAB# #TAB# resp['Content-Type'] = 'application/pdf' #LINE# #TAB# #TAB# resp['Content-Length'] = len(content) #LINE# #TAB# #TAB# resp['Content-Disposition'] = 'attachment; filename=%s' % filename #LINE# #TAB# #TAB# return resp #LINE# #TAB# raise Http404"
"#LINE# #TAB# if isinstance(tree, NameCollector): #LINE# #TAB# #TAB# return tree.collect(ctx) #LINE# #TAB# return False"
"#LINE# #TAB# command_args_package_name = [cls.executable, '-f', file_path, 'Package'] #LINE# #TAB# command_args_version = [cls.executable, '-f', file_path, 'Version'] #LINE# #TAB# package_name = CM.run_command_check_output(command_args_package_name) #LINE# #TAB# package_version = CM.run_command_check_output(command_args_version) #LINE# #TAB# package_info = PackageInfo() #LINE# #TAB# package_info.package = package_name.strip() #LINE# #TAB# package_info.version = package_version.strip() #LINE# #TAB# return package_info"
#LINE# #TAB# if t == 0: #LINE# #TAB# #TAB# p = l[1] #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# p = l[0] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# if not p: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# p = ast.literal_eval(p) #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# pass #LINE# #TAB# l[0] = p #LINE# #TAB# return l
#LINE# #TAB# roles = [] #LINE# #TAB# status = client.list_account_roles() #LINE# #TAB# if status['account']!= '': #LINE# #TAB# #TAB# roles = status['account']['roles'] #LINE# #TAB# else: #LINE# #TAB# #TAB# roles = status['account']['roles'] #LINE# #TAB# return roles
"#LINE# #TAB# assert confirm #LINE# #TAB# path = os.path.expanduser('~/.config/chkpt.yaml') #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# v = {} #LINE# #TAB# for k, v in DEFAULTS.items(): #LINE# #TAB# #TAB# if os.path.isfile(v): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# f = open(v, 'r') #LINE# #TAB# #TAB# #TAB# #TAB# v = f.read() #LINE# #TAB# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# v[k] = v #LINE# #TAB# return v"
#LINE# #TAB# if x == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return x.bit_length() - 1
#LINE# #TAB# if type(s) == str: #LINE# #TAB# #TAB# s = [n for n in s if n not in df.columns] #LINE# #TAB# if type(s) == list: #LINE# #TAB# #TAB# for n in s: #LINE# #TAB# #TAB# #TAB# df = df[df.columns.apply(lambda x: not x.startswith(n))] #LINE# #TAB# return df
#LINE# #TAB# if param is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# result = [] #LINE# #TAB# if 'type' in param: #LINE# #TAB# #TAB# result.append(param['type']) #LINE# #TAB# #TAB# result.append(param['name']) #LINE# #TAB# else: #LINE# #TAB# #TAB# result.append(param['name']) #LINE# #TAB# #TAB# if 'default' in param: #LINE# #TAB# #TAB# #TAB# result.append(param['default']) #LINE# #TAB# #TAB# if 'class' in param: #LINE# #TAB# #TAB# #TAB# result.append(param['class']) #LINE# #TAB# #TAB# if 'traits' in param: #LINE# #TAB# #TAB# #TAB# result.append(param['traits']) #LINE# #TAB# return result
#LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += calculate_size_data(new_value) #LINE# #TAB# return data_size
#LINE# #TAB# fname = fname.split('.')[-1] #LINE# #TAB# if fname.endswith('.gz'): #LINE# #TAB# #TAB# fname = fname[:-4] #LINE# #TAB# if fname.endswith('.bz2'): #LINE# #TAB# #TAB# fname = fname[:-4] #LINE# #TAB# if fname.endswith('.h5'): #LINE# #TAB# #TAB# fname = fname[:-8] #LINE# #TAB# if fname.endswith('.gz'): #LINE# #TAB# #TAB# fname = fname[:-9] #LINE# #TAB# return fname
"#LINE# #TAB# task_collection = [] #LINE# #TAB# directory = os.path.join(engineio_server, engineio_endpoint) #LINE# #TAB# if not os.path.isdir(directory): #LINE# #TAB# #TAB# return task_collection #LINE# #TAB# for path in glob.glob(directory): #LINE# #TAB# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# #TAB# task_collection.append((path, {'path': path})) #LINE# #TAB# task_collection.append((engineio_server, engineio_endpoint)) #LINE# #TAB# return task_collection"
"#LINE# #TAB# config = [] #LINE# #TAB# for string in string_set: #LINE# #TAB# #TAB# if isinstance(string, bytes): #LINE# #TAB# #TAB# #TAB# string = string.decode('utf-8') #LINE# #TAB# #TAB# if isinstance(string, str): #LINE# #TAB# #TAB# #TAB# config.append(string) #LINE# #TAB# return config"
"#LINE# #TAB# if not isinstance(my_list, list): #LINE# #TAB# #TAB# raise TypeError('my_list should be of type list') #LINE# #TAB# if my_list[0].__class__!= my_element.__class__.__name__: #LINE# #TAB# #TAB# raise ValueError('my_list should be of type list') #LINE# #TAB# my_list.append(my_element) #LINE# #TAB# return True"
#LINE# #TAB# out = {} #LINE# #TAB# for key in dico: #LINE# #TAB# #TAB# out[dico[key]] = key #LINE# #TAB# return out
#LINE# #TAB# table = load_table(str1) #LINE# #TAB# if type(table) is not list: #LINE# #TAB# #TAB# str1 = [str1] #LINE# #TAB# str2 = ''.join(table) #LINE# #TAB# if len(str2) > 10: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# if not table[str2].isdigit(): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return table
#LINE# #TAB# if options.input: #LINE# #TAB# #TAB# for string in options.input: #LINE# #TAB# #TAB# #TAB# yield string #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string
#LINE# #TAB# link_pages = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(links): #LINE# #TAB# #TAB# link_page = [] #LINE# #TAB# #TAB# while i < len(links) and len(link_page) < 10: #LINE# #TAB# #TAB# #TAB# link_page.append(links[i]) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# link_pages.append(link_page) #LINE# #TAB# return link_pages
"#LINE# #TAB# ""for json output add a full stop if ends in et al"" #LINE# #TAB# sequence = None #LINE# #TAB# tags = raw_parser.chop_sequence(soup) #LINE# #TAB# if tags: #LINE# #TAB# #TAB# for tag in tags: #LINE# #TAB# #TAB# #TAB# if tag['text'][-1] =='': #LINE# #TAB# #TAB# #TAB# #TAB# sequence = tag #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return sequence"
"#LINE# #TAB# joystick_c = unbox(joystick, 'SDL_Joystick *') #LINE# #TAB# rc = lib.sdl_joysticknumhats(joystick_c) #LINE# #TAB# return rc"
"#LINE# #TAB# content = response.content #LINE# #TAB# if isinstance(content, dict): #LINE# #TAB# #TAB# return Resource(**content) #LINE# #TAB# if isinstance(content, list): #LINE# #TAB# #TAB# return ResourceList(content) #LINE# #TAB# if isinstance(content, dict): #LINE# #TAB# #TAB# return Resource(**content) #LINE# #TAB# return content"
"#LINE# #TAB# values = value.split(',') #LINE# #TAB# if len(values)!= 1: #LINE# #TAB# #TAB# raise ValueError('Window size must be a single element') #LINE# #TAB# elif isinstance(values[0], int): #LINE# #TAB# #TAB# values = [values] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Window size must be a single element') #LINE# #TAB# return values"
#LINE# #TAB# XsdValidator = XsdValidator(schema_version='1.0.0') #LINE# #TAB# if require: #LINE# #TAB# #TAB# XsdValidator.bC_Mg_ClO4_PM73_require(require) #LINE# #TAB# return XsdValidator
#LINE# #TAB# if id_ in channel_dict(chid): #LINE# #TAB# #TAB# return channel_dict[chid]['state'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# fail = {'type': id_type, 'value': id_value, 'quality': quality} #LINE# #TAB# return fail"
"#LINE# #TAB# res = {} #LINE# #TAB# for k, v in attrs.items(): #LINE# #TAB# #TAB# if not k.startswith('_'): #LINE# #TAB# #TAB# #TAB# setattr(res, k, v) #LINE# #TAB# return res"
"#LINE# #TAB# if isinstance(params, dict): #LINE# #TAB# #TAB# return params #LINE# #TAB# if isinstance(params, list): #LINE# #TAB# #TAB# return [sample_batch(x) for x in params] #LINE# #TAB# return params"
#LINE# #TAB# if type_name not in data_dict.keys(): #LINE# #TAB# #TAB# raise ValueError(f'Invalid value for {type_name}.') #LINE# #TAB# return data_dict
"#LINE# #TAB# threshold = abs(threshold) #LINE# #TAB# position = 0 #LINE# #TAB# for position, basescore in enumerate(scores): #LINE# #TAB# #TAB# if basescore >= threshold: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return position"
"#LINE# #TAB# model = create_knn_model(fname) #LINE# #TAB# issues = model.issues #LINE# #TAB# issues = issues.issues #LINE# #TAB# return model, issues"
"#LINE# #TAB# outfile = os.path.join(outdir, 'plot') #LINE# #TAB# os.makedirs(outfile, exist_ok=True) #LINE# #TAB# return outfile"
"#LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# return value #LINE# #TAB# value = quote(value, safe='~') #LINE# #TAB# if not value.startswith('/'): #LINE# #TAB# #TAB# return quote(value) #LINE# #TAB# return value"
"#LINE# #TAB# r = subprocess.run(path, shell=True, stdout=subprocess.PIPE, stderr= #LINE# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# if r.returncode == 0: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return r.returncode"
"#LINE# #TAB# if not txt: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif len(txt) == 1: #LINE# #TAB# #TAB# return txt[0] #LINE# #TAB# elif len(txt) == 2: #LINE# #TAB# #TAB# return txt[0], txt[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None, None"
#LINE# #TAB# result = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = berdecoder.decode(content) #LINE# #TAB# #TAB# #TAB# result.append(obj) #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return result
"#LINE# #TAB# count = stdio.readInt() #LINE# #TAB# array = create_empty_array(count, None) #LINE# #TAB# for i in range(count): #LINE# #TAB# #TAB# array[i] = stdio.readInt() #LINE# #TAB# return array"
"#LINE# #TAB# return [ #LINE# #TAB# #TAB# os.path.join(tmp_dir, 'corpora-%s' % i) #LINE# #TAB# #TAB# for i in os.listdir(tmp_dir) #LINE# #TAB# ]"
"#LINE# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# return val.split() #LINE# #TAB# else: #LINE# #TAB# #TAB# return val"
"#LINE# #TAB# left_bytes = list(left_bytes) #LINE# #TAB# right_bytes = list(right_bytes) #LINE# #TAB# l2m = len(left_bytes) #LINE# #TAB# r2m = len(right_bytes) #LINE# #TAB# distance = 0 #LINE# #TAB# for l, r in zip(left_bytes, right_bytes): #LINE# #TAB# #TAB# l2m += l #LINE# #TAB# #TAB# r2m += r #LINE# #TAB# distance /= l2m #LINE# #TAB# return distance"
"#LINE# #TAB# return {'id': card.id, 'type': card.type, 'title': card.title, 'value': #LINE# #TAB# #TAB# card.value, 'notes': card.notes, 'created_at': card.created_at, #LINE# #TAB# #TAB# 'updated_at': card.updated_at}"
"#LINE# #TAB# global kmers #LINE# #TAB# if kmers is not None: #LINE# #TAB# #TAB# return kmers #LINE# #TAB# shm_fd = os.open('/dev/shm', os.O_RDONLY) #LINE# #TAB# try: #LINE# #TAB# #TAB# kmer = shm_fd.readline().strip() #LINE# #TAB# finally: #LINE# #TAB# #TAB# if shm_fd is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return kmers"
#LINE# #TAB# global deformer_history #LINE# #TAB# if ota is None: #LINE# #TAB# #TAB# if not os.path.exists(define_deformer_history_path): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# ota_history = read_deformer_history(ota=ota) #LINE# #TAB# return ota_history
#LINE# #TAB# entities = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# match = re_saved_tracks.search(text) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# entities.append(match.group(1)) #LINE# #TAB# #TAB# #TAB# text = match.group(2) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return entities
#LINE# #TAB# #TAB# cls._status = None #LINE# #TAB# #TAB# pass
#LINE# #TAB# social_auth_user = get_social_auth_user(lock) #LINE# #TAB# if not social_auth_user: #LINE# #TAB# #TAB# return None #LINE# #TAB# return social_auth_user.user
"#LINE# #TAB# result = nodes.Assign(target_id, ['config'], []) #LINE# #TAB# result.env = env #LINE# #TAB# return result"
"#LINE# #TAB# if isinstance(obj, sa.Model): #LINE# #TAB# #TAB# return dict((k, path_to_regex(v)) for k, v in obj.all_fields.items()) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# return [path_to_regex(v) for v in obj] #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
#LINE# #TAB# ts = _get_tweets_cache.get(fn) #LINE# #TAB# if ts is not None: #LINE# #TAB# #TAB# return ts #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if isinstance(vals, (list, tuple, set)): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(vals, (tuple, set)): #LINE# #TAB# #TAB# return all(isinstance(e, str) for e in vals) #LINE# #TAB# return False"
"#LINE# #TAB# y_pred = 0.0 #LINE# #TAB# for i in range(len(X)): #LINE# #TAB# #TAB# w = X[i] #LINE# #TAB# #TAB# coef_ = coef[w] #LINE# #TAB# #TAB# if link: #LINE# #TAB# #TAB# #TAB# y_pred += w * y_pred #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# y_pred -= w * y #LINE# #TAB# if y_pred == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# d_pred = y_pred / y_pred #LINE# #TAB# #TAB# return d_pred, y_pred"
"#LINE# #TAB# if asse_equal_type_re.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'SL317: assertEqual(type(A), B) sentences not allowed'"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# base = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# #TAB# host, port = ip_address.split(':') #LINE# #TAB# #TAB# request = 'https{}:{}/api'.format(host, port) #LINE# #TAB# #TAB# with open(request) as f: #LINE# #TAB# #TAB# #TAB# api = f.read() #LINE# #TAB# #TAB# return api #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# logging.error('Could not determine API for IP {}: {}'.format( #LINE# #TAB# #TAB# #TAB# ip_address, request)) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# fsm = fsm_helpers.get_fsm() #LINE# #TAB# return {x['region']: {'name': x['name'],'start_time': x['startTime'], #LINE# #TAB# #TAB# 'end_time': x['endTime']} for x in fsm.get_regions()}"
#LINE# #TAB# parts = path.split('/') #LINE# #TAB# if len(parts) < 2: #LINE# #TAB# #TAB# return '' #LINE# #TAB# alt_text = parts[-1] #LINE# #TAB# if alt_text.endswith('.ipynb'): #LINE# #TAB# #TAB# alt_text = alt_text[:-4] #LINE# #TAB# if len(parts) > 2: #LINE# #TAB# #TAB# alt_text = parts[-2] #LINE# #TAB# return alt_text
#LINE# #TAB# if value and max_width is not None and len(value) > max_width: #LINE# #TAB# #TAB# return value[:max_width - 3] + '...' #LINE# #TAB# return value
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = get_text(section, option, ignore_whitespace_symbols=True) #LINE# #TAB# #TAB# return value #LINE# #TAB# except (NoOptionError, NoSectionError) as err: #LINE# #TAB# #TAB# if raise_exception and default is None: #LINE# #TAB# #TAB# #TAB# raise err #LINE# #TAB# #TAB# return default"
#LINE# #TAB# bit_arr = [] #LINE# #TAB# for i in range(len(arr)): #LINE# #TAB# #TAB# for j in range(8): #LINE# #TAB# #TAB# #TAB# bit_arr.append(arr[i][j]) #LINE# #TAB# return bit_arr
"#LINE# #TAB# if not retries > 0: #LINE# #TAB# #TAB# retries = 1 #LINE# #TAB# connection = mysql.connect(host=host, db=db, user=user, password=password, #LINE# #TAB# #TAB# retry_count=retries) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# diff = connection.execute('SELECT * FROM ratings') #LINE# #TAB# #TAB# except mysql.OperationalError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if diff: #LINE# #TAB# #TAB# #TAB# time.sleep(sleep) #LINE# #TAB# #TAB# #TAB# connection.execute('SELECT * FROM ratings') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return connection"
"#LINE# #TAB# inputs = {} #LINE# #TAB# for i in example_command: #LINE# #TAB# #TAB# for k, v in i.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# #TAB# if 'radius' in k: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# inputs[k] = np.array(v['radius']) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# inputs[k] = v #LINE# #TAB# return inputs"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# c = models.ContentType.objects.get(app_name=app_name, model_name= #LINE# #TAB# #TAB# #TAB# model_name) #LINE# #TAB# except models.ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if c.content_type not in VALID_CONTENT_TYPES: #LINE# #TAB# #TAB# raise Http404"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# mod = apps.get_model(app_name, model_name) #LINE# #TAB# except LookupError: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if isinstance(mod, type): #LINE# #TAB# #TAB# return mod #LINE# #TAB# try: #LINE# #TAB# #TAB# return ContentType.objects.get(app_name=app_name, model_name=model_name #LINE# #TAB# #TAB# #TAB# ).content_type #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404"
"#LINE# #TAB# env_name = str(env) #LINE# #TAB# if env_name.endswith('.py'): #LINE# #TAB# #TAB# env_name = env_name[:-3] #LINE# #TAB# if env_name in ('c',): #LINE# #TAB# #TAB# return 'gamma' #LINE# #TAB# try: #LINE# #TAB# #TAB# return pkg_resources.resource_filename('ravello', env_name) #LINE# #TAB# except pkg_resources.DistributionNotFound: #LINE# #TAB# #TAB# return 'gamma'"
#LINE# #TAB# message = error.response.content #LINE# #TAB# try: #LINE# #TAB# #TAB# return yaml.load(message) #LINE# #TAB# except YAMLError as e: #LINE# #TAB# #TAB# message = str(e) #LINE# #TAB# #TAB# if 'Message' in message: #LINE# #TAB# #TAB# #TAB# message = message['Message'] #LINE# #TAB# return message
"#LINE# #TAB# if direction == 'parent': #LINE# #TAB# #TAB# name = '{0}_{1}'.format(name, uuid.uuid4().hex) #LINE# #TAB# elif direction == 'owner': #LINE# #TAB# #TAB# name = '{0}_{1}'.format(name, uuid.uuid4().hex) #LINE# #TAB# elif direction =='sibling': #LINE# #TAB# #TAB# name = '{0}_{1}'.format(name, uuid.uuid4().hex) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('direction must be ""parent"" or ""sibling""') #LINE# #TAB# return name"
"#LINE# #TAB# dst_ns = int((extent[2] - extent[0])/res + 0.001) #LINE# #TAB# dst_nl = int((extent[3] - extent[1])/res + 0.001) #LINE# #TAB# dst_nl = int((extent[5] - extent[3])/res + 0.001) #LINE# #TAB# m = gdal.GetGeoTransform() #LINE# #TAB# m.SetGeoTransform(dst_ns, dst_nl) #LINE# #TAB# if srs is not None: #LINE# #TAB# #TAB# m.SetProjection(srs.ExportToWkt()) #LINE# #TAB# out = gdal.GetDataset(dst_ns, dtype) #LINE# #TAB# out.SetGeoTransform(m) #LINE# #TAB# return out"
"#LINE# #TAB# colnames, indices = [], [] #LINE# #TAB# with open(filename, 'r') as csvfile: #LINE# #TAB# #TAB# reader = csv.reader(csvfile, delimiter='\t') #LINE# #TAB# #TAB# for i, row in enumerate(reader): #LINE# #TAB# #TAB# #TAB# if row[0]!= '#': #LINE# #TAB# #TAB# #TAB# #TAB# colnames.append(row[0]) #LINE# #TAB# #TAB# #TAB# #TAB# indices.append(i) #LINE# #TAB# return colnames, indices"
"#LINE# #TAB# vals = [] #LINE# #TAB# vecs = [] #LINE# #TAB# for i in range(len(esys_mapdata[0])): #LINE# #TAB# #TAB# esys_mapdata[i][0] = esys_mapdata[0][i] #LINE# #TAB# #TAB# for j in range(len(esys_mapdata[0])): #LINE# #TAB# #TAB# #TAB# esys_mapdata[i][j] = esys_mapdata[0][j + 1] #LINE# #TAB# #TAB# #TAB# vals.append(esys_mapdata[j][i]) #LINE# #TAB# #TAB# #TAB# vecs.append(np.array([esys_mapdata[1][i] for i in range(len( #LINE# #TAB# #TAB# #TAB# #TAB# esys_mapdata[1]))])) #LINE# #TAB# return vals, vecs"
"#LINE# #TAB# grains = {} #LINE# #TAB# docker_dev = get_docker_dev(proxy=proxy) #LINE# #TAB# if 'network_containers' not in docker_dev: #LINE# #TAB# #TAB# return {} #LINE# #TAB# docker_dev['network_containers'] = docker_dev['network_containers'] #LINE# #TAB# for container in docker_dev['network_containers']: #LINE# #TAB# #TAB# name = container['name'] #LINE# #TAB# #TAB# data = docker_dev.get('properties', {}) #LINE# #TAB# #TAB# if name not in grains: #LINE# #TAB# #TAB# #TAB# grains[name] = {} #LINE# #TAB# #TAB# if 'id' not in data: #LINE# #TAB# #TAB# #TAB# data['id'] = container['id'] #LINE# #TAB# #TAB# grains[name]['data'] = data #LINE# #TAB# return grains"
#LINE# #TAB# if regex.startswith('ecma://'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# parsed = OrderedDict(type=in_16(arg)) #LINE# #TAB# parsed['name'] = arg.__name__ #LINE# #TAB# parsed['signature'] = str(signature_inspect(arg)) #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getfullargspec(arg)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# parsed['fullargspec'] = str(inspect.getargspec(arg)) #LINE# #TAB# parsed['isbuiltin'] = inspect.isbuiltin(arg) #LINE# #TAB# return parsed
"#LINE# #TAB# if os.path.isfile(template): #LINE# #TAB# #TAB# file_path = template #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# file_path = find_template(template) #LINE# #TAB# #TAB# except TemplateDoesNotExist: #LINE# #TAB# #TAB# #TAB# file_path = None #LINE# #TAB# #TAB# file_path = file_path.replace('\\', '/') #LINE# #TAB# return file_path"
#LINE# #TAB# #TAB# ret = ReviewConfig() #LINE# #TAB# #TAB# ret.add_mechfile(ini_config) #LINE# #TAB# #TAB# if app_config: #LINE# #TAB# #TAB# #TAB# ret.add_app_config(app_config) #LINE# #TAB# #TAB# if get_default_mechfile(ini_config): #LINE# #TAB# #TAB# #TAB# ret.add_default_mechfile(ini_config) #LINE# #TAB# #TAB# return ret
"#LINE# #TAB# output = read_csv(os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# 'data/conditional_entropy.csv'), index_col=0) #LINE# #TAB# return output"
#LINE# #TAB# compression_method = decompressor.COMPRESSION_METHOD.lower() #LINE# #TAB# if compression_method not in cls._decompressors: #LINE# #TAB# #TAB# raise KeyError('No such compression method: {0:s}'.format( #LINE# #TAB# #TAB# #TAB# decompressor)) #LINE# #TAB# cls._decompressors[compression_method] = None
#LINE# #TAB# checksum = None #LINE# #TAB# try: #LINE# #TAB# #TAB# data_str = base64.b64decode(data) #LINE# #TAB# #TAB# checksum = hashlib.sha256(data_str.encode('utf-8')).hexdigest() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return {'checksum': checksum}
#LINE# #TAB# z = x.copy() #LINE# #TAB# z.update(y) #LINE# #TAB# return z
#LINE# #TAB# value = True #LINE# #TAB# if url_dict['url'] in authn_subj_list: #LINE# #TAB# #TAB# value = False #LINE# #TAB# return value
"#LINE# #TAB# psi = -0.0094 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
#LINE# #TAB# code = Code() #LINE# #TAB# code.extend(code_object) #LINE# #TAB# return code
"#LINE# #TAB# for pattern, _ in matches: #LINE# #TAB# #TAB# if fnmatch.fnmatch(name, pattern): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# for sda in sdas: #LINE# #TAB# #TAB# mib_max = np.max(mib) #LINE# #TAB# #TAB# xi_complement_max = np.max(xi_complement) #LINE# #TAB# #TAB# b = (mib_max + xi_complement) / 2.0 #LINE# #TAB# #TAB# sda[1] = b #LINE# #TAB# ensure_binary(sdas, reachability_plot) #LINE# #TAB# return"
"#LINE# #TAB# proc = Process(name=commands, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# args=(process_start, process_finish), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# kwargs={ #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'env': env}) #LINE# #TAB# try: #LINE# #TAB# #TAB# proc.start() #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == 2: #LINE# #TAB# #TAB# #TAB# proc.terminate() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return proc"
"#LINE# #TAB# from pyproj.units import SkyCoord #LINE# #TAB# if isinstance(value, SkyCoord): #LINE# #TAB# #TAB# return value._class #LINE# #TAB# elif isinstance(value, int): #LINE# #TAB# #TAB# return SkyCoord(value) #LINE# #TAB# elif isinstance(value, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return SkyCoord(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# raise ValueError"
"#LINE# #TAB# path = PF_PATH / name #LINE# #TAB# pf = open(path, 'r') #LINE# #TAB# func = pf.read() #LINE# #TAB# pf.close() #LINE# #TAB# return func"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# ean = next(gen) #LINE# #TAB# #TAB# #TAB# if strategy == 'random': #LINE# #TAB# #TAB# #TAB# #TAB# ean = parse_isbn_ean_random(ean) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ean = parse_isbn_ean(ean, strategy) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return"
"#LINE# #TAB# scheduled_text = get_scheduled_text(section, min_paragraph_length) #LINE# #TAB# paragraphs = [] #LINE# #TAB# if scheduled_text: #LINE# #TAB# #TAB# for paragraph in scheduled_text.split('\n\n'): #LINE# #TAB# #TAB# #TAB# paragraph = strip_tags(paragraph) #LINE# #TAB# #TAB# #TAB# if len(paragraph) > 0: #LINE# #TAB# #TAB# #TAB# #TAB# paragraphs.append(paragraph) #LINE# #TAB# return paragraphs"
"#LINE# #TAB# if width == 1: #LINE# #TAB# #TAB# return [0] * 2 #LINE# #TAB# msb = np.frombuffer(data, dtype='uint8') #LINE# #TAB# lsb = np.frombuffer(data, dtype='uint8') #LINE# #TAB# lsb[0] = 1 #LINE# #TAB# lsb[-1] = 2 #LINE# #TAB# if width == 2: #LINE# #TAB# #TAB# return [msb, lsb] #LINE# #TAB# elif width == 3: #LINE# #TAB# #TAB# return [msb, lsb] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [msb, lsb]"
"#LINE# #TAB# field.setText(str(field.text()).replace(',','')) #LINE# #TAB# for txt in field.text(): #LINE# #TAB# #TAB# if len(txt) < num: #LINE# #TAB# #TAB# #TAB# _show_consistency(field, message, True) #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"#LINE# #TAB# from.groups import as_bsgs #LINE# #TAB# from.transversals import read_config_by_gens #LINE# #TAB# config = read_config_by_gens(base, gens) #LINE# #TAB# transversals = {} #LINE# #TAB# for k, v in config.items(): #LINE# #TAB# #TAB# if k.startswith('bsgs.'): #LINE# #TAB# #TAB# #TAB# v = read_config_by_gens(base, gens, v) #LINE# #TAB# #TAB# #TAB# transversals[k] = v #LINE# #TAB# return transversals"
"#LINE# #TAB# path, filename = os.path.split(fullpath) #LINE# #TAB# filename, ext = os.path.splitext(filename) #LINE# #TAB# if ext == '.py': #LINE# #TAB# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# #TAB# filename = filename.replace('.py', '') #LINE# #TAB# #TAB# elif ext == '.pyc': #LINE# #TAB# #TAB# #TAB# filename = filename.replace('.pyc', '') #LINE# #TAB# module = importlib.import_module(filename) #LINE# #TAB# params = dict(module.as_dict()) #LINE# #TAB# if ext == '.pyo': #LINE# #TAB# #TAB# params['pyo'] = importlib.import_module(filename) #LINE# #TAB# return params"
"#LINE# #TAB# if name.startswith('build_'): #LINE# #TAB# #TAB# return name, size, unsize #LINE# #TAB# elif name.startswith('copy_'): #LINE# #TAB# #TAB# return name, location, size #LINE# #TAB# return location, size, unsize"
"#LINE# #TAB# task_config, sequence_number = cls.build_task_config(config, dependencies) #LINE# #TAB# return task_config, sequence_number"
"#LINE# #TAB# vault_relation_list = [model_a, model_b] #LINE# #TAB# for index_a, module_b in enumerate(vault_relation_list): #LINE# #TAB# #TAB# if index_a!= index_b: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# for index_b, module_a in enumerate(vault_relation_list): #LINE# #TAB# #TAB# if index_b!= index_a: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"#LINE# #TAB# xml_files = [] #LINE# #TAB# for file_name in files: #LINE# #TAB# #TAB# this_dir = os.path.dirname(os.path.realpath(file_name)) #LINE# #TAB# #TAB# xml_files += get_xml_for_file(file_name, zip_filename, base_path, #LINE# #TAB# #TAB# #TAB# this_dir) #LINE# #TAB# return xml_files"
"#LINE# #TAB# mount_point_json = mount_point + '/json' #LINE# #TAB# if os.path.exists(mount_point_json): #LINE# #TAB# #TAB# with open(mount_point_json, 'r') as f: #LINE# #TAB# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# mount_point_json['data'] = data #LINE# #TAB# return mount_point_json"
"#LINE# #TAB# if elapsed < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# nih_image_header = read_nih_image(lease_time, elapsed) #LINE# #TAB# if nih_image_header == b'': #LINE# #TAB# #TAB# return nih_image_header #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# tzi = getattr(dt, 'tzinfo', None) #LINE# #TAB# if tzi is not None: #LINE# #TAB# #TAB# dt = dt.astimezone(UTC) #LINE# #TAB# #TAB# tzi = UTC #LINE# #TAB# base = float(dt.replace(tzinfo=tzi)) #LINE# #TAB# cdate = getattr(dt, 'date', None) #LINE# #TAB# if cdate is not None: #LINE# #TAB# #TAB# midnight_time = datetime.time(0, tzinfo=tzi) #LINE# #TAB# #TAB# rdt = datetime.datetime.combine(cdate, midnight_time) #LINE# #TAB# #TAB# base += (dt - rdt).total_seconds() / SEC_PER_DAY #LINE# #TAB# return base"
#LINE# #TAB# child = node.getparent() #LINE# #TAB# if child is None: #LINE# #TAB# #TAB# return #LINE# #TAB# if child.data is None: #LINE# #TAB# #TAB# child.data = value #LINE# #TAB# else: #LINE# #TAB# #TAB# child.data = value
#LINE# #TAB# try: #LINE# #TAB# #TAB# int(tag) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# chr_list = list(query) #LINE# #TAB# while 'chr' in chr_list: #LINE# #TAB# #TAB# chr_list.remove('chr') #LINE# #TAB# for chr in chr_list: #LINE# #TAB# #TAB# if chr == 'chr': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# chr_list.append(chr) #LINE# #TAB# return chr_list
"#LINE# #TAB# vertices_neighbours = _get_vertices_neighbours(nets) #LINE# #TAB# for subgraph_vertices in _get_connected_subgraphs(vertices_resources, #LINE# #TAB# #TAB# vertices_neighbours): #LINE# #TAB# #TAB# for subgraph in _get_subgraphs_from_map(subgraph_vertices, vertices_neighbours): #LINE# #TAB# #TAB# #TAB# yield subgraph"
#LINE# #TAB# out = [] #LINE# #TAB# for row in cursor.fetchall(): #LINE# #TAB# #TAB# out.append(row) #LINE# #TAB# return out
#LINE# #TAB# global CONFIG #LINE# #TAB# if not CONFIG: #LINE# #TAB# #TAB# CONFIG = yaml.SafeConfigParser() #LINE# #TAB# return CONFIG
"#LINE# #TAB# installed_dists_by_name = {} #LINE# #TAB# for installed_dist in installed_dists: #LINE# #TAB# #TAB# installed_dists_by_name[installed_dist.project_name] = installed_dist #LINE# #TAB# for requirement in dist.requires(): #LINE# #TAB# #TAB# present_dist = installed_dists_by_name.get(requirement.project_name) #LINE# #TAB# #TAB# if present_dist: #LINE# #TAB# #TAB# #TAB# yield requirement, present_dist"
#LINE# #TAB# if text.endswith(suffix): #LINE# #TAB# #TAB# return text[:-len(suffix)] #LINE# #TAB# return text
"#LINE# #TAB# if isinstance(xyz, list): #LINE# #TAB# #TAB# return xyz[0], xyz[1], True #LINE# #TAB# return xyz"
#LINE# #TAB# if i >= 2 ** 28: #LINE# #TAB# #TAB# raise ValueError('value of {} is too large'.format(i)) #LINE# #TAB# elif i < 0: #LINE# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# return i
"#LINE# #TAB# params = {} #LINE# #TAB# file_name = module.params['file'] #LINE# #TAB# if file_name == '': #LINE# #TAB# #TAB# return params #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# #TAB# params = yaml.safe_load(f) #LINE# #TAB# except IOError as e: #LINE# #TAB# #TAB# module.fail_json(msg='Failed to open the allocation file %s for reading' % #LINE# #TAB# #TAB# #TAB# file_name) #LINE# #TAB# except yaml.YAMLError as e: #LINE# #TAB# #TAB# module.fail_json(msg='Failed to parse the allocation file %s as YAML' % #LINE# #TAB# #TAB# #TAB# file_name) #LINE# #TAB# if params: #LINE# #TAB# #TAB# module.params['ip_addresses'] = params #LINE# #TAB# return params"
"#LINE# #TAB# sock = DDPSocket() #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1) #LINE# #TAB# _taskotron_release_critical_task(sock, timeout) #LINE# #TAB# return sock"
"#LINE# #TAB# modules = [] #LINE# #TAB# for path in get_serial_paths(directory): #LINE# #TAB# #TAB# if path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# path = path[:-3] #LINE# #TAB# #TAB# if flush_local_modules: #LINE# #TAB# #TAB# #TAB# path = path.replace('\\', '/') #LINE# #TAB# #TAB# init_path = os.path.join(path, '__init__.py') #LINE# #TAB# #TAB# modules.append((init_path, path)) #LINE# #TAB# return modules"
"#LINE# #TAB# if'' in comm: #LINE# #TAB# #TAB# return comm.split(' ') #LINE# #TAB# obj = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# json_obj = json.loads(comm) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# for part in json_obj['args']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj[part] = json_obj['args'][part] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return obj['cmd'], obj['args']"
#LINE# #TAB# if pg_name not in pg_default_port_config['atomic_abundance']: #LINE# #TAB# #TAB# return {} #LINE# #TAB# return pg_default_port_config['atomic_abundance'][pg_name]
"#LINE# #TAB# parsed = urlparse(url) #LINE# #TAB# try: #LINE# #TAB# #TAB# username = unquote(parsed.username), unquote(parsed.password) #LINE# #TAB# except (AttributeError, TypeError): #LINE# #TAB# #TAB# username = parsed.username #LINE# #TAB# try: #LINE# #TAB# #TAB# password = unquote(parsed.password) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# password = None #LINE# #TAB# return username, password"
"#LINE# #TAB# matches = [] #LINE# #TAB# with open(filename, 'r') as stream: #LINE# #TAB# #TAB# for line in stream: #LINE# #TAB# #TAB# #TAB# match = line.rstrip().split('\t') #LINE# #TAB# #TAB# #TAB# confidence = match[1][1:] #LINE# #TAB# #TAB# #TAB# if confidence == 1: #LINE# #TAB# #TAB# #TAB# #TAB# matches.append((int(match[0]), match[1])) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# matches.append((int(match[0]), match[1])) #LINE# #TAB# return matches"
#LINE# #TAB# if type(elements) == int: #LINE# #TAB# #TAB# elements = int(elements) #LINE# #TAB# if type(elements) == list: #LINE# #TAB# #TAB# for x in elements: #LINE# #TAB# #TAB# #TAB# del x #LINE# #TAB# else: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# #TAB# started = False #LINE# #TAB# #TAB# for idx, s in enumerate(mol2_lst): #LINE# #TAB# #TAB# #TAB# if s.startswith('@<TRIPOS>ATOM'): #LINE# #TAB# #TAB# #TAB# #TAB# first_idx = idx + 1 #LINE# #TAB# #TAB# #TAB# #TAB# started = True #LINE# #TAB# #TAB# #TAB# elif started and s.startswith('@<TRIPOS>'): #LINE# #TAB# #TAB# #TAB# #TAB# last_idx_plus1 = idx #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# return mol2_lst[first_idx:last_idx_plus1]"
"#LINE# #TAB# all_text = [] #LINE# #TAB# all_tail = [] #LINE# #TAB# for dictnode in dictnode_tree: #LINE# #TAB# #TAB# if isinstance(dictnode, dict): #LINE# #TAB# #TAB# #TAB# all_text.extend(dictnode.keys()) #LINE# #TAB# #TAB# #TAB# all_tail.extend(dictnode_tree[dictnode]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# all_tail.append(dictnode) #LINE# #TAB# return all_text, all_tail"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return options['return'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return base_context.returning if base_context else ReturnType.Records
"#LINE# #TAB# if use_ordered_dict: #LINE# #TAB# #TAB# dict = OrderedDict #LINE# #TAB# columns = df.columns #LINE# #TAB# if index_col is not None: #LINE# #TAB# #TAB# columns = [col for col in index_col] #LINE# #TAB# data = [] #LINE# #TAB# for row in df: #LINE# #TAB# #TAB# if isinstance(row, dict): #LINE# #TAB# #TAB# #TAB# data.append(type_of_script(row, use_ordered_dict=use_ordered_dict)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data.append(row) #LINE# #TAB# return data"
#LINE# #TAB# if not now: #LINE# #TAB# #TAB# now = datetime.datetime.utcnow() #LINE# #TAB# delta = timestamp - now #LINE# #TAB# return delta.days * 24 * 60 * 60 + delta.seconds
"#LINE# #TAB# handlers = _create_handlers(nicknames, relations, format, program, directed) #LINE# #TAB# graph = DotGraph(handlers) #LINE# #TAB# graph.graph['name'] = name #LINE# #TAB# graph.graph['relations'] = nicknames #LINE# #TAB# return graph"
"#LINE# #TAB# all_columns = list(filter(lambda el: el[-2:] == '_I', df.columns)) #LINE# #TAB# for column in all_columns: #LINE# #TAB# #TAB# del df[column]"
"#LINE# #TAB# out_dict = {} #LINE# #TAB# for key, value in in_dict.items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# #TAB# out_dict[key] = combine_hex(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# out_dict[key] = value #LINE# #TAB# return out_dict"
#LINE# #TAB# handler = logging.StreamHandler() #LINE# #TAB# handler.setLevel(logging.DEBUG) #LINE# #TAB# formatter = logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s - %(levelname)s - %(module)s - %(message)s') #LINE# #TAB# handler.setFormatter(formatter) #LINE# #TAB# handler.addFilter(EGG_DIST_FILTER) #LINE# #TAB# return handler
"#LINE# #TAB# new_d = {} #LINE# #TAB# for key, value in d.items(): #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# value = value[0] #LINE# #TAB# #TAB# if key not in skip: #LINE# #TAB# #TAB# #TAB# new_d[key] = value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_d[key] = value #LINE# #TAB# return new_d"
"#LINE# #TAB# modules = _load_frontend_modules(config, internal_attributes) #LINE# #TAB# for module_name in modules: #LINE# #TAB# #TAB# module = import_module(module_name) #LINE# #TAB# #TAB# callback(module, config) #LINE# #TAB# return None"
#LINE# #TAB# data = {} #LINE# #TAB# data['file'] = func.__file__ #LINE# #TAB# data['line'] = func.__code__.strip('\n') #LINE# #TAB# data['function'] = func.__name__ #LINE# #TAB# data['line_number'] = str(inspect.getsourcelines(func)[1]) #LINE# #TAB# data['source'] = source_from_env(func) #LINE# #TAB# return data
#LINE# #TAB# if sys.version_info[0] < 3: #LINE# #TAB# #TAB# return obj_class() #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj_class
"#LINE# #TAB# #TAB# return [models.DbReference(reference=x.text) for x in entry.iterfind( #LINE# #TAB# #TAB# #TAB# './dbReference', namespaces=NAMESPACES)]"
"#LINE# #TAB# result = {} #LINE# #TAB# for op, histories_list in histories.items(): #LINE# #TAB# #TAB# for history in histories_list: #LINE# #TAB# #TAB# #TAB# op_name = op.split('/')[-1].split('.')[0] #LINE# #TAB# #TAB# #TAB# result[op_name] = {} #LINE# #TAB# #TAB# #TAB# for key in op_name: #LINE# #TAB# #TAB# #TAB# #TAB# val = histories_list[key].pop(op_name) #LINE# #TAB# #TAB# #TAB# #TAB# op_name = op_name + '_' + str(val) #LINE# #TAB# #TAB# #TAB# #TAB# result[op_name][val] = val #LINE# #TAB# return result"
#LINE# #TAB# tol = None #LINE# #TAB# for i in range(sys.maxsize): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# tol = str(sys.getmaxsize()[i]) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# for i in range(sys.maxsize): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# tol = str(sys.getmaxsize()[i]) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return tol
"#LINE# #TAB# csv_out = [] #LINE# #TAB# for f in os.listdir(dirpath): #LINE# #TAB# #TAB# full_path = os.path.join(dirpath, f) #LINE# #TAB# #TAB# if os.path.isdir(full_path): #LINE# #TAB# #TAB# #TAB# csv_out.extend(parse_csv(full_path, cond=cond)) #LINE# #TAB# return csv_out"
"#LINE# #TAB# value = int(value, 0) #LINE# #TAB# if value > 0: #LINE# #TAB# #TAB# value = value % 2 ** 32 #LINE# #TAB# return value"
"#LINE# #TAB# with open(jsonParams, 'r') as f: #LINE# #TAB# #TAB# jsonFile = f.read() #LINE# #TAB# parsedJSON = json.loads(jsonFile) #LINE# #TAB# return parsedJSON[blockNames.ControlFileParams.generalParams], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.spawningBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.simulationBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.clusteringBlockname]"
"#LINE# #TAB# result = [] #LINE# #TAB# for i in dir(obj): #LINE# #TAB# #TAB# if callable(getattr(obj, i)) and not i.startswith('_'): #LINE# #TAB# #TAB# #TAB# result.append((i, getattr(obj, i))) #LINE# #TAB# return result"
"#LINE# #TAB# global _perc_of_rt #LINE# #TAB# if _perc_of_rt is not None: #LINE# #TAB# #TAB# return _perc_of_rt #LINE# #TAB# p = _get_rt_dir() / 'data' / format.lower() #LINE# #TAB# if os.path.exists(p): #LINE# #TAB# #TAB# with open(p, 'r') as f: #LINE# #TAB# #TAB# #TAB# _perc_of_rt = [f.read().lower() for f in f] #LINE# #TAB# return _perc_of_rt"
"#LINE# #TAB# keys, values = p[2:] #LINE# #TAB# result = {} #LINE# #TAB# for k in keys: #LINE# #TAB# #TAB# result[k] = [] #LINE# #TAB# #TAB# for j in values: #LINE# #TAB# #TAB# #TAB# result[k].append(j) #LINE# #TAB# p[0] = result"
"#LINE# #TAB# theta = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"#LINE# #TAB# if match.end >= len(chars): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""Match does not end in sequence: {0!r}"".format(match.end)) #LINE# #TAB# i = match.end - 1 #LINE# #TAB# if i >= len(chars): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""Match does not end in sequence: {0!r}"".format(match.end)) #LINE# #TAB# return i"
"#LINE# #TAB# body = cls.client.set_id_generator(subnet_id=subnet_id, router_id= #LINE# #TAB# #TAB# router_id, name=name) #LINE# #TAB# vpn_service = body['vpn_service'] #LINE# #TAB# cls.vpn_services.append(vpn_service) #LINE# #TAB# return vpn_service"
"#LINE# #TAB# table = formatting.Table(['rule_id', 'theta_B3O3OH4_Cl_FW86'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# columns=['rule_id', 'theta_B3OH4_Cl_FW86']) #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# table[rule['rule_id']] = rule['theta_B3O3OH4_Cl_FW86'] #LINE# #TAB# return table"
"#LINE# #TAB# if isinstance(schema, basestring): #LINE# #TAB# #TAB# schema = get_item_schema(schema) #LINE# #TAB# ap = ICachedItemMapper() #LINE# #TAB# for key, value in schema.items(): #LINE# #TAB# #TAB# if isinstance(value, (bool, int)): #LINE# #TAB# #TAB# #TAB# ap.setdefault(key, False) #LINE# #TAB# #TAB# #TAB# return ap #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# ap.setdefault(key, True) #LINE# #TAB# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# #TAB# ap.setdefault(key, []) #LINE# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# ap[key].append(item) #LINE# #TAB# return ap"
#LINE# #TAB# if uri.startswith('file:'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# components = [] #LINE# #TAB# base_url = resource.get_base_url() #LINE# #TAB# if base_url is not None: #LINE# #TAB# #TAB# components += find_components(base_url) #LINE# #TAB# components += find_components_direct(resource.get_base()) #LINE# #TAB# return components
"#LINE# #TAB# res = False #LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen([command], stdout=subprocess.PIPE, stderr= #LINE# #TAB# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# #TAB# res = p.communicate()[0] #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return res"
"#LINE# #TAB# config = Config() #LINE# #TAB# for result in apply_func(fget, fset, fdel): #LINE# #TAB# #TAB# if result!= value_not_found: #LINE# #TAB# #TAB# #TAB# return config #LINE# #TAB# return config"
"#LINE# #TAB# q = q.copy() #LINE# #TAB# for i in range(q.shape[1]): #LINE# #TAB# #TAB# if q[i, 0]!= x[i]: #LINE# #TAB# #TAB# #TAB# add_v(q, x) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# df_num_den = df_num - df_den #LINE# #TAB# df_num = pd.to_numeric(df_num) #LINE# #TAB# df_den = pd.to_numeric(df_den) #LINE# #TAB# if not np.isfinite(f): #LINE# #TAB# #TAB# return False #LINE# #TAB# num_den = df_den - df_num #LINE# #TAB# den_num = pd.to_numeric(f) #LINE# #TAB# return num_den > den_num
#LINE# #TAB# method = request_handler.request.method #LINE# #TAB# if method == 'OPTIONS': #LINE# #TAB# #TAB# return request_handler.response #LINE# #TAB# elif method == 'GET': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# path = request_handler.path #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# validator.validate(path) #LINE# #TAB# #TAB# #TAB# except ValidationError: #LINE# #TAB# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return path
"#LINE# #TAB# if isinstance(path, Path): #LINE# #TAB# #TAB# yield path #LINE# #TAB# #TAB# return #LINE# #TAB# if isinstance(path, str): #LINE# #TAB# #TAB# path = Path(path) #LINE# #TAB# for child in path: #LINE# #TAB# #TAB# for res in multigraph_collect(child): #LINE# #TAB# #TAB# #TAB# yield res"
"#LINE# #TAB# N = X.shape[0] #LINE# #TAB# L = np.zeros(N) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# L[i] = append_random_len(X[i], cells[i]) #LINE# #TAB# return L"
"#LINE# #TAB# trun['exit'] = True #LINE# #TAB# trun['failures'] = 0 #LINE# #TAB# size = 0 #LINE# #TAB# for hook in tsuite['hooks']['exit']: #LINE# #TAB# #TAB# run_hook(trun, hook) #LINE# #TAB# for hook in trun['hooks']['exit']: #LINE# #TAB# #TAB# size += calculate_size(trun, hook) #LINE# #TAB# return size"
"#LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# if os.path.isfile(os.path.join(path, filename)): #LINE# #TAB# #TAB# #TAB# yield filename"
"#LINE# #TAB# if nsname and nspid: #LINE# #TAB# #TAB# nspath = join_namespace(nspath, nsname) #LINE# #TAB# elif nspath is None: #LINE# #TAB# #TAB# nspath = join_namespace(os.getcwd(), 'root', nsname) #LINE# #TAB# elif nspath is None: #LINE# #TAB# #TAB# nspath = join_namespace(os.getcwd(), 'root', nsname) #LINE# #TAB# if nspid is not None: #LINE# #TAB# #TAB# nspid = join_namespace(os.getcwd(), nspid) #LINE# #TAB# return nspath, nspid"
#LINE# #TAB# for item in items: #LINE# #TAB# #TAB# if item.groupid not in groupids: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del items[item.groupid] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# if 'type' in schema: #LINE# #TAB# #TAB# return schema['type'] #LINE# #TAB# elif isinstance(schema, dict): #LINE# #TAB# #TAB# types = [] #LINE# #TAB# #TAB# for key, value in schema.items(): #LINE# #TAB# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# #TAB# types.extend(value) #LINE# #TAB# #TAB# #TAB# elif isinstance(value, str): #LINE# #TAB# #TAB# #TAB# #TAB# types.append(value) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# types.append(str(value)) #LINE# #TAB# #TAB# return types #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
"#LINE# #TAB# if isinstance(node, str): #LINE# #TAB# #TAB# yield node.lower() == 'true' #LINE# #TAB# elif isinstance(node, list): #LINE# #TAB# #TAB# for n in node: #LINE# #TAB# #TAB# #TAB# for v in str_to_bool(n): #LINE# #TAB# #TAB# #TAB# #TAB# yield v #LINE# #TAB# else: #LINE# #TAB# #TAB# yield node"
#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# colocation_status = get_signature_input_colocation_status(running_tm_ver) #LINE# #TAB# #TAB# #TAB# if colocation_status['colocation'] == 'invalid': #LINE# #TAB# #TAB# #TAB# #TAB# return colocation_status #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return None
"#LINE# #TAB# samples = np.asarray(samples) #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# assert samples.shape[0] == 2 #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# assert samples.shape[1] == 4 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Metric must be ""l1"" or ""l2"".') #LINE# #TAB# return samples"
"#LINE# #TAB# for key, val in cls._fields.items(): #LINE# #TAB# #TAB# if isinstance(val, aes_to_scale): #LINE# #TAB# #TAB# #TAB# yield key, val"
"#LINE# #TAB# data = [] #LINE# #TAB# for k, v in state.summary.items(): #LINE# #TAB# #TAB# if match in [x.lower() for x in v]: #LINE# #TAB# #TAB# #TAB# data.append([k, v]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# data.append([(k, v)]) #LINE# #TAB# if col_names is not None: #LINE# #TAB# #TAB# data.sort(key=lambda x: x[col_names[0]]) #LINE# #TAB# if sort: #LINE# #TAB# #TAB# data.sort(key=lambda x: x[0]) #LINE# #TAB# return data"
"#LINE# #TAB# javac_cmd = None #LINE# #TAB# key = variants.get('key') #LINE# #TAB# if key is not None: #LINE# #TAB# #TAB# javac_cmd = key.decode('utf-8') #LINE# #TAB# if append_fields(javac_cmd, variants): #LINE# #TAB# #TAB# javac_cmd = append_fields(javac_cmd, variants) #LINE# #TAB# return javac_cmd"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# status_code = requests.head(url, headers={'User-Agent': #LINE# #TAB# #TAB# #TAB# 'Mozilla/5.0'})[0].status_code #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# status_code = 500 #LINE# #TAB# return status_code"
#LINE# #TAB# diff_input = np.zeros(len(riskinputs)) #LINE# #TAB# for i in range(len(riskinputs)): #LINE# #TAB# #TAB# diff_input[i] = riskinputs[i] - riskmodel[i] #LINE# #TAB# if monitor is not None: #LINE# #TAB# #TAB# monitor.update({'diff_dow': diff_input}) #LINE# #TAB# return diff_input
#LINE# #TAB# plugin_name = plugin_class.NAME.lower() #LINE# #TAB# if plugin_name in cls._plugin_classes: #LINE# #TAB# raise KeyError('Plugin class already set for name: {0:s}.'.format( #LINE# #TAB# #TAB# plugin_class.NAME)) #LINE# #TAB# cls._plugin_classes[plugin_name] = plugin_class
"#LINE# #TAB# data = get_apps(app, env=env, region=region) #LINE# #TAB# return {k.lower(): v for (k, v) in data.items() if v}"
#LINE# #TAB# bV = normalize_V(V) #LINE# #TAB# txDx = np.zeros(len(bV)) #LINE# #TAB# for i in range(len(bV)): #LINE# #TAB# #TAB# for j in range(len(bV)): #LINE# #TAB# #TAB# #TAB# txDx[i][j] = bV[i][j] * eps / dx ** (len(bV[i]) - 1) #LINE# #TAB# return txDx
#LINE# #TAB# floating_ip = cls.by_id(external_network_id) #LINE# #TAB# if floating_ip is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return floating_ip
#LINE# #TAB# if type(rows)!= list: #LINE# #TAB# #TAB# rows = [rows] #LINE# #TAB# if left: #LINE# #TAB# #TAB# return [rows[0]] #LINE# #TAB# else: #LINE# #TAB# #TAB# return rows
"#LINE# #TAB# padding = '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# key = snakecase_to_padding(key) #LINE# #TAB# #TAB# if key == 'extra': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif value is True: #LINE# #TAB# #TAB# #TAB# padding +='' #LINE# #TAB# #TAB# elif value is False: #LINE# #TAB# #TAB# #TAB# padding +='' #LINE# #TAB# return padding"
"#LINE# #TAB# outfile = outdir / 'xray_wavelength.tsv' #LINE# #TAB# df.to_csv(outfile, sep='\t', index=False) #LINE# #TAB# return outfile"
#LINE# #TAB# config.LOGGER.info('unpacking object...') #LINE# #TAB# files_to_diff = set() #LINE# #TAB# for file_id in files_to_diff: #LINE# #TAB# #TAB# with tree.lock: #LINE# #TAB# #TAB# #TAB# if file_id not in config.FILES: #LINE# #TAB# #TAB# #TAB# #TAB# config.FILES.add(file_id) #LINE# #TAB# #TAB# #TAB# files_to_diff.add(file_id) #LINE# #TAB# return files_to_diff
#LINE# #TAB# if project_folder == '': #LINE# #TAB# #TAB# project_folder = os.path.expanduser('~') #LINE# #TAB# if project_folder == '.': #LINE# #TAB# #TAB# project_folder = os.path.dirname(os.path.realpath(__file__)) #LINE# #TAB# return project_folder
"#LINE# #TAB# s_start = ts.index[0] #LINE# #TAB# s_end = ts.index[1] #LINE# #TAB# if start < s_end: #LINE# #TAB# #TAB# start = s_end #LINE# #TAB# if end > s_start: #LINE# #TAB# #TAB# end = s_start #LINE# #TAB# pad_length = max(pad, end - s_start) #LINE# #TAB# if pad_length > 0: #LINE# #TAB# #TAB# return ts.pad((pad_length, pad), mode='constant', constant_values=( #LINE# #TAB# #TAB# #TAB# 0, 0, 0))) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ts"
"#LINE# #TAB# global _sampling_rate #LINE# #TAB# if _sampling_rate is None: #LINE# #TAB# #TAB# res = subprocess.run(['gh', 'token'], shell=True, stdout= #LINE# #TAB# #TAB# #TAB# subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# #TAB# _sampling_rate = res.stdout.decode().strip() #LINE# #TAB# return _sampling_rate"
#LINE# #TAB# if Nu > Nc: #LINE# #TAB# #TAB# clust = 0 #LINE# #TAB# elif Nu == Nc: #LINE# #TAB# #TAB# clust = 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# clust = 0 #LINE# #TAB# return clust
"#LINE# #TAB# max_size = powerline.segment_conf('cwd','max_dir_size') #LINE# #TAB# if max_size: #LINE# #TAB# #TAB# return name[:max_size] #LINE# #TAB# return name"
"#LINE# #TAB# if experiment.metadata.raw_payload: #LINE# #TAB# #TAB# return json.dumps(experiment.metadata.raw_payload, sort_keys=True, #LINE# #TAB# #TAB# #TAB# indent=2) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if isinstance(data, pd.DataFrame): #LINE# #TAB# #TAB# y = data.index.get_level_values('time') #LINE# #TAB# elif isinstance(data, pd.Series): #LINE# #TAB# #TAB# y = data.index.get_level_values('time') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError(""detuning_of_monomial must be a pandas DataFrame, not {}"" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB#.format(type(data))) #LINE# #TAB# return y"
"#LINE# #TAB# if isinstance(obj, datetime): #LINE# #TAB# #TAB# obj = obj.isoformat() #LINE# #TAB# return {'date': obj.isoformat(), 'name': obj.get('name'), 'year': obj.get( #LINE# #TAB# #TAB# 'year'),'month': obj.get('month'), 'day': obj.get('day'), #LINE# #TAB# #TAB# 'hour': obj.get('hour'),'minute': obj.get('minute'), #LINE# #TAB# #TAB#'second': obj.get('second'),'microsecond': obj.get('microsecond')}"
"#LINE# #TAB# if os.name == 'posix': #LINE# #TAB# #TAB# data_dir = '/var/run/resnet50' #LINE# #TAB# elif os.name == 'nt': #LINE# #TAB# #TAB# data_dir = '/var/run/resnet50' #LINE# #TAB# elif os.name == 'posix': #LINE# #TAB# #TAB# data_dir = '/var/run/resnet50' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError( #LINE# #TAB# #TAB# #TAB# 'OS %s is not supported by Python %s' % (os.name, #LINE# #TAB# #TAB# #TAB# sys.version_info[0])) #LINE# #TAB# if lock_base_model: #LINE# #TAB# #TAB# os.makedirs(data_dir) #LINE# #TAB# return data_dir"
"#LINE# #TAB# path = '' #LINE# #TAB# for line in data.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if re.match(gdal_regex, line): #LINE# #TAB# #TAB# #TAB# path = line #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if path: #LINE# #TAB# #TAB# return path #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# has_eof = False #LINE# #TAB# if fhpatch.headers.get('Content-Type') == 'application/json': #LINE# #TAB# #TAB# has_eof = True #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if fhpatch.get('Content-Length') == 1: #LINE# #TAB# #TAB# #TAB# #TAB# has_eof = True #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if has_eof: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# print('Getting sequence length...') #LINE# #TAB# sequence_length = input('Enter sequence length: ') #LINE# #TAB# try: #LINE# #TAB# #TAB# sequence_length = int(sequence_length) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('Error: sequence length must be an integer') #LINE# #TAB# #TAB# raise #LINE# #TAB# return sequence_length
"#LINE# #TAB# if not hasattr(cls, '__return_letters_from_string__'): #LINE# #TAB# #TAB# cls.__return_letters_from_string__ = [c.upper() for c in dir(cls #LINE# #TAB# #TAB# #TAB# ) if not c.startswith('_')] #LINE# #TAB# return cls.__return_letters_from_string__"
#LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# with open(path) as f: #LINE# #TAB# #TAB# #TAB# solr_tags = json.load(f) #LINE# #TAB# #TAB# return solr_tags #LINE# #TAB# else: #LINE# #TAB# #TAB# return []
"#LINE# #TAB# #TAB# instances = {} #LINE# #TAB# #TAB# for clazz_name in clazz.__module__.split('.'): #LINE# #TAB# #TAB# #TAB# java_class = importlib.import_module( #LINE# #TAB# #TAB# #TAB# #TAB# '.'.join(clazz_name.split('.')[:-1]), clazz_name) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# instance = java_class() #LINE# #TAB# #TAB# #TAB# #TAB# instances[clazz_name] = instance #LINE# #TAB# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# #TAB# logger.error( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'Failed to import {} instance. Error: {}.'. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# format(clazz_name, e)) #LINE# #TAB# #TAB# return"
"#LINE# #TAB# if isinstance(rate, int): #LINE# #TAB# #TAB# if rate < 50: #LINE# #TAB# #TAB# #TAB# return rate * 10 #LINE# #TAB# #TAB# return rate #LINE# #TAB# if not isinstance(rate, (float, int)): #LINE# #TAB# #TAB# raise TypeError('unsupported rate type: %r' % type(rate)) #LINE# #TAB# return rate * 10"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return job.function_exists() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return job
"#LINE# #TAB# text = re.sub(r'<.*?>', r'\1', text) #LINE# #TAB# text = re.sub(r'<.+?>', r'\1', text) #LINE# #TAB# text = re.sub(r'<.+?>', r'\1', text) #LINE# #TAB# return text"
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for key in obj.keys(): #LINE# #TAB# #TAB# #TAB# obj[key] = ratio_mat(obj[key]) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# obj[item] = [] #LINE# #TAB# #TAB# for val in obj: #LINE# #TAB# #TAB# #TAB# if isinstance(val, str): #LINE# #TAB# #TAB# #TAB# #TAB# val = val.strip() #LINE# #TAB# #TAB# #TAB# #TAB# obj[item].append(val) #LINE# #TAB# return obj"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# prop = getattr(model_cls, prop_name) #LINE# #TAB# #TAB# return isinstance(prop, property) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# try: #LINE# #TAB# #TAB# msg = request.form.get('msg') #LINE# #TAB# #TAB# if msg: #LINE# #TAB# #TAB# #TAB# db.session.delete(msg) #LINE# #TAB# #TAB# #TAB# db.session.commit() #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False
#LINE# #TAB# if context.lock_runtime_dir is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# context.lock_runtime_dir = tempfile.mkdtemp() #LINE# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# context.lock_runtime_dir = None #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with context.lock_runtime_dir: #LINE# #TAB# #TAB# #TAB# #TAB# yield #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# #TAB# context.lock_runtime_dir.close() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass
#LINE# #TAB# new_gmt_datadir = os.environ.get('GMT_DATADIR') #LINE# #TAB# if new_gmt_datadir == old_gmt_datadir: #LINE# #TAB# #TAB# del os.environ['GMT_DATADIR'] #LINE# #TAB# else: #LINE# #TAB# #TAB# os.environ['GMT_DATADIR'] = old_gmt_datadir #LINE# #TAB# return new_gmt_datadir
"#LINE# #TAB# file_list = [] #LINE# #TAB# if os.path.exists(default_config_path): #LINE# #TAB# #TAB# for root, dirs, files in os.walk(default_config_path): #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# if filename.endswith('.json'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# file_list.append(os.path.join(root, filename)) #LINE# #TAB# return file_list"
#LINE# #TAB# if file_name.endswith('.jpg'): #LINE# #TAB# #TAB# return Image.open(file_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# if file_name.endswith('.png'): #LINE# #TAB# #TAB# #TAB# return Image.open(file_name) #LINE# #TAB# #TAB# return None
"#LINE# #TAB# wide_char = None #LINE# #TAB# if cluster: #LINE# #TAB# #TAB# wide_char = Cluster.get_by_name(context, cluster) #LINE# #TAB# #TAB# if show_progress: #LINE# #TAB# #TAB# #TAB# show_progress(context, ""wide_char: %s"" % wide_char) #LINE# #TAB# #TAB# return wide_char #LINE# #TAB# return cluster"
#LINE# #TAB# encoded = {} #LINE# #TAB# for key in to_encode: #LINE# #TAB# #TAB# if to_encode[key] == 'name': #LINE# #TAB# #TAB# #TAB# name = to_encode[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# name = str(to_encode[key]) #LINE# #TAB# #TAB# encoded[name] = name #LINE# #TAB# return encoded
"#LINE# #TAB# df = pd.DataFrame(x) #LINE# #TAB# for i in range(num_blocks): #LINE# #TAB# #TAB# l = int(np.ceil(num_filters * i / num_blocks)) #LINE# #TAB# #TAB# df = add_conv2d(df, l, stride=2) #LINE# #TAB# #TAB# df = add_conv2d(df, l, stride=2) #LINE# #TAB# #TAB# df = add_conv2d(df, l, stride=2) #LINE# #TAB# return df"
"#LINE# #TAB# e1 = compute_epicyclic_gearing(c) #LINE# #TAB# e2 = compute_annulus_planet(c) #LINE# #TAB# e3 = compute_geodesicyclic_gearing(c) #LINE# #TAB# e4 = compute_annulus_planet(c) #LINE# #TAB# e5 = compute_distance_matrix(e1, e2) #LINE# #TAB# e6 = compute_distance_matrix(e3, e4) #LINE# #TAB# e7 = compute_distance_matrix(e5, e6) #LINE# #TAB# return e1, e2, e3, e4, e5, e6"
#LINE# #TAB# coordinates = deepcopy(frac_coordinates) #LINE# #TAB# for coord in range(coordinates.shape[0]): #LINE# #TAB# #TAB# coordinates[coord] = time(coordinates[coord]) #LINE# #TAB# return coordinates
"#LINE# #TAB# if ccd == 'aperture': #LINE# #TAB# #TAB# return None #LINE# #TAB# if prefix is None: #LINE# #TAB# #TAB# prefix = b'' #LINE# #TAB# expnum = str(expnum) #LINE# #TAB# ccd = str(ccd) #LINE# #TAB# if version == 'p': #LINE# #TAB# #TAB# aperture = detect_p_aperture(expnum, ccd, version, prefix=prefix) #LINE# #TAB# elif version == 'n': #LINE# #TAB# #TAB# aperture = detect_n_aperture(expnum, ccd, version, prefix=prefix) #LINE# #TAB# else: #LINE# #TAB# #TAB# aperture = detect_aperture(expnum, ccd, version, prefix) #LINE# #TAB# return aperture"
#LINE# #TAB# result = [] #LINE# #TAB# for i in range(len(stack)): #LINE# #TAB# #TAB# for j in range(len(stack)): #LINE# #TAB# #TAB# #TAB# result.append(custom_prefix_strict(stack[i])) #LINE# #TAB# return result
"#LINE# #TAB# if s.startswith(start): #LINE# #TAB# #TAB# raise ValueError('Value {0} not prefixed with {1}'.format(s, start) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# mass = 1 #LINE# #TAB# while s.startswith(start): #LINE# #TAB# #TAB# mass += 1 #LINE# #TAB# #TAB# s = s[len(start):] #LINE# #TAB# return mass"
"#LINE# #TAB# if not isinstance(value, (int, float)): #LINE# #TAB# #TAB# raise TypeError(""Glyph left margin must be an :ref:`type-int-float`, not "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""%s."" % type(value).__name__) #LINE# #TAB# return value / 10.0"
"#LINE# #TAB# if counts.sum() <= n: #LINE# #TAB# #TAB# return counts #LINE# #TAB# cumsum = np.cumsum(counts, dtype=dtype) #LINE# #TAB# nz = counts.nonzero()[0] #LINE# #TAB# new_counts = cumsum[nz] #LINE# #TAB# return new_counts"
#LINE# #TAB# result = {} #LINE# #TAB# for key in d: #LINE# #TAB# #TAB# result[key] = d[key] #LINE# #TAB# return result
"#LINE# #TAB# if isinstance(o, type(None)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return o.to_json() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return o"
"#LINE# #TAB# method = request.method.lower() #LINE# #TAB# if method == 'GET': #LINE# #TAB# #TAB# return request.data.get('next', '') #LINE# #TAB# if method == 'POST': #LINE# #TAB# #TAB# request.data.remove('next') #LINE# #TAB# #TAB# return request.data.get('next', '') #LINE# #TAB# try: #LINE# #TAB# #TAB# gpio_command = request.method #LINE# #TAB# #TAB# if gpio_command == 'gpio': #LINE# #TAB# #TAB# #TAB# return request.data.get('next', '') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return request.data.get('next', '') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return ''"
#LINE# #TAB# for s in structures: #LINE# #TAB# #TAB# s.lattice.assert_same_lattice() #LINE# #TAB# return structures
#LINE# #TAB# hasher = hashlib.sha1() #LINE# #TAB# hasher.update(message) #LINE# #TAB# if '\n' in hasher.hexdigest(): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if month > 13: #LINE# #TAB# #TAB# raise ValueError('Incorrect month index') #LINE# #TAB# if month in (IYYAR, TAMMUZ, ELUL, TEVETH, VEADAR): #LINE# #TAB# #TAB# return 29 #LINE# #TAB# if month == ADAR and not leap(year): #LINE# #TAB# #TAB# return 29 #LINE# #TAB# if month == HESHVAN and (year_days(year) % 10)!= 5: #LINE# #TAB# #TAB# return 29 #LINE# #TAB# return 30"
#LINE# #TAB# if url.startswith('__') and url.endswith('__'): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif url.startswith('file://'): #LINE# #TAB# #TAB# return is_up_voted_by_file(url) #LINE# #TAB# return False
#LINE# #TAB# for i in range(len(l)): #LINE# #TAB# #TAB# if i in l: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if i % 2 == 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# for channel_type in fitted_params: #LINE# #TAB# #TAB# control_board.channel_types[channel_type] = fitted_params[channel_type]
#LINE# #TAB# measurements = {} #LINE# #TAB# for country in instance.countries.all(): #LINE# #TAB# #TAB# deaths = instance.get_deaths(country) #LINE# #TAB# #TAB# measurements[country] = {'deaths': deaths} #LINE# #TAB# return measurements
#LINE# #TAB# json_data = {} #LINE# #TAB# for child in tree.iter(): #LINE# #TAB# #TAB# if child.tag == 'json': #LINE# #TAB# #TAB# #TAB# json_data.update(json_reader(child)) #LINE# #TAB# #TAB# elif child.tag =='match': #LINE# #TAB# #TAB# #TAB# text = child.text #LINE# #TAB# #TAB# #TAB# json_data.update(json_reader(child)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# text = child.text #LINE# #TAB# return json_data
#LINE# #TAB# styles = get_lines_in_text(search_string) #LINE# #TAB# if not styles: #LINE# #TAB# #TAB# return [] #LINE# #TAB# new_styles = [] #LINE# #TAB# for style in styles: #LINE# #TAB# #TAB# new_styles.append(style + replacement_line) #LINE# #TAB# return new_styles
#LINE# #TAB# nextp = p.parent #LINE# #TAB# while nextp is not None: #LINE# #TAB# #TAB# if p.isTerminal(): #LINE# #TAB# #TAB# #TAB# nextp = nextp.parent #LINE# #TAB# #TAB# fn = p.FN() #LINE# #TAB# #TAB# if fn.hasChildNodes(): #LINE# #TAB# #TAB# #TAB# return fn #LINE# #TAB# #TAB# nextp = p.parent #LINE# #TAB# return None
#LINE# #TAB# if align < 0: #LINE# #TAB# #TAB# return align - 1 #LINE# #TAB# if base & 1 == 0: #LINE# #TAB# #TAB# return align - 1 #LINE# #TAB# return 0
#LINE# #TAB# unassigned = [] #LINE# #TAB# for analysis in analysis_request.getAnalyses(): #LINE# #TAB# #TAB# analysis = api.get_object(analysis) #LINE# #TAB# #TAB# if api.get_workflow_status_of(analysis) == 'unassigned': #LINE# #TAB# #TAB# #TAB# assigned.append(analysis) #LINE# #TAB# #TAB# elif api.get_workflow_status_of(analysis) == 'assigned': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if host is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if host == 'localhost': #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if isinstance(arg, list): #LINE# #TAB# #TAB# lambd = [arg] #LINE# #TAB# elif isinstance(arg, str): #LINE# #TAB# #TAB# lambd = lookup_dict[arg] #LINE# #TAB# elif isinstance(arg, alert.Alert): #LINE# #TAB# #TAB# lambd = alert.alert.id #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('invalid backend identifier') #LINE# #TAB# return lambd"
"#LINE# #TAB# sql = ( #LINE# #TAB# #TAB# ""SELECT name FROM %s WHERE type='table' AND name='{tbl}'"" #LINE# #TAB# #TAB# ) #LINE# #TAB# result = connection.execute(sql, db=db, tbl=tbl, index=index) #LINE# #TAB# fields = list() #LINE# #TAB# for row in result: #LINE# #TAB# #TAB# fields.append(row[0]) #LINE# #TAB# return fields"
#LINE# #TAB# file_type = None #LINE# #TAB# basename = os.path.basename(path) #LINE# #TAB# if os.path.isdir(basename): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(basename) #LINE# #TAB# #TAB# except OSError as exc: #LINE# #TAB# #TAB# #TAB# if exc.errno == errno.EEXIST: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# elif exc.errno == errno.EISDIR: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return file_type
#LINE# #TAB# project_repository_handler = get_repository_handler(project_element) #LINE# #TAB# if project_repository_handler and project_repository_handler.is_cancelled: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# bool=False) ->Dict[str, str]: #LINE# #TAB# if is_msc: #LINE# #TAB# #TAB# table_desc = read_msc_table(table_code, table_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# table_desc = read_table(table_code, table_name) #LINE# #TAB# tag_desc = dict() #LINE# #TAB# for key in table_desc.keys(): #LINE# #TAB# #TAB# if key == 1: #LINE# #TAB# #TAB# #TAB# tag_desc[key] = 1.0 #LINE# #TAB# return tag_desc"
#LINE# #TAB# global script_list #LINE# #TAB# recipes = node.run_list #LINE# #TAB# if not recipes: #LINE# #TAB# #TAB# return {} #LINE# #TAB# for script in recipes: #LINE# #TAB# #TAB# name = script[0].split(':')[0] #LINE# #TAB# #TAB# if name not in script_list: #LINE# #TAB# #TAB# #TAB# script_list.append(name) #LINE# #TAB# return script_list
"#LINE# #TAB# base, ext = os.path.splitext(name) #LINE# #TAB# return {'base': base, 'ext': ext}"
"#LINE# #TAB# mean = [0.485, 0.456, 0.406] #LINE# #TAB# std = [0.229, 0.224, 0.225] #LINE# #TAB# image = image.convert('RGB') #LINE# #TAB# arr = np.array(image) #LINE# #TAB# im = crop_and_resize(arr, 224) #LINE# #TAB# arr = arr / 255.0 #LINE# #TAB# arr[..., 0] -= mean[0] #LINE# #TAB# arr[..., 1] -= mean[1] #LINE# #TAB# arr[..., 2] -= mean[2] #LINE# #TAB# arr[..., 0] /= std[0] #LINE# #TAB# arr[..., 1] /= std[1] #LINE# #TAB# return arr"
#LINE# #TAB# ws = load_workbook(template_file) #LINE# #TAB# ws.load() #LINE# #TAB# return ws
"#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# return os.path.abspath(os.path.join(os.path.expanduser('~'), '.config')) #LINE# #TAB# return os.path.sep!= '/'"
#LINE# #TAB# pieces = line.split('\t') #LINE# #TAB# if len(pieces) < 2: #LINE# #TAB# #TAB# return #LINE# #TAB# prop_dict[pieces[1]] = pieces[0]
#LINE# #TAB# if ':' in s: #LINE# #TAB# #TAB# ts = str(s).split(':') #LINE# #TAB# #TAB# ts = [int(ts[0]) for ts in ts] #LINE# #TAB# #TAB# return int(ts[1]) if len(ts) > 1 else 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
#LINE# #TAB# try: #LINE# #TAB# #TAB# num = int(os.environ['ANA_NUM']) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# num = 0 #LINE# #TAB# #TAB# os.environ['ANA_NUM'] = num #LINE# #TAB# return num
"#LINE# #TAB# receivers = [] #LINE# #TAB# for attr in dir(cls): #LINE# #TAB# #TAB# if attr.startswith('_'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# getattr(cls, attr) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if not hasattr(cls, attr) or getattr(cls, attr #LINE# #TAB# #TAB# #TAB# #TAB# ) == attr: #LINE# #TAB# #TAB# #TAB# #TAB# receivers.append(attr) #LINE# #TAB# return receivers"
#LINE# #TAB# conn = sqlite3.connect(database_name) #LINE# #TAB# curs = conn.cursor() #LINE# #TAB# curs.execute('SELECT * from devices;') #LINE# #TAB# references = curs.fetchone() #LINE# #TAB# conn.close() #LINE# #TAB# if references is None: #LINE# #TAB# #TAB# return #LINE# #TAB# new_devices = [] #LINE# #TAB# for device in references: #LINE# #TAB# #TAB# d = json.loads(device) #LINE# #TAB# #TAB# for key in new_devices: #LINE# #TAB# #TAB# #TAB# if search_string in d[key]: #LINE# #TAB# #TAB# #TAB# #TAB# references.append(device) #LINE# #TAB# conn.commit() #LINE# #TAB# conn.close() #LINE# #TAB# return new_devices
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for k, v in obj.items(): #LINE# #TAB# #TAB# #TAB# obj[k] = merge_unkown_celltypes(v) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for i, v in enumerate(obj): #LINE# #TAB# #TAB# #TAB# obj[i] = merge_unkown_celltypes(v) #LINE# #TAB# elif isinstance(obj, datetime): #LINE# #TAB# #TAB# return obj.dtend #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj.duration"
#LINE# #TAB# res = {} #LINE# #TAB# for typalias in typealiases: #LINE# #TAB# #TAB# typ = TypeAlias.from_string(typalias) #LINE# #TAB# #TAB# if typ.optional: #LINE# #TAB# #TAB# #TAB# typ = typ.optional #LINE# #TAB# #TAB# res[typ.alias] = typ #LINE# #TAB# return res
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = getattr(settings, section, name) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# #TAB# return value"
"#LINE# #TAB# choices = {'q': ''} #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# choice = input('{0} [{1}]: '.format(state.selected_option, state. #LINE# #TAB# #TAB# #TAB# #TAB# choices[state.selected_option])) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# choice = '' #LINE# #TAB# #TAB# if choice not in choices: #LINE# #TAB# #TAB# #TAB# choices.append(choice) #LINE# #TAB# #TAB# choice = input('{0} [{1}]: '.format(state.selected_option, state.choices[ #LINE# #TAB# #TAB# #TAB# choice])) #LINE# #TAB# return choices"
"#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# if not logger.hasHandlers(): #LINE# #TAB# #TAB# path = os.path.join(os.path.dirname(__file__), 'log') #LINE# #TAB# #TAB# logger.addHandler(logging.NullHandler()) #LINE# #TAB# else: #LINE# #TAB# #TAB# path = os.path.join(os.getcwd(), 'log') #LINE# #TAB# #TAB# logger.addHandler(logging.NullHandler()) #LINE# #TAB# return logger"
#LINE# #TAB# try: #LINE# #TAB# #TAB# child.comment = alias_map[child.comment] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# return
"#LINE# #TAB# sort_dict = {} #LINE# #TAB# for key in list(request_parameters.keys()): #LINE# #TAB# #TAB# new_key = request_parameters[key] #LINE# #TAB# #TAB# if new_key in ['submit_date','submit_datetime']: #LINE# #TAB# #TAB# #TAB# sort_dict[new_key] = request_parameters[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sort_dict[new_key] = request_parameters[key] #LINE# #TAB# return sort_dict"
"#LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# header_length = struct.unpack('!L', f.read(4))[0] #LINE# #TAB# return header_length"
"#LINE# #TAB# filename = 'experiments_%s_%s.png' % (exp_ind[0], exp_ind[-1]) #LINE# #TAB# with open(filename, 'w') as f: #LINE# #TAB# #TAB# f.write(swim_speed) #LINE# #TAB# return filename"
"#LINE# #TAB# #TAB# cert = cls(token=api_token, id=cert_id) #LINE# #TAB# #TAB# cert.load() #LINE# #TAB# #TAB# return cert"
"#LINE# #TAB# is_stake = os.path.isfile(path) #LINE# #TAB# if not is_stake: #LINE# #TAB# #TAB# dirname = os.path.dirname(path) #LINE# #TAB# #TAB# is_stake = os.path.isfile(os.path.join(dirname, os.pardir, path)) #LINE# #TAB# if not is_stake: #LINE# #TAB# #TAB# is_stake = False #LINE# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# return is_stake"
"#LINE# #TAB# if isinstance(txt, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(txt) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# elif txt == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return txt"
"#LINE# #TAB# if isinstance(value, NoneValue): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif hasattr(type, '__fields__'): #LINE# #TAB# #TAB# fields = type.__fields__ #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# for field in fields: #LINE# #TAB# #TAB# if hasattr(value, field['name']): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# time_step = 0.5 #LINE# #TAB# global log_status #LINE# #TAB# log_status = None #LINE# #TAB# global global_log = [] #LINE# #TAB# global_log = {} #LINE# #TAB# global_log = {} #LINE# #TAB# time_step = 2 #LINE# #TAB# global_log = {} #LINE# #TAB# global_log = {} #LINE# #TAB# logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s - %(message)s' #LINE# #TAB# #TAB#, **global_log) #LINE# #TAB# global_log = {} #LINE# #TAB# global_log = {} #LINE# #TAB# global_log.clear() #LINE# #TAB# global_log = {} #LINE# #TAB# global_log = {} #LINE# #TAB# global_log.clear() #LINE# #TAB# return"
"#LINE# #TAB# for ind, val in enumerate(indir): #LINE# #TAB# #TAB# full_path = os.path.join(ind + 1, val) #LINE# #TAB# #TAB# if os.path.isfile(full_path): #LINE# #TAB# #TAB# #TAB# return full_path"
"#LINE# #TAB# existing_path = os.path.join(os.getcwd(), RESOURCE_GROUP_NAME) #LINE# #TAB# if path is None or path in existing_path: #LINE# #TAB# #TAB# return RESOURCE_GROUP_NAME #LINE# #TAB# else: #LINE# #TAB# #TAB# new_path = os.path.join(path, RESOURCE_GROUP_NAME) #LINE# #TAB# #TAB# if not os.path.exists(new_path): #LINE# #TAB# #TAB# #TAB# os.makedirs(new_path) #LINE# #TAB# return new_path"
"#LINE# #TAB# while True: #LINE# #TAB# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# s.bind((start_port, 0)) #LINE# #TAB# #TAB# #TAB# return s.getsockname()[1] #LINE# #TAB# #TAB# except socket.error: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# start_port += 1"
"#LINE# #TAB# old_mtime = None #LINE# #TAB# mtime = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file_path, 'r+') as fd: #LINE# #TAB# #TAB# #TAB# old_mtime = os.stat(fd.fileno()).st_mtime #LINE# #TAB# #TAB# #TAB# mtime = os.stat(fd.fileno()).st_mtime #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# log.warning('Could not find file: {}'.format(file_path)) #LINE# #TAB# return old_mtime, mtime"
#LINE# #TAB# if name.startswith('http'): #LINE# #TAB# #TAB# return False #LINE# #TAB# if name.endswith('.html'): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# config = configparser.ConfigParser() #LINE# #TAB# config.read(DEFAULT_CONFIG) #LINE# #TAB# if ipython_dir is not None: #LINE# #TAB# #TAB# config.read(ipython_dir) #LINE# #TAB# fix_python(config) #LINE# #TAB# return
#LINE# #TAB# if 'transient' in options: #LINE# #TAB# #TAB# return options['transient'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return options
"#LINE# #TAB# items = os.listdir(os.getcwd()) #LINE# #TAB# d = [] #LINE# #TAB# for item in items: #LINE# #TAB# #TAB# f = item.split('/') #LINE# #TAB# #TAB# if len(f) == 2: #LINE# #TAB# #TAB# #TAB# d.append(f[1]) #LINE# #TAB# #TAB# elif len(f) == 3: #LINE# #TAB# #TAB# #TAB# d.append(f[2]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d.append(f) #LINE# #TAB# df = pd.DataFrame(d, columns=['x', 'y', 'z']) #LINE# #TAB# df.columns = ['x', 'y', 'z', 'd'] #LINE# #TAB# return df"
#LINE# #TAB# new_record = SeqRecord(record.seq) #LINE# #TAB# new_record.start = record.start + n_bases #LINE# #TAB# return new_record
"#LINE# #TAB# mask = zeros_like(points) #LINE# #TAB# if priority is not None: #LINE# #TAB# #TAB# threshold = priority #LINE# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# mask[i] = convert_asset_timestamp_field(point, radius, threshold) #LINE# #TAB# else: #LINE# #TAB# #TAB# for i, point in enumerate(points): #LINE# #TAB# #TAB# #TAB# mask[i] = convert_asset_timestamp_field(point, radius, priority) #LINE# #TAB# return mask"
#LINE# #TAB# key = 'Image_Name:' #LINE# #TAB# if inst_img_id in cache: #LINE# #TAB# #TAB# aminame = cache[inst_img_id] #LINE# #TAB# else: #LINE# #TAB# #TAB# aminame = 'unknown_image_id:%s' % inst_img_id #LINE# #TAB# return aminame
#LINE# #TAB# try: #LINE# #TAB# #TAB# int_response = int(response) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return response #LINE# #TAB# error_code = int_response.split(':')[1] #LINE# #TAB# if error_code == -1: #LINE# #TAB# #TAB# return 'Command error' #LINE# #TAB# return error_code
"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_ports = ast.walk(node) #LINE# #TAB# except ast.ParseError: #LINE# #TAB# #TAB# return None #LINE# #TAB# for module_port, function in module_ports.items(): #LINE# #TAB# #TAB# if isinstance(function, ast.Module): #LINE# #TAB# #TAB# #TAB# return getattr(module_port, function.name) #LINE# #TAB# return None"
"#LINE# #TAB# return { #LINE# #TAB# #TAB# route.name: dict( #LINE# #TAB# #TAB# #TAB# [( #LINE# #TAB# #TAB# #TAB# #TAB# blueprint.name, #LINE# #TAB# #TAB# #TAB# #TAB# (blueprint.path.startswith(base_path)), #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# for blueprint in app.blueprints #LINE# #TAB# #TAB# ] #LINE# #TAB# }"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
#LINE# #TAB# all_resource_managers = ResourceManager.objects.all() #LINE# #TAB# if admin_required is None or not admin_required: #LINE# #TAB# #TAB# return all_resource_managers #LINE# #TAB# return admin_resource_managers
"#LINE# #TAB# module_name = 'forms.widgets' #LINE# #TAB# try: #LINE# #TAB# #TAB# module_name += '.DateTimeBaseInput' #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# if test: #LINE# #TAB# #TAB# #TAB# raise ImproperlyConfigured( #LINE# #TAB# #TAB# #TAB# #TAB# ""django.forms.widgets is not available on this system."") #LINE# #TAB# #TAB# module_name += '.DateTimeBaseInput' #LINE# #TAB# module = import_module(module_name) #LINE# #TAB# return module"
"#LINE# #TAB# out_files = [] #LINE# #TAB# for in_file in in_files: #LINE# #TAB# #TAB# if 'ipacharser' in metadata: #LINE# #TAB# #TAB# #TAB# glob_pattern = metadata['ipacharser']['glob'] #LINE# #TAB# #TAB# #TAB# matches = fnmatch.filter(in_file, glob_pattern) #LINE# #TAB# #TAB# #TAB# if matches: #LINE# #TAB# #TAB# #TAB# #TAB# out_files.append((in_file, True)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# out_files.append((in_file, False)) #LINE# #TAB# return out_files"
"#LINE# #TAB# conn = sqlite3.connect(os.environ['database_url'], detect_types=sqlite3. #LINE# #TAB# #TAB# PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES) #LINE# #TAB# c = conn.cursor() #LINE# #TAB# c.execute('select key, value from database') #LINE# #TAB# json_object = c.fetchall() #LINE# #TAB# result = [] #LINE# #TAB# for x in json_object: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# json.loads(x[0]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# print('{} is not a valid GTFS database'.format(x[0])) #LINE# #TAB# #TAB# #TAB# result.append([x[0], x[1], x[2]]) #LINE# #TAB# conn.close() #LINE# #TAB# return result"
"#LINE# #TAB# if not raw_annotations: #LINE# #TAB# #TAB# return {} #LINE# #TAB# result = {} #LINE# #TAB# for name, value in raw_annotations.items(): #LINE# #TAB# #TAB# if name =='return': #LINE# #TAB# #TAB# #TAB# result[name] = value #LINE# #TAB# #TAB# elif isinstance(value, AnyType): #LINE# #TAB# #TAB# #TAB# result[name] = escape_attribute_value(value, module_name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[name] = value #LINE# #TAB# return result"
#LINE# #TAB# if not rule: #LINE# #TAB# #TAB# return None #LINE# #TAB# args = {} #LINE# #TAB# for arg in rule: #LINE# #TAB# #TAB# if arg.startswith('--'): #LINE# #TAB# #TAB# #TAB# if arg.endswith('--'): #LINE# #TAB# #TAB# #TAB# #TAB# arg = arg[:-1] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# arg = arg[:-1] #LINE# #TAB# #TAB# args[arg] = rule[arg] #LINE# #TAB# if args: #LINE# #TAB# #TAB# return {name: args} #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# git_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), '.git') #LINE# #TAB# if os.path.isdir(git_dir): #LINE# #TAB# #TAB# return {'git_dir': git_dir, 'git_dir': git_dir} #LINE# #TAB# repo_root = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__)), #LINE# #TAB# #TAB#'master')) #LINE# #TAB# return {'git_dir': repo_root, 'git_dir': repo_root}"
"#LINE# #TAB# for testcase_root, _, files in os.walk(testcases_root): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# if filename.endswith("".py""): #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(testcases_root, testcase_root, filename) #LINE# #TAB# #TAB# #TAB# #TAB# testcase_bytes_index = os.path.getsize(os.path.join(svn_dir, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# filename)) #LINE# #TAB# #TAB# #TAB# #TAB# yield testcase_bytes_index, filename"
"#LINE# #TAB# if season not in mds_ops_df_cache: #LINE# #TAB# #TAB# data = get_mds_ops_df(season) #LINE# #TAB# #TAB# mds_ops_df_cache[season] = pd.read_csv(data, sep='\t') #LINE# #TAB# return mds_ops_df_cache[season]"
"#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# return value #LINE# #TAB# for input_name in input_names: #LINE# #TAB# #TAB# input_value = get_child_value(input_name, value) #LINE# #TAB# #TAB# if input_value is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# new_value = {} #LINE# #TAB# #TAB# for input_value in input_value: #LINE# #TAB# #TAB# #TAB# new_value[input_name] = get_child_value(input_value, input_name) #LINE# #TAB# #TAB# return new_value #LINE# #TAB# return None"
"#LINE# #TAB# log_events = mod.get_log_events() #LINE# #TAB# try: #LINE# #TAB# #TAB# events_docstring = mod.get_docstring() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return {} #LINE# #TAB# now = datetime.now().strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# new_events = {} #LINE# #TAB# for event_name, event_dict in log_events.items(): #LINE# #TAB# #TAB# event_name = normalize_event_name(event_name) #LINE# #TAB# #TAB# new_events[event_name] = make_mod_event_change(mod, event_dict, #LINE# #TAB# #TAB# #TAB# event_name, now) #LINE# #TAB# return {'new_events': new_events,'modded_events': new_events}"
"#LINE# #TAB# result = boxed_type(data) #LINE# #TAB# if not result: #LINE# #TAB# #TAB# return {} #LINE# #TAB# template_data = {} #LINE# #TAB# for key, value in result.items(): #LINE# #TAB# #TAB# template_data[key] = _template_value(value, boxed_type) #LINE# #TAB# return template_data"
#LINE# #TAB# rule_list = list() #LINE# #TAB# seen = set() #LINE# #TAB# for repo in repo_list: #LINE# #TAB# #TAB# if repo.name in seen: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# seen.add(repo.name) #LINE# #TAB# #TAB# rule_list.append(repo) #LINE# #TAB# return rule_list
"#LINE# #TAB# count = re.search('(.*?)\\n', text).group(0) + 1 #LINE# #TAB# try: #LINE# #TAB# #TAB# count = int(re.search('(.*?)\\n', text).group(0)) #LINE# #TAB# except: #LINE# #TAB# #TAB# count = 0 #LINE# #TAB# return count"
"#LINE# #TAB# property = vicon.properties.get(name,'subject') #LINE# #TAB# if not property: #LINE# #TAB# #TAB# return None #LINE# #TAB# docs = property.get('docs') #LINE# #TAB# if not docs: #LINE# #TAB# #TAB# return None #LINE# #TAB# if len(docs) == 1: #LINE# #TAB# #TAB# return docs[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if not isinstance(a, np.ndarray): #LINE# #TAB# #TAB# raise TypeError('Expected input array type.') #LINE# #TAB# if a.dtype!= dtype: #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# f""Input array type '{a.dtype}' is not of type '{dtype}'."") #LINE# #TAB# return a"
"#LINE# #TAB# if isinstance(mem_size_str, str): #LINE# #TAB# #TAB# size = int(mem_size_str) #LINE# #TAB# elif isinstance(mem_size_str, int) and reserve_time is not None: #LINE# #TAB# #TAB# size = float(reserve_time) #LINE# #TAB# else: #LINE# #TAB# #TAB# size = int(mem_size_str) #LINE# #TAB# return size"
"#LINE# #TAB# ns = pd.read_csv(fn, sep='\t') #LINE# #TAB# cols = range(ns.shape[0] + 1) #LINE# #TAB# cidrs = [] #LINE# #TAB# for col in range(ns.shape[0]): #LINE# #TAB# #TAB# for i in range(ns.shape[1]): #LINE# #TAB# #TAB# #TAB# if i in cols: #LINE# #TAB# #TAB# #TAB# #TAB# cidrs.append(str(i)) #LINE# #TAB# if usecols is not None: #LINE# #TAB# #TAB# cidrs = set(cidrs) #LINE# #TAB# return cidrs"
#LINE# #TAB# if not cloud.startswith('gat'): #LINE# #TAB# #TAB# return cloud #LINE# #TAB# cloud = cloud[3:] #LINE# #TAB# size = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# memory = os.stat(cloud).st_size #LINE# #TAB# #TAB# if memory < 1024: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# size += float(memory) / 1024 #LINE# #TAB# if size > 1024: #LINE# #TAB# #TAB# for i in range(size - 1): #LINE# #TAB# #TAB# #TAB# if memory > 1024: #LINE# #TAB# #TAB# #TAB# #TAB# return cloud #LINE# #TAB# #TAB# cloud = cloud[:-i] #LINE# #TAB# return cloud
"#LINE# #TAB# t = {} #LINE# #TAB# for k, v in mapping.items(): #LINE# #TAB# #TAB# t[k] = set(v) #LINE# #TAB# return t"
#LINE# #TAB# idx = 0 #LINE# #TAB# while idx < len(s): #LINE# #TAB# #TAB# if s[idx] == char: #LINE# #TAB# #TAB# #TAB# return idx #LINE# #TAB# #TAB# idx += 1 #LINE# #TAB# return -1
"#LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# px4_state = DCM_State(ATT.Roll, ATT.Pitch, ATT.Yaw) #LINE# #TAB# px4_state.update(IMU) #LINE# #TAB# return px4_state"
"#LINE# #TAB# if hasattr(sys, 'getpwuid'): #LINE# #TAB# #TAB# pwuid = sys.getpwuid() #LINE# #TAB# #TAB# if not pwuid: #LINE# #TAB# #TAB# #TAB# return '/bin/sh' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return'sh' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '/bin/sh'"
#LINE# #TAB# vec_magnitude = {} #LINE# #TAB# for gateway in vpc.vpcs.values(): #LINE# #TAB# #TAB# name = gateway.name #LINE# #TAB# #TAB# if name not in vec_magnitude: #LINE# #TAB# #TAB# #TAB# vec_magnitude[name] = 0 #LINE# #TAB# #TAB# vec_magnitude[name] += 1 #LINE# #TAB# return vec_magnitude
"#LINE# #TAB# if k not in o: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# return [cast_arr_to_float(o, n) for n in v] #LINE# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# return {k: cast_arr_to_float(o[k], n) for k, n in v.items()} #LINE# #TAB# return v"
"#LINE# #TAB# prefix = QLabel() #LINE# #TAB# prefix.setPixmap(QPixmap(name, default)) #LINE# #TAB# return prefix"
"#LINE# #TAB# if p.has_field('fibonacci'): #LINE# #TAB# #TAB# f = item.font(0) #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# #TAB# item.setFont(0, f) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# filename = os.path.splitext(path)[0] #LINE# #TAB# filename = os.path.join(os.path.dirname(filename), ext) #LINE# #TAB# logger = logging.getLogger(filename) #LINE# #TAB# if hasattr(obj, 'iteritems'): #LINE# #TAB# #TAB# iterator = obj.iteritems() #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# iterator = obj.items() #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# logger.addHandler(logging.StreamHandler(item)) #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.addHandler(obj) #LINE# #TAB# return logger"
"#LINE# #TAB# if isinstance(x, tuple): #LINE# #TAB# #TAB# return x #LINE# #TAB# else: #LINE# #TAB# #TAB# return [config_from_environment(t) for t in x]"
"#LINE# #TAB# if textfile!= 'trip.txt': #LINE# #TAB# #TAB# raise ValueError('{} is not a proper text file'.format(textfile)) #LINE# #TAB# df = pd.read_csv(os.path.join(textfile_path, textfile), dtype={ #LINE# #TAB# #TAB#'service_id': object}, low_memory=False) #LINE# #TAB# if len(df) == 0: #LINE# #TAB# #TAB# raise ValueError('{} has no records'.format(os.path.join( #LINE# #TAB# #TAB# #TAB# textfile_path, textfile))) #LINE# #TAB# df['service_id'] = df['service_id'].astype(object) #LINE# #TAB# df.rename(columns={'service_id': 'trip'}, inplace=True) #LINE# #TAB# return df"
"#LINE# #TAB# git_dir = join(expanduser('~'), '.git') #LINE# #TAB# if isfile(git_dir): #LINE# #TAB# #TAB# return join(git_dir, 'templates') #LINE# #TAB# git_dir = join(expanduser('~'), '.git') #LINE# #TAB# if isfile(git_dir): #LINE# #TAB# #TAB# return join(git_dir, 'templates') #LINE# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# ipv4_hostname = cls.__name__ #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ipv4_hostname = cls.__bases__[0].get_ip_address() #LINE# #TAB# #TAB# except (AttributeError, IndexError): #LINE# #TAB# #TAB# #TAB# ipv4_hostname = None #LINE# #TAB# if ipv4_hostname is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ipv4_hostname = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# #TAB# except (AttributeError, UnicodeEncodeError): #LINE# #TAB# #TAB# #TAB# ipv4_hostname = None #LINE# #TAB# return ipv4_hostname"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return node.registration_version #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return 0
#LINE# #TAB# plugin_data = dashboard_entry.plugin_data.copy() #LINE# #TAB# if request: #LINE# #TAB# #TAB# plugin_data.update(request) #LINE# #TAB# return plugin_data
"#LINE# #TAB# file_handle = fits.open(extname) #LINE# #TAB# try: #LINE# #TAB# #TAB# params = file_handle[0].header #LINE# #TAB# #TAB# coords = [float(x) for x in params['coords']] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# print('File does not exist.') #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# return coords, params"
"#LINE# #TAB# theta = 0.12 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"#LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_card_server_url(), school_id) #LINE# #TAB# resp = do_get_request(url) #LINE# #TAB# columns_dict = resp.json() #LINE# #TAB# return columns_dict"
"#LINE# #TAB# blueprint = app.copy_template( #LINE# #TAB# #TAB# 'loan_item_sequences/remove-sequences.html', #LINE# #TAB# #TAB# template_name='loan_item_sequences.html') #LINE# #TAB# blueprint.template_name ='remove-sequences.html' #LINE# #TAB# return blueprint"
"#LINE# #TAB# transitions = {} #LINE# #TAB# for k, v in ns.items(): #LINE# #TAB# #TAB# if isinstance(v, numbers.Number): #LINE# #TAB# #TAB# #TAB# transitions[k] = v #LINE# #TAB# #TAB# elif isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# get_index_transitions(ns, v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# transitions[k] = v #LINE# #TAB# return transitions"
#LINE# #TAB# title = str(obj) #LINE# #TAB# if title.startswith('['): #LINE# #TAB# #TAB# title = title[1:] #LINE# #TAB# if title.endswith(']'): #LINE# #TAB# #TAB# title = title[:-1] #LINE# #TAB# if title.endswith(']'): #LINE# #TAB# #TAB# title = title[:-1] #LINE# #TAB# return title
"#LINE# #TAB# try: #LINE# #TAB# #TAB# score = score_hmm_poisson(config, exception=exception) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# if exception is not None: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# return score"
#LINE# #TAB# first_nick = input_dict.pop('first_nick') #LINE# #TAB# input_dict['nick'] = input_dict.pop('nick') #LINE# #TAB# input_dict['year'] = input_dict.pop('year') #LINE# #TAB# input_dict['month'] = input_dict.pop('month') #LINE# #TAB# input_dict['day'] = input_dict.pop('day') #LINE# #TAB# input_dict['stem'] = first_nick + '-' + input_dict.pop('stem') #LINE# #TAB# return input_dict
"#LINE# #TAB# if not alphabet: #LINE# #TAB# #TAB# alphabet = alphabet_out #LINE# #TAB# elif base_num.startswith('1'): #LINE# #TAB# #TAB# base = int(base_num[1:]) #LINE# #TAB# num = base_num[2:] #LINE# #TAB# elif base_num.startswith('2'): #LINE# #TAB# #TAB# num = int(base_num[2:]) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unknown base representation: {num}'.format(num= #LINE# #TAB# #TAB# #TAB# base_num, base=base)) #LINE# #TAB# return num"
#LINE# #TAB# tags = [] #LINE# #TAB# for pred in entity_predictions: #LINE# #TAB# #TAB# if pred['type']!= 'tag': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# tags.append(pred['tag']) #LINE# #TAB# return tags
#LINE# #TAB# g = settings.G() #LINE# #TAB# g.joint_sfs_incr = 1.0 / g.joint_sfs_incr #LINE# #TAB# g.joint_sfs_outcr = 1.0 / g.joint_sfs_incr #LINE# #TAB# g.joint_sfs_outw = 1.0 / g.joint_sfs_outw #LINE# #TAB# return g
#LINE# #TAB# cleaned_array = [] #LINE# #TAB# for i in range(nd4j_array.shape[0]): #LINE# #TAB# #TAB# cleaned_array.append(nd4j_array[i]) #LINE# #TAB# return cleaned_array
"#LINE# #TAB# if name == 'lambda': #LINE# #TAB# #TAB# return lambda x: x #LINE# #TAB# if not name.startswith('__') and not name.endswith('__'): #LINE# #TAB# #TAB# return lambda x: x #LINE# #TAB# name = name[len('__'):] #LINE# #TAB# module = __import__(name) #LINE# #TAB# for name in get_module_names(): #LINE# #TAB# #TAB# mod = getattr(module, name) #LINE# #TAB# #TAB# if not hasattr(mod, '__call__'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# func = getattr(mod, name) #LINE# #TAB# #TAB# if callable(func): #LINE# #TAB# #TAB# #TAB# return func #LINE# #TAB# return None"
#LINE# #TAB# value = data.get(attribute_name) #LINE# #TAB# return value
#LINE# #TAB# if auth.has_password(): #LINE# #TAB# #TAB# password = auth.get_password() #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# p = urlparse(url) #LINE# #TAB# if p.scheme == 'https': #LINE# #TAB# #TAB# if p.hostname == '127.0.0.1': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# existing_file = os.path.join(base_file, csv_name) #LINE# #TAB# file_exists = os.path.exists(existing_file) #LINE# #TAB# extension = os.path.splitext(os.path.basename(csv_name))[1] #LINE# #TAB# if sep!= ',' and extension not in ['', '.']: #LINE# #TAB# #TAB# extension = sep + extension #LINE# #TAB# if convert_float: #LINE# #TAB# #TAB# extension = float(extension) #LINE# #TAB# is_float = extension == 'float' #LINE# #TAB# if is_float and extension: #LINE# #TAB# #TAB# if extension: #LINE# #TAB# #TAB# #TAB# csv_value = float(csv_value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# csv_value = int(csv_value) #LINE# #TAB# return is_float, csv_value"
"#LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.filename, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
"#LINE# #TAB# result = [] #LINE# #TAB# temp = [0] * vector_length #LINE# #TAB# for i in range(0, vector_length): #LINE# #TAB# #TAB# temp[i] = vector[i + shift] #LINE# #TAB# #TAB# result.append(temp) #LINE# #TAB# return result"
#LINE# #TAB# output = [] #LINE# #TAB# for directory in os.listdir(name): #LINE# #TAB# #TAB# full_name = camel_to_underscore(directory) #LINE# #TAB# #TAB# if os.path.isdir(full_name): #LINE# #TAB# #TAB# #TAB# output.append(directory) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if os.path.isdir(full_name): #LINE# #TAB# #TAB# #TAB# #TAB# output.append(directory) #LINE# #TAB# return output
"#LINE# #TAB# resp = {} #LINE# #TAB# with open(cls._api_url, 'r') as f: #LINE# #TAB# #TAB# content = f.read() #LINE# #TAB# for line in content.splitlines(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fields = line.split() #LINE# #TAB# #TAB# #TAB# if len(fields) < 2: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# resp[fields[0].lower()] = fields[1].strip() #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return resp"
"#LINE# #TAB# if value in [True, 'y', 'yes']: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# if type_string == 'default': #LINE# #TAB# #TAB# return 'default' #LINE# #TAB# parts = type_string.split('.') #LINE# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# return parts[0] #LINE# #TAB# return f'<{parts[0]}>'
"#LINE# #TAB# L = len(contactMap) #LINE# #TAB# eigenvalues = np.zeros(L, dtype=np.float64) #LINE# #TAB# values = np.zeros(L, dtype=np.float64) #LINE# #TAB# for c in range(L): #LINE# #TAB# #TAB# contact = np.array(contactMap[c]) #LINE# #TAB# #TAB# eigenvalues[c] = np.linalg.norm(contact - contactMap[c]) #LINE# #TAB# #TAB# values[c] = np.linalg.norm(contact - contactMap[c]) #LINE# #TAB# return eigenvalues, values"
"#LINE# #TAB# poly = bucket_poly(nodes, deep) #LINE# #TAB# if points is not None: #LINE# #TAB# #TAB# vtk_points = vtk.vtkPolyData(points, dtype=dtype) #LINE# #TAB# #TAB# poly.SetPoints(vtk_points) #LINE# #TAB# output = [] #LINE# #TAB# index = 0 #LINE# #TAB# for i in range(len(poly)): #LINE# #TAB# #TAB# if isinstance(poly[i], int): #LINE# #TAB# #TAB# #TAB# output.append(poly[i]) #LINE# #TAB# #TAB# #TAB# index += 1 #LINE# #TAB# #TAB# elif isinstance(poly[i], list): #LINE# #TAB# #TAB# #TAB# output.extend(bucket_pipeline(poly[i], points=poly[i])) #LINE# #TAB# #TAB# #TAB# index += 1 #LINE# #TAB# return output"
"#LINE# #TAB# key = 'ip_address' #LINE# #TAB# response = nmap.search(key, ip_address) #LINE# #TAB# if response!= 0: #LINE# #TAB# #TAB# print('found %s' % response) #LINE# #TAB# #TAB# return 0 #LINE# #TAB# response = nmap.search(key, ip_address) #LINE# #TAB# if response!= 0: #LINE# #TAB# #TAB# print('found %s' % response) #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# print('ip address not found') #LINE# #TAB# #TAB# return 0"
#LINE# #TAB# parser = cls.shebangKeyword[ext.lower()] #LINE# #TAB# if parser not in cls.shebangParsers: #LINE# #TAB# #TAB# parser = cls.shebangParsers[parser] = cls.guessParser(text) #LINE# #TAB# cls.shebangParsers[parser] = None #LINE# #TAB# return parser
"#LINE# #TAB# source_list = cls() #LINE# #TAB# for file_name in os.listdir(path): #LINE# #TAB# #TAB# if file_name.endswith('.sdetect'): #LINE# #TAB# #TAB# #TAB# source_list.append(cls(path=path, file_name=file_name)) #LINE# #TAB# return source_list"
"#LINE# #TAB# images = [] #LINE# #TAB# preload_images = soup.find_all('preload_images') #LINE# #TAB# for img in preload_images: #LINE# #TAB# #TAB# if img.get('href') and img.get('src'): #LINE# #TAB# #TAB# #TAB# images.append(img.get('src')) #LINE# #TAB# filenames = [f['filename'] for f in images] #LINE# #TAB# return images, filenames"
"#LINE# #TAB# import pngio #LINE# #TAB# if not isinstance(pointlist, list): #LINE# #TAB# #TAB# f = open(pointlist[0], 'r') #LINE# #TAB# else: #LINE# #TAB# #TAB# f = pngio.open(pointlist[0], 'rb') #LINE# #TAB# pngdata = f.read() #LINE# #TAB# f.close() #LINE# #TAB# return pngdata"
"#LINE# #TAB# hours = int(timestamp / 3600) #LINE# #TAB# minutes = int(timestamp / 60) #LINE# #TAB# seconds = int(timestamp % 60) #LINE# #TAB# return [hours, minutes, seconds]"
#LINE# #TAB# iter_result = True #LINE# #TAB# try: #LINE# #TAB# #TAB# iter_result = os.listdir(path) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# return iter_result
#LINE# #TAB# name = name.lower() #LINE# #TAB# if name.startswith('p_'): #LINE# #TAB# #TAB# name = name[5:] #LINE# #TAB# if name.endswith('_B'): #LINE# #TAB# #TAB# name = name[:-2] #LINE# #TAB# if name == '': #LINE# #TAB# #TAB# name = 'B_' #LINE# #TAB# return name
#LINE# #TAB# s = slice_obj.start.copy() #LINE# #TAB# if s.stop is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# n = s.stop - s.start + 1 #LINE# #TAB# if n < 1: #LINE# #TAB# #TAB# return None #LINE# #TAB# if n > slice_obj.step: #LINE# #TAB# #TAB# return None #LINE# #TAB# s = slice_obj.step #LINE# #TAB# if s.start is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# s = int(s.start) #LINE# #TAB# if s.stop < 0: #LINE# #TAB# #TAB# s += 1 #LINE# #TAB# return s
"#LINE# #TAB# update_time = '' #LINE# #TAB# date = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# update_time = os.path.getmtime(update_time) #LINE# #TAB# #TAB# date = datetime.datetime.strptime(update_time, '%Y-%m-%d %H:%M:%S') #LINE# #TAB# #TAB# if date == '': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return date"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# from pkg_resources import get_distribution #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return get_distribution(package_name).version #LINE# #TAB# except (pkg_resources.DistributionNotFound, AttributeError): #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# aic = np.zeros(yk.size) #LINE# #TAB# aic[-1] = 0.0 #LINE# #TAB# for K in range(yk.size - 2, -1, -1): #LINE# #TAB# #TAB# aic[K] = aic[K + 1] + 2 * aic[K + 1] / (NF - K - 2) #LINE# #TAB# return aic"
"#LINE# #TAB# if hasattr(normalisation_parameters, 'new_y'): #LINE# #TAB# #TAB# new_y = normalisation_parameters.new_y(y) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_y = y #LINE# #TAB# return new_y"
"#LINE# #TAB# assert isinstance(var_instance, SymbolVAR) #LINE# #TAB# from symbols import BOUNDLIST #LINE# #TAB# from symbols import VARARRAY #LINE# #TAB# assert isinstance(bounds, BOUNDLIST) #LINE# #TAB# var_instance.__class__ = VARARRAY #LINE# #TAB# var_instance.class_ = CLASS.array #LINE# #TAB# var_instance.bounds = bounds #LINE# #TAB# return var_instance"
"#LINE# #TAB# data_disk_size = 0 #LINE# #TAB# for command in commands: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# command_output = subprocess.check_output([cmake_with_sdist, command] #LINE# #TAB# #TAB# #TAB# #TAB# ).decode('utf-8') #LINE# #TAB# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if command_output in data_disk_size: #LINE# #TAB# #TAB# #TAB# data_disk_size = data_disk_size #LINE# #TAB# return data_disk_size"
#LINE# #TAB# n = float(n) #LINE# #TAB# Ht_est = sum(Ht_est) / float(n) #LINE# #TAB# Hs_est = sum(Hs_est) / float(n) #LINE# #TAB# if (n - 1.0) * Ht_est == 0.0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# Hs_est_ = sum(Hs_est) / float(n) #LINE# #TAB# d_est = (Ht_est - Hs_est_) / float(n) #LINE# #TAB# return d_est
"#LINE# #TAB# b0 = 0.903 * 2 / 3 #LINE# #TAB# b1 = 8.181 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.0909 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['SM'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# label_to_id = {} #LINE# #TAB# for i, label in enumerate(labels): #LINE# #TAB# #TAB# if label in label_to_id: #LINE# #TAB# #TAB# #TAB# label_to_id[label] += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# label_to_id[label] = i + 1 #LINE# #TAB# mid_label = ''.join(label_to_id.keys()) #LINE# #TAB# return mid_label"
"#LINE# #TAB# split_path = ses_path.split('/') #LINE# #TAB# if not split_path[0]: #LINE# #TAB# #TAB# return [] #LINE# #TAB# for root, dirs, files in os.walk(split_path): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# full_path = os.path.join(root, f) #LINE# #TAB# #TAB# #TAB# if is_build_linked(full_path): #LINE# #TAB# #TAB# #TAB# #TAB# return [f] #LINE# #TAB# return []"
"#LINE# #TAB# psi = -0.0102 #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"#LINE# #TAB# config = get_default(filename) #LINE# #TAB# matches = parse_requirements(config_string, based_on) #LINE# #TAB# if matches.version == 0: #LINE# #TAB# #TAB# return config #LINE# #TAB# else: #LINE# #TAB# #TAB# matches = [] #LINE# #TAB# #TAB# for match in matches: #LINE# #TAB# #TAB# #TAB# match_parts = match.split('=') #LINE# #TAB# #TAB# #TAB# if len(match_parts) == 2: #LINE# #TAB# #TAB# #TAB# #TAB# matches.append((match_parts[0], match_parts[1])) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# matches.append((match_parts[0], match_parts[1])) #LINE# #TAB# #TAB# return config"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# next_link = grab.xpath_one('//a[contains(@class, ""b-pager__next"")]') #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# logging.debug('No results found') #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
#LINE# #TAB# count = 0 #LINE# #TAB# mean = [] #LINE# #TAB# for obj in qs: #LINE# #TAB# #TAB# mean.append(obj.mean()) #LINE# #TAB# #TAB# count += 1 #LINE# #TAB# return count
"#LINE# #TAB# x_value = np.array([1.0, 0.0, 0.0, 0.0]) #LINE# #TAB# y_value = np.array([0.0, 1.0, 0.0, 0.0]) #LINE# #TAB# if _is_present('fact-tools'): #LINE# #TAB# #TAB# return x_value, y_value #LINE# #TAB# else: #LINE# #TAB# #TAB# res = _is_present('fact-tools') #LINE# #TAB# #TAB# if res: #LINE# #TAB# #TAB# #TAB# return x_value, y_value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return x_value, y_value #LINE# #TAB# else: #LINE# #TAB# #TAB# return None, None"
"#LINE# #TAB# suite_paths = action.get_suite_paths(data_dict) #LINE# #TAB# if'metadata_modified' not in data_dict: #LINE# #TAB# #TAB# return suite_paths #LINE# #TAB# sorted_results = sorted(suite_paths, key=lambda x: x['metadata_modified'], #LINE# #TAB# #TAB# reverse=True) #LINE# #TAB# try: #LINE# #TAB# #TAB# data_dict['limit'] = suite_paths['limit'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# data_dict['limit'] = DEFAULT_ITEM_LIMIT #LINE# #TAB# return data_dict"
"#LINE# #TAB# kernel = np.ones(shape, dtype=np.float64) #LINE# #TAB# for i in range(1, shape[0] - 1): #LINE# #TAB# #TAB# for j in range(i + 1): #LINE# #TAB# #TAB# #TAB# kernel[j, i] = 1 #LINE# #TAB# return kernel"
#LINE# #TAB# region = {} #LINE# #TAB# edges = y.edges() #LINE# #TAB# for edge in edges: #LINE# #TAB# #TAB# if edge[0] == 0: #LINE# #TAB# #TAB# #TAB# region[edge[1]] = edge[1] #LINE# #TAB# #TAB# elif edge[1] == 1: #LINE# #TAB# #TAB# #TAB# region[edge[0]] = edge[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# region[edge[0]] = edge[1] #LINE# #TAB# return region
"#LINE# #TAB# if root is None: #LINE# #TAB# #TAB# root = '' #LINE# #TAB# result = depth_first_search(graph, root) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# errmsg = 'no root found' #LINE# #TAB# #TAB# print(errmsg) #LINE# #TAB# #TAB# return None #LINE# #TAB# output = '\n'.join(result) #LINE# #TAB# click.echo(output) #LINE# #TAB# return output"
#LINE# #TAB# if uri in filter_list: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# cursor = framework_var.create_parameter(attr_name) #LINE# #TAB# lines = cursor.get_lines() #LINE# #TAB# while not lines: #LINE# #TAB# #TAB# cursor.close() #LINE# #TAB# #TAB# lines = cursor.get_lines() #LINE# #TAB# return lines
"#LINE# #TAB# if path is None: #LINE# #TAB# #TAB# return command #LINE# #TAB# cwd = os.getcwd() #LINE# #TAB# if cwd == '': #LINE# #TAB# #TAB# os.chdir(path) #LINE# #TAB# result = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# cwd = os.getcwd() #LINE# #TAB# result = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# os.chdir(cwd) #LINE# #TAB# return result"
"#LINE# #TAB# if 'amount' not in data: #LINE# #TAB# #TAB# return None #LINE# #TAB# payload = data['amount'] #LINE# #TAB# if isinstance(payload, str): #LINE# #TAB# #TAB# payload = serializer.loads(payload) #LINE# #TAB# if data['datetime'] is None: #LINE# #TAB# #TAB# raise ValueError('Datetime is required') #LINE# #TAB# return payload"
"#LINE# #TAB# if path.is_file(): #LINE# #TAB# #TAB# return {'is_integer': True, 'path': str(path)} #LINE# #TAB# return {}"
#LINE# #TAB# out = '' #LINE# #TAB# for written_file in written_files: #LINE# #TAB# #TAB# xml = minidom.parse(written_file) #LINE# #TAB# #TAB# if xml.tag == '{': #LINE# #TAB# #TAB# #TAB# LOG.info('Skipping input file: {}'.format(written_file)) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for input_file in written_file: #LINE# #TAB# #TAB# #TAB# if input_file.endswith('.xml'): #LINE# #TAB# #TAB# #TAB# #TAB# xml = xml.find('.//input') #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# out += xml #LINE# #TAB# return out
"#LINE# #TAB# if isinstance(memory, str): #LINE# #TAB# #TAB# memory = memory.split(':') #LINE# #TAB# config = OrderedDict() #LINE# #TAB# config['sparkSession'] = SparkSession.builder.getOrCreate() #LINE# #TAB# config['memory'] = memory #LINE# #TAB# return config"
#LINE# #TAB# ret = np.zeros_like(arr) #LINE# #TAB# ret[index] = g #LINE# #TAB# for i in range(len(arr)): #LINE# #TAB# #TAB# ret[i] = arr[i] #LINE# #TAB# return ret
"#LINE# #TAB# rows, cols = A.shape #LINE# #TAB# l = [] #LINE# #TAB# for i in range(rows): #LINE# #TAB# #TAB# for j in range(cols): #LINE# #TAB# #TAB# #TAB# if A.data[i, j] < 0: #LINE# #TAB# #TAB# #TAB# #TAB# l.append(i) #LINE# #TAB# return l"
"#LINE# #TAB# matching = [] #LINE# #TAB# for request in items: #LINE# #TAB# #TAB# matching.append((request.index, request)) #LINE# #TAB# return matching"
"#LINE# #TAB# return {'sids': obj.sids,'schedule': obj.schedule,'status': {'id': obj. #LINE# #TAB# #TAB# status.id, 'naam': obj.status.naam, 'definitie': obj.status.definitie}}"
#LINE# #TAB# cache = _get_cache() #LINE# #TAB# if cache is not None: #LINE# #TAB# #TAB# return cache.get(user) #LINE# #TAB# issue_counts = _get_issue_counts(user) #LINE# #TAB# if issue_counts is None: #LINE# #TAB# #TAB# return ISecurityPolicy() #LINE# #TAB# return issue_counts
"#LINE# #TAB# words = [] #LINE# #TAB# for i in range(0, len(byteData), 2): #LINE# #TAB# #TAB# word = byteData[i] #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# word = '0' #LINE# #TAB# #TAB# words.append(word) #LINE# #TAB# return words"
#LINE# #TAB# n_sig = params[0] #LINE# #TAB# n_bkg = params[1] #LINE# #TAB# if n_sig == n_bkg: #LINE# #TAB# #TAB# return -np.inf #LINE# #TAB# if n_bkg == n_sig: #LINE# #TAB# #TAB# return signal_2d[0][0] * n_bkg #LINE# #TAB# if n_sig == n_bkg: #LINE# #TAB# #TAB# return background_2d[0][0] * n_bkg #LINE# #TAB# if n_sig == n_bkg: #LINE# #TAB# #TAB# return -np.inf #LINE# #TAB# return np.inf
"#LINE# #TAB# terms = [] #LINE# #TAB# prefix = flow_name[0] #LINE# #TAB# if prefix.startswith('/'): #LINE# #TAB# #TAB# prefix = '/' #LINE# #TAB# flow_dir = os.path.join(config.FLOW_BASE_DIR, prefix) #LINE# #TAB# if not os.path.exists(flow_dir): #LINE# #TAB# #TAB# return [] #LINE# #TAB# for part in os.listdir(flow_dir): #LINE# #TAB# #TAB# term = '%s/%s' % (flow_dir, part) #LINE# #TAB# #TAB# if not term.startswith('/'): #LINE# #TAB# #TAB# #TAB# term = '/' + term #LINE# #TAB# #TAB# terms.append(term) #LINE# #TAB# return terms"
#LINE# #TAB# if model.description!= '': #LINE# #TAB# #TAB# model.description += '\n' #LINE# #TAB# sbo = '' #LINE# #TAB# if model.objective_type == 'C': #LINE# #TAB# #TAB# sbo +='sbo-c' #LINE# #TAB# if model.objective_type == 'D': #LINE# #TAB# #TAB# sbo +='sbo-d' #LINE# #TAB# if model.objective_type == 'E': #LINE# #TAB# #TAB# sbo +='sbo-e' #LINE# #TAB# return sbo
"#LINE# #TAB# if extent[0] == 'A': #LINE# #TAB# #TAB# xmin = extent[1] #LINE# #TAB# #TAB# xmax = extent[2] #LINE# #TAB# #TAB# ymax = extent[3] #LINE# #TAB# else: #LINE# #TAB# #TAB# xmin = extent[0] #LINE# #TAB# #TAB# xmax = extent[2] #LINE# #TAB# #TAB# ymax = extent[3] #LINE# #TAB# if xmax >= xmax and ymax <= ymax: #LINE# #TAB# #TAB# return xmin, xmax, ymax #LINE# #TAB# else: #LINE# #TAB# #TAB# return xmin, xmax, ymax"
"#LINE# #TAB# if criteria_is_array_3(module_scope, model_name): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# if nparray > 2: #LINE# #TAB# #TAB# rgb = np.empty((nparray, nparray), dtype=np.uint8) #LINE# #TAB# #TAB# bgr = np.empty((nparray, nparray), dtype=np.uint8) #LINE# #TAB# #TAB# rgb.fill(0) #LINE# #TAB# #TAB# bgr.fill(0) #LINE# #TAB# else: #LINE# #TAB# #TAB# rgb = np.empty((nparray, nparray), dtype=np.uint8) #LINE# #TAB# #TAB# bgr = np.empty((nparray, nparray), dtype=np.uint8) #LINE# #TAB# #TAB# rgb.fill(0) #LINE# #TAB# #TAB# bgr.fill(0) #LINE# #TAB# return rgb, bgr"
#LINE# #TAB# if epoch_time is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# t = str(epoch_time) #LINE# #TAB# if t.isdigit(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# t = float(t) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return t
"#LINE# #TAB# if not os.path.isdir(custom_directory): #LINE# #TAB# #TAB# raise ValueError('Cannot parse additional directory {}'.format(custom_directory) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# cache_file = os.path.join(custom_directory, 'parse_addition.h5') #LINE# #TAB# if not os.path.isfile(cache_file): #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(cache_file, 'r') as f: #LINE# #TAB# #TAB# parsed_dataset = json.load(f) #LINE# #TAB# #TAB# if 'parse_addition' in parsed_dataset: #LINE# #TAB# #TAB# #TAB# return parsed_dataset['parse_addition'] #LINE# #TAB# return None"
#LINE# #TAB# layer_nr = None #LINE# #TAB# target_name = target.GetName() #LINE# #TAB# if target_name!= None: #LINE# #TAB# #TAB# architecture = target_name.GetArchitecture() #LINE# #TAB# #TAB# if architecture: #LINE# #TAB# #TAB# #TAB# layer_nr = architecture #LINE# #TAB# return layer_nr
"#LINE# #TAB# if ordchr in string.printable or ordchr == '.': #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# with codecs.open(FILE_NAME, 'r', encoding='utf-8') as fd: #LINE# #TAB# #TAB# #TAB# return json.loads(fd.read()) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for snap in sorted(root.children, key=lambda x: x['name']): #LINE# #TAB# #TAB# #TAB# if name == snap['name']: #LINE# #TAB# #TAB# #TAB# #TAB# return float(snap['score']) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# a = np.array(vec1) #LINE# #TAB# r = np.linalg.norm(a, axis=-1) #LINE# #TAB# return a / r"
"#LINE# #TAB# description = '' #LINE# #TAB# lines = open(os.devnull, 'r').readlines() #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith('Output:'): #LINE# #TAB# #TAB# #TAB# output_name = line[7:].replace('MIDI', '') #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# output_name = 'Unknown' #LINE# #TAB# try: #LINE# #TAB# #TAB# number = int(number) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if number > 0 and number < 15: #LINE# #TAB# #TAB# description = output_name + '_' + str(number) #LINE# #TAB# return description"
"#LINE# #TAB# from. import minlength_regex #LINE# #TAB# name = minlength_regex.sub('_', name) #LINE# #TAB# name = name.capitalize() #LINE# #TAB# while keyword.iskeyword(name): #LINE# #TAB# #TAB# name = name + '_' #LINE# #TAB# return name"
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t') #LINE# #TAB# if isdatetime: #LINE# #TAB# #TAB# df = df.dt.to_datetime() #LINE# #TAB# return df"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# import uav.example #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# for handler in control.handlers: #LINE# #TAB# #TAB# if isinstance(handler, uav.example.UavHandler): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# select = function_name #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(argument_list): #LINE# #TAB# #TAB# arg_name = f'{function_name}{i}' #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# arg = argument_list[i] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# select += f'={arg}' #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# return select
"#LINE# #TAB# terminal_output = '' #LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# terminal_output = str(e) #LINE# #TAB# return terminal_output"
#LINE# #TAB# from_columns = from_table.columns #LINE# #TAB# to_columns = to_table.columns #LINE# #TAB# diff = defaultdict(lambda : []) #LINE# #TAB# for column in from_columns: #LINE# #TAB# #TAB# for field in from_table.columns: #LINE# #TAB# #TAB# #TAB# if field.target_column == to_columns[column].target_column: #LINE# #TAB# #TAB# #TAB# #TAB# diff[column].append(column) #LINE# #TAB# for field in from_table.foreign_keys: #LINE# #TAB# #TAB# if field.target_column not in to_columns: #LINE# #TAB# #TAB# #TAB# diff[field.target_column] = [] #LINE# #TAB# return diff
#LINE# #TAB# if a['date'] > b['date']: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# elif a['timestamp'] > b['timestamp']: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0
#LINE# #TAB# if job_class is None: #LINE# #TAB# #TAB# job_class = get_job_from_RQ() #LINE# #TAB# if is_string(job_class): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = import_module(job_class) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# raise ImproperlyConfigured( #LINE# #TAB# #TAB# #TAB# #TAB# 'Error importing spanning forest: %s' % job_class) #LINE# #TAB# else: #LINE# #TAB# #TAB# return job_class
"#LINE# #TAB# global KEYRING #LINE# #TAB# if hasattr(request, 'country'): #LINE# #TAB# #TAB# KEYRING = request.country #LINE# #TAB# elif hasattr(request,'session') and request.session.get('keyring'): #LINE# #TAB# #TAB# KEYRING = request.session['keyring'] #LINE# #TAB# return KEYRING"
#LINE# #TAB# home_folder = os.path.expanduser('~') + '/gridCAl' #LINE# #TAB# if not os.path.exists(home_folder): #LINE# #TAB# #TAB# os.makedirs(home_folder) #LINE# #TAB# return home_folder
#LINE# #TAB# conf_file = open(path) #LINE# #TAB# try: #LINE# #TAB# #TAB# conf_file.read() #LINE# #TAB# except FileNotFoundError as error: #LINE# #TAB# #TAB# if error.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# conf_file = {} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# conf_dict = yaml.load(conf_file) #LINE# #TAB# conf_file.close() #LINE# #TAB# return conf_dict
"#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# response = request.json_body #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid request body') #LINE# #TAB# #TAB# if 400 <= response.status_code < 500: #LINE# #TAB# #TAB# #TAB# raise exc.InvalidRequest( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid response code: %d - %d' % (response.status_code, #LINE# #TAB# #TAB# #TAB# #TAB# response.reason)) #LINE# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# num_rows = 0 #LINE# #TAB# num_columns = 0 #LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = f.read(1024).copy() #LINE# #TAB# #TAB# #TAB# if data!= '': #LINE# #TAB# #TAB# #TAB# #TAB# if num_rows == 0: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# num_rows += 1 #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return num_rows, num_columns"
"#LINE# #TAB# dt = row['mtime'] #LINE# #TAB# dt = dt.replace(hour=sd['hour'], minute=sd['minute'], second=sd[ #LINE# #TAB# #TAB#'second'], microsecond=sd['microsecond']) #LINE# #TAB# dt = dt.replace(hour=sd['hour'], minute=sd['minute'], second=sd[ #LINE# #TAB# #TAB#'second'], microsecond=sd['microsecond']) #LINE# #TAB# return dt"
"#LINE# #TAB# cwd = os.getcwd() #LINE# #TAB# if cwd not in (None, ''): #LINE# #TAB# #TAB# cwd = join(cwd, '.freezer') #LINE# #TAB# if not exists(cwd): #LINE# #TAB# #TAB# os.makedirs(cwd) #LINE# #TAB# xml_to_member = cwd + '/logs/xml_to_member.xml' #LINE# #TAB# else: #LINE# #TAB# #TAB# with open(xml_to_member, 'w') as f: #LINE# #TAB# #TAB# #TAB# f.write(xml_to_member) #LINE# #TAB# return xml_to_member"
"#LINE# #TAB# with open(path, 'rb') as cih_file: #LINE# #TAB# #TAB# info_dict = cih_file.read() #LINE# #TAB# return info_dict"
#LINE# #TAB# for record in load_dolphin_records(): #LINE# #TAB# #TAB# if label in record['label']: #LINE# #TAB# #TAB# #TAB# is_variable = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# is_variable = False #LINE# #TAB# return is_variable
#LINE# #TAB# units = convert_units(library_name) #LINE# #TAB# return units
"#LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# return smart_text(value) #LINE# #TAB# if isinstance(value, bool): #LINE# #TAB# #TAB# return 'true' if value else 'false' #LINE# #TAB# if isinstance(value, (list, tuple)): #LINE# #TAB# #TAB# value = [smart_text(a) for a in value] #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# return dict((key, anonymize_sensitive_words(val, type_)) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for key, val in value.items()) #LINE# #TAB# return value"
"#LINE# #TAB# text = text.rstrip('\n') #LINE# #TAB# try: #LINE# #TAB# #TAB# with gzip.open(text, 'w', 'utf-8') as f: #LINE# #TAB# #TAB# #TAB# text = yaml.safe_dump(text, f, default_flow_style=False) #LINE# #TAB# except gzip.Error as e: #LINE# #TAB# #TAB# log.error('gzip_encode failed: %s' % e) #LINE# #TAB# #TAB# raise #LINE# #TAB# return text"
#LINE# #TAB# if len(augs) > 0: #LINE# #TAB# #TAB# percentile = 0 #LINE# #TAB# #TAB# for a in augs: #LINE# #TAB# #TAB# #TAB# if a.pct is None: #LINE# #TAB# #TAB# #TAB# #TAB# percentile = a #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# percentile = 100 #LINE# #TAB# return percentile
"#LINE# #TAB# sql = """""" #LINE# #TAB# SELECT #LINE# #TAB# #TAB# SELECT #LINE# #TAB# #TAB# #TAB# SELECT #LINE# #TAB# #TAB# #TAB# FROM pg_catalog.pg_class #LINE# #TAB# #TAB# #TAB# WHERE #LINE# #TAB# #TAB# (SELECT #LINE# #TAB# #TAB# #TAB# #TAB# pg_catalog.pg_namespace.word #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# FROM pg_catalog.pg_class #LINE# #TAB# #TAB# WHERE #LINE# #TAB# #TAB# (SELECT #LINE# #TAB# #TAB# #TAB# pg_catalog.pg_namespace.word #LINE# #TAB# #TAB# ) #LINE# #TAB# #TAB# WHERE #LINE# #TAB# #TAB# (SELECT #LINE# #TAB# #TAB# #TAB# pg_catalog.pg_namespace.word #LINE# #TAB# #TAB# ) #LINE# #TAB# """""" #LINE# #TAB#"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for attr in ('ASP', 'PTX4'): #LINE# #TAB# #TAB# #TAB# if getattr(mlog, attr, None) is not None: #LINE# #TAB# #TAB# #TAB# #TAB# return attr #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# diff = '' #LINE# #TAB# for line in source: #LINE# #TAB# #TAB# if mlog.getline(line)!= '': #LINE# #TAB# #TAB# #TAB# diff += line #LINE# #TAB# return diff"
"#LINE# #TAB# import redis #LINE# #TAB# redis_connection = redis.StrictRedis(host=REDIS_HOST, decode_responses=True) #LINE# #TAB# conn = redis_connection.pipeline() #LINE# #TAB# favicon = {} #LINE# #TAB# for key, value in Favoricon.items(): #LINE# #TAB# #TAB# key = key.lower() #LINE# #TAB# #TAB# value = value.lower() #LINE# #TAB# #TAB# if key in redis_connection: #LINE# #TAB# #TAB# #TAB# redis_connection[key] = redis_connection[key].lru #LINE# #TAB# return {'favicon': favicon}"
#LINE# #TAB# if cls.__from_class__: #LINE# #TAB# #TAB# cls = cls.__from_class__ #LINE# #TAB# return cls.__table__.primary_key.columns.values()[0].name
"#LINE# #TAB# mn = torch.mean(minval, dim=0) #LINE# #TAB# mx = torch.var(maxval, dim=0) #LINE# #TAB# return mn, mx"
"#LINE# #TAB# scale = 1.0 / ntaps #LINE# #TAB# if window == 'hanning': #LINE# #TAB# #TAB# scale = 1.0 / ntaps #LINE# #TAB# impulse = np.roll(impulse, scale) #LINE# #TAB# if window == 'hanning': #LINE# #TAB# #TAB# impulse = np.roll(impulse, -scale) #LINE# #TAB# return impulse"
"#LINE# #TAB# if options.get('count', 0)!= 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# last_oneliners = term.split()[-1] #LINE# #TAB# top_margin = max(top_margin, last_oneliners[0]) #LINE# #TAB# adj_offset = offset - (top_margin - last_oneliners[0]) #LINE# #TAB# return top_margin, adj_offset"
"#LINE# #TAB# if isinstance(obs_dict, dict): #LINE# #TAB# #TAB# obs_array = obs_dict #LINE# #TAB# else: #LINE# #TAB# #TAB# obs_array = obs_dict #LINE# #TAB# return obs_array"
"#LINE# #TAB# ret = {} #LINE# #TAB# for k, v in _annotations.items(): #LINE# #TAB# #TAB# if isinstance(v, CallableType): #LINE# #TAB# #TAB# #TAB# _type_annotations[k] = convert_strong(v) #LINE# #TAB# #TAB# elif isinstance(v, str): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# ret[k] = tuple(v) #LINE# #TAB# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# #TAB# ret[k] = v, None #LINE# #TAB# return ret"
"#LINE# #TAB# conf['jupyterlab_script_path'] = target['jupyterlab_script_path'] #LINE# #TAB# if target['jupyterlab_script_path']: #LINE# #TAB# #TAB# conf['jupyterlab_script_path'] = '%s/%s' % (target['jupyterlab_script_path'], #LINE# #TAB# #TAB# #TAB# project) #LINE# #TAB# if target['jupyterlab_script_path']: #LINE# #TAB# #TAB# target['jupyterlab_script_path'] = '%s/%s' % (target['jupyterlab_script_path'], #LINE# #TAB# #TAB# #TAB# repository) #LINE# #TAB# conf['jupyterlab_script_path'] = target['jupyterlab_script_path']"
"#LINE# #TAB# output = [] #LINE# #TAB# row = 0 #LINE# #TAB# for i, relation in enumerate(correlations): #LINE# #TAB# #TAB# for j, relation in enumerate(relation): #LINE# #TAB# #TAB# #TAB# output.append((i, j, relation)) #LINE# #TAB# #TAB# #TAB# row += 1 #LINE# #TAB# return output"
#LINE# #TAB# obos = [] #LINE# #TAB# while len(msgs) > 0: #LINE# #TAB# #TAB# if len(msgs[0]) <= max_len: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# for msg in msgs[1:]: #LINE# #TAB# #TAB# #TAB# if len(msg) > max_len - 3: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# obos.append(msg) #LINE# #TAB# return obos
"#LINE# #TAB# if filepath.lower().endswith('.gz'): #LINE# #TAB# #TAB# zf = gzip.open(filepath, 'r') #LINE# #TAB# else: #LINE# #TAB# #TAB# zf = open(filepath, 'r') #LINE# #TAB# data = zf.read() #LINE# #TAB# data = fix_type(data) #LINE# #TAB# zf.close() #LINE# #TAB# return data"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# data = json.dumps(string, sort_keys=True, indent=4) #LINE# #TAB# #TAB# with tempfile.NamedTemporaryFile(delete=False) as f: #LINE# #TAB# #TAB# #TAB# f.write(data) #LINE# #TAB# #TAB# #TAB# f.flush() #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# pass #LINE# #TAB# finally: #LINE# #TAB# #TAB# f.close() #LINE# #TAB# if fmt is not None: #LINE# #TAB# #TAB# return f.name"
"#LINE# #TAB# bytes = bytearray() #LINE# #TAB# for key, value in ctx.items(): #LINE# #TAB# #TAB# if key == 'ignore': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# bytes += value #LINE# #TAB# #TAB# elif isinstance(value, str): #LINE# #TAB# #TAB# #TAB# bytes += value.encode('utf-8') #LINE# #TAB# return bytes"
"#LINE# #TAB# config = get_last_commit_tag_config(rootdir) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.path.join(rootdir, config['authfile'])) as f: #LINE# #TAB# #TAB# #TAB# last_commit = json.load(f) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return last_commit == 1"
#LINE# #TAB# if is_volatile(type_): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
#LINE# #TAB# data = {} #LINE# #TAB# for item in grp.values(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# subval = data[item] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# subval = [] #LINE# #TAB# #TAB# data[item] = subval #LINE# #TAB# return data
"#LINE# #TAB# url = '{}/api/volcano/class_device/{}/'.format(class_card_server_url(), sn) #LINE# #TAB# resp = do_get_request(url=url, token=class_card_server_token(), #LINE# #TAB# #TAB# school_id=school_id) #LINE# #TAB# code = resp.code #LINE# #TAB# data = resp.data.get('data', {}) if not code else resp.msg #LINE# #TAB# if code: #LINE# #TAB# #TAB# logger.error('Error: Request: {}, Detail: {}'.format(url, data)) #LINE# #TAB# return code, data"
"#LINE# #TAB# #TAB# fields = [] #LINE# #TAB# #TAB# for field in instance._meta.fields: #LINE# #TAB# #TAB# #TAB# if field.get_api_value(instance): #LINE# #TAB# #TAB# #TAB# #TAB# fields.append(field) #LINE# #TAB# #TAB# if not instance._meta.pk: #LINE# #TAB# #TAB# #TAB# if hasattr(instance, '_meta'): #LINE# #TAB# #TAB# #TAB# #TAB# fields += instance._meta.get_fields() #LINE# #TAB# #TAB# return fields"
"#LINE# #TAB# import re #LINE# #TAB# dirname = os.path.abspath(os.path.dirname(__file__)) #LINE# #TAB# version_file = os.path.join(dirname, ""version.py"") #LINE# #TAB# with open(version_file) as f: #LINE# #TAB# #TAB# raw_version = f.readlines() #LINE# #TAB# version_dict = {} #LINE# #TAB# for line in raw_version: #LINE# #TAB# #TAB# match = re.search(""__version__ = '([^']+)'"", line) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# version_dict['__version__'] = match.group(1) #LINE# #TAB# return version_dict"
"#LINE# #TAB# for row in results['rows']: #LINE# #TAB# #TAB# for k, v in row.items(): #LINE# #TAB# #TAB# #TAB# if k == 'timestamp': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# data = json.loads(v) #LINE# #TAB# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# data = '' #LINE# #TAB# #TAB# #TAB# yield k, data #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield k, v"
"#LINE# #TAB# if not concept_cd: #LINE# #TAB# #TAB# return '', '' #LINE# #TAB# cat_cd, data_label = concept_cd.split('/', 1) #LINE# #TAB# data_label = join_char.join(data_label) #LINE# #TAB# return cat_cd, data_label"
"#LINE# #TAB# error = None #LINE# #TAB# if c14n_exc: #LINE# #TAB# #TAB# raise InvalidSignatureError('Invalid signature') #LINE# #TAB# sha1 = RSA.new(key, signature) #LINE# #TAB# der = RSA.new(key, signature) #LINE# #TAB# try: #LINE# #TAB# #TAB# der.verify(xml) #LINE# #TAB# except InvalidSignature: #LINE# #TAB# #TAB# error = 'Invalid signature' #LINE# #TAB# if error: #LINE# #TAB# #TAB# raise InvalidSignatureError('Invalid signature') #LINE# #TAB# if not der.verify(xml): #LINE# #TAB# #TAB# error = 'Invalid signature' #LINE# #TAB# return error == 0"
#LINE# #TAB# if shape == 'gaus': #LINE# #TAB# #TAB# max_sz = 2 * np.max(sigma) #LINE# #TAB# #TAB# if shape == 'x': #LINE# #TAB# #TAB# #TAB# return max_sz #LINE# #TAB# #TAB# elif shape == 'y': #LINE# #TAB# #TAB# #TAB# return max_sz // 2 #LINE# #TAB# #TAB# elif shape == 'z': #LINE# #TAB# #TAB# #TAB# return max_sz // 2 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return max_sz // 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# sig_len = np.sqrt(sigma) #LINE# #TAB# #TAB# return sig_len
"#LINE# #TAB# beg_sep = '\n#TAB#'#LINE# #TAB# end_sep = '\n#TAB#'#LINE# #TAB# if len(name) > 20: #LINE# #TAB# #TAB# name = name[:17] + ellipsis + name[-20:] #LINE# #TAB# #TAB# beg_sep = '\n' #LINE# #TAB# beg_mark = name.strip(beg_sep) #LINE# #TAB# end_mark = name.strip(end_sep) #LINE# #TAB# return beg_mark, end_mark"
"#LINE# #TAB# if hr_data and filename is not None: #LINE# #TAB# #TAB# with open(filename, 'w') as f: #LINE# #TAB# #TAB# #TAB# for row in hr_data: #LINE# #TAB# #TAB# #TAB# #TAB# f.write(row[0]) #LINE# #TAB# #TAB# return filename #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# hash_generator = hashlib.sha512() #LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# data = f.read(size) #LINE# #TAB# #TAB# #TAB# hash_generator.update(data) #LINE# #TAB# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return hash_generator"
#LINE# #TAB# if not config_dir: #LINE# #TAB# #TAB# config_dir = DEFAULT_CONFIG_DIR #LINE# #TAB# config = _get_config_file(config_dir) #LINE# #TAB# if not config: #LINE# #TAB# #TAB# raise ValueError('s3acc configuration not found in {}'.format( #LINE# #TAB# #TAB# #TAB# config_dir)) #LINE# #TAB# return config
"#LINE# #TAB# if isinstance(params, dict): #LINE# #TAB# #TAB# params = tokenize_versions_dir(params) #LINE# #TAB# #TAB# params = normalize_versions_dir(params) #LINE# #TAB# if not os.path.exists(params['versions_dir']): #LINE# #TAB# #TAB# os.mkdir(params['versions_dir']) #LINE# #TAB# return"
"#LINE# #TAB# result = [] #LINE# #TAB# for el in node.iter(): #LINE# #TAB# #TAB# if isinstance(el, etree._Element): #LINE# #TAB# #TAB# #TAB# result.append(parse_options(el)) #LINE# #TAB# return result"
"#LINE# #TAB# for directory in os.listdir(BUILDOUT_DIR): #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(BUILDOUT_DIR, directory)): #LINE# #TAB# #TAB# #TAB# yield directory"
"#LINE# #TAB# url = '%s/%s/order/%s/compiler' % (order_url, account, order_url) #LINE# #TAB# data = requests.get(url) #LINE# #TAB# if data.status_code!= 200: #LINE# #TAB# #TAB# _LOGGER.warning('Could not retrieve compiler path for order %s', url) #LINE# #TAB# #TAB# return None #LINE# #TAB# data = data.json() #LINE# #TAB# if 'compiler' not in data: #LINE# #TAB# #TAB# _LOGGER.warning('Could not retrieve compiler path for order %s', order_url) #LINE# #TAB# return data['compiler']"
"#LINE# #TAB# notifications = registered_notifications() #LINE# #TAB# for notification_type in ['created', 'updated', 'error','merged']: #LINE# #TAB# #TAB# if not notifications: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# headers = [(notification_type, notification.data) for notification in #LINE# #TAB# #TAB# #TAB# notifications] #LINE# #TAB# _sort_headers(headers) #LINE# #TAB# return headers"
#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# qry_id = int(id) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# qry_id = None #LINE# #TAB# #TAB# if not qry_id: #LINE# #TAB# #TAB# #TAB# msg = 'unknown identifier %s' % id #LINE# #TAB# #TAB# #TAB# cls.error(msg) #LINE# #TAB# #TAB# return qry_id
#LINE# #TAB# frame = inspect.currentframe() #LINE# #TAB# try: #LINE# #TAB# #TAB# if 'ipython' not in frame.f_back.f_globals: #LINE# #TAB# #TAB# #TAB# if frame.f_globals['__name__'].startswith('ipython.'): #LINE# #TAB# #TAB# #TAB# #TAB# return [frame] #LINE# #TAB# #TAB# return [] #LINE# #TAB# finally: #LINE# #TAB# #TAB# del frame
"#LINE# #TAB# resource_uri = resource_uri.replace('/', '_') #LINE# #TAB# if resource_uri.find('/')!= -1: #LINE# #TAB# #TAB# resource_uri = resource_uri[:resource_uri.rfind('/')] #LINE# #TAB# return resource_uri"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# iter(scopes.keys()) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(scopes.itervalues(), collections.Iterable): #LINE# #TAB# #TAB# return True #LINE# #TAB# for scope in scopes.itervalues(): #LINE# #TAB# #TAB# if isinstance(scope, collections.Iterator): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# covariance_matrix = 1.0 #LINE# #TAB# for boundary in points: #LINE# #TAB# #TAB# covariance_matrix += np.linalg.inv(np.dot(boundary.T, points)) #LINE# #TAB# return covariance_matrix"
#LINE# #TAB# #TAB# if not os.path.isdir(dirpath): #LINE# #TAB# #TAB# #TAB# os.makedirs(dirpath) #LINE# #TAB# #TAB# #TAB# model_instance = Model.load(dirpath) #LINE# #TAB# #TAB# #TAB# return model_instance
"#LINE# #TAB# assert A.ndim > 1 #LINE# #TAB# assert A.shape[1] == 1 #LINE# #TAB# assert A.shape[2] == 3 #LINE# #TAB# if not A.shape[2]: #LINE# #TAB# #TAB# A = np.reshape(A, (A.shape[1], 1)) #LINE# #TAB# if not A.shape[2]: #LINE# #TAB# #TAB# A = np.reshape(A, (A.shape[2], 1)) #LINE# #TAB# return A"
"#LINE# #TAB# id_recs = {'year': date.strftime('%Y'),'month': date.strftime('%m'), #LINE# #TAB# #TAB# 'day': date.strftime('%d'), 'hour': date.strftime('%H'),'minute': #LINE# #TAB# #TAB# date.strftime('%M'),'second': date} #LINE# #TAB# return id_recs"
"#LINE# #TAB# with open(fastq, 'rb') as f: #LINE# #TAB# #TAB# pickle.dump(f, f, pickle.HIGHEST_PROTOCOL) #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# data = f.read(1024) #LINE# #TAB# #TAB# #TAB# #TAB# pickle.dump(data, f, pickle.HIGHEST_PROTOCOL) #LINE# #TAB# #TAB# #TAB# #TAB# yield data #LINE# #TAB# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# break"
"#LINE# #TAB# if hasattr(filename, 'decode'): #LINE# #TAB# #TAB# filename = filename.decode() #LINE# #TAB# parts = filename.split('.') #LINE# #TAB# if len(parts) >= 3: #LINE# #TAB# #TAB# return parts[2] #LINE# #TAB# if len(parts) >= 2: #LINE# #TAB# #TAB# if parts[1] == 'color': #LINE# #TAB# #TAB# #TAB# return parts[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return parts[1] #LINE# #TAB# elif len(parts) >= 1: #LINE# #TAB# #TAB# if parts[0] == 'color': #LINE# #TAB# #TAB# #TAB# return parts[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# return default"
#LINE# #TAB# if ':' not in val: #LINE# #TAB# #TAB# raise ValueError('Not a technical string: %r' % val) #LINE# #TAB# queue = val.split(':') #LINE# #TAB# if len(queue) == 1: #LINE# #TAB# #TAB# return queue[0] #LINE# #TAB# return queue
"#LINE# #TAB# return {'fingerprint': base64.b64decode(publication.get_pubkey()).decode( #LINE# #TAB# #TAB# 'utf-8'), 'algorithm': publication.get_algorithm(), 'public_key': #LINE# #TAB# #TAB# public_key.get_public_key()}"
"#LINE# #TAB# out = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# if f.endswith("".py""): #LINE# #TAB# #TAB# #TAB# out.append(f[:-3]) #LINE# #TAB# #TAB# elif f.endswith("".pyc""): #LINE# #TAB# #TAB# #TAB# out.append(f[:-5]) #LINE# #TAB# return out"
#LINE# #TAB# if not description: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not description.startswith('a'): #LINE# #TAB# #TAB# return description #LINE# #TAB# description = description[1:] #LINE# #TAB# description = description.capitalize() #LINE# #TAB# if not description.endswith('.'): #LINE# #TAB# #TAB# return description #LINE# #TAB# return description
#LINE# #TAB# output = '' #LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# output += cell.source #LINE# #TAB# #TAB# elif cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# output += cell.source #LINE# #TAB# return output
"#LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# return s.decode('utf8') #LINE# #TAB# return s"
"#LINE# #TAB# old = env.copy() #LINE# #TAB# env.pop(key, None) #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# env.pop(key, None) #LINE# #TAB# #TAB# if old is not None: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# del env[key] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# #TAB# if sys.version_info >= (3, 5): #LINE# #TAB# #TAB# #TAB# return child_class #LINE# #TAB# #TAB# if not issubclass(child_class, this_abc): #LINE# #TAB# #TAB# #TAB# raise KappaError('Cannot fix docs of class that is not decendent.') #LINE# #TAB# #TAB# for name in dir(child_class): #LINE# #TAB# #TAB# #TAB# if name.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if name in this_abc.__abstractmethods__: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# child_func = getattr(this_abc, name) #LINE# #TAB# #TAB# #TAB# child_func.__doc__ = child_func.__doc__ #LINE# #TAB# #TAB# return child_class"
#LINE# #TAB# ret = [] #LINE# #TAB# for d in dir_list: #LINE# #TAB# #TAB# if preprocess and d in DIR_STRS: #LINE# #TAB# #TAB# #TAB# ret.append(DIR_STRS[d]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret.append(d) #LINE# #TAB# return ret
"#LINE# #TAB# method_map = {'console': _DEFAULT_LOG_console, 'info': _DEFAULT_LOG_INFO, #LINE# #TAB# #TAB# 'warning': _DEFAULT_LOG_WARNING, 'error': _DEFAULT_LOG_ERROR, #LINE# #TAB# #TAB# 'critical': _DEFAULT_LOG_CRITICAL} #LINE# #TAB# for method in METHODS: #LINE# #TAB# #TAB# if method not in method_map: #LINE# #TAB# #TAB# #TAB# method_map[method] = METHODS[method] #LINE# #TAB# return method_map"
#LINE# #TAB# operations = {} #LINE# #TAB# for reference in references_json: #LINE# #TAB# #TAB# for model_collection in reference['collection_items']: #LINE# #TAB# #TAB# #TAB# operation = test_operation_from_json(reference[model_collection]) #LINE# #TAB# #TAB# #TAB# operations[model_collection] = operation #LINE# #TAB# return operations
#LINE# #TAB# if cls._context is None: #LINE# #TAB# #TAB# raise ContextIsNotInitializedError #LINE# #TAB# return cls._context
#LINE# #TAB# if num < 1: #LINE# #TAB# #TAB# return [num] #LINE# #TAB# if scheme == 'two': #LINE# #TAB# #TAB# return [num] #LINE# #TAB# if scheme == 'two': #LINE# #TAB# #TAB# return [num] #LINE# #TAB# if num >= scheme [0]: #LINE# #TAB# #TAB# return [num - 1] #LINE# #TAB# if num < scheme[1]: #LINE# #TAB# #TAB# return [num] #LINE# #TAB# return [num]
"#LINE# #TAB# global _enabled #LINE# #TAB# if not _enabled: #LINE# #TAB# #TAB# _enabled = False #LINE# #TAB# #TAB# return #LINE# #TAB# while not _enabled: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if stop() is True: #LINE# #TAB# #TAB# #TAB# #TAB# _enabled = True #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# time.sleep(0.1) #LINE# #TAB# #TAB# except (KeyboardInterrupt, SystemExit): #LINE# #TAB# #TAB# #TAB# _enabled = False"
#LINE# #TAB# cmdlist = [cmd] #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# cmdlist.append('v') #LINE# #TAB# else: #LINE# #TAB# #TAB# cmdlist.append('l') #LINE# #TAB# cmdlist.append(archive) #LINE# #TAB# return cmdlist
"#LINE# #TAB# use_addr = np.array([(0) for c in connections]) #LINE# #TAB# use_addr.sort() #LINE# #TAB# return use_addr[::-1], use_addr[1::-1]"
"#LINE# #TAB# matrix = np.zeros((shape[0], shape[1]), dtype=float) #LINE# #TAB# for i in range(shape[1]): #LINE# #TAB# #TAB# x = score_x_of_a_kind_zahtzee(shape, node_status=node_status) #LINE# #TAB# #TAB# matrix[i] = x #LINE# #TAB# if return_count: #LINE# #TAB# #TAB# return matrix #LINE# #TAB# else: #LINE# #TAB# #TAB# return matrix"
"#LINE# #TAB# items = path.split('.') #LINE# #TAB# if len(items) == 1: #LINE# #TAB# #TAB# tdict = tdict[items[0]] #LINE# #TAB# elif len(items) == 2: #LINE# #TAB# #TAB# tdict = tdict[items[0]] #LINE# #TAB# else: #LINE# #TAB# #TAB# assert len(items) == len(path) #LINE# #TAB# #TAB# for item in items[1:]: #LINE# #TAB# #TAB# #TAB# fmt_str(tdict, item) #LINE# #TAB# return tdict"
"#LINE# #TAB# if not hasattr(engine, 'execute'): #LINE# #TAB# #TAB# return False #LINE# #TAB# variables = dict(engine.execute( #LINE# #TAB# #TAB#'show variables where variable_name like ""innodb_large_prefix"" or variable_name like ""innodb_file_format"";' #LINE# #TAB# #TAB# ).fetchall()) #LINE# #TAB# if variables.get('innodb_file_format', 'Barracuda' #LINE# #TAB# #TAB# ) == 'Barracuda' and variables.get('innodb_large_prefix', 'ON' #LINE# #TAB# #TAB# ) == 'ON': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# component = cache.get(id, None) #LINE# #TAB# if component: #LINE# #TAB# #TAB# return getattr(component, 'no_author', None) #LINE# #TAB# return None"
"#LINE# #TAB# credentials = np.zeros(x.shape[0]) #LINE# #TAB# for i in range(x.shape[0]): #LINE# #TAB# #TAB# for j in range(x.shape[1]): #LINE# #TAB# #TAB# #TAB# credentials[i, j] = 1 #LINE# #TAB# return credentials"
"#LINE# #TAB# if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) > timedelta(0): #LINE# #TAB# #TAB# logging.warning('Warning: aware datetimes are interpreted as if they were naive') #LINE# #TAB# return {'date': dt.strftime('%Y-%m-%dT%H:%M:%S'), 'time': dt. #LINE# #TAB# #TAB# strftime('%H:%M:%S'), 'tzinfo': dt.tzinfo}"
#LINE# #TAB# options_dict = {} #LINE# #TAB# options_dict['x'] = x - pagesize / 2 #LINE# #TAB# options_dict['y'] = y - pagesize / 2 #LINE# #TAB# options_dict['width'] = w #LINE# #TAB# options_dict['height'] = h #LINE# #TAB# options_dict['pagesize'] = pagesize #LINE# #TAB# return options_dict
#LINE# #TAB# stream = io.StringIO(data) #LINE# #TAB# for line in stream: #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# steps = line.strip().split(' ') #LINE# #TAB# #TAB# #TAB# if len(steps) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# yield steps #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break
"#LINE# #TAB# p = subprocess.Popen(['git', 'ls-files', '--error-unmatch', filename], #LINE# #TAB# #TAB# stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# output, _ = p.communicate() #LINE# #TAB# if p.returncode == 0: #LINE# #TAB# #TAB# return output.strip() #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# country = _get_local_user(code) #LINE# #TAB# if country is None: #LINE# #TAB# #TAB# raise voluptuous.Invalid('Invalid country code {}'.format(code)) #LINE# #TAB# return country
"#LINE# #TAB# out_file = out_txt + '.pbar' #LINE# #TAB# cmd =''.join(['pbar', '-h', out_file]) #LINE# #TAB# run(cmd) #LINE# #TAB# with open(out_file, 'w') as out_handle: #LINE# #TAB# #TAB# for line in open(bam): #LINE# #TAB# #TAB# #TAB# line = line.rstrip() #LINE# #TAB# #TAB# #TAB# line = line.split('\t') #LINE# #TAB# #TAB# #TAB# out_handle.write(line[0]) #LINE# #TAB# return out_file"
"#LINE# #TAB# metadata = get_metadata(provider, region, builder) #LINE# #TAB# if metadata is None: #LINE# #TAB# #TAB# raise ValueError('Provider {} has no metadata.'.format(provider)) #LINE# #TAB# vocab_path = get_vocab_filename(metadata, builder) #LINE# #TAB# if not os.path.exists(vocab_path): #LINE# #TAB# #TAB# logger.warning(""Vocab file %s does not exist."", vocab_path) #LINE# #TAB# else: #LINE# #TAB# #TAB# with open(vocab_path, 'rb') as f: #LINE# #TAB# #TAB# #TAB# content = f.read() #LINE# #TAB# #TAB# vocab = to_unicode(content) #LINE# #TAB# return vocab"
"#LINE# #TAB# if isinstance(p[1], ast.Union): #LINE# #TAB# #TAB# p[0] = p[1][1] #LINE# #TAB# else: #LINE# #TAB# #TAB# p[0] = p[1]"
"#LINE# #TAB# cols = ((cols - 6) *.85) #LINE# #TAB# if shorten is False or len(url) < cols: #LINE# #TAB# #TAB# return url #LINE# #TAB# split = int(cols *.5) #LINE# #TAB# return url[:split] + ""..."" + url[-split:]"
"#LINE# #TAB# response = request_handler.make_request('GET', '/reports') #LINE# #TAB# matches = filter(lambda x: x['name'] == name, response.json()) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return None #LINE# #TAB# report = cls.get_by_uuid(request_handler, matches) #LINE# #TAB# return report"
#LINE# #TAB# if'minimum' not in tag: #LINE# #TAB# #TAB# return True #LINE# #TAB# if'maximum' not in tag: #LINE# #TAB# #TAB# return True #LINE# #TAB# return tag <= schema['maximum']
"#LINE# #TAB# import tempfile #LINE# #TAB# import networkx as nx #LINE# #TAB# filelist = [] #LINE# #TAB# for u, v in graph.edges(): #LINE# #TAB# #TAB# filelist.append(join(u, v)) #LINE# #TAB# _plot_circ_layout(filelist, graph) #LINE# #TAB# return filelist"
#LINE# #TAB# prompt = ( #LINE# #TAB# #TAB# ) #LINE# #TAB# if sys.platform == 'darwin': #LINE# #TAB# #TAB# if click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ) and click.confirm( #LINE# #TAB# #TAB# #TAB# ): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# t = {} #LINE# #TAB# for catalog in find_ec2_catalog(): #LINE# #TAB# #TAB# t[catalog.name] = { #LINE# #TAB# #TAB# #TAB#'region': catalog.region, #LINE# #TAB# #TAB# #TAB# 'name': catalog.name, #LINE# #TAB# #TAB# #TAB# 'description': catalog.description, #LINE# #TAB# #TAB# } #LINE# #TAB# return t"
#LINE# #TAB# app = NetifyApplication(config_file) #LINE# #TAB# app.register_blueprint(routes.blueprint) #LINE# #TAB# return app
#LINE# #TAB# if response[0] == 'Gaussian': #LINE# #TAB# #TAB# return 100 #LINE# #TAB# elif response[0] == 'Unknown': #LINE# #TAB# #TAB# return -1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1
#LINE# #TAB# if ranges.shape[0] < 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if ranges.shape[1] == 1: #LINE# #TAB# #TAB# return ranges[0] == 0 #LINE# #TAB# edges = np.zeros(ranges.shape) #LINE# #TAB# edges[1:-1] = (ranges[:-1] + ranges[1:]) / 2 #LINE# #TAB# edges[0] = ranges[0] - (ranges[1] - ranges[0]) / 2 #LINE# #TAB# edges[-1] = ranges[-1] + (ranges[-1] - ranges[-2]) / 2 #LINE# #TAB# if edges.sum() == 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# pos = lut.label_to_index(label) #LINE# #TAB# dir_labels = use_lut.get(label, []) #LINE# #TAB# if dir_labels is None: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# return dir_labels[pos]"
"#LINE# #TAB# if not isinstance(string, string_types): #LINE# #TAB# #TAB# string = string.decode('utf8') #LINE# #TAB# try: #LINE# #TAB# #TAB# id_pairs = re.findall('[0-9a-zA-Z_-]+', string) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# id_pairs = [string] #LINE# #TAB# try: #LINE# #TAB# #TAB# id_pairs = re.findall('[0-9a-zA-Z_]+', string) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# id_pairs = [string] #LINE# #TAB# try: #LINE# #TAB# #TAB# return id_pairs #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if len(control) > len(test): #LINE# #TAB# #TAB# return 0 #LINE# #TAB# if test[0] > control[1]: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0
#LINE# #TAB# global ASYNC_EVENTLET_POOL #LINE# #TAB# if ASYNC_EVENTLET_POOL is None: #LINE# #TAB# #TAB# ASYNC_EVENTLET_POOL = threading.EventletPool() #LINE# #TAB# return ASYNC_EVENTLET_POOL
#LINE# #TAB# dvs = session.query(models.DVS).filter_by(name=dvs_name).first() #LINE# #TAB# if dvs is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# port_groups = session.query(models.PortGroup).filter_by(portgroup_name= #LINE# #TAB# #TAB# pg_name).all() #LINE# #TAB# for port_group in port_groups: #LINE# #TAB# #TAB# vlan_id = port_group.vlan_id #LINE# #TAB# #TAB# return vlan_id
#LINE# #TAB# if option is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [option]
"#LINE# #TAB# if not module_exists(): #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# __import__(module_name) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# module = sys.modules[module_name] #LINE# #TAB# if not hasattr(module, '__path__'): #LINE# #TAB# #TAB# return None #LINE# #TAB# path = os.path.join(module.__path__[0], '__init__.py') #LINE# #TAB# try: #LINE# #TAB# #TAB# imp.find_module(path) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return module"
#LINE# #TAB# sel = [] #LINE# #TAB# if database_name not in project_list(): #LINE# #TAB# #TAB# print('Database %s not in project list' % database_name) #LINE# #TAB# #TAB# return None #LINE# #TAB# for database in project_list(): #LINE# #TAB# #TAB# if not database_name.endswith('/'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# sel.append(database) #LINE# #TAB# if len(sel) > 0: #LINE# #TAB# #TAB# for database in sel: #LINE# #TAB# #TAB# #TAB# print('Database %s has activities' % database) #LINE# #TAB# return database_name
#LINE# #TAB# match_name = song_name.lower() + song_title.lower() #LINE# #TAB# match_title = song_title.lower() + song_name.lower() #LINE# #TAB# if match_name == match_title: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# if match_name == match_title: #LINE# #TAB# #TAB# return 10 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1
#LINE# #TAB# canonical_dialect = None #LINE# #TAB# for dialect in dialects: #LINE# #TAB# #TAB# if not dialect: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# canonical_dialect = dialect #LINE# #TAB# if canonical_dialect is None: #LINE# #TAB# #TAB# raise ValueError('Unrecognized dialect: {}'.format(dialects)) #LINE# #TAB# return canonical_dialect
#LINE# #TAB# if initialized: #LINE# #TAB# #TAB# pp.debug('trunc_normal_expval is initialized') #LINE# #TAB# else: #LINE# #TAB# #TAB# pp.debug('trunc_normal_expval is not None') #LINE# #TAB# global _initialized #LINE# #TAB# _initialized = initialized
"#LINE# #TAB# if not isinstance(run, bool): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# run = bool(run) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not isinstance(run, int): #LINE# #TAB# #TAB# return False #LINE# #TAB# run = int(run) #LINE# #TAB# return True"
"#LINE# #TAB# global filedir_pdf #LINE# #TAB# try: #LINE# #TAB# #TAB# if not os.path.exists(filedir_pdf): #LINE# #TAB# #TAB# #TAB# os.makedirs(filedir_pdf) #LINE# #TAB# #TAB# with open(filedir_pdf, 'wb') as f: #LINE# #TAB# #TAB# #TAB# f.write(pdf_bytes) #LINE# #TAB# #TAB# filedir_pdf += '.pdf'"
"#LINE# #TAB# index = {v: k for k, v in graph.edges(keys=True) if graph.has_edge(v, k)} #LINE# #TAB# for v in graph.edges(keys=True): #LINE# #TAB# #TAB# if graph.has_edge(v, k): #LINE# #TAB# #TAB# #TAB# graph.remove_edge(v, k) #LINE# #TAB# return index"
"#LINE# #TAB# rows = [None] * numLabels #LINE# #TAB# for f in os.listdir(dirPath): #LINE# #TAB# #TAB# if f.endswith('.csv'): #LINE# #TAB# #TAB# #TAB# f = open(dirPath + f, 'r') #LINE# #TAB# #TAB# #TAB# row = next(f) #LINE# #TAB# #TAB# #TAB# if modify: #LINE# #TAB# #TAB# #TAB# #TAB# row = np.array(row) #LINE# #TAB# #TAB# #TAB# #TAB# rows[row == 0] = np.nan #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# rows[row] = row #LINE# #TAB# return rows"
#LINE# #TAB# expires = datetime.datetime.fromtimestamp(expiration) #LINE# #TAB# return {'sig': {'sig': base64.b64encode(string_to_sign.encode('utf-8')). #LINE# #TAB# #TAB# decode('utf-8')}}
"#LINE# #TAB# try: #LINE# #TAB# #TAB# api.selinux(domain, logger) #LINE# #TAB# #TAB# return True #LINE# #TAB# except libvirtError as e: #LINE# #TAB# #TAB# logger.error('API error message: %s' % str(e)) #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# file_stats = {} #LINE# #TAB# status = None #LINE# #TAB# if os.path.exists(file_path): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(file_path, 'r', encoding='utf-8') as f: #LINE# #TAB# #TAB# #TAB# #TAB# status = json.load(f) #LINE# #TAB# #TAB# except JSONDecodeError: #LINE# #TAB# #TAB# #TAB# logging.error('Failed to json_file_to_dict: %s', file_path) #LINE# #TAB# #TAB# #TAB# raise OSError('Failed to json_file_to_dict') #LINE# #TAB# if not status: #LINE# #TAB# #TAB# logging.error('Failed to fsync: %s', file_path) #LINE# #TAB# return file_stats"
"#LINE# #TAB# if isinstance(obj, argparse.Namespace): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB#'version': obj.version, #LINE# #TAB# #TAB# #TAB# 'format': obj.format, #LINE# #TAB# #TAB# #TAB# 'help': obj.help, #LINE# #TAB# #TAB# } #LINE# #TAB# elif isinstance(obj, optparse.OptionParser): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB#'version': str(obj.version), #LINE# #TAB# #TAB# #TAB# 'options': obj.options, #LINE# #TAB# #TAB# } #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
"#LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fd = getfd() #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if fd.isatty(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# sock = socket.socket() #LINE# #TAB# #TAB# sock.getsockopt(socket.SOL_SOCKET, socket.SO_TYPE) #LINE# #TAB# #TAB# return True #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return False"
"#LINE# #TAB# found = [] #LINE# #TAB# for name, logger in loggers.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# found.append((name, logger)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return found"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return next(os.walk(directory))[0] #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# if len(rois) == 1: #LINE# #TAB# #TAB# return df[(rois == 0), :, :] #LINE# #TAB# else: #LINE# #TAB# #TAB# return df[(rois == 1), :, :]"
"#LINE# #TAB# fjac = infodic['fjac'] #LINE# #TAB# ipvt = infodic['ipvt'] #LINE# #TAB# n = len(p) #LINE# #TAB# cov_x = np.zeros((n, n)) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(i + 1, n): #LINE# #TAB# #TAB# #TAB# cov_x[i, j] = fjac[i][j] / p[j] #LINE# #TAB# return cov_x"
#LINE# #TAB# iterable = iter(iterable) #LINE# #TAB# for i in range(n - 1): #LINE# #TAB# #TAB# avg = sum(iterable) / n #LINE# #TAB# #TAB# if avg!= iterable[i]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# cmd = 'ipa -l' #LINE# #TAB# if check_platform: #LINE# #TAB# #TAB# result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr= #LINE# #TAB# #TAB# #TAB# subprocess.PIPE) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = subprocess.run(cmd, stdout=subprocess.PIPE) #LINE# #TAB# return result.returncode == 0"
"#LINE# #TAB# if t_x[0]!= t_x[1]: #LINE# #TAB# #TAB# return 0, 0 #LINE# #TAB# elif i == 0: #LINE# #TAB# #TAB# return t_x[0], 1 #LINE# #TAB# elif i == 1: #LINE# #TAB# #TAB# return t_x[2], 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return t_x[0], t_x[1], t_x[2]"
"#LINE# #TAB# n = len(x) #LINE# #TAB# stetson_means = np.zeros(n) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# if fills[i, 0]: #LINE# #TAB# #TAB# #TAB# stetson_means[i] += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# stetson_means[i] = 0 #LINE# #TAB# return stetson_means"
"#LINE# #TAB# lib = Library() #LINE# #TAB# lib.add_polynomial(polynomial_q_to_deg(a), polynomial_q_to_deg(b)) #LINE# #TAB# lib.add_polynomial(polynomial_r_to_deg(b), polynomial_r_to_deg(a)) #LINE# #TAB# return lib"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = sqlite3.connect(db_file) #LINE# #TAB# #TAB# return conn #LINE# #TAB# except Error as e: #LINE# #TAB# #TAB# print(bcolors.FAIL, e, bcolors.ENDC) #LINE# #TAB# return None"
"#LINE# #TAB# if not os.path.exists(cachefname): #LINE# #TAB# #TAB# return False, None #LINE# #TAB# if not os.path.isdir(cachefname): #LINE# #TAB# #TAB# return False, None #LINE# #TAB# try: #LINE# #TAB# #TAB# open(cachefname, 'rb').read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return False, None #LINE# #TAB# return True, cachefname"
#LINE# #TAB# if cur_dev_size < thres_dev_size: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# interval = int(cur_dev_size / thres_dev_size) #LINE# #TAB# if interval == 0: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# return interval
"#LINE# #TAB# grams = {} #LINE# #TAB# for pos in range(1, 13): #LINE# #TAB# #TAB# if year == pos: #LINE# #TAB# #TAB# #TAB# ngrams[pos] = 1 #LINE# #TAB# #TAB# elif year > 13: #LINE# #TAB# #TAB# #TAB# ngrams[pos + 1] = 1 #LINE# #TAB# return grams"
"#LINE# #TAB# L = zeros(S.shape[1]) #LINE# #TAB# for i in range(S.shape[1]): #LINE# #TAB# #TAB# for j in range(i): #LINE# #TAB# #TAB# #TAB# L[i, j] = S[i, j] * np.log(variance) #LINE# #TAB# return L"
#LINE# #TAB# if node is not None: #LINE# #TAB# #TAB# pubdate = node.find('.//pubdate') #LINE# #TAB# #TAB# if pubdate is None: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# elif len(pubdate.text) == 0: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return pubdate.text
"#LINE# #TAB# return {record.name for record in SeqIO.parse(trimmed_aln, 'fasta') if #LINE# #TAB# #TAB# str(record.name)!= 'X'}"
#LINE# #TAB# client = ofxclient.Client() #LINE# #TAB# client.download_accounts(accounts) #LINE# #TAB# config = client.configuration() #LINE# #TAB# if days: #LINE# #TAB# #TAB# config['days'] = days #LINE# #TAB# else: #LINE# #TAB# #TAB# config['days'] = days #LINE# #TAB# return config
"#LINE# #TAB# if not isinstance(mats[0], sp.spmatrix): #LINE# #TAB# #TAB# raise ValueError('Input must be a list of sp.spmatrix objects.') #LINE# #TAB# output = np.zeros(mats[0].shape, dtype=mats[0].dtype) #LINE# #TAB# for m in mats: #LINE# #TAB# #TAB# output[(m.nonzero()), :] = m.pixel_median() #LINE# #TAB# return output"
"#LINE# #TAB# geni_input = geni_input if geni_input is not None else geni_input[0] #LINE# #TAB# profile_iso = profile_to_iso(profile, geni_input, type_geni) #LINE# #TAB# time_format = '%Y%m%d%H%M%S' #LINE# #TAB# return time_format % profile"
"#LINE# #TAB# if mn is None and mx is None: #LINE# #TAB# #TAB# raise ValueError('{} cannot be of type None'.format(dtype)) #LINE# #TAB# if mn is None: #LINE# #TAB# #TAB# mn = np.array([], dtype=dtype) #LINE# #TAB# if mx is None: #LINE# #TAB# #TAB# mx = np.array([], dtype=dtype) #LINE# #TAB# if dtype!= 'float64': #LINE# #TAB# #TAB# raise ValueError('{} cannot be converted to {}'.format(dtype, #LINE# #TAB# #TAB# #TAB# mn)) #LINE# #TAB# if dtype!= 'float64': #LINE# #TAB# #TAB# raise ValueError('{} cannot be converted to {}'.format(dtype, #LINE# #TAB# #TAB# #TAB# mx)) #LINE# #TAB# return mn, mx"
#LINE# #TAB# if when is None: #LINE# #TAB# #TAB# when = time.time() #LINE# #TAB# m = hashlib.sha1() #LINE# #TAB# m.update(str(time.time()).encode('utf8')) #LINE# #TAB# m.update(str(when).encode('utf8')) #LINE# #TAB# m.hexdigest() #LINE# #TAB# return m.hexdigest()[:10]
#LINE# #TAB# name = slot.name +'s' #LINE# #TAB# if slot.cardinality and not slot.explicit: #LINE# #TAB# #TAB# name += 'c' #LINE# #TAB# return name
"#LINE# #TAB# for test in [lambda x: ipaddress.IPv6Network(x)._prefixlen!= 128, lambda #LINE# #TAB# #TAB# x: ipaddress.IPv4Address(x)._prefixlen!= 128]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return bool(test(value)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
"#LINE# #TAB# dir_path = os.path.dirname(filename) #LINE# #TAB# if not os.path.exists(dir_path): #LINE# #TAB# #TAB# os.makedirs(dir_path) #LINE# #TAB# a = os.stat(filename) #LINE# #TAB# rows = [float(a.st_size[1]), float(a.st_size[0]), float(a.st_size[1]), #LINE# #TAB# #TAB# float(a.st_size[2])] #LINE# #TAB# tileset = [] #LINE# #TAB# for i in range(len(rows)): #LINE# #TAB# #TAB# tileset.append([rows[i], rows[i + 1], rows[i + 2]]) #LINE# #TAB# return tileset"
#LINE# #TAB# lease_id = cls.int_to_string(lease_id) #LINE# #TAB# status = cls.int_to_string(status) #LINE# #TAB# if status is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if status not in cls.STATUSES: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# if os.path.isfile(LOG_FILE_NAME): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# _, filename = os.path.split(filename) #LINE# #TAB# parameters = None #LINE# #TAB# if filename!= '': #LINE# #TAB# #TAB# base_name = os.path.basename(filename) #LINE# #TAB# #TAB# if not os.path.exists(base_name): #LINE# #TAB# #TAB# #TAB# os.makedirs(base_name) #LINE# #TAB# for key in parameters.keys(): #LINE# #TAB# #TAB# parameters[key] = parameters[key] #LINE# #TAB# return parameters"
#LINE# #TAB# logger = logging.getLogger(__name__) #LINE# #TAB# logger.setLevel(logging.INFO) #LINE# #TAB# logger.propagate = True
"#LINE# #TAB# trimmed_alignment_size = float(untrimmed_alignment_size) #LINE# #TAB# if trimmed_alignment_size < 0: #LINE# #TAB# #TAB# raise ValueError('""untrimmed_alignment_size"" must be >= 0') #LINE# #TAB# elif untrimmed_alignment_size >= no_sites_trimmed: #LINE# #TAB# #TAB# raise ValueError('""no_sites_trimmed"" must be >= no_sites_trimmed') #LINE# #TAB# return 0 if trimmed_alignment_size == 0 else trimmed_alignment_size / no_sites_trimmed"
"#LINE# #TAB# if not color[0] == '#': #LINE# #TAB# #TAB# color = color[1:] #LINE# #TAB# if len(color)!= 6: #LINE# #TAB# #TAB# raise ValueError('Input color must be in 6 digit hex string format: {}'. #LINE# #TAB# #TAB# #TAB# format(color)) #LINE# #TAB# r, g, b = color[:2], color[2:4], color[4:] #LINE# #TAB# r = int(r, 16) #LINE# #TAB# g = int(g, 16) #LINE# #TAB# b = int(b, 16) #LINE# #TAB# return r, g, b"
"#LINE# #TAB# contsign = np.sum(v * v, axis=0) #LINE# #TAB# kv = dict(k=k, contsign=contsign) #LINE# #TAB# return kv"
"#LINE# #TAB# temp_file = tempfile.NamedTemporaryFile(delete=False) #LINE# #TAB# temp_file.write(csv_content) #LINE# #TAB# hooks = [] #LINE# #TAB# with open(temp_file.name, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# hook_dict = json.loads(line) #LINE# #TAB# #TAB# #TAB# temp_file.close() #LINE# #TAB# #TAB# #TAB# hooks.append(hook_dict) #LINE# #TAB# temp_file.close() #LINE# #TAB# return hooks"
"#LINE# #TAB# url = SHIFTS_URL.format(code) #LINE# #TAB# headers = {'Accept': 'application/json'} #LINE# #TAB# resp = requests.get(url, headers=headers) #LINE# #TAB# resp.raise_for_status() #LINE# #TAB# data = json.loads(resp.text) #LINE# #TAB# cycle = {'code': code} #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# if key not in cycle: #LINE# #TAB# #TAB# #TAB# cycle[key] = data[key] #LINE# #TAB# return cycle"
#LINE# #TAB# l = [] #LINE# #TAB# for x in tab: #LINE# #TAB# #TAB# if x not in l: #LINE# #TAB# #TAB# #TAB# l.append(x) #LINE# #TAB# return l
"#LINE# #TAB# X = np.atleast_2d(X) #LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter(""ignore"") #LINE# #TAB# #TAB# H = np.zeros(X.shape[0]) #LINE# #TAB# #TAB# for i in range(X.shape[0]): #LINE# #TAB# #TAB# #TAB# H[i] = gamma * np.log(np.sum(X[i] ** 2, axis=0) + #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# np.log(np.sum(X[i] ** 2, axis=0) / np.sum(X[i] ** 2, axis=0))) #LINE# #TAB# return H"
#LINE# #TAB# cls.command_sub = 'operatingsystem' #LINE# #TAB# result = cls.execute(cls._construct_command(options)) #LINE# #TAB# return result
#LINE# #TAB# pos = m.position_of(t) #LINE# #TAB# neg = m.position_of(t) #LINE# #TAB# return pos - neg
#LINE# #TAB# padding = max_value - min_value #LINE# #TAB# return (im - padding) / padding
"#LINE# #TAB# if z < 0: #LINE# #TAB# #TAB# raise ValueError(z) #LINE# #TAB# if z2 < 0: #LINE# #TAB# #TAB# raise ValueError(z2) #LINE# #TAB# binding = 0 #LINE# #TAB# for i in range(1, len(z)): #LINE# #TAB# #TAB# if z[i]!= z2[i]: #LINE# #TAB# #TAB# #TAB# binding += 1 / (1 + z2[i] * NA) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# binding -= 1 / (1 + z[i] * NA) #LINE# #TAB# return binding"
#LINE# #TAB# vocab_path = trim_vocab_path(vocab_path) #LINE# #TAB# if not vocab_path: #LINE# #TAB# #TAB# return [] #LINE# #TAB# words = vocab_path.split('_') #LINE# #TAB# label_list = [] #LINE# #TAB# for word in words: #LINE# #TAB# #TAB# if word == 'field-key': #LINE# #TAB# #TAB# #TAB# label_list.append(word) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# label_list.append(word) #LINE# #TAB# return label_list
"#LINE# #TAB# if allowedreacs is None: #LINE# #TAB# #TAB# allowedreacs = [model] #LINE# #TAB# out = model.copy() #LINE# #TAB# for reac in model: #LINE# #TAB# #TAB# out.reactants = [julian_century(r, allowedreacs=allowedreacs, #LINE# #TAB# #TAB# #TAB# tol=tol) for r in reac.reactions] #LINE# #TAB# #TAB# out.reactions = [julian_century(r, allowedreacs=allowedreacs, #LINE# #TAB# #TAB# #TAB# tol=tol) for r in reac.reactions] #LINE# #TAB# return out"
"#LINE# #TAB# new_paths = [] #LINE# #TAB# for path in list_of_paths: #LINE# #TAB# #TAB# if path.startswith('./'): #LINE# #TAB# #TAB# #TAB# path = path[1:] #LINE# #TAB# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# #TAB# raise IOError('File not found: {}'.format(path)) #LINE# #TAB# #TAB# new_paths.append(path) #LINE# #TAB# json_path = '/'.join(new_paths) #LINE# #TAB# with h5py.File(json_path, 'r') as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# return data"
"#LINE# #TAB# dt = datetime.datetime.combine(date, datetime.time()) #LINE# #TAB# dt = tzinfo.localize(dt) #LINE# #TAB# return dt"
"#LINE# #TAB# for i in range(n_adult): #LINE# #TAB# #TAB# for j in range(n_child): #LINE# #TAB# #TAB# #TAB# participants[i][j] = participants[i][j + 1].replace('.csv', '') #LINE# #TAB# participants = list(set(participants.values())) #LINE# #TAB# return participants"
"#LINE# #TAB# injectable = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# for ini_path in ini_paths: #LINE# #TAB# #TAB# #TAB# if os.path.isfile(ini_path): #LINE# #TAB# #TAB# #TAB# #TAB# with open(ini_path, 'r') as ini_file: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if 'P.getParameters' in ini_file: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# injectable.extend(ini_file.readlines()) #LINE# #TAB# return injectable"
"#LINE# #TAB# x = read_and_sort_centrimo_df(x, shape=[len(x.shape) - 1]) #LINE# #TAB# return x"
"#LINE# #TAB# s = re.sub('[^\\w\\s]', '_', s) #LINE# #TAB# s = re.sub('\\s+', '_', s) #LINE# #TAB# return s"
"#LINE# #TAB# for pattern, repl in TEMPLATE_PATTERNS.items(): #LINE# #TAB# #TAB# match = pattern.match(date) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# date = date[:match.start()] + repl #LINE# #TAB# #TAB# #TAB# if match.groupdict()!= {'date': #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return date #LINE# #TAB# return ''"
"#LINE# #TAB# jump = offset % 4 #LINE# #TAB# if jump: #LINE# #TAB# #TAB# offset += 4 - jump #LINE# #TAB# (default, low, high), offset = _unpack(_STRUCT_TYPE_PREFIX, bc, offset) #LINE# #TAB# joffs = list() #LINE# #TAB# for _index in range(high - low + 1): #LINE# #TAB# #TAB# j, offset = _unpack(_STRUCT_TYPE_PREFIX, bc, offset) #LINE# #TAB# #TAB# joffs.append(j) #LINE# #TAB# return (default, low, high, joffs), offset"
"#LINE# #TAB# res = asse_equal_start_with_none_re.match(logical_line #LINE# #TAB# #TAB# ) or asse_equal_end_with_none_re.match(logical_line) #LINE# #TAB# if res: #LINE# #TAB# #TAB# yield 0, 'N318: assertEqual(A, None) or assertEqual(None, A) sentences not allowed'"
"#LINE# #TAB# if not text: #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(text, unicode): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return text.decode('UTF-8') #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return text #LINE# #TAB# elif isinstance(text, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return text.decode('UTF-8') #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return text"
#LINE# #TAB# n = int_like(n) #LINE# #TAB# if n > 0: #LINE# #TAB# #TAB# if big_endian: #LINE# #TAB# #TAB# #TAB# return [bytearray(i) for i in range(n)] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return [bytearray(i) for i in range(n)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [bytearray(i) for i in range(n)]
"#LINE# #TAB# cat_ctx = getattr(g, 'cat_ctx', None) #LINE# #TAB# if not cat_ctx: #LINE# #TAB# #TAB# cat_ctx = { #LINE# #TAB# #TAB# #TAB#'request': {}, #LINE# #TAB# #TAB# #TAB# 'categories': {} #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# g.cat_ctx = cat_ctx #LINE# #TAB# return g.cat_ctx"
"#LINE# #TAB# Optional[Exception]=None) ->Any: #LINE# #TAB# value = request.GET.get(name, None) #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# value = request.META.get(name) #LINE# #TAB# #TAB# if not value: #LINE# #TAB# #TAB# #TAB# raise web.HTTPBadRequest(body=f'Missing parameter: {name}') #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# raise web.HTTPBadRequest(body=f'Missing parameter: {name}') #LINE# #TAB# if error_if_missing is not None: #LINE# #TAB# #TAB# raise error_if_missing() #LINE# #TAB# return value"
#LINE# #TAB# func_name = None #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# if token.kind == TokenKind.NAME and func_name is None: #LINE# #TAB# #TAB# #TAB# func_name = token.value.lower() #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return func_name
#LINE# #TAB# try: #LINE# #TAB# #TAB# if '=' in arg: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# docker_arg = arg_dict[arg.split('=')[1]] #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# docker_arg = arg #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# docker_arg = arg_dict #LINE# #TAB# #TAB# return docker_arg #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return arg_dict
"#LINE# #TAB# db_files = os.listdir(os.getcwd()) #LINE# #TAB# db_tables = [] #LINE# #TAB# for fn in os.listdir(db_files): #LINE# #TAB# #TAB# if fn.endswith('.db'): #LINE# #TAB# #TAB# #TAB# db_tables.append(os.path.join(db_files, fn)) #LINE# #TAB# db_files = [os.path.join(db_files, '__init__.db')] #LINE# #TAB# for fn in db_tables: #LINE# #TAB# #TAB# if not os.path.exists(fn): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# with open(fn, 'r') as f: #LINE# #TAB# #TAB# #TAB# db_tables.append(parse_table(f.read())) #LINE# #TAB# return db_tables"
"#LINE# #TAB# output = os.environ.get('IACA_OUTPUT', None) #LINE# #TAB# if output is None: #LINE# #TAB# #TAB# output = {} #LINE# #TAB# #TAB# for key, value in DEFAULTS.items(): #LINE# #TAB# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# #TAB# output[key] = os.path.join(os.path.expanduser('~'), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# value) #LINE# #TAB# return output"
"#LINE# #TAB# M = nx.to_numpy_matrix(G, nodelist=nodelist) #LINE# #TAB# return M"
#LINE# #TAB# d = {} #LINE# #TAB# for i in range(num_peaks): #LINE# #TAB# #TAB# t = image[i - 1:i + 2] #LINE# #TAB# #TAB# d[t[0]] = t[1] #LINE# #TAB# return d
"#LINE# #TAB# template_dir = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# plugin_templates = os.listdir(template_dir) #LINE# #TAB# dialog_list = [] #LINE# #TAB# for template in plugin_templates: #LINE# #TAB# #TAB# file_path = os.path.join(template_dir, template) #LINE# #TAB# #TAB# if not os.path.isfile(file_path): #LINE# #TAB# #TAB# #TAB# dialog_list.append((template, os.path.basename(file_path))) #LINE# #TAB# dialog_list.sort() #LINE# #TAB# if len(dialog_list) == 1: #LINE# #TAB# #TAB# return dialog_list[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return dialog_list[0]"
"#LINE# #TAB# theta = 0.036 #LINE# #TAB# if target == 400: #LINE# #TAB# #TAB# theta = theta_Cl_SO4_PK74E(w, h) #LINE# #TAB# elif target == 5: #LINE# #TAB# #TAB# theta = theta_Cl_SO4_PK74W(w, h) #LINE# #TAB# elif target == 6: #LINE# #TAB# #TAB# theta = theta_Cl_SO4_PK74W(w, h) #LINE# #TAB# return theta"
"#LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# logger = bytes() #LINE# #TAB# elif isinstance(value, bytes): #LINE# #TAB# #TAB# logger = value #LINE# #TAB# else: #LINE# #TAB# #TAB# logger = bytes(value, 'utf-8') #LINE# #TAB# return logger"
"#LINE# #TAB# for node in doc.iter(): #LINE# #TAB# #TAB# if node.text not in ('',): #LINE# #TAB# #TAB# #TAB# node.text = ''.join(node.text.split()) #LINE# #TAB# return doc"
#LINE# #TAB# if blacklist_hostnames is None: #LINE# #TAB# #TAB# blacklist_hostnames = [DEFAULT_BLACKLIST_HOSTNAME] #LINE# #TAB# cmdclass = None #LINE# #TAB# if url.endswith('/'): #LINE# #TAB# #TAB# url = url[:-1] #LINE# #TAB# cmdclass = blacklist_hostnames + cmdclass #LINE# #TAB# return cmdclass
#LINE# #TAB# rc = lib.sdl_sempost(sem) #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return rc
"#LINE# #TAB# job_binary = JobBinary(values.copy(), context) #LINE# #TAB# job_binary.find_components() #LINE# #TAB# return job_binary"
#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# err = q.get_nowait() #LINE# #TAB# #TAB# #TAB# if err: #LINE# #TAB# #TAB# #TAB# #TAB# yield err #LINE# #TAB# #TAB# except Queue.Empty: #LINE# #TAB# #TAB# #TAB# return
"#LINE# #TAB# d = {} #LINE# #TAB# for k, v in editions.items(): #LINE# #TAB# #TAB# path = k.replace('.py', '') #LINE# #TAB# #TAB# file_path = os.path.join(path, k) #LINE# #TAB# #TAB# if os.path.isfile(file_path): #LINE# #TAB# #TAB# #TAB# with open(file_path, 'r') as fp: #LINE# #TAB# #TAB# #TAB# #TAB# data = fp.read() #LINE# #TAB# #TAB# #TAB# d.update(data) #LINE# #TAB# return d"
"#LINE# #TAB# data = {} #LINE# #TAB# data[const.MSG_ID] = data_type #LINE# #TAB# code = ctypes.c_int() #LINE# #TAB# data[const.MSG_CODE] = code #LINE# #TAB# data[const.MSG_DATA] = data.value #LINE# #TAB# return {'code': code, 'data': data.value}"
"#LINE# #TAB# close_expr = get_close_expr(l, r, sep, expr) #LINE# #TAB# if not allow_missing_close: #LINE# #TAB# #TAB# assert close_expr is not None #LINE# #TAB# names = [] #LINE# #TAB# for i in range(0, l, r): #LINE# #TAB# #TAB# name = expr[:i] #LINE# #TAB# #TAB# names.append(name) #LINE# #TAB# return names"
"#LINE# #TAB# if not os.path.exists(WORK_DIRECTORY): #LINE# #TAB# #TAB# os.mkdir(WORK_DIRECTORY) #LINE# #TAB# filepath = os.path.join(WORK_DIRECTORY, filename) #LINE# #TAB# response = requests.get(filepath) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# return response.content"
"#LINE# #TAB# catch_errors.check_for_period_error(data, period) #LINE# #TAB# wma = make_wma(data, period) #LINE# #TAB# sum_log = np.sum(wma) #LINE# #TAB# return [wma - sum_log]"
#LINE# #TAB# params = [] #LINE# #TAB# for i in range(len(paramMat)): #LINE# #TAB# #TAB# params.append(paramMat[i]) #LINE# #TAB# return params
#LINE# #TAB# frequencies = np.all(abs(entry.data) > 0) #LINE# #TAB# if np.any(frequencies): #LINE# #TAB# #TAB# return -1.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return frequencies
#LINE# #TAB# ustr = ustr.encode('utf-8') #LINE# #TAB# if is_null_unit(ustr): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# as_unit(ustr) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# if len(phi.atoms) == 0: #LINE# #TAB# #TAB# return #LINE# #TAB# filenames = [] #LINE# #TAB# for a in phi.atoms: #LINE# #TAB# #TAB# if a.is_atom(): #LINE# #TAB# #TAB# #TAB# if not filename_test(a): #LINE# #TAB# #TAB# #TAB# #TAB# filenames.append(a) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# errors = [] #LINE# #TAB# #TAB# #TAB# #TAB# if a.is_literal(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# errors.append('{0} is not a valid filename'.format(a)) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# filenames.append(a) #LINE# #TAB# return filenames
"#LINE# #TAB# for offset in count(0, limit): #LINE# #TAB# #TAB# r = q.offset(offset).limit(limit).all() #LINE# #TAB# #TAB# for row in r: #LINE# #TAB# #TAB# #TAB# yield row"
"#LINE# #TAB# logging.debug(dict_str) #LINE# #TAB# c = dict_str #LINE# #TAB# try: #LINE# #TAB# #TAB# if str_ok: #LINE# #TAB# #TAB# #TAB# c = json.loads(c) #LINE# #TAB# except ValueError as e: #LINE# #TAB# #TAB# logging.error(e) #LINE# #TAB# #TAB# return None #LINE# #TAB# obj = {} #LINE# #TAB# for key, value in c.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj[key] = json.loads(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# logging.error(e) #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return obj"
"#LINE# #TAB# if hasattr(td, '.Aosm_MarChemSpec'): #LINE# #TAB# #TAB# return td.Aosm_MarChemSpec() #LINE# #TAB# else: #LINE# #TAB# #TAB# return td.MarChemSpec"
"#LINE# #TAB# choices = [] #LINE# #TAB# for image in images: #LINE# #TAB# #TAB# size = image.size #LINE# #TAB# #TAB# if size < 6: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# elif size > 7: #LINE# #TAB# #TAB# #TAB# label = f'Source {image.filename}' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# label = f'Source {image.filename}' #LINE# #TAB# #TAB# choices.append((label, image)) #LINE# #TAB# return choices"
"#LINE# #TAB# lst = sorted(lst) #LINE# #TAB# if len(lst) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# b, a = lst[0], lst[1] #LINE# #TAB# #TAB# while b == a: #LINE# #TAB# #TAB# #TAB# b, a = lst[0], lst[1] #LINE# #TAB# #TAB# while a in lst: #LINE# #TAB# #TAB# #TAB# b, a = lst[0], lst[1] #LINE# #TAB# return b, a"
"#LINE# #TAB# assert ""preorder_hash"" in preorder_hash, ""BUG: missing preorder_hash"" #LINE# #TAB# try: #LINE# #TAB# #TAB# cur.execute(""DELETE FROM preorder_records WHERE preorder_hash =?;"", (preorder_hash,)) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass"
#LINE# #TAB# focus_widget = get_focus_widget() #LINE# #TAB# widget_id = focus_widget.id() #LINE# #TAB# declaration = parse_sweep_box(widget_id) #LINE# #TAB# return declaration
"#LINE# #TAB# if edges == 0: #LINE# #TAB# #TAB# return None, [] #LINE# #TAB# forward = [] #LINE# #TAB# reverse = [] #LINE# #TAB# for i, j in edges: #LINE# #TAB# #TAB# forward.append(i) #LINE# #TAB# #TAB# if j == 0: #LINE# #TAB# #TAB# #TAB# reverse.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# reverse.append(j - 1) #LINE# #TAB# if reverse: #LINE# #TAB# #TAB# return forward, reverse #LINE# #TAB# else: #LINE# #TAB# #TAB# return forward, reverse"
#LINE# #TAB# result = radius * np.cos(0.5 * radius) + height * np.sin(0.5 * radius #LINE# #TAB# #TAB# ) * np.cos(0.5 * radius) #LINE# #TAB# if transform is not None: #LINE# #TAB# #TAB# result = transform * result #LINE# #TAB# return result
#LINE# #TAB# filtered_repos = [] #LINE# #TAB# for repo in repos: #LINE# #TAB# #TAB# if repo in ignore_repos: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if repo.installed: #LINE# #TAB# #TAB# #TAB# filtered_repos.append(repo) #LINE# #TAB# return filtered_repos
"#LINE# #TAB# if value < CELERY_START_TIME or value > CELERY_END_TIME: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return arrow.get(value, 'datetime') #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# value = '' #LINE# #TAB# if isinstance(delta, datetime.timedelta): #LINE# #TAB# #TAB# value = '{0} per {1}'.format(delta.days, delta.seconds) #LINE# #TAB# elif isinstance(delta, float): #LINE# #TAB# #TAB# value = '{0} per {1}'.format(delta.days, delta.seconds) #LINE# #TAB# elif isinstance(delta, datetime.datetime): #LINE# #TAB# #TAB# value = '{0} seconds'.format(delta.total_seconds()) #LINE# #TAB# return value"
#LINE# #TAB# with NamedTemporaryFile(delete=True) as tempfile: #LINE# #TAB# #TAB# pdf2_f = NamedTemporaryFile(delete=True) #LINE# #TAB# #TAB# pdf2_f.write(filename) #LINE# #TAB# #TAB# pdf2_f.flush() #LINE# #TAB# #TAB# res = _from_message(filename) #LINE# #TAB# pdf2_f.close() #LINE# #TAB# return res
"#LINE# #TAB# if isinstance(args, six.string_types): #LINE# #TAB# #TAB# args = [args] #LINE# #TAB# ret = [] #LINE# #TAB# for arg in args: #LINE# #TAB# #TAB# if argname in arg: #LINE# #TAB# #TAB# #TAB# arg_list = arg.split(',') #LINE# #TAB# #TAB# #TAB# for perm in arg_list: #LINE# #TAB# #TAB# #TAB# #TAB# if perm not in ret: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ret.append(perm) #LINE# #TAB# #TAB# return ret #LINE# #TAB# return []"
#LINE# #TAB# data = {} #LINE# #TAB# if auth is not None: #LINE# #TAB# #TAB# auth = auth.split('&') #LINE# #TAB# #TAB# for item in auth: #LINE# #TAB# #TAB# #TAB# key = item[0] #LINE# #TAB# #TAB# #TAB# val = item[1] #LINE# #TAB# #TAB# #TAB# data[key] = val #LINE# #TAB# return data
"#LINE# #TAB# if mime_type is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# mime_type = mime_type.split('/')[-1] #LINE# #TAB# max_length = len(mime_type) #LINE# #TAB# for i in range(max_length): #LINE# #TAB# #TAB# ext = mime_type[-i] #LINE# #TAB# #TAB# if ext in ('.jpeg', '.jpe'): #LINE# #TAB# #TAB# #TAB# ext = mime_type[:i] #LINE# #TAB# #TAB# mime_type = '.'.join(ext) #LINE# #TAB# return mime_type"
"#LINE# #TAB# chEBI = get_gui_thread(chebi_id) #LINE# #TAB# if not chEBI: #LINE# #TAB# #TAB# raise ValueError('Invalid ChEBI ID: %s' % chebi_id) #LINE# #TAB# for thread in chEBI[::-1]: #LINE# #TAB# #TAB# if thread.isAlive(): #LINE# #TAB# #TAB# #TAB# log.debug('Stopping gui thread %s', thread.name) #LINE# #TAB# #TAB# #TAB# thread.quit() #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# log.debug('Stopping gui thread %s', thread.name) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# thread.join() #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return chEBI"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls.objects.get(uri=actor_urn) #LINE# #TAB# except cls.DoesNotExist: #LINE# #TAB# #TAB# return None
#LINE# #TAB# bookmark = str(bookmark) #LINE# #TAB# res = db.session.query(Resource).filter(Resource.bookmark_id == bookmark).one() #LINE# #TAB# if res: #LINE# #TAB# #TAB# return res[0] #LINE# #TAB# return {}
"#LINE# #TAB# new_paths = [] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if not isinstance(path, list): #LINE# #TAB# #TAB# #TAB# path = [path] #LINE# #TAB# #TAB# new_paths.append(path) #LINE# #TAB# for path in new_paths: #LINE# #TAB# #TAB# if isinstance(path, str): #LINE# #TAB# #TAB# #TAB# new_path = path +'' + idx #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_path = path +'' + idx #LINE# #TAB# return new_paths"
"#LINE# #TAB# if isinstance(obj, RO): #LINE# #TAB# #TAB# return obj #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# return [build_prefix(c) for c in obj] #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# return {k: build_prefix(v) for k, v in obj.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
"#LINE# #TAB# ssh = str(ssh_command(material)) #LINE# #TAB# result = [] #LINE# #TAB# for line in ssh.splitlines(): #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# if not re.match('^[a-zA-Z0-9-]+$', line): #LINE# #TAB# #TAB# #TAB# #TAB# result.append(' '.join([line])) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(line) #LINE# #TAB# return result"
#LINE# #TAB# settings = fs.read(settings_path) #LINE# #TAB# if settings is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# cards = {} #LINE# #TAB# for card in settings.split('\n'): #LINE# #TAB# #TAB# if not card: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# card = int(card) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if card == 'unknown': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# cards[card] = int(card) #LINE# #TAB# return cards
"#LINE# #TAB# #TAB# charge_balance = 0 #LINE# #TAB# #TAB# for parameter in struct.get('parameters', []): #LINE# #TAB# #TAB# #TAB# if parameter['charge']!= 0: #LINE# #TAB# #TAB# #TAB# #TAB# charge_balance += 1 #LINE# #TAB# #TAB# return charge_balance == 0"
"#LINE# #TAB# assert issparse(A) #LINE# #TAB# if A.dim() not in [2, 3]: #LINE# #TAB# #TAB# normalize_a = A.tocsc() #LINE# #TAB# else: #LINE# #TAB# #TAB# normalize_a = tf.divide(A, k=3) #LINE# #TAB# for _ in range(A.shape[0]): #LINE# #TAB# #TAB# for _ in range(A.shape[1]): #LINE# #TAB# #TAB# #TAB# normalize_a.data[(_), :] = tf.sqrt(norm(A.data[(_), :, :])) #LINE# #TAB# return normalize_a"
#LINE# #TAB# global native_idx #LINE# #TAB# if idx < 0: #LINE# #TAB# #TAB# native_idx = idx #LINE# #TAB# else: #LINE# #TAB# #TAB# native_idx = native_idx - 1 #LINE# #TAB# return native_idx
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('temp_retrieve', dtype='int32', direction=function #LINE# #TAB# #TAB#.OUT, description='maximum number of MIDI outputs') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
#LINE# #TAB# output_array = [] #LINE# #TAB# for x in input_array: #LINE# #TAB# #TAB# output_array.append(x + '\n') #LINE# #TAB# return output_array
"#LINE# #TAB# sorted_versions = [] #LINE# #TAB# for version in versions: #LINE# #TAB# #TAB# if reverse: #LINE# #TAB# #TAB# #TAB# sorted_versions.append((version.version, version.number)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sorted_versions.append((version.version, version.number)) #LINE# #TAB# return sorted_versions"
"#LINE# #TAB# for arg in ['x', 'y', 'width', 'height']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if getattr(x, arg) == getattr(y, arg): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return False"
#LINE# #TAB# if not signature.startswith(withSignature): #LINE# #TAB# #TAB# raise ValueError('Invalid signature: %s' % withSignature) #LINE# #TAB# if len(signature)!= len(withSignature): #LINE# #TAB# #TAB# raise ValueError('Invalid signature: %s' % signature) #LINE# #TAB# return True
#LINE# #TAB# if 'CONTENT_LENGTH' in environ: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return environ['CONTENT_LENGTH'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if 'CONTENT_LENGTH' in environ: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return environ['CONTENT_LENGTH'] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return 0
"#LINE# #TAB# notes = [] #LINE# #TAB# for name, table in tables.items(): #LINE# #TAB# #TAB# if isinstance(table, BaseX): #LINE# #TAB# #TAB# #TAB# notes.extend(get_release_notes(table, table_ctor)) #LINE# #TAB# #TAB# elif isinstance(table, Iterable): #LINE# #TAB# #TAB# #TAB# for i, f in enumerate(table): #LINE# #TAB# #TAB# #TAB# #TAB# notes.extend(get_release_notes(f, table_ctor)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# notes.append(table_ctor(table)) #LINE# #TAB# return notes"
"#LINE# #TAB# for c in channels: #LINE# #TAB# #TAB# bin_input_data = _reads_per_bin(input_data, converter_type, c) #LINE# #TAB# #TAB# bin_output_data = _reads_per_bin(output_data, converter_type, c) #LINE# #TAB# #TAB# yield bin_input_data, bin_output_data"
#LINE# #TAB# try: #LINE# #TAB# #TAB# port = int(host) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# port = None #LINE# #TAB# if scheme: #LINE# #TAB# #TAB# port = int(scheme.lower()) #LINE# #TAB# return port
"#LINE# #TAB# can_create = True #LINE# #TAB# try: #LINE# #TAB# #TAB# create_topic = Topic.objects.create(category=category, forum= #LINE# #TAB# #TAB# #TAB# forum, user=user).can_create() #LINE# #TAB# except ObjectDoesNotExist: #LINE# #TAB# #TAB# pass #LINE# #TAB# finally: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# create_topic.delete() #LINE# #TAB# #TAB# except ObjectDoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return can_create"
"#LINE# #TAB# table = OrderedDict() #LINE# #TAB# if opts.bid: #LINE# #TAB# #TAB# table['bid'] = opts.bid #LINE# #TAB# for row in table.values(): #LINE# #TAB# #TAB# for col in row: #LINE# #TAB# #TAB# #TAB# val = getattr(row, col) #LINE# #TAB# #TAB# #TAB# if val is None: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if not isinstance(val, list): #LINE# #TAB# #TAB# #TAB# #TAB# val = [val] #LINE# #TAB# #TAB# #TAB# table[col] = val #LINE# #TAB# return table"
#LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = '' #LINE# #TAB# cfg.parentdir_prefix = 'dask-cloudprovider-' #LINE# #TAB# cfg.versionfile_source = 'dask_cloudprovider/_version.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"#LINE# if len(attempt) <= 2 and not longopt_list: #LINE# #TAB# return [] #LINE# images = [] #LINE# option_names = [v[0] for v in longopt_list] #LINE# if len(attempt) == 1 and option_names[0] in attempt: #LINE# #TAB# attempt = attempt[0] #LINE# for option in option_names: #LINE# #TAB# try: #LINE# #TAB# #TAB# images.append(load_image(attempt, option)) #LINE# #TAB# except IOError: #LINE# #TAB# images.append(""Couldn't load image %s"" % option) #LINE# return images"
"#LINE# #TAB# if type(claimset_data)!= list: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# sum = sum(map(int, claimset_data)) #LINE# #TAB# #TAB# if sum == 0: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# if cigar_tuple[0] == 0 or cigar_tuple[1] <= 10: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# global measure_name #LINE# #TAB# measure_name = m.replace(' ', '_') #LINE# #TAB# if 'jupyter' in sys.modules: #LINE# #TAB# #TAB# return #LINE# #TAB# global scan_directory #LINE# #TAB# if'scan_directory' in os.environ: #LINE# #TAB# #TAB# del os.environ['scan_directory'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if prefix == '': #LINE# #TAB# #TAB# prefix = 'default' #LINE# #TAB# from os.path import dirname, join #LINE# #TAB# if not exists(join(dirname(__file__),'stars')): #LINE# #TAB# #TAB# makedirs(join(dirname(__file__),'stars')) #LINE# #TAB# from importlib import import_module #LINE# #TAB# module = import_module(prefix) #LINE# #TAB# if not hasattr(module, '__name__'): #LINE# #TAB# #TAB# module.__name__ = basename(prefix) #LINE# #TAB# return module.__name__"
"#LINE# #TAB# if not hasattr(find_column_widths, 'col_widths'): #LINE# #TAB# #TAB# from importlib import import_module #LINE# #TAB# #TAB# find_column_widths.col_widths = import_module('col_widths') #LINE# #TAB# return find_column_widths.col_widths"
"#LINE# #TAB# sandbox_client = FluidDBClient(app, sandbox=sandbox) #LINE# #TAB# return sandbox_client.client"
#LINE# #TAB# c = np.sqrt(x ** 2 + y ** 2 + x ** 2 + y ** 2) #LINE# #TAB# if np.abs(c) > 1e-08: #LINE# #TAB# #TAB# return c #LINE# #TAB# elif np.abs(c) > 1e-08: #LINE# #TAB# #TAB# return c / 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return c
"#LINE# #TAB# for key in spec.keys(): #LINE# #TAB# #TAB# if key.startswith('route_'): #LINE# #TAB# #TAB# #TAB# spec.loc[spec['route_id'] == key, 'key'] = 'route_' + key #LINE# #TAB# #TAB# if key.startswith('wave'): #LINE# #TAB# #TAB# #TAB# spec.loc[spec['wave'] == key, 'key'] = 'wave_' + key #LINE# #TAB# return spec"
#LINE# #TAB# if json: #LINE# #TAB# #TAB# if json_fields and node in json: #LINE# #TAB# #TAB# #TAB# val = json[node] #LINE# #TAB# #TAB# #TAB# del json[node] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# val = None #LINE# #TAB# else: #LINE# #TAB# #TAB# val = node #LINE# #TAB# return val
"#LINE# #TAB# if not isinstance(toDecode, bytes): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# decoded = toDecode.decode('utf-8') #LINE# #TAB# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# #TAB# return toDecode #LINE# #TAB# else: #LINE# #TAB# #TAB# return decoded"
#LINE# #TAB# if operator.shape[0] == 1: #LINE# #TAB# #TAB# parameters = [] #LINE# #TAB# elif operator.shape[1] == 2: #LINE# #TAB# #TAB# parameters = open_crate_dataset_2d(operator) #LINE# #TAB# elif operator.shape[1] == 3: #LINE# #TAB# #TAB# parameters = open_crate_dataset_3d(operator) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Invalid operator shape.') #LINE# #TAB# return parameters
"#LINE# #TAB# mochad_controller = hass.data[DOMAIN] #LINE# #TAB# devs = config.get(CONF_DEVICES) #LINE# #TAB# add_entities([MochadColours(hass, mochad_controller.ctrl, dev) for dev in #LINE# #TAB# #TAB# devs]) #LINE# #TAB# return True"
"#LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# file_name = open(file_name, 'r', encoding=encoding) #LINE# #TAB# schema = parse_uri(file_name) #LINE# #TAB# if encode: #LINE# #TAB# #TAB# schema = schema.encode(encoding) #LINE# #TAB# return schema"
"#LINE# #TAB# for i, t in enumerate(triangle_list): #LINE# #TAB# #TAB# v = lut.get(t[0], t[1]) #LINE# #TAB# #TAB# if v is not None: #LINE# #TAB# #TAB# #TAB# yield v"
"#LINE# #TAB# if hasattr(cls, '__init__'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# params = unpacked_spo.params.copy() #LINE# #TAB# if 'algo' in params and params['algo'] == 'zero': #LINE# #TAB# #TAB# params['address'] = None #LINE# #TAB# #TAB# del params['algo'] #LINE# #TAB# return params
#LINE# #TAB# vtkActor = vtk.vtkActor() #LINE# #TAB# vtkActor.SetTransform(transformation) #LINE# #TAB# delta = vtkActor.GetDelta() #LINE# #TAB# vtkActor.GetProperty().SetDelta(delta) #LINE# #TAB# return delta
#LINE# #TAB# try: #LINE# #TAB# #TAB# date = pd.to_datetime(df[date_col]) #LINE# #TAB# #TAB# instant_dir = df[cluster_id].groupby(date) #LINE# #TAB# except: #LINE# #TAB# #TAB# raise RuntimeError('Date column must be onset or pd.DataFrame') #LINE# #TAB# return instant_dir
"#LINE# #TAB# p = subprocess.Popen(namedex_bin, stdout=subprocess.PIPE) #LINE# #TAB# p.communicate() #LINE# #TAB# tag_list = p.returncode #LINE# #TAB# if tag_list == '': #LINE# #TAB# #TAB# tag_list = 'unknown' #LINE# #TAB# return tag_list"
#LINE# #TAB# L = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# line = frame.f_back.f_back #LINE# #TAB# #TAB# #TAB# if line is None: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# L.append(line) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass
#LINE# #TAB# url_size = 4 #LINE# #TAB# while True: #LINE# #TAB# #TAB# data = os.urandom(url_size) #LINE# #TAB# #TAB# if not data: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# url_size += len(data) #LINE# #TAB# return url_size
"#LINE# #TAB# if isinstance(node, str): #LINE# #TAB# #TAB# query = node #LINE# #TAB# else: #LINE# #TAB# #TAB# query = node #LINE# #TAB# used_tables = set() #LINE# #TAB# for query_string in query.split(';'): #LINE# #TAB# #TAB# tables = get_tables_from_query(query_string) #LINE# #TAB# #TAB# for table in tables: #LINE# #TAB# #TAB# #TAB# if table not in used_tables: #LINE# #TAB# #TAB# #TAB# #TAB# used_tables.add(table) #LINE# #TAB# return used_tables"
#LINE# #TAB# if char.is_utf8(): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# if (gtype == cls.SHUFFLE or gtype == cls.ALL or gtype == cls.LOWEST or #LINE# #TAB# #TAB# gtype == cls.NONE): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(gtype, cls.FIELDS): #LINE# #TAB# #TAB# return gtype.gtype == topology_pb2.Grouping.Value('FIELDS' #LINE# #TAB# #TAB# #TAB# ) and gtype.fields is not None #LINE# #TAB# elif isinstance(gtype, cls.CUSTOM): #LINE# #TAB# #TAB# return gtype.gtype == topology_pb2.Grouping.Value('CUSTOM' #LINE# #TAB# #TAB# #TAB# ) and gtype.python_serialized is not None #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# outer_indices = [] #LINE# #TAB# for tensor in inputs: #LINE# #TAB# #TAB# outer_index = slice(i1_cut_i2_wo_output[tensor], i1_union_i2[tensor]) #LINE# #TAB# #TAB# season_date_tensor = _date_range(g, outer_index, all_tensors, s, inputs, #LINE# #TAB# #TAB# #TAB# i1_cut_i2_wo_output, i1_union_i2) #LINE# #TAB# #TAB# outer_indices.append(season_date_tensor) #LINE# #TAB# return outer_indices"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# value = socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, #LINE# #TAB# #TAB# #TAB# socket.SOCK_STREAM) #LINE# #TAB# #TAB# return value #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# log.debug('Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port)) #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# if Pylint is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# try: #LINE# #TAB# #TAB# return super(SimpleLogger, cls).get_symmetries() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return super(SimpleLogger, cls).get_symmetries() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return []"
"#LINE# #TAB# global _classic_mapping #LINE# #TAB# if not net.is_op: #LINE# #TAB# #TAB# return _classic_mapping #LINE# #TAB# mapping = {} #LINE# #TAB# for p in net.params: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mapping.setdefault(p.device, []).append(p.op) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mapping[p.op].append(p.device) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# _classic_mapping[net.op] = mapping #LINE# #TAB# return _classic_mapping"
"#LINE# #TAB# config = {} #LINE# #TAB# for key, val in CONFIG.items(): #LINE# #TAB# #TAB# if re.match(key, email): #LINE# #TAB# #TAB# #TAB# config = val #LINE# #TAB# return config"
"#LINE# #TAB# data = None #LINE# #TAB# with nc.open_file(nc_filename, 'r') as nc_file: #LINE# #TAB# #TAB# data = nc_file.read() #LINE# #TAB# return data"
"#LINE# #TAB# if is_accessible: #LINE# #TAB# #TAB# result = [] #LINE# #TAB# #TAB# for i in range(start, stop, step): #LINE# #TAB# #TAB# #TAB# w = W(i, is_accessible) #LINE# #TAB# #TAB# #TAB# if w is not None: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(w) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(w) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = [] #LINE# #TAB# #TAB# for i in range(size): #LINE# #TAB# #TAB# #TAB# w = W(i, is_accessible) #LINE# #TAB# #TAB# #TAB# result.append(w) #LINE# #TAB# #TAB# return result"
#LINE# #TAB# if url.endswith('/'): #LINE# #TAB# #TAB# url = url[:-1] #LINE# #TAB# p = urlparse(url) #LINE# #TAB# return p
#LINE# #TAB# url = current_url() #LINE# #TAB# if not url.endswith('/'): #LINE# #TAB# #TAB# url += '/' #LINE# #TAB# url = url + '/' + d #LINE# #TAB# r = requests.get(url) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# if d.get('id'): #LINE# #TAB# #TAB# r.raise_for_status() #LINE# #TAB# if d.get('status') == 'ok': #LINE# #TAB# #TAB# return {'id': d.get('id')}
"#LINE# #TAB# global _db_groups #LINE# #TAB# group = _db_groups.get(name, _Group()) #LINE# #TAB# _db_groups[name] = group #LINE# #TAB# return group"
#LINE# #TAB# for field in dist.get_translatable_fields(): #LINE# #TAB# #TAB# if not field.endswith('-pyc'): #LINE# #TAB# #TAB# #TAB# yield field
"#LINE# #TAB# context = {'exchange': exchange, 'queue': queue_name, 'routing': routing} #LINE# #TAB# return context"
#LINE# #TAB# if '=' not in value: #LINE# #TAB# #TAB# return () #LINE# #TAB# parts = value.split('=') #LINE# #TAB# if len(parts)!= 2: #LINE# #TAB# #TAB# return () #LINE# #TAB# return [part.strip() for part in parts]
"#LINE# #TAB# res = {} #LINE# #TAB# for n in sorted(_config, reverse=True): #LINE# #TAB# #TAB# if n is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# r = int(n / config[n]) #LINE# #TAB# #TAB# if r!= num_n: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res[n] = r #LINE# #TAB# return res"
#LINE# #TAB# player_config = PlayerConfig(team=team) #LINE# #TAB# for part in player_config_path.parts[1:]: #LINE# #TAB# #TAB# if part.startswith('__'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# player_config.add_part(part) #LINE# #TAB# return player_config
"#LINE# #TAB# bot_configs: Set[BotConfigBundle] = set() #LINE# #TAB# for root, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# if filename.endswith('_geometric_average'): #LINE# #TAB# #TAB# #TAB# #TAB# p = Path(root) #LINE# #TAB# #TAB# #TAB# #TAB# bot_config = bot_config_bundle_from_filepath(os.path.join(p, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# filename)) #LINE# #TAB# #TAB# #TAB# #TAB# bot_configs.add(bot_config) #LINE# #TAB# return bot_configs"
#LINE# #TAB# if credentials.type_indicator not in cls._isolated_credentials: #LINE# #TAB# #TAB# raise KeyError('Credential object already set for type indicator: {0:s}.' #LINE# #TAB# #TAB# #TAB#.format(credentials.type_indicator)) #LINE# #TAB# del cls._isolated_credentials[credentials.type_indicator]
"#LINE# #TAB# for item in l: #LINE# #TAB# #TAB# obj = cls(item[0], item[1]) #LINE# #TAB# return obj"
#LINE# #TAB# subclasses = [c] #LINE# #TAB# while c.data: #LINE# #TAB# #TAB# c = c.data #LINE# #TAB# #TAB# s = alg.decompress(c) #LINE# #TAB# #TAB# if s in subclasses: #LINE# #TAB# #TAB# #TAB# subclasses.remove(s) #LINE# #TAB# #TAB# #TAB# c = subclasses[s] #LINE# #TAB# return subclasses
"#LINE# #TAB# _check_contour_levels(contour_levels, contourf_idx) #LINE# #TAB# if unit == 'px': #LINE# #TAB# #TAB# return {'stroke_width': stroke_width, 'fcolor': fcolor, 'fill_opacity': #LINE# #TAB# #TAB# #TAB# fill_opacity} #LINE# #TAB# elif unit == 'pt': #LINE# #TAB# #TAB# return {'stroke_width': stroke_width, 'fcolor': fcolor, #LINE# #TAB# #TAB# #TAB# 'fill_opacity': fill_opacity} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {'stroke_width': stroke_width, 'fcolor': fcolor, 'fill_opacity': #LINE# #TAB# #TAB# #TAB# fill_opacity}"
#LINE# #TAB# label_vec = [0.0] * len(points) #LINE# #TAB# for i in range(len(points)): #LINE# #TAB# #TAB# for j in range(len(points)): #LINE# #TAB# #TAB# #TAB# label_vec[i] = points[i][j] #LINE# #TAB# label_vec = np.array(label_vec) #LINE# #TAB# return label_vec
#LINE# #TAB# #TAB# obj = cls() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj.from_bytes(data) #LINE# #TAB# #TAB# except GIError as err: #LINE# #TAB# #TAB# #TAB# raise GIError(err) #LINE# #TAB# #TAB# return obj
"#LINE# #TAB# request_hooks = [hooks.JSONErrorHook()] #LINE# #TAB# if transactional: #LINE# #TAB# #TAB# request_hooks.append(hooks.OSVmExpireTransactionHook()) #LINE# #TAB# file_loader = wsgi.FileLoader(controller or versions.AVAILABLE_VERSIONS[ #LINE# #TAB# #TAB# versions.DEFAULT_VERSION](), hooks=request_hooks, force_canonical=False #LINE# #TAB# #TAB# ) #LINE# #TAB# file_loader.load_hooks(file_loader) #LINE# #TAB# return file_loader"
#LINE# #TAB# gene_dict = geno.gene_dict #LINE# #TAB# projs = [gene_dict[x] for x in gene_dict.keys()] #LINE# #TAB# projs.append(gene_dict[0]) #LINE# #TAB# projs.append(gene_dict[1]) #LINE# #TAB# return projs
"#LINE# #TAB# N, M = mat.shape #LINE# #TAB# K = np.zeros((N, M)) #LINE# #TAB# for i in range(N): #LINE# #TAB# #TAB# for j in range(M): #LINE# #TAB# #TAB# #TAB# K[i, j] = cholesky(mat[(i), :, :]) #LINE# #TAB# #TAB# K[i, j] -= eps #LINE# #TAB# return K"
"#LINE# #TAB# for i, row in enumerate(cat_table): #LINE# #TAB# #TAB# if i in cuts: #LINE# #TAB# #TAB# #TAB# yield row"
"#LINE# #TAB# cache_file = os.path.join(settings.CACHE_DIR, 'weights.csv') #LINE# #TAB# with io.open(cache_file, 'w', encoding='utf-8') as csvfile: #LINE# #TAB# #TAB# reader = csv.reader(csvfile, delimiter=',', quotechar='""') #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# csvfile.write(row) #LINE# #TAB# return cache_file"
#LINE# #TAB# a = 0 #LINE# #TAB# b = 0 #LINE# #TAB# while a < len(player) and b < len(player): #LINE# #TAB# #TAB# if player[a] == b: #LINE# #TAB# #TAB# #TAB# a += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# b += 1 #LINE# #TAB# return a
#LINE# #TAB# catalog = cls() #LINE# #TAB# for combination in cls.combinations(): #LINE# #TAB# #TAB# catalog.sanitize_and_check(combination) #LINE# #TAB# for combination in cls.combinations(): #LINE# #TAB# #TAB# catalog.sanitize_and_check(combination) #LINE# #TAB# return catalog
"#LINE# #TAB# if has_start_stop: #LINE# #TAB# #TAB# return py3compat.text_type( #LINE# #TAB# #TAB# #TAB# has_start_stop, #LINE# #TAB# #TAB# #TAB# 'Start stop activation time threshold [s]', #LINE# #TAB# #TAB# #TAB# default='s') #LINE# #TAB# else: #LINE# #TAB# #TAB# return py3compat.text_type( #LINE# #TAB# #TAB# #TAB# has_start_stop, #LINE# #TAB# #TAB# #TAB# 'Start activation time threshold [s]', #LINE# #TAB# #TAB# #TAB# default='s') #LINE# #TAB# return py3compat.text_type(DEFAULT_START_STOP_ACTIVITY_TIME_THRESHOLD, #LINE# #TAB# #TAB# default='s') + py3compat.text_type(DEFAULT_STOP_ACTIVITY_TIME_THRESHOLD, #LINE# #TAB# #TAB#'s')"
"#LINE# #TAB# request = context['request'] #LINE# #TAB# if article: #LINE# #TAB# #TAB# categories = article.get('categories', []) #LINE# #TAB# else: #LINE# #TAB# #TAB# categories = [] #LINE# #TAB# if limit: #LINE# #TAB# #TAB# categories = [p for p in categories if p['title'] == request.GET[ #LINE# #TAB# #TAB# #TAB# 'page_id']] #LINE# #TAB# categories = [p for p in categories if p['title']!= request.GET[ #LINE# #TAB# #TAB# 'page_id']] #LINE# #TAB# context['categories'] = categories #LINE# #TAB# return context"
"#LINE# #TAB# G, o, g, h = pos #LINE# #TAB# n = 0.0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# if o > g: #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# elif o < g: #LINE# #TAB# #TAB# #TAB# n += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return n #LINE# #TAB# return n"
#LINE# #TAB# from nurbs import Renderers #LINE# #TAB# newRenderer = Renderers() #LINE# #TAB# for i in range(len(baseFunction._arrs)): #LINE# #TAB# #TAB# if baseFunction._arrs[i] is not None: #LINE# #TAB# #TAB# #TAB# newRenderer.splitArray(baseFunction._arrs[i]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# newRenderer.splitArray(baseFunction._arrs[i]) #LINE# #TAB# return newRenderer
"#LINE# #TAB# obj = Rule(name, table.name, inobj.pop('description', None), inobj. #LINE# #TAB# #TAB# pop('owner', None), inobj.pop('privileges', []), inobj.pop( #LINE# #TAB# #TAB# 'ruleset', []), inobj.pop('handler', None), inobj.pop( #LINE# #TAB# #TAB# 'validator', None)) #LINE# #TAB# obj.fix_privileges() #LINE# #TAB# obj.set_oldname(inobj) #LINE# #TAB# return obj"
#LINE# #TAB# for node in dx_nodes: #LINE# #TAB# #TAB# if node.value in dx_node.value: #LINE# #TAB# #TAB# #TAB# yield node
#LINE# #TAB# a = np.array(a) #LINE# #TAB# other = np.array(other) #LINE# #TAB# a = np.array(particle) #LINE# #TAB# other = np.array(other) #LINE# #TAB# tmp = (a * other).sum(axis=1) + (other * particle).sum(axis=0) #LINE# #TAB# if side == 'left': #LINE# #TAB# #TAB# tmp /= 2.0 #LINE# #TAB# daemons_template = np.zeros_like(tmp) #LINE# #TAB# for i in range(len(particle)): #LINE# #TAB# #TAB#daemons_template[i] = particle[i] - other[i] #LINE# #TAB# if normalize: #LINE# #TAB# #TAB# return normalize_daemons_template(daemons_template) #LINE# #TAB# return daemons_template
"#LINE# #TAB# data = get_branch(profile, name) #LINE# #TAB# new_sha = data.get(""head"") #LINE# #TAB# if new_sha is None: #LINE# #TAB# #TAB# new_sha = sha #LINE# #TAB# new_sha = ""heads/"" + new_sha #LINE# #TAB# data[""head""] = new_sha #LINE# #TAB# return new_sha"
"#LINE# #TAB# name = re.sub('([a-z])([A-Z])', lambda m: m.group(1).upper(), name) #LINE# #TAB# name = re.sub('([0-9])([A-Z])', lambda m: m.group(1).upper(), name) #LINE# #TAB# return name"
#LINE# #TAB# if token is not None: #LINE# #TAB# #TAB# headers = {'Authorization': 'Bearer {}'.format(token)} #LINE# #TAB# else: #LINE# #TAB# #TAB# headers = {} #LINE# #TAB# return headers
#LINE# #TAB# element_name_absolute = inflection.camelize(str(model_name)) #LINE# #TAB# element_name_list = [element_name_absolute] #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# element_name_list.append(model_name + '.' + str(name)) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return element_name_list
#LINE# #TAB# res = [] #LINE# #TAB# for t in x: #LINE# #TAB# #TAB# if t == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if t[0].isupper() and len(t) > 1 and t[1:].islower(): #LINE# #TAB# #TAB# #TAB# res.append(TK_MAJ) #LINE# #TAB# #TAB# res.append(t.lower()) #LINE# #TAB# return res
#LINE# #TAB# request = requests.Request() #LINE# #TAB# request.method = 'GET' #LINE# #TAB# request.headers = {'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' #LINE# #TAB# #TAB# } #LINE# #TAB# request.headers['X-Requested-With'] = 'XMLHttpRequest' #LINE# #TAB# request.headers['Content-Length'] = str(num_images) #LINE# #TAB# return request
"#LINE# #TAB# with open(file_path, 'rb') as fh: #LINE# #TAB# #TAB# clf = pickle.load(fh) #LINE# #TAB# (x1, y1), (x2, y2) = clf.predict(x1, y2) #LINE# #TAB# return clf"
"#LINE# #TAB# #TAB# separator = '=' #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# for line in cls._parse_list(value): #LINE# #TAB# #TAB# #TAB# key, sep, val = line.partition(separator) #LINE# #TAB# #TAB# #TAB# if sep!= separator: #LINE# #TAB# #TAB# #TAB# #TAB# raise DistutilsOptionError( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'Unable to parse option value to dict: %s' % value) #LINE# #TAB# #TAB# #TAB# result[key.strip()] = val.strip() #LINE# #TAB# #TAB# return result"
"#LINE# #TAB# identity_refs = [] #LINE# #TAB# for identity_ref in reviewers: #LINE# #TAB# #TAB# identity_ref = IdentityRefWithVote(identity_ref, team_instance) #LINE# #TAB# #TAB# identity_refs.append(identity_ref) #LINE# #TAB# return identity_refs"
#LINE# #TAB# if resource_id not in _cached_query_string_data: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(_cached_query_string_data[resource_id]) as file: #LINE# #TAB# #TAB# #TAB# #TAB# return file.read() #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# raise ResourceNotFoundError('No such resource: {0}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# resource_id)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return _cached_query_string_data[resource_id]
"#LINE# #TAB# logger.debug('convert_args: %s', t) #LINE# #TAB# atomList = t[1] #LINE# #TAB# if not isinstance(atomList, list): #LINE# #TAB# #TAB# return t #LINE# #TAB# ret = [] #LINE# #TAB# if atomList[0] in HOOK_ATOMS: #LINE# #TAB# #TAB# ret.append(HOOK_ATOMS[atomList[0]]) #LINE# #TAB# #TAB# ret.append(atomList[1]) #LINE# #TAB# elif len(atomList) > 1: #LINE# #TAB# #TAB# for i in range(len(atomList)): #LINE# #TAB# #TAB# #TAB# ret.append(HOOK_ATOMS[atomList[i][0]]) #LINE# #TAB# #TAB# ret.append(atomList[i][1]) #LINE# #TAB# logger.debug('convert_args: %s', ret) #LINE# #TAB# return ret"
#LINE# #TAB# x = cor_norm_rot[0] #LINE# #TAB# y = cor_norm_rot[1] #LINE# #TAB# R = np.sqrt(x ** 2 + y ** 2) #LINE# #TAB# if R > 1e-12: #LINE# #TAB# #TAB# R = 1e-12 #LINE# #TAB# elif R < 0: #LINE# #TAB# #TAB# R = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# R = 1 #LINE# #TAB# return R
#LINE# #TAB# encoded_key = urlsafe_b64encode(secret_key) #LINE# #TAB# uri = 'http' + encoded_key.decode('utf-8') #LINE# #TAB# return uri
"#LINE# #TAB# version, pkg_name = args #LINE# #TAB# if version is None: #LINE# #TAB# #TAB# return #LINE# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# output = run('git rev-parse --verify {}'.format(pkg_name), #LINE# #TAB# #TAB# #TAB# capture=True) #LINE# #TAB# if len(output) > 0: #LINE# #TAB# #TAB# signed = output[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# match = regex.match(package_name) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# signed = match.group(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# signed = package_name #LINE# #TAB# return signed"
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(dir) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# if err.errno == errno.EEXIST and os.path.isdir(dir): #LINE# #TAB# #TAB# #TAB# app_name = os.path.basename(dir) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# app_name = os.path.splitext(app_name)[0] #LINE# #TAB# #TAB# return app_name
"#LINE# #TAB# customers = Customer.objects.create(name=name) #LINE# #TAB# customers.attach(email=email, phone=phone) #LINE# #TAB# return customers"
#LINE# #TAB# padding = 0 #LINE# #TAB# n = len(node) #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# if n % 2 == 0: #LINE# #TAB# #TAB# #TAB# padding += 1 #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# return padding
#LINE# #TAB# desktop_entry = _get_desktop_entry() #LINE# #TAB# if desktop_entry: #LINE# #TAB# #TAB# del _desktop_entry[0]
"#LINE# #TAB# if not isinstance(logger, str): #LINE# #TAB# #TAB# logger = logging.getLogger(logger) #LINE# #TAB# roman_string = logger.encode('utf-8') #LINE# #TAB# if roman_string.startswith('roman'): #LINE# #TAB# #TAB# logger = roman_string[3:] #LINE# #TAB# if '.' not in logger: #LINE# #TAB# #TAB# logger = 'roman' #LINE# #TAB# return logger"
"#LINE# #TAB# if input is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(data, str): #LINE# #TAB# #TAB# return data #LINE# #TAB# if isinstance(data, list): #LINE# #TAB# #TAB# if len(data) > 1: #LINE# #TAB# #TAB# #TAB# x = random.choice(data) #LINE# #TAB# #TAB# #TAB# if x =='subfeature': #LINE# #TAB# #TAB# #TAB# #TAB# return'subfeature' #LINE# #TAB# #TAB# #TAB# elif x in data: #LINE# #TAB# #TAB# #TAB# #TAB# return data[x] #LINE# #TAB# return None"
#LINE# #TAB# if up: #LINE# #TAB# #TAB# return [arch.lower() for arch in arch_info.get_supported_architectures()] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [arch.lower() for arch in arch_info.get_supported_architectures()]
"#LINE# #TAB# names = [] #LINE# #TAB# for arg in argument_spec: #LINE# #TAB# #TAB# if arg == '=': #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# elif isinstance(arg, list): #LINE# #TAB# #TAB# #TAB# names.extend(skip_if(arg)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# names.append(arg) #LINE# #TAB# return names"
"#LINE# #TAB# samples = utils.to_single_data(samples) #LINE# #TAB# if not all(isinstance(sample, (list, tuple)) for sample in samples): #LINE# #TAB# #TAB# raise ValueError(""Expected a list of samples, got {0}"".format(type(samples))) #LINE# #TAB# for data in samples: #LINE# #TAB# #TAB# if ""class"" not in data: #LINE# #TAB# #TAB# #TAB# data[""class""] = ""File"" #LINE# #TAB# #TAB# if ""path"" not in data: #LINE# #TAB# #TAB# #TAB# data[""path""] = dd.get_sample_dir(data) #LINE# #TAB# #TAB# #TAB# data[""path""] = os.path.join(dd.get_sample_dir(data), data[""path""]) #LINE# #TAB# return samples"
"#LINE# #TAB# return {'zzz_subversionPage': [QCoreApplication.translate('VcsPySvnPlugin', #LINE# #TAB# #TAB# 'Subversion'), os.path.join('VcsPlugins', 'vcsPySvn', #LINE# #TAB# #TAB# 'vcsPySvn', 'icons', 'preferences-subversion.svg'), #LINE# #TAB# #TAB# createConfigurationPage, 'vcsPage', None]}"
#LINE# #TAB# new_seq = '' #LINE# #TAB# for c in seq: #LINE# #TAB# #TAB# if c in '{}': #LINE# #TAB# #TAB# #TAB# new_seq +='' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_seq += c #LINE# #TAB# return new_seq
#LINE# #TAB# for item in iterator: #LINE# #TAB# #TAB# yield item
"#LINE# #TAB# columns = [col['name'] for col in data] #LINE# #TAB# df = pd.DataFrame(np.array(columns), index=columns) #LINE# #TAB# df.columns = [col['name'] for col in data] #LINE# #TAB# return df"
"#LINE# #TAB# df = pd.read_csv(filename, sep='\t', header=None, usecols=[0, 1, 2], #LINE# #TAB# #TAB# names=['chrom', 'pos', 'genotype']) #LINE# #TAB# if 'chrom' not in df.columns: #LINE# #TAB# #TAB# table = 'chrom' #LINE# #TAB# #TAB# df['chrom'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['pos'] = df['chrom'].astype(str) #LINE# #TAB# #TAB# df['genotype'] = df['chrom'].astype(str) #LINE# #TAB# return df"
#LINE# #TAB# print('add line breaks') #LINE# #TAB# yield #LINE# #TAB# print('-' * 80) #LINE# #TAB# for line in ctx.doc.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# yield line #LINE# #TAB# #TAB# print('') #LINE# #TAB# yield
#LINE# #TAB# vals = VarInt.from_hex(raw_hex[1:]) #LINE# #TAB# neighborhood_distance_singvals = vals[0] #LINE# #TAB# return neighborhood_distance_singvals
"#LINE# #TAB# if getattr(obj, '_mounted', False): #LINE# #TAB# #TAB# return '/{}/'.format(obj._mounted) #LINE# #TAB# state = obj.state() #LINE# #TAB# return '/{}/'.format(state.get('label', '')) if state.get('label', #LINE# #TAB# #TAB# '') else ''"
"#LINE# #TAB# if name.endswith('.complex'): #LINE# #TAB# #TAB# name = name[:-1] #LINE# #TAB# parts = name.split('.') #LINE# #TAB# for i in range(1, len(parts)): #LINE# #TAB# #TAB# if parts[i] == 'complex': #LINE# #TAB# #TAB# #TAB# name = name[:-i] #LINE# #TAB# return name"
"#LINE# #TAB# if not condition: #LINE# #TAB# #TAB# return user #LINE# #TAB# result = [] #LINE# #TAB# for keypath, value in condition.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# result.append((keypath, value)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# #TAB# for item in value: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if item in user: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# result.append((keypath, item)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# result.append((keypath, value)) #LINE# #TAB# if result: #LINE# #TAB# #TAB# return result"
#LINE# #TAB# exchange_dict = {} #LINE# #TAB# if not dict_a: #LINE# #TAB# #TAB# return dict_b #LINE# #TAB# for key in dict_b: #LINE# #TAB# #TAB# if key in dict_a: #LINE# #TAB# #TAB# #TAB# dict_a[key] = dict_b[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# exchange_dict[key] = dict_a[key] #LINE# #TAB# for key in dict_a: #LINE# #TAB# #TAB# if key in dict_b: #LINE# #TAB# #TAB# #TAB# dict_b[key] = dict_b[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dict_a[key] = dict_b[key] #LINE# #TAB# return exchange_dict
"#LINE# #TAB# bit_object_list, rest0 = get_bit_object(str_input) #LINE# #TAB# yt_url, rest1 = get_youtube_url(rest0) #LINE# #TAB# time_codes, rest2 = get_time_codes(rest1) #LINE# #TAB# titles, rest3 = get_titles(rest2) #LINE# #TAB# if rest3: #LINE# #TAB# #TAB# description = rest3.strip() #LINE# #TAB# elif bitobject_list: #LINE# #TAB# #TAB# description = bitobject_to_description(rest2) #LINE# #TAB# else: #LINE# #TAB# #TAB# description = None #LINE# #TAB# return time_codes, titles, yt_url, description"
"#LINE# #TAB# f = urllib.request.urlopen(url) #LINE# #TAB# data = f.read() #LINE# #TAB# match = re.search('https?://archiveofourown.org/(.*)', data) #LINE# #TAB# return match.group(1) if match else None"
"#LINE# #TAB# pages = Parallel(n_jobs)(delayed(_get_page)(i, k, MI_FS, F, s, are_data_binned) for #LINE# #TAB# #TAB# i in range(n_jobs)) #LINE# #TAB# for j in range(n_jobs): #LINE# #TAB# #TAB# Page = Parallel(n_jobs)(delayed(_get_page)(i, MI_FS, k, s, are_data_binned) for #LINE# #TAB# #TAB# #TAB# i in range(len(F))] #LINE# #TAB# #TAB# yield Page"
"#LINE# #TAB# if RE_NO_GRAPH.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'S360: Project graph can not be used in sahara code'"
#LINE# #TAB# result = [] #LINE# #TAB# for degradation_args in degradations_args: #LINE# #TAB# #TAB# degradation = Degradation(**degradation_args) #LINE# #TAB# #TAB# result.append(degradation) #LINE# #TAB# return result
"#LINE# #TAB# references = [] #LINE# #TAB# ref_finder = HTMLReferenceFinder(xml) #LINE# #TAB# for elm, uri_attr in ref_finder: #LINE# #TAB# #TAB# type_ = _discover_uri_type(elm.get(uri_attr)) #LINE# #TAB# #TAB# references.append(Reference(elm, type_, uri_attr)) #LINE# #TAB# return references"
"#LINE# #TAB# goldenratio = 0.2 #LINE# #TAB# session_width = figwidth / goldenratio #LINE# #TAB# return session_width, session_height"
#LINE# #TAB# result = 0 #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if filter_function(item): #LINE# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# return result
"#LINE# #TAB# new_dictionary = {} #LINE# #TAB# for k, v in dictionary.items(): #LINE# #TAB# #TAB# if isinstance(v, str): #LINE# #TAB# #TAB# #TAB# new_value = dateutil.parser.parse(v) #LINE# #TAB# #TAB# elif isinstance(v, datetime.datetime): #LINE# #TAB# #TAB# #TAB# new_value = v.isoformat() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_value = str(v) #LINE# #TAB# #TAB# new_dictionary[k] = new_value #LINE# #TAB# return new_dictionary"
#LINE# #TAB# schedule = [] #LINE# #TAB# all_images = pbclient.list_images() #LINE# #TAB# for image in all_images['items']: #LINE# #TAB# #TAB# if image['properties']['name'] == image_name: #LINE# #TAB# #TAB# #TAB# schedule.append(image) #LINE# #TAB# return schedule
"#LINE# #TAB# rot = np.array([[0, 0, 0], [0, np.sin(2 * np.pi / 180.0), -np.cos(2 * #LINE# #TAB# #TAB# np.pi / 180.0), 0], [0, np.sin(2 * np.pi / 180.0), np.cos(2 * np.pi / #LINE# #TAB# #TAB# 180.0)]]) #LINE# #TAB# return rot"
"#LINE# #TAB# if arrays is None: #LINE# #TAB# #TAB# arrays = [ss] #LINE# #TAB# file_paths = [] #LINE# #TAB# for ss_module in ss: #LINE# #TAB# #TAB# if ss_module.jackknife_module: #LINE# #TAB# #TAB# #TAB# file_paths.append(os.path.join(ss_module.jackknife_module, ss_module.name + #LINE# #TAB# #TAB# #TAB# #TAB# '.py')) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# file_paths.append(os.path.join(ss_module.path, ss_module.name + #LINE# #TAB# #TAB# #TAB# #TAB# '.py')) #LINE# #TAB# return file_paths"
#LINE# #TAB# user = context.user #LINE# #TAB# return 'corosync_%s' % user.pk
#LINE# #TAB# cloud = cloud_name.lower() #LINE# #TAB# if cloud == 'eu-west-2': #LINE# #TAB# #TAB# return 'anon' #LINE# #TAB# elif cloud =='redhat': #LINE# #TAB# #TAB# return 'anon' #LINE# #TAB# elif cloud =='redhat-west': #LINE# #TAB# #TAB# return'redhat' #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# files = ctx.obj['conf'].get('files', []) #LINE# #TAB# split_str = ctx.obj['conf'].get('split_str', []) #LINE# #TAB# if len(files) > 1: #LINE# #TAB# #TAB# raise click.BadParameter('deploy_files_size: %s' % s) #LINE# #TAB# for file in files: #LINE# #TAB# #TAB# if file: #LINE# #TAB# #TAB# #TAB# split_str = ','.join(split_str) #LINE# #TAB# #TAB# #TAB# if len(split_str) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# raise click.BadParameter( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'deploy_files_size: %s > %s' % (len(split_str), file)) #LINE# #TAB# return files"
"#LINE# #TAB# sorted_encoders = sorted(encoders, key=lambda enc: int(np.percentile(enc.trainX, #LINE# #TAB# #TAB# percentile)), reverse=True) #LINE# #TAB# return sorted_encoders"
"#LINE# #TAB# if len(tok) == 3: #LINE# #TAB# #TAB# tok[0] = LogicalBinOpRule(tok[2], tok[1], tok[3]) #LINE# #TAB# else: #LINE# #TAB# #TAB# tok[0] = tok[1] #LINE# #TAB# return tok"
"#LINE# #TAB# CFG = load_config() #LINE# #TAB# masses = CFG.get_masses(filename) #LINE# #TAB# reg_errors = CFG.get_errors(filename) #LINE# #TAB# reg_score = round(reg_score * len(masses), 2) #LINE# #TAB# if not reg_error and reg_warning: #LINE# #TAB# #TAB# log.info('Registering masses failed.') #LINE# #TAB# #TAB# reg_warning = round(reg_warning * len(masses), 2) #LINE# #TAB# if not reg_warning and reg_error: #LINE# #TAB# #TAB# log.info('Registering masses failed.') #LINE# #TAB# return {'all_atoms': masses,'reg_errors': reg_errors,'reg_score': #LINE# #TAB# #TAB# reg_score,'reg_warning': reg_warning,'reg_error': reg_error}"
"#LINE# #TAB# #TAB# if filename is None: #LINE# #TAB# #TAB# #TAB# filename = hashlib.md5(url.encode('utf-8')).hexdigest() #LINE# #TAB# #TAB# cache_file = os.path.join(cache_dir, filename) #LINE# #TAB# #TAB# if os.path.isfile(cache_file): #LINE# #TAB# #TAB# #TAB# with open(cache_file, 'wb') as f: #LINE# #TAB# #TAB# #TAB# #TAB# f.write(url) #LINE# #TAB# #TAB# #TAB# return cls.sanitize_file(cache_file) #LINE# #TAB# #TAB# return url"
"#LINE# #TAB# host, port = hosts_and_ports #LINE# #TAB# if isinstance(hosts_and_ports, tuple): #LINE# #TAB# #TAB# host, port = hosts_and_ports #LINE# #TAB# elif len(hosts_and_ports) == 2: #LINE# #TAB# #TAB# host = hosts_and_ports[0] #LINE# #TAB# #TAB# port = None #LINE# #TAB# elif len(hosts) == 1: #LINE# #TAB# #TAB# host = hosts[0] #LINE# #TAB# #TAB# port = int(hosts[1]) #LINE# #TAB# return host, port"
"#LINE# #TAB# cmd = ['gpg', '--version'] #LINE# #TAB# cmd.extend(in_file_list) #LINE# #TAB# cmd.append('--out-file') #LINE# #TAB# with open(out_file, 'w') as out_handle: #LINE# #TAB# #TAB# subprocess.check_call(cmd) #LINE# #TAB# return out_handle"
"#LINE# #TAB# scenarios = [] #LINE# #TAB# for line in scencmd.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line and not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# parts = line.split(' ') #LINE# #TAB# #TAB# #TAB# scenarios.append((parts[0], parts[1].strip())) #LINE# #TAB# return scenarios"
"#LINE# #TAB# folders = [] #LINE# #TAB# for dir_ in os.listdir(os.getcwd()): #LINE# #TAB# #TAB# mod = os.path.join(dir_, '__init__.py') #LINE# #TAB# #TAB# if os.path.isdir(mod): #LINE# #TAB# #TAB# #TAB# packages = [] #LINE# #TAB# #TAB# #TAB# for file_name in os.listdir(mod): #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(mod, file_name) #LINE# #TAB# #TAB# #TAB# #TAB# if is_python_package(filename): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# packages.append(os.path.basename(filename)) #LINE# #TAB# return ', '.join(packages) + '.py'"
#LINE# #TAB# D = collections.defaultdict(float) #LINE# #TAB# n = counts['nrows'] #LINE# #TAB# k = counts['ncols'] #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# D[i][k] += observable[i] #LINE# #TAB# return D
#LINE# #TAB# if url.startswith('/'): #LINE# #TAB# #TAB# return url #LINE# #TAB# return ''
"#LINE# #TAB# if all(map(lambda x: x < 150, sync)): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# ip = str(ip) #LINE# #TAB# try: #LINE# #TAB# #TAB# IPv4Address(ip) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# if ip == '0.0.0.0': #LINE# #TAB# #TAB# #TAB# return '127.0.0.1' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return '127.0.0.1' #LINE# #TAB# except AddressValueError: #LINE# #TAB# #TAB# return False
#LINE# #TAB# if argument is None: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# return argument
#LINE# #TAB# my_choices = [] #LINE# #TAB# for project_line in serialized: #LINE# #TAB# #TAB# project_split = project_line.split() #LINE# #TAB# #TAB# if len(project_split) == 1: #LINE# #TAB# #TAB# #TAB# my_choices.append(project_split[0]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# my_choices.append(project_split) #LINE# #TAB# return my_choices
"#LINE# #TAB# if y is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif y > 0: #LINE# #TAB# #TAB# return y + (1,) #LINE# #TAB# else: #LINE# #TAB# #TAB# return y"
"#LINE# #TAB# container_exists = False #LINE# #TAB# for volume in volumes: #LINE# #TAB# #TAB# key = volume['private_key'] #LINE# #TAB# #TAB# key_path = os.path.join(container_dir, key) #LINE# #TAB# #TAB# if not os.path.exists(key_path): #LINE# #TAB# #TAB# #TAB# os.makedirs(key_path) #LINE# #TAB# #TAB# container_private_key = read_private_key(key_path, volume['private_key']) #LINE# #TAB# #TAB# container_certs = read_certs(key_path, volume['certs']) #LINE# #TAB# #TAB# if not container_private_key or not container_certs: #LINE# #TAB# #TAB# #TAB# container_private_key = container_private_key #LINE# #TAB# return container_private_key, container_certs"
"#LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# return value"
"#LINE# #TAB# if not isinstance(artist, str): #LINE# #TAB# #TAB# artist = artist.decode('utf-8') #LINE# #TAB# if not isinstance(title, str): #LINE# #TAB# #TAB# title = title.decode('utf-8') #LINE# #TAB# tempo = _get_tempo() #LINE# #TAB# for song in tempo: #LINE# #TAB# #TAB# if song['artist'] == artist and song['title'] == title: #LINE# #TAB# #TAB# #TAB# return song #LINE# #TAB# return None"
"#LINE# #TAB# update_dict = {} #LINE# #TAB# update_dict[str(key)] = 'yes' #LINE# #TAB# update_dict[str(key).replace('-','')] = 'no' #LINE# #TAB# update_dict[str(key).replace('_','')] = 'true' #LINE# #TAB# update_dict[str(key).replace('-','')] = 'true' #LINE# #TAB# return update_dict"
"#LINE# #TAB# if not isinstance(templates, (list, tuple)): #LINE# #TAB# #TAB# templates = [templates] #LINE# #TAB# device = device_from_request(request) #LINE# #TAB# alltemplates = [] #LINE# #TAB# for template in templates: #LINE# #TAB# #TAB# if device: #LINE# #TAB# #TAB# #TAB# alltemplates.append('%s/%s' % (device, template)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# alltemplates.append(template) #LINE# #TAB# alltemplates.reverse() #LINE# #TAB# return alltemplates"
#LINE# #TAB# rxn_index = 0 #LINE# #TAB# for rxn in model.reactions: #LINE# #TAB# #TAB# if len(rxn.source) < 2: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# rxn_index += 1 #LINE# #TAB# #TAB# if rxn.source[0] == 'C': #LINE# #TAB# #TAB# #TAB# yield rxn #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield rxn
#LINE# #TAB# devices = [] #LINE# #TAB# for device_name in ALL_DEVICES: #LINE# #TAB# #TAB# devices += rotate_one_device(device_name) #LINE# #TAB# return devices
#LINE# #TAB# global SEP #LINE# #TAB# if type in SEP: #LINE# #TAB# #TAB# return SEP[type] #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
#LINE# #TAB# while True: #LINE# #TAB# #TAB# line = os.readline(10) #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# g = line.split() #LINE# #TAB# #TAB# if len(g) < count: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# l = [g[0]] #LINE# #TAB# #TAB# l.append(g[1]) #LINE# #TAB# #TAB# if len(g) > count: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return l
"#LINE# #TAB# exposure_class = layer.keywords['exposure_class'] #LINE# #TAB# if not exposure_class: #LINE# #TAB# #TAB# return [] #LINE# #TAB# matches = re.findall(exposure_class, exposure_class) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return matches"
#LINE# #TAB# assert a >= 0 and p > 0 #LINE# #TAB# if a == 1 and p == 2: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# elif a == 2 and p == 3: #LINE# #TAB# #TAB# return 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1
"#LINE# #TAB# if cm.ndim!= 2: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be 2-dim.') #LINE# #TAB# if cm.shape[0]!= cm.shape[1]: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# if cm.shape[1]!= cm.shape[0]: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# for i in range(cm.shape[1]): #LINE# #TAB# #TAB# for j in range(cm.shape[1]): #LINE# #TAB# #TAB# #TAB# if cm[i, j]!= 1: #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# return cm.shape[1], cm.shape[0]"
"#LINE# #TAB# rid = rid or action['rid'] #LINE# #TAB# unit = unit or action['unit'] #LINE# #TAB# try: #LINE# #TAB# #TAB# with settings(hide('running','stdout','stderr', 'warnings'), #LINE# #TAB# #TAB# #TAB# warn_only=True): #LINE# #TAB# #TAB# #TAB# res = action['command'][0]['result'] #LINE# #TAB# #TAB# #TAB# res == 'OK' and set_user_handle_type(action, rid, unit) == 'COMPLETE': #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# return [(f[0], f[1]) for f in list(flag_dict.values()) if isinstance(f[0 #LINE# #TAB# #TAB# ], (int, float)) and f[1].is_integer()]"
"#LINE# #TAB# if rand: #LINE# #TAB# #TAB# stat_id = str(stat) #LINE# #TAB# else: #LINE# #TAB# #TAB# stat_id = stat #LINE# #TAB# keys_in_compartments = func.get_metabolites_in_compartments(stat, #LINE# #TAB# #TAB# set_, min_, max_) #LINE# #TAB# for key in keys_in_compartments: #LINE# #TAB# #TAB# stat_dict = {'stat_id': key, 'values': val} #LINE# #TAB# #TAB# stat_dict['values'] = sorted(stat_dict[key]) #LINE# #TAB# return stat_dict"
#LINE# #TAB# vmdk_name = PNR_NAME_FROM_VMDK.get(pnr) #LINE# #TAB# if not vmdk_name: #LINE# #TAB# #TAB# raise ValueError('{} is not a valid VMDK name.'.format(pnr)) #LINE# #TAB# return vmdk_name
"#LINE# #TAB# tools = find_installed_tools(plugin_dir) #LINE# #TAB# output = [] #LINE# #TAB# for tool in tools: #LINE# #TAB# #TAB# report = generate_asset_allocation_report_for_tool(tool, plugin_dir) #LINE# #TAB# #TAB# output.append(report) #LINE# #TAB# return output"
"#LINE# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# values = get_orig_distr(values) #LINE# #TAB# hist, bin_edges = np.histogram(values, edges, density=True) #LINE# #TAB# values = np.rollaxis(hist, -1, axis=0) #LINE# #TAB# bin_edges = np.rollaxis(bin_edges, -1, axis=1) #LINE# #TAB# density = bin_edges / np.sum(bin_edges) #LINE# #TAB# return density"
#LINE# #TAB# final_list = [] #LINE# #TAB# for item in val.split('_'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# final_list.append(item.capitalize()) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# final_list.append(item) #LINE# #TAB# return ''.join(final_list) if len(final_list) > 0 else final_list[0]
"#LINE# #TAB# warnings.warn(' '.join(['check_content_consistency']), DeprecationWarning, #LINE# #TAB# #TAB# stacklevel=2) #LINE# #TAB# return [seq[loc_ind[i]:loc_ind[i + 1] + freq_offset[i]] for i in range(len( #LINE# #TAB# #TAB# loc_ind))]"
"#LINE# #TAB# return [hash_function(w, minimize_indices=minimize_indices) for w in words #LINE# #TAB# #TAB# ]"
"#LINE# #TAB# sub_retries = {k: (RetryConfig(**c) if isinstance(c, RetryConfig) else #LINE# #TAB# #TAB# RetryConfig(**c)) for k, c in retries.items()} #LINE# #TAB# sub_retries = {k: (RetryConfig(**c) if isinstance(c, RetryConfig) else #LINE# #TAB# #TAB# RetryConfig(**c)) for k, c in sub_retries.items()} #LINE# #TAB# return sub_retries"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# #TAB# return f.readline().split()[2] #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# keys = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common AppData', #LINE# #TAB# #TAB# 'CSIDL_LOCAL_APPDATA': 'Local AppData'}[csidl_name] #LINE# #TAB# dd_skips_list = [] #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in dd_skips_list: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# dd_skips_list.append(key) #LINE# #TAB# return {'CSIDL_NAME': csidl_name, 'CSIDL_SKIPS': dd_skips_list}"
#LINE# #TAB# omero_home = os.getenv('OMOUR_HOME') #LINE# #TAB# if not omero_home: #LINE# #TAB# #TAB# return #LINE# #TAB# os.environ['OMOUR_HOME'] = omero_home
"#LINE# #TAB# if vcfutils.file_exists(infile): #LINE# #TAB# #TAB# with vcfutils.open_gzipsafe(infile) as in_handle: #LINE# #TAB# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# #TAB# parts = line.strip().split() #LINE# #TAB# #TAB# #TAB# #TAB# psi = float(parts[0]) / float(parts[1]) #LINE# #TAB# #TAB# #TAB# #TAB# if psi == 0.0: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# #TAB# yield psi, parts[0], float(parts[2]) #LINE# #TAB# else: #LINE# #TAB# #TAB# yield infile, None"
"#LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print('Activating %i header files.' % len(extension_names)) #LINE# #TAB# for extension_name in extension_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# require_header(extension_name, verbose=verbose) #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# pass"
"#LINE# #TAB# if not qs: #LINE# #TAB# #TAB# return False #LINE# #TAB# qs = qs.exclude(id__in=qs.values_list('id', flat=True)) #LINE# #TAB# if limit: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# qs = qs.limit(limit) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return qs"
"#LINE# #TAB# widths_temp = [0.0] * num_samples #LINE# #TAB# for i in range(num_samples): #LINE# #TAB# #TAB# base = np.random.randint(0, len(widths)) #LINE# #TAB# #TAB# x_temp = widths[i] - base #LINE# #TAB# #TAB# for j in range(i + 1, num_samples): #LINE# #TAB# #TAB# #TAB# if base < x_temp: #LINE# #TAB# #TAB# #TAB# #TAB# width_temp[j] = base / x_temp #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# width_temp[j] = base / x_temp #LINE# #TAB# return widths_temp"
#LINE# #TAB# initial_guess = initial_guess #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n = _format.format(initial_guess) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if n == '': #LINE# #TAB# #TAB# #TAB# return initial_guess #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return n
"#LINE# #TAB# if data.ndim == 1: #LINE# #TAB# #TAB# data = data.reshape((-1, 1)) #LINE# #TAB# ds = pd.read_csv(data, delimiter='|', engine='F1') #LINE# #TAB# ds['freq'] = ds['freq'].astype(float) #LINE# #TAB# if axis == 'F1': #LINE# #TAB# #TAB# ds['x'] = ds['x'].astype(float) #LINE# #TAB# elif axis == 'F2': #LINE# #TAB# #TAB# ds['y'] = ds['y'].astype(float) #LINE# #TAB# #TAB# ds['z'] = ds['z'].astype(float) #LINE# #TAB# return ds"
#LINE# #TAB# path_info = environ['PATH_INFO'] #LINE# #TAB# if path_info[-1]!= '/': #LINE# #TAB# #TAB# path_info = path_info[:-1] #LINE# #TAB# matched_re = cls(path_info) #LINE# #TAB# return matched_re
"#LINE# #TAB# json_formatter = JSONFormatter() #LINE# #TAB# col_names = list(col_types) #LINE# #TAB# for col_name in col_names: #LINE# #TAB# #TAB# col = pd.DataFrame(col_types[col_name]) #LINE# #TAB# #TAB# col = col.astype(str) #LINE# #TAB# #TAB# first_value = col.iloc[0] #LINE# #TAB# #TAB# last_value = col.iloc[-1] #LINE# #TAB# #TAB# if overrides: #LINE# #TAB# #TAB# #TAB# json_formatter.update(overrides) #LINE# #TAB# #TAB# json_formatter.update({'first': first_value, 'last': last_value}) #LINE# #TAB# return json_formatter"
#LINE# #TAB# chunk_length = len(outputs) #LINE# #TAB# for i in range(chunk_length): #LINE# #TAB# #TAB# yield outputs[i]
"#LINE# #TAB# assert _environment.obj is not None #LINE# #TAB# mapp = _environment.obj.make_mapp(autospace) #LINE# #TAB# for k, v in mapp.items(): #LINE# #TAB# #TAB# if not isinstance(v, string_types): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# v = v.encode('utf8') #LINE# #TAB# #TAB# v = v.strip() #LINE# #TAB# #TAB# if v: #LINE# #TAB# #TAB# #TAB# v = v.encode('utf8') #LINE# #TAB# #TAB# setattr(mapp, k, v) #LINE# #TAB# return mapp"
"#LINE# #TAB# dialog = scrolledmessage.ScrolledMessageDialog(parent, text, caption) #LINE# #TAB# dialog.exec_() #LINE# #TAB# return dialog"
#LINE# #TAB# cbasois = {} #LINE# #TAB# for child in root.get_children(): #LINE# #TAB# #TAB# cbasois[child.get_cbasois_id()] = create_cbasois_column(child) #LINE# #TAB# return cbasois
"#LINE# #TAB# qis = query_aqi(city_name=city_name) #LINE# #TAB# if not qis: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif len(qis) == 1: #LINE# #TAB# #TAB# return qis[0].get('data_dependent', False) #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"#LINE# #TAB# hsv1 = np.linspace(0, 1, N) #LINE# #TAB# hsv2 = np.linspace(0, 1, N) #LINE# #TAB# if hsv: #LINE# #TAB# #TAB# color1 = hsv1 * 255 + color2 #LINE# #TAB# elif N == 1: #LINE# #TAB# #TAB# color1 = np.array([color1]) #LINE# #TAB# return [(str(color1[i]) + ',' + str(color2[i])) for i in range(N)]"
#LINE# #TAB# global _repository #LINE# #TAB# _repository = repository #LINE# #TAB# if should_list: #LINE# #TAB# #TAB# repo_files = [f for f in os.listdir(repository) if f.endswith('.view')] #LINE# #TAB# else: #LINE# #TAB# #TAB# repo_files = [f for f in os.listdir('{}/entry/'.format(repository) #LINE# #TAB# #TAB# #TAB# ) if f.endswith('.view')] #LINE# #TAB# if len(_repository_files) == 0: #LINE# #TAB# #TAB# repos = list() #LINE# #TAB# else: #LINE# #TAB# #TAB# repos = [_repository_files[i] for i in sorted(_repository_files)] #LINE# #TAB# return repos
"#LINE# #TAB# if six.PY3: #LINE# #TAB# #TAB# if isinstance(string, bytes): #LINE# #TAB# #TAB# #TAB# string = string.decode('utf-8') #LINE# #TAB# #TAB# interval = float(string) / 1000 #LINE# #TAB# else: #LINE# #TAB# #TAB# interval = string #LINE# #TAB# return interval"
#LINE# #TAB# new_rules = {} #LINE# #TAB# if create_copy: #LINE# #TAB# #TAB# new_rules = copy.deepcopy(left) #LINE# #TAB# for rule in right: #LINE# #TAB# #TAB# if rule == '*': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if rule not in new_rules: #LINE# #TAB# #TAB# #TAB# new_rules[rule] = [] #LINE# #TAB# #TAB# new_rules[rule].append(rule) #LINE# #TAB# return new_rules
#LINE# #TAB# if s.group_by_pattern: #LINE# #TAB# #TAB# return s.group_by_pattern[0].selector #LINE# #TAB# else: #LINE# #TAB# #TAB# return s
"#LINE# #TAB# if isinstance(value, bool): #LINE# #TAB# #TAB# return 'true' if value else 'false' #LINE# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# return [get_yaml_editor(item) for item in value] #LINE# #TAB# else: #LINE# #TAB# #TAB# return value"
#LINE# #TAB# w1 = np.sqrt(data[0][0] ** 2 + data[0][1] ** 2) #LINE# #TAB# w2 = np.sqrt(data[1][1] ** 2 + data[1][1] ** 2) #LINE# #TAB# cm = w1 * (w2 - w1) / (w1 * np.sqrt(data[1][0] ** 2 + data[1][1] ** 2)) #LINE# #TAB# return cm
#LINE# #TAB# ref = cls._weakrefs.get(component) #LINE# #TAB# if ref is not None: #LINE# #TAB# #TAB# del cls._weakrefs[component]
"#LINE# #TAB# kwargs = {} #LINE# #TAB# for item in header['extras']: #LINE# #TAB# #TAB# if not isinstance(item, six.text_type): #LINE# #TAB# #TAB# #TAB# if six.PY3: #LINE# #TAB# #TAB# #TAB# #TAB# item = item.decode('utf-8') #LINE# #TAB# #TAB# #TAB# item = item.encode('utf-8') #LINE# #TAB# #TAB# kwargs[item] = header['extra'] #LINE# #TAB# return kwargs"
#LINE# #TAB# columns = set() #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# query = cursor.execute( #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# for row in query: #LINE# #TAB# #TAB# #TAB# #TAB# columns.add(row[0]) #LINE# #TAB# return columns
#LINE# #TAB# d = {} #LINE# #TAB# libsvm = load_libsvm(filename) #LINE# #TAB# for dataset in libsvm: #LINE# #TAB# #TAB# idx = dataset['idx'] #LINE# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# if idx == 0: #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# elif idx < len(libsvm): #LINE# #TAB# #TAB# #TAB# d[dataset['name']] = dataset #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opt_type = dataset['opt_type'] #LINE# #TAB# #TAB# #TAB# d[dataset['name']][opt_type] = idx #LINE# #TAB# return d
"#LINE# #TAB# parts = version_string.split('-') #LINE# #TAB# major = int(parts[0]) #LINE# #TAB# minor = int(parts[1]) #LINE# #TAB# revision = int(parts[2]) #LINE# #TAB# prerelease = False #LINE# #TAB# if len(parts) == 4: #LINE# #TAB# #TAB# major = int(parts[3]) #LINE# #TAB# #TAB# minor = int(parts[5]) #LINE# #TAB# #TAB# revision = int(parts[6]) #LINE# #TAB# return major, minor, revision, prerelease"
#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# if not logger.handlers: #LINE# #TAB# #TAB# return None #LINE# #TAB# if len(logger.handlers) > 0: #LINE# #TAB# #TAB# return logger.handlers[0].short_sha #LINE# #TAB# return None
"#LINE# #TAB# os.makedirs(f, exist_ok=True) #LINE# #TAB# if not os.path.isdir(f): #LINE# #TAB# #TAB# logger.info('Creating folder %s' % f) #LINE# #TAB# #TAB# os.mkdir(f) #LINE# #TAB# time_bin_dir = os.path.join(f, 'time_bins') #LINE# #TAB# if not os.path.isdir(time_bin_dir): #LINE# #TAB# #TAB# logger.info('Creating folder %s' % f) #LINE# #TAB# #TAB# os.mkdir(time_bin_dir) #LINE# #TAB# return time_bin_dir"
#LINE# #TAB# message_json = json_format.MessageToJson(message_proto) #LINE# #TAB# message_dict = json.loads(message_json) #LINE# #TAB# return message_dict
#LINE# #TAB# base = agg #LINE# #TAB# if base.endswith('/'): #LINE# #TAB# #TAB# base = base[:-1] #LINE# #TAB# for l in _get_dates_file(base): #LINE# #TAB# #TAB# yield l #LINE# #TAB# if base.endswith('/'): #LINE# #TAB# #TAB# base = base[:-1] #LINE# #TAB# #TAB# yield base #LINE# #TAB# #TAB# for l in _get_dates_file(base[:-1]): #LINE# #TAB# #TAB# #TAB# yield l #LINE# #TAB# else: #LINE# #TAB# #TAB# yield base
"#LINE# #TAB# cmd = shlex.split(cmd) #LINE# #TAB# stdout = subprocess.check_output(cmd, stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# if not stdout: #LINE# #TAB# #TAB# raise subprocess.CalledProcessError(cmd, stdout=stdout) #LINE# #TAB# if not stderr: #LINE# #TAB# #TAB# raise subprocess.CalledProcessError(cmd, stderr=stderr) #LINE# #TAB# return stdout, stderr"
"#LINE# #TAB# old_country = state.country #LINE# #TAB# new_country = update_country(state, updates, dist) #LINE# #TAB# if new_country == old_country: #LINE# #TAB# #TAB# return old_country #LINE# #TAB# else: #LINE# #TAB# #TAB# return new_country"
#LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# context = jinja_context.copy() #LINE# #TAB# context['md5'] = md5.hexdigest() #LINE# #TAB# context['filename'] = new_filename #LINE# #TAB# return context
#LINE# #TAB# upper = lookfor.upper() #LINE# #TAB# if upper == 'CRITICAL': #LINE# #TAB# #TAB# level = 0 #LINE# #TAB# elif upper == 'INFO': #LINE# #TAB# #TAB# level = 4 #LINE# #TAB# elif upper == 'WARNING': #LINE# #TAB# #TAB# level = 5 #LINE# #TAB# elif upper == 'ERROR': #LINE# #TAB# #TAB# level = 6 #LINE# #TAB# elif upper == 'CRITICAL': #LINE# #TAB# #TAB# level = 5 #LINE# #TAB# else: #LINE# #TAB# #TAB# level = 0 #LINE# #TAB# return level
"#LINE# #TAB# r = int(r, 10) #LINE# #TAB# g = int(g, 10) #LINE# #TAB# b = int(b, 10) #LINE# #TAB# if r > 255 or g > 255 or b > 255: #LINE# #TAB# #TAB# return '' #LINE# #TAB# s = '' #LINE# #TAB# r = int(r, 10) #LINE# #TAB# g = int(g, 10) #LINE# #TAB# b = int(b, 10) #LINE# #TAB# if r < 0 or g < 0 or b < 0: #LINE# #TAB# #TAB# s += '#' #LINE# #TAB# return s"
#LINE# #TAB# attrs = {'balanced': 0} #LINE# #TAB# for key in set(solution): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# attrs[key] = solution[key][prediction] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# attrs[key] = 1 #LINE# #TAB# return attrs
"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('memory_limit', dtype='int64', direction=function.OUT, #LINE# #TAB# #TAB# description='The memory limit in gigabytes.') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"#LINE# #TAB# #TAB# token_request_data = { #LINE# #TAB# #TAB# #TAB# 'client_id': OAUTH2_CLIENT_ID, #LINE# #TAB# #TAB# #TAB# 'client_secret': OAUTH2_CLIENT_SECRET, #LINE# #TAB# #TAB# #TAB# 'grant_type':'refresh_token', #LINE# #TAB# #TAB# #TAB#'refresh_token': refresh_token, #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# res = session.post(OAUTH2_AUTHORIZATION_ENDPOINT, data=token_request_data) #LINE# #TAB# #TAB# if res.status_code!= 200: #LINE# #TAB# #TAB# #TAB# raise GoogleAuthError(res.json()) #LINE# #TAB# #TAB# return res.json()['access_token']"
"#LINE# #TAB# out = False #LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# s = s.encode('ascii','strict') #LINE# #TAB# try: #LINE# #TAB# #TAB# s.encode('ascii') #LINE# #TAB# #TAB# out = True #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# for m in s.split('|'): #LINE# #TAB# #TAB# #TAB# if not m.isdigit(): #LINE# #TAB# #TAB# #TAB# #TAB# out = False #LINE# #TAB# #TAB# #TAB# #TAB# out = True #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if out: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
#LINE# #TAB# if infotype == 'phone': #LINE# #TAB# #TAB# response['phone_number'] = str(response['value']) #LINE# #TAB# elif infotype == 'first_name': #LINE# #TAB# #TAB# response['phone_number'] = response['value'][0] #LINE# #TAB# elif infotype == 'last_name': #LINE# #TAB# #TAB# response['phone_number'] = response['value'][0] #LINE# #TAB# return response
"#LINE# #TAB# host = dictionary['host'] #LINE# #TAB# port = dictionary['port'] #LINE# #TAB# program_id = dictionary['program_id'] #LINE# #TAB# name = dictionary['name'] #LINE# #TAB# display_name = dictionary['display_name'] #LINE# #TAB# s3_key ='s3://' + host + ':' + port + name #LINE# #TAB# return program_id, display_name, s3_key"
"#LINE# #TAB# code = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# code = int(input('Enter code: ')) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# print('Invalid input', file=sys.stderr) #LINE# #TAB# #TAB# #TAB# return code #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return code"
#LINE# #TAB# style = {} #LINE# #TAB# style['alpha'] = path.get_alpha() #LINE# #TAB# if style['alpha'] is None: #LINE# #TAB# #TAB# style['alpha'] = 1 #LINE# #TAB# style['edgecolor'] = export_color(path.get_edgecolor()) #LINE# #TAB# if fill: #LINE# #TAB# #TAB# style['facecolor'] = export_color(path.get_facecolor()) #LINE# #TAB# else: #LINE# #TAB# #TAB# style['facecolor'] = 'none' #LINE# #TAB# style['edgewidth'] = path.get_linewidth() #LINE# #TAB# style['dasharray'] = get_dasharray(path) #LINE# #TAB# style['zorder'] = path.get_zorder() #LINE# #TAB# return style
"#LINE# #TAB# if getattr(settings, 'UNIT_NAME', None): #LINE# #TAB# #TAB# return settings.UNIT_NAME"
"#LINE# #TAB# sentences = [] #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# i = 0 #LINE# #TAB# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# #TAB# word = line[i] #LINE# #TAB# #TAB# #TAB# #TAB# if word: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# sentences.append(word) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return sentences"
#LINE# #TAB# digest = hashlib.sha1() #LINE# #TAB# digest.update(data) #LINE# #TAB# encoded = base64.b64decode(digest.digest()).decode('utf-8') #LINE# #TAB# return encoded
#LINE# #TAB# cookie = get_cookie(name) #LINE# #TAB# if cookie and cookie[0].value == value: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError
#LINE# #TAB# if os.path.exists(dest): #LINE# #TAB# #TAB# return None #LINE# #TAB# hostedzone_id = get_hostedzone(src) #LINE# #TAB# if hostedzone_id is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return hostedzone_id
"#LINE# #TAB# mX = np.asarray(mX, dtype=float) #LINE# #TAB# mY = np.asarray(mY, dtype=float) #LINE# #TAB# mX += 0.5 #LINE# #TAB# mY += 0.5 #LINE# #TAB# pX = ((mX - geoTransform[0]) / geoTransform[3]) - 0.5 #LINE# #TAB# pY = ((mY - geoTransform[1]) / geoTransform[5]) - 0.5 #LINE# #TAB# mX = ((mX - geoTransform[0]) / geoTransform[3]) - 0.5 #LINE# #TAB# mY = ((mY - geoTransform[1]) / geoTransform[5]) - 0.5 #LINE# #TAB# return mX, mY"
#LINE# #TAB# if atom.symbol == 'X': #LINE# #TAB# #TAB# return Atom.X #LINE# #TAB# elif atom.symbol == 'Y': #LINE# #TAB# #TAB# return Atom.Y #LINE# #TAB# elif atom.symbol == 'Z': #LINE# #TAB# #TAB# return Atom.Z #LINE# #TAB# return atom
#LINE# #TAB# request.clipboard.clear() #LINE# #TAB# return {'status':'success'}
#LINE# #TAB# if passwd is None: #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# os.environ['ONNX_MODEL_PASSWORD'] = passwd #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# del os.environ['ONNX_MODEL_PASSWORD']
#LINE# #TAB# try: #LINE# #TAB# #TAB# Bluetooth.from_address(address) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# if x < 0: #LINE# #TAB# #TAB# return -a * np.log(x) #LINE# #TAB# else: #LINE# #TAB# #TAB# n = a.shape[0] #LINE# #TAB# #TAB# out = np.zeros(n) #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# z = np.exp(-a[i] * x) #LINE# #TAB# #TAB# #TAB# out[i] = np.log(z) #LINE# #TAB# #TAB# return out
#LINE# #TAB# router.refresh() #LINE# #TAB# return None
"#LINE# #TAB# identifier = base64.b64decode(bookmark[1:]) #LINE# #TAB# if len(identifier)!= 2: #LINE# #TAB# #TAB# raise ValueError('Invalid bookmark: {}'.format(bookmark)) #LINE# #TAB# return identifier[0], identifier[1:]"
"#LINE# #TAB# #TAB# files = options.files #LINE# #TAB# #TAB# output = [] #LINE# #TAB# #TAB# for filepath in files: #LINE# #TAB# #TAB# #TAB# with open(filepath, 'r') as fp: #LINE# #TAB# #TAB# #TAB# #TAB# reader = csv.reader(fp, delimiter='\t') #LINE# #TAB# #TAB# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# row = [row[0]] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# row[0] = row[1] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# output.append(row) #LINE# #TAB# #TAB# return output"
#LINE# #TAB# type_indicator = resolver_helper.type_indicator #LINE# #TAB# if type_indicator not in cls._resolver_helpers: #LINE# #TAB# #TAB# raise KeyError('Resolver helper object not set for type indicator: {0:s}.' #LINE# #TAB# #TAB# #TAB#.format(type_indicator)) #LINE# #TAB# cls._resolver_helpers[type_indicator] = resolver_helper
"#LINE# #TAB# cores = multiprocessing.cpu_count() #LINE# #TAB# request_cpus = [] #LINE# #TAB# for r1, r2 in zip(R1, R2): #LINE# #TAB# #TAB# n = random.randint(0, len(r1)) #LINE# #TAB# #TAB# request_cpus.append(os.path.basename(r1).split('.')[0]) #LINE# #TAB# return request_cpus"
#LINE# #TAB# session = db.get_reader_session() #LINE# #TAB# indices = set() #LINE# #TAB# with session.begin(): #LINE# #TAB# #TAB# for row in session.query(Network).filter(Network.tenant_id == tenant_id).all(): #LINE# #TAB# #TAB# #TAB# n_rows = [row.network_id for row in row] #LINE# #TAB# #TAB# #TAB# for port_id in range(len(n_rows)): #LINE# #TAB# #TAB# #TAB# #TAB# if network.port_id!= port_id: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# indices.add(n_rows[port_id]) #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# indices.add(r[0]) #LINE# #TAB# return indices
"#LINE# #TAB# t = time.time() #LINE# #TAB# while True: #LINE# #TAB# #TAB# if t < num: #LINE# #TAB# #TAB# #TAB# l = random.randint(1, num) #LINE# #TAB# #TAB# #TAB# yield l + 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break"
#LINE# #TAB# if score_c == 5: #LINE# #TAB# #TAB# return 'Fair' #LINE# #TAB# elif score_c == 6: #LINE# #TAB# #TAB# return 'Moderate' #LINE# #TAB# elif score_c == 7: #LINE# #TAB# #TAB# return 'Good' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Excellent'
#LINE# #TAB# f.width = 0.8 #LINE# #TAB# f.height = 0.2 #LINE# #TAB# f.strokeColor = colors.black #LINE# #TAB# f.fillColor = colors.white #LINE# #TAB# return f
#LINE# #TAB# bottom = sqrt((y - y1) ** 2 + (z - z2) ** 2) #LINE# #TAB# top = sqrt((y - y1) ** 2 + (z - z2) ** 2) #LINE# #TAB# bottom = sqrt((z - z1) ** 2 + (y - y2) ** 2) #LINE# #TAB# for i in range(len(top)): #LINE# #TAB# #TAB# if pt_1_id == top[i]: #LINE# #TAB# #TAB# #TAB# return i #LINE# #TAB# #TAB# elif pt_1_id == bottom[i]: #LINE# #TAB# #TAB# #TAB# return i #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# for parser in _header_parsers: #LINE# #TAB# #TAB# if not title.startswith(parser.title): #LINE# #TAB# #TAB# #TAB# if not exact: #LINE# #TAB# #TAB# #TAB# #TAB# return parser #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# title = parser.title #LINE# #TAB# #TAB# #TAB# if title in parser.titles: #LINE# #TAB# #TAB# #TAB# #TAB# return parser #LINE# #TAB# return None
"#LINE# #TAB# response = remote.get(CERN_TENSOR_INFO_URL) #LINE# #TAB# user_info = json.loads(response.text) #LINE# #TAB# groups = response.get('groups') #LINE# #TAB# return [tuple(map(int, group.split('/'))) for group in groups]"
"#LINE# #TAB# file_path = os.path.join(os.environ[main_section], 'log.file') #LINE# #TAB# file_path = os.path.expanduser(file_path) #LINE# #TAB# if not os.path.isfile(file_path): #LINE# #TAB# #TAB# return main_section #LINE# #TAB# with open(file_path, 'r') as file_handle: #LINE# #TAB# #TAB# for line in file_handle: #LINE# #TAB# #TAB# #TAB# if line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if os.path.isfile(os.path.join(file_handle, line)): #LINE# #TAB# #TAB# #TAB# #TAB# return os.path.abspath(line) #LINE# #TAB# return file_path"
"#LINE# #TAB# parent = node #LINE# #TAB# while parent is not None: #LINE# #TAB# #TAB# if isinstance(parent, TableColumn): #LINE# #TAB# #TAB# #TAB# return parent.column_type #LINE# #TAB# #TAB# parent = parent.parent #LINE# #TAB# return DEFAULT_COLUMN_TYPE"
#LINE# #TAB# state0_fidelity = bk.BKTensor(0) #LINE# #TAB# state1_fidelity = bk.BKTensor(0) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# if state0_fidelity <= state1_fidelity: #LINE# #TAB# #TAB# #TAB# return bk.BKTensor(2) #LINE# #TAB# #TAB# if state0_fidelity > state1_fidelity: #LINE# #TAB# #TAB# #TAB# return bk.BKTensor(1) #LINE# #TAB# return None
"#LINE# #TAB# vector_ab = [(y - x) for x, y in zip(a, b)] #LINE# #TAB# vector_ap1 = [(y - x) for x, y in zip(a, p)] #LINE# #TAB# vector_ap2 = [(y - x) for x, y in zip(a, p)] #LINE# #TAB# cross_vab_ap1 = vector_ab[0] * vector_ap1[1] - vector_ab[1] * vector_ap1[0] #LINE# #TAB# cross_vab_ap2 = vector_ab[0] * vector_ap2[1] - vector_ab[1] * vector_ap2[0] #LINE# #TAB# return cross_vab_ap1, cross_vab_ap2, vector_ap1, cross_vab_ap2"
"#LINE# #TAB# body = request_body #LINE# #TAB# if body is None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# try: #LINE# #TAB# #TAB# body = ET.fromstring(body) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return '' #LINE# #TAB# if isinstance(body, list): #LINE# #TAB# #TAB# return body[0] #LINE# #TAB# if isinstance(body, dict): #LINE# #TAB# #TAB# return {key: read_exactly(value) for key, value in body.items()} #LINE# #TAB# try: #LINE# #TAB# #TAB# return request_body.toxml() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return body"
"#LINE# #TAB# ref_dict = {} #LINE# #TAB# for key, val in input.items(): #LINE# #TAB# #TAB# if isinstance(val, unicode): #LINE# #TAB# #TAB# #TAB# ref_dict[key] = val.encode('utf-8', errors) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ref_dict[key] = val.decode('utf-8', errors) #LINE# #TAB# return ref_dict"
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.remove(module) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# pass
"#LINE# #TAB# c = r if isinstance(r, list) else [r, g, b] #LINE# #TAB# best = {} #LINE# #TAB# for index, item in enumerate(colors): #LINE# #TAB# #TAB# d = __distance(item, c) #LINE# #TAB# #TAB# if not best or d <= best['distance']: #LINE# #TAB# #TAB# #TAB# best = {'distance': d, 'index': index} #LINE# #TAB# if 'index' in best: #LINE# #TAB# #TAB# return best['index'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1"
#LINE# #TAB# host_string = socket.gethostname() #LINE# #TAB# if ':' in host_string: #LINE# #TAB# #TAB# host_ip = host_string.split(':')[0] #LINE# #TAB# #TAB# rabit_host = host_ip #LINE# #TAB# else: #LINE# #TAB# #TAB# host_ip = socket.gethostbyname(socket.gethostname())[2] #LINE# #TAB# #TAB# rabit_host = host_ip #LINE# #TAB# return rabit_host
#LINE# #TAB# result = np.zeros(maglen) #LINE# #TAB# for i in range(int(lag*maglen)): #LINE# #TAB# #TAB# result[i] = (mags[i] - magmed) ** 2 / (maglen[i] - magmed) #LINE# #TAB# return result
#LINE# #TAB# if symbol == 'X': #LINE# #TAB# #TAB# return 4 #LINE# #TAB# else: #LINE# #TAB# #TAB# m = _ES_MODIFIER_DICT.get(symbol) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# return 2 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 4
"#LINE# #TAB# env =iq_array.shape[-1] #LINE# #TAB# hilbert_transform = np.array([1.0, 0.0, 0.0, 1.0]) #LINE# #TAB# for i in range(env): #LINE# #TAB# #TAB# for j in range(i): #LINE# #TAB# #TAB# #TAB# hilbert_transform[i, j] = hilbert_transform[j, i] #LINE# #TAB# #TAB# if len(hilbert_transform) > 0: #LINE# #TAB# #TAB# #TAB# env[i, j] = 'hilbert' #LINE# #TAB# return env"
#LINE# #TAB# request_func._read_section_by_section = True #LINE# #TAB# return request_func
"#LINE# #TAB# _, extension = os.path.splitext(value) #LINE# #TAB# for module in this.config['processes']: #LINE# #TAB# #TAB# if module =='main': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mod = importlib.import_module(module) #LINE# #TAB# #TAB# #TAB# if hasattr(mod, 'file_type'): #LINE# #TAB# #TAB# #TAB# #TAB# return mod.file_type #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return extension"
"#LINE# #TAB# if reference_fasta_map_param is None: #LINE# #TAB# #TAB# return {} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {s.split('.')[0]: s.split('/')[-1] for s in #LINE# #TAB# #TAB# #TAB# reference_fasta_map_param.split(',')}"
"#LINE# #TAB# if ctypes is None: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# pydoc_page_name = ctypes.c_string_p() #LINE# #TAB# if hasattr(pydoc_page_name, 'decode'): #LINE# #TAB# #TAB# return pydoc_page_name.decode('ascii') #LINE# #TAB# else: #LINE# #TAB# #TAB# return pydoc_page_name.value"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# mod = import_module(package + '.' + script_information['name']) #LINE# #TAB# except: #LINE# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# if verbose: #LINE# #TAB# #TAB# #TAB# print('get_script_module: script_information={'name': script_information[ #LINE# #TAB# #TAB# #TAB# #TAB# 'name'], 'package': package}) #LINE# #TAB# #TAB# #TAB# del mod #LINE# #TAB# return mod"
"#LINE# #TAB# perms = [] #LINE# #TAB# if hasattr(task, 'perms'): #LINE# #TAB# #TAB# for perm in task.perms: #LINE# #TAB# #TAB# #TAB# if user.has_perm(perm): #LINE# #TAB# #TAB# #TAB# #TAB# perms.append(perm) #LINE# #TAB# elif hasattr(task, 'task'): #LINE# #TAB# #TAB# for perm in task.task: #LINE# #TAB# #TAB# #TAB# if user.has_perm(perm): #LINE# #TAB# #TAB# #TAB# #TAB# perms.append(perm) #LINE# #TAB# return {'perms': perms}"
#LINE# #TAB# alphas = [None] * len(node_ids) #LINE# #TAB# for node_id in node_ids: #LINE# #TAB# #TAB# node = Node.objects.get(id=node_id) #LINE# #TAB# #TAB# for i in range(len(node_ids)): #LINE# #TAB# #TAB# #TAB# alphas[node_id][i] = 1.0 / sum(alphas[node_id][i] for i in range(len( #LINE# #TAB# #TAB# #TAB# #TAB# node_ids)) #LINE# #TAB# return alphas
"#LINE# #TAB# seq = list(map(Sequence, seq)) #LINE# #TAB# for i in range(len(seq)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# i = tuple(map(int, seq)) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# raise ValueError('Argument must be two-dimensional.') #LINE# #TAB# #TAB# if i < 0: #LINE# #TAB# #TAB# #TAB# raise ValueError('Argument must be positive.') #LINE# #TAB# return seq"
"#LINE# #TAB# method = None #LINE# #TAB# for klass_name, klass in klass.__bases__: #LINE# #TAB# #TAB# if klass.__name__ == name: #LINE# #TAB# #TAB# #TAB# method = klass_.get_method(klass_name) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return method"
"#LINE# #TAB# if len(tokens) == 2: #LINE# #TAB# #TAB# psi, value = tokens[1], tokens[3] #LINE# #TAB# elif len(tokens) == 4: #LINE# #TAB# #TAB# psi, value = tokens #LINE# #TAB# elif len(tokens) == 5: #LINE# #TAB# #TAB# psi, value = tokens #LINE# #TAB# else: #LINE# #TAB# #TAB# raise CoconutInternalException(""Invalid assignment function definition"") #LINE# #TAB# return psi, value"
"#LINE# #TAB# if 'image' not in params: #LINE# #TAB# #TAB# return False #LINE# #TAB# if method in ('get', 'post', 'put'): #LINE# #TAB# #TAB# tensor = params['image'] #LINE# #TAB# elif 'image' in params: #LINE# #TAB# #TAB# tensor = params['image'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# return tensor"
"#LINE# #TAB# if encoding is None: #LINE# #TAB# #TAB# encoding = 'utf-8' #LINE# #TAB# try: #LINE# #TAB# #TAB# return np.array(data).reshape((-1, 1))[0] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return np.array(data).reshape((-1, 1))[0]"
"#LINE# s = [] #LINE# inputBits = int(numActiveInputBits) #LINE# for _ in range(numDims): #LINE# #TAB# s.append(random.randint(0, 2 ** 32 - 1)) #LINE# for _ in range(numDims): #LINE# #TAB# s.append(np.random.randint(0, 2 ** 32 - 1)) #LINE# return s"
"#LINE# #TAB# db_handler = RedisHandler(host=REDIS_HOST, port=REDIS_PORT) #LINE# #TAB# db_name = '%s.db' % cls.get_db_name() #LINE# #TAB# return db_handler, db_name"
#LINE# #TAB# with open(location) as csvfile: #LINE# #TAB# #TAB# reader = csv.DictReader(csvfile) #LINE# #TAB# #TAB# row_list = [row for row in reader] #LINE# #TAB# #TAB# if row_list: #LINE# #TAB# #TAB# #TAB# sorted_row_list = sorted(row_list) #LINE# #TAB# #TAB# #TAB# yield sorted_row_list
"#LINE# #TAB# try: #LINE# #TAB# #TAB# log = _hid.get_agent_log(dev_ref, key) #LINE# #TAB# #TAB# if not log: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return int(log[0]) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# scope = [] #LINE# #TAB# for d in unitaries: #LINE# #TAB# #TAB# for p1 in d: #LINE# #TAB# #TAB# #TAB# for p2 in d: #LINE# #TAB# #TAB# #TAB# #TAB# if p1.commute == p2.commute: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# scope.append(p1.indece_time) #LINE# #TAB# return scope
#LINE# #TAB# client = ControllerClient() #LINE# #TAB# result = client.get_pathext(name) #LINE# #TAB# if result is not None: #LINE# #TAB# #TAB# click.echo(util.format_result(result)) #LINE# #TAB# return result
#LINE# #TAB# l = task.logs #LINE# #TAB# if run_name not in l: #LINE# #TAB# #TAB# return True #LINE# #TAB# for item in l[run_name]: #LINE# #TAB# #TAB# if item is task: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# model_one = S12.diff(model_one) #LINE# #TAB# model_two = S12.diff(model_two) #LINE# #TAB# U1 = model_one.dot(model_two) #LINE# #TAB# U2 = model_two.dot(model_one) #LINE# #TAB# U3 = model_one.dot(S12.diff(U1)) #LINE# #TAB# U4 = model_two.dot(S12.diff(U2)) #LINE# #TAB# return U1, U2, U3, U4"
#LINE# #TAB# old_value = numpy.asarray(old_value) #LINE# #TAB# new_value = numpy.asarray(new_value) #LINE# #TAB# if old_value.dtype == new_value.dtype: #LINE# #TAB# #TAB# if old_value.shape == new_value.shape: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif old_value.shape!= new_value.shape: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if name1 is None or name2 is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# match = wildcard_pattern.match(name1) #LINE# #TAB# if match: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# match = wildcard_pattern.match(name2) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if not match: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if FQ_NOT_FOUND in config: #LINE# #TAB# #TAB# logger.warning('Fq not found in config file.') #LINE# #TAB# #TAB# return config #LINE# #TAB# if config['fq'] is None: #LINE# #TAB# #TAB# logger.warning('Fq not found in config file.') #LINE# #TAB# #TAB# return config #LINE# #TAB# return {'fq': config['fq']}
#LINE# #TAB# labelmap = {} #LINE# #TAB# for table in database.get_tables(table_name): #LINE# #TAB# #TAB# for column in table.columns: #LINE# #TAB# #TAB# #TAB# labelmap[column.name] = column.value #LINE# #TAB# return labelmap
"#LINE# #TAB# try: #LINE# #TAB# #TAB# for node in tree.iter(): #LINE# #TAB# #TAB# #TAB# node.tag = '{}{}'.format(URI, node.tag) #LINE# #TAB# #TAB# yield node #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass"
"#LINE# #TAB# if not hasattr(cls, 'DESCRIPTION'): #LINE# #TAB# #TAB# raise NotImplementedError #LINE# #TAB# result = cls.DESCRIPTION #LINE# #TAB# if len(cls.DESCRIPTION)!= 6: #LINE# #TAB# #TAB# raise NotImplementedError #LINE# #TAB# return result"
#LINE# #TAB# bytecode_md5 = hashlib.md5(bytecode_path.encode('utf-8')).hexdigest() #LINE# #TAB# if PY3: #LINE# #TAB# #TAB# return bytecode_md5.decode('latin-1') #LINE# #TAB# else: #LINE# #TAB# #TAB# return bytecode_md5
"#LINE# #TAB# dirname = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# data = np.loadtxt(os.path.join(dirname, 'data')) #LINE# #TAB# is_extension = False #LINE# #TAB# if data[0] == 'HID': #LINE# #TAB# #TAB# is_extension = True #LINE# #TAB# if data[1] == 'Household': #LINE# #TAB# #TAB# is_extension = True #LINE# #TAB# if data[2] == 'AREA': #LINE# #TAB# #TAB# is_extension = True #LINE# #TAB# return is_extension"
#LINE# #TAB# if i % 4 == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"#LINE# #TAB# obj_function_file = config.get('objective_function') #LINE# #TAB# if obj_function_file: #LINE# #TAB# #TAB# with open(obj_function_file, 'r') as f: #LINE# #TAB# #TAB# #TAB# obj_function_dict = json.load(f) #LINE# #TAB# return obj_function_dict"
"#LINE# #TAB# if isinstance(maybe_dttm, datetime.datetime): #LINE# #TAB# #TAB# maybe_dttm = _format_datetime(maybe_dttm) #LINE# #TAB# return maybe_dttm"
"#LINE# #TAB# if index == len(lst): #LINE# #TAB# #TAB# return lst #LINE# #TAB# min_index = min(index, len(lst)) #LINE# #TAB# new_lst = [] #LINE# #TAB# for i, val in enumerate(lst): #LINE# #TAB# #TAB# if val < min_index: #LINE# #TAB# #TAB# #TAB# new_lst.append(i) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_lst.append(val) #LINE# #TAB# return new_lst"
"#LINE# #TAB# response = requests.get(USER_ACCESS_TOKEN_URL, params={'grant_type': #LINE# #TAB# #TAB# 'client_credentials', 'client_id': user_access_token['client_id']}) #LINE# #TAB# if response.status_code == 200: #LINE# #TAB# #TAB# return response.json()['startTime'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
#LINE# #TAB# if obj.experiment_type =='multivariate': #LINE# #TAB# #TAB# return {'create_sections': True} #LINE# #TAB# return {}
"#LINE# #TAB# col = dataframe[colname] #LINE# #TAB# col_numerics = col.loc[col.apply(lambda x: isinstance(x, (int, float)))] #LINE# #TAB# return col_numerics.mode"
"#LINE# #TAB# q = [] #LINE# #TAB# while len(q) < length: #LINE# #TAB# #TAB# q.append(rnd.randint(0, 255)) #LINE# #TAB# return q"
"#LINE# #TAB# if method == 'between': #LINE# #TAB# #TAB# window = time.groupby(col).apply(cdp_split_between) #LINE# #TAB# #TAB# out = time.drop(window, axis=1) #LINE# #TAB# elif method == 'date': #LINE# #TAB# #TAB# window = time.groupby(col).apply(cdp_split_date) #LINE# #TAB# #TAB# out = time.drop(window, axis=1) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('method must be'+ method +'or'+ #LINE# #TAB# #TAB# #TAB# 'between') #LINE# #TAB# return out"
#LINE# #TAB# session = boto3.Session(profile_name=None) #LINE# #TAB# client = session.client('ec2') #LINE# #TAB# instance_key = None #LINE# #TAB# try: #LINE# #TAB# #TAB# with session.begin(): #LINE# #TAB# #TAB# #TAB# instance_key = client.get_instance_key(Username=username) #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# if e.response['Error']['Code'] == 'NoSuchEntity': #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise e #LINE# #TAB# return instance_key
#LINE# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# ip = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# elif os.name == 'posix': #LINE# #TAB# #TAB# ip = os.getenv('APPDATA') #LINE# #TAB# else: #LINE# #TAB# #TAB# ip = socket.gethostbyname(socket.gethostname()) #LINE# #TAB# return ip
#LINE# #TAB# result = [] #LINE# #TAB# row_count = 0 #LINE# #TAB# for _ in range(num_rows): #LINE# #TAB# #TAB# new_row = sbody.pop(row_count) #LINE# #TAB# #TAB# if new_row == row_count: #LINE# #TAB# #TAB# #TAB# row_count += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(new_row) #LINE# #TAB# #TAB# #TAB# row_count += 1 #LINE# #TAB# return result
"#LINE# #TAB# scale = float('inf') #LINE# #TAB# for key in vertex_json: #LINE# #TAB# #TAB# if key =='scale': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(vertex_json[key], dict): #LINE# #TAB# #TAB# #TAB# scale += get_scale(vertex_json[key]) #LINE# #TAB# #TAB# elif isinstance(vertex_json[key], list): #LINE# #TAB# #TAB# #TAB# for item in vertex_json[key]: #LINE# #TAB# #TAB# #TAB# #TAB# scale += get_scale(item) #LINE# #TAB# return scale"
"#LINE# #TAB# for x in range(w - room_size): #LINE# #TAB# #TAB# for y in range(h - room_size): #LINE# #TAB# #TAB# #TAB# yield x, y #LINE# #TAB# for x in range(w - room_size): #LINE# #TAB# #TAB# for y in range(h - room_size): #LINE# #TAB# #TAB# #TAB# yield x, y"
"#LINE# #TAB# group_name = np.random.choice(list(group_sizes), size=min_n_obs, #LINE# #TAB# #TAB# replace=False) #LINE# #TAB# return group_name"
"#LINE# #TAB# for base, dirs, files in os.walk(root_dir): #LINE# #TAB# #TAB# if base.endswith('.ubv'): #LINE# #TAB# #TAB# #TAB# if not empty_only: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# dirs[:] = [d for d in dirs if os.path.isdir(os.path.join(base, d))] #LINE# #TAB# #TAB# yield base, dirs, files"
#LINE# #TAB# logger.info('Shuffling the data') #LINE# #TAB# model.data.shuffle() #LINE# #TAB# return model
"#LINE# #TAB# x_xor_axes = ax.get_xlim() #LINE# #TAB# y_xor_axes = ax.get_ylim() #LINE# #TAB# return len(x_xor_axes) == 2 and all([(i in x_and_axes[i] == y_xor_axes[i]) for #LINE# #TAB# #TAB# i in range(1, len(y_xor_axes[i])) for i in range(0, len( #LINE# #TAB# #TAB# y_xor_axes[i]))]) and not np.all(np.diff(x_and_axes[i]) == [0, 1] #LINE# #TAB# #TAB# ) and not np.all(np.diff(y_and_axes[i]) == [0, 1]): #LINE# #TAB# #TAB# ax.colorbar() #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# compressed = False #LINE# #TAB# path = filename #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if filename.endswith('.gz'): #LINE# #TAB# #TAB# #TAB# #TAB# compressed = True #LINE# #TAB# #TAB# #TAB# #TAB# path = filename[:-4] #LINE# #TAB# #TAB# #TAB# elif filename.endswith('.bz2'): #LINE# #TAB# #TAB# #TAB# #TAB# path = filename[:-4] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return path, compressed"
"#LINE# #TAB# unique_dict = {} #LINE# #TAB# for key, val in ordered_pairs: #LINE# #TAB# #TAB# if key in unique_dict: #LINE# #TAB# #TAB# #TAB# raise KeyError('Duplicate key: %r' % (key,)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unique_dict[key] = val #LINE# #TAB# return unique_dict"
"#LINE# #TAB# main = MainWindow() #LINE# #TAB# main.main_window_size = 75 #LINE# #TAB# bottom = main.main_window_size[0] #LINE# #TAB# top = main.main_window_size[1] #LINE# #TAB# bottom = main.main_window_size[2] #LINE# #TAB# for i in range(top): #LINE# #TAB# #TAB# if bottom >= bottom: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# bottom = top #LINE# #TAB# #TAB# window = MainWindow(bottom, top) #LINE# #TAB# #TAB# window.add_child(window, bottom) #LINE# #TAB# return main"
"#LINE# #TAB# popsi = np.shape(ftrue) #LINE# #TAB# fval = _rand(popsi) ** beta * ftrue * np.maximum(1.0, (1000000000.0 / ( #LINE# #TAB# #TAB# ftrue + 1e-99)) ** (alpha * _rand(popsi))) #LINE# #TAB# tol = 1e-08 #LINE# #TAB# fval = fval + 1.01 * tol #LINE# #TAB# idx = ftrue < tol #LINE# #TAB# try: #LINE# #TAB# #TAB# fval[idx] = ftrue[idx] #LINE# #TAB# except (IndexError, TypeError): #LINE# #TAB# #TAB# if idx: #LINE# #TAB# #TAB# #TAB# fval = ftrue #LINE# #TAB# return fval"
"#LINE# #TAB# members = {} #LINE# #TAB# version_info = get_version() #LINE# #TAB# if version_info['major'] > 2: #LINE# #TAB# #TAB# return #LINE# #TAB# for machine in get_members(): #LINE# #TAB# #TAB# members[machine['id']] = {'version': version_info['major'],'minor': #LINE# #TAB# #TAB# #TAB# machine['minor'],'machine_info': {'hostname': machine['hostname'], #LINE# #TAB# #TAB# #TAB# 'python_version': platform.python_version()} #LINE# #TAB# return {'members': members}"
#LINE# #TAB# headers = {} #LINE# #TAB# for line in sacct_stream.decode('utf-8').splitlines(): #LINE# #TAB# #TAB# if not line or line[0] == '#': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if 'Access-Control-Allow-Origin' in line: #LINE# #TAB# #TAB# #TAB# headers['Access-Control-Allow-Origin'] = line[1:-1] #LINE# #TAB# #TAB# if 'Access-Control-Allow-Headers' in line: #LINE# #TAB# #TAB# #TAB# headers['Access-Control-Allow-Headers'] = line[1:-1] #LINE# #TAB# return headers
"#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# f = formatter('randomInt') #LINE# #TAB# #TAB# return f(value) #LINE# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# value = {k: formatter(v) for k, v in value.items()} #LINE# #TAB# return value"
"#LINE# #TAB# for pos, refl, iBeg, iFin in profList: #LINE# #TAB# #TAB# yc[iBeg:iFin] += refl[11 + im] * refl[9 + im] * yd[iBeg:iFin] #LINE# #TAB# return yc"
"#LINE# #TAB# db_map = {} #LINE# #TAB# if os.path.exists('custom/pypi/map.txt'): #LINE# #TAB# #TAB# with open('custom/pypi/map.txt') as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# if line.strip(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# package, url = line.strip().split('\t') #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# db_map[package] = url #LINE# #TAB# return db_map"
#LINE# #TAB# if doc is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# tags = [] #LINE# #TAB# for item in doc: #LINE# #TAB# #TAB# if item[0] == '@': #LINE# #TAB# #TAB# #TAB# tags.append(item[1:]) #LINE# #TAB# #TAB# elif item[0] == '/': #LINE# #TAB# #TAB# #TAB# tags.append(item[1:]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tags.append(item) #LINE# #TAB# return tags
"#LINE# #TAB# try: #LINE# #TAB# #TAB# template_path = 'templates/%s' % template_name #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise http.Http404('Template not found.') #LINE# #TAB# if template_path.endswith('.htm'): #LINE# #TAB# #TAB# template_path += '.htm' #LINE# #TAB# template = server_group_field_data(request, template_path) #LINE# #TAB# if template is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return template"
"#LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# s = s.decode('utf-8') #LINE# #TAB# return s"
#LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# set_options(text_color=color) #LINE# #TAB# return DEFAULT_CCD
"#LINE# #TAB# return [re.compile('^([0-9a-z\\-_]+)', re.IGNORECASE), re.compile( #LINE# #TAB# #TAB# '^[0-9a-z\\-_]+$'), re.compile('^([0-9a-z\\-_]+)$', re.IGNORECASE), #LINE# #TAB# #TAB# re.compile('^([0-9a-z\\-_]+)$', re.IGNORECASE), re.compile( #LINE# #TAB# #TAB# '^([0-9a-z\\-_]+)$', re.IGNORECASE), re.compile('^([0-9a-z\\-_]+)$', #LINE# #TAB# #TAB# re.IGNORECASE)]"
"#LINE# #TAB# if not protcol: #LINE# #TAB# #TAB# protcol = mzidtsvdata.HEADER_PROT #LINE# #TAB# top_psms = generate_top_psms(psms, protcol) #LINE# #TAB# for protein in proteins: #LINE# #TAB# #TAB# prot_acc = protein[prottabledata.HEADER_PROTEIN] #LINE# #TAB# #TAB# prec_area = calculate_precursor_quant(top_psms, prot_acc) #LINE# #TAB# #TAB# outprotein = {k: v for k, v in protein.items()} #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][None]] += prec_area #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][None]] = str(prec_area) #LINE# #TAB# return outprotein"
"#LINE# #TAB# string = '' #LINE# #TAB# if dedupe: #LINE# #TAB# #TAB# pattern = re.compile('\\s+') #LINE# #TAB# #TAB# text = pattern.sub('', text) #LINE# #TAB# lines = re.findall(pattern, text) #LINE# #TAB# if not lines: #LINE# #TAB# #TAB# return text #LINE# #TAB# else: #LINE# #TAB# #TAB# for line in lines: #LINE# #TAB# #TAB# #TAB# line = re.sub('\\s+','', line) #LINE# #TAB# #TAB# #TAB# if line.strip() in text: #LINE# #TAB# #TAB# #TAB# #TAB# lines.remove(line) #LINE# #TAB# #TAB# text = ''.join(lines) #LINE# #TAB# return text"
"#LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if isinstance(item, int): #LINE# #TAB# #TAB# #TAB# yield item #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield item"
#LINE# #TAB# w = event.app.layout.current_window #LINE# #TAB# b = event.app.current_buffer #LINE# #TAB# if w: #LINE# #TAB# #TAB# if w.render_info: #LINE# #TAB# #TAB# #TAB# info = w.render_info #LINE# #TAB# #TAB# #TAB# if w.vertical_scroll < info.content_height - info.window_height: #LINE# #TAB# #TAB# #TAB# #TAB# if info.cursor_position.y <= info.configured_scroll_offsets.top: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# b.cursor_position += b.document.get_cursor_down_position() #LINE# #TAB# #TAB# #TAB# #TAB# w.vertical_scroll += 1
#LINE# #TAB# url = ':'.join(url.split(':')) #LINE# #TAB# for r in requests.get(url): #LINE# #TAB# #TAB# if r.status_code == 200: #LINE# #TAB# #TAB# #TAB# data = r.json() #LINE# #TAB# #TAB# #TAB# if data['repository_fullname'] == url: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# elif data['repository_fullname'] == url: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# yield data
"#LINE# #TAB# calc_dtype = type(a_dtype).__name__ #LINE# #TAB# res_dtype = type(b_dtype).__name__ #LINE# #TAB# if calc_dtype == 'float64': #LINE# #TAB# #TAB# res_dtype = 'float32' #LINE# #TAB# elif calc_dtype == 'int8': #LINE# #TAB# #TAB# res_dtype = 'int8' #LINE# #TAB# elif calc_dtype == 'float32': #LINE# #TAB# #TAB# res_dtype = 'float32' #LINE# #TAB# return calc_dtype, res_dtype"
#LINE# #TAB# if not content: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [line.strip() for line in content.splitlines() if line and not line.startswith('#' #LINE# #TAB# #TAB# )]
#LINE# #TAB# for cell in nb.cells: #LINE# #TAB# #TAB# if cell.cell_type == 'code': #LINE# #TAB# #TAB# #TAB# if 'outputs' in cell: #LINE# #TAB# #TAB# #TAB# #TAB# for output in cell['outputs']: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if 'last_modified' in output: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# output['last_modified'] = output['last_modified'] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield cell
#LINE# #TAB# target_hosts = [] #LINE# #TAB# for col in df.columns: #LINE# #TAB# #TAB# if 'Target_Host' in df[col].unique(): #LINE# #TAB# #TAB# #TAB# target_hosts.append(df[col]) #LINE# #TAB# return target_hosts
"#LINE# #TAB# media = {} #LINE# #TAB# if request.GET: #LINE# #TAB# #TAB# for prop_name, prop_val in request.GET.items(): #LINE# #TAB# #TAB# #TAB# plugin = request.registry[prop_name] #LINE# #TAB# #TAB# #TAB# if plugin.is_cacheable(): #LINE# #TAB# #TAB# #TAB# #TAB# save_media(media, prop_val, plugin) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# media[prop_name] = prop_val #LINE# #TAB# return media"
#LINE# #TAB# batch_c = batch #LINE# #TAB# r = lib.SDL_WriteU8(batch_c) #LINE# #TAB# return r
#LINE# #TAB# es = cls.get_organism_hosts() #LINE# #TAB# result = {} #LINE# #TAB# for i in es: #LINE# #TAB# #TAB# result[i] = [i] #LINE# #TAB# return result
"#LINE# #TAB# choice = '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# choice = random.randint(0, 10000) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# choice = random.randint(10000, 999999999) #LINE# #TAB# return choice"
"#LINE# #TAB# x = P[0] #LINE# #TAB# y = P[1] #LINE# #TAB# return x, y"
#LINE# #TAB# new_state = copy.deepcopy(state) #LINE# #TAB# new_state.encode(pickup) #LINE# #TAB# new_state.gain = pickup.gain #LINE# #TAB# return new_state
"#LINE# #TAB# conn = boto.connect_s3() #LINE# #TAB# return conn.get_bucket(bucket), conn.get_key(bucket)['Key']"
"#LINE# #TAB# marker_tuple = [x, y] #LINE# #TAB# if marker in ['o', 'x', 'y']: #LINE# #TAB# #TAB# marker_tuple[0] = marker #LINE# #TAB# elif marker in ['w', 'h']: #LINE# #TAB# #TAB# marker_tuple[0] = x #LINE# #TAB# #TAB# marker_tuple[1] = y #LINE# #TAB# assert len(marker_tuple) == 4 #LINE# #TAB# return marker_tuple[0], marker_tuple[1], marker_tuple[2]"
#LINE# #TAB# c = root #LINE# #TAB# t = target #LINE# #TAB# try: #LINE# #TAB# #TAB# while c!= t: #LINE# #TAB# #TAB# #TAB# n = pred[c] #LINE# #TAB# #TAB# #TAB# c = pred[n] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# while c!= t: #LINE# #TAB# #TAB# #TAB# if c == root: #LINE# #TAB# #TAB# #TAB# #TAB# t = n #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return c
"#LINE# #TAB# bridge = [] #LINE# #TAB# sensitivity_in_db = np.array(sensitivity, dtype=np.float64) #LINE# #TAB# for i in range(0, len(sensitivity_in_db), 2): #LINE# #TAB# #TAB# bridge.append(sensitivity_in_db[i]) #LINE# #TAB# bridge[1:, 0] = (sensitivity_in_db[i + 1] - sensitivity_in_db[i]) / 1000 #LINE# #TAB# return bridge"
"#LINE# #TAB# terms_to_return = [] #LINE# #TAB# for term in facets_terms: #LINE# #TAB# #TAB# if isinstance(term, FacetTerm): #LINE# #TAB# #TAB# #TAB# terms_to_return.append(term) #LINE# #TAB# return terms_to_return"
#LINE# #TAB# if 'uri' not in configuration: #LINE# #TAB# #TAB# return configuration #LINE# #TAB# root = ET.Element('uri') #LINE# #TAB# root.append(configuration['uri']) #LINE# #TAB# application.append(root) #LINE# #TAB# return configuration
#LINE# #TAB# try: #LINE# #TAB# #TAB# return _registry_stack[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# return { #LINE# #TAB# #TAB#'models': { #LINE# #TAB# #TAB# #TAB# 'args': module.jsonpointer_args(), #LINE# #TAB# #TAB# #TAB# 'kwargs': module.jsonpointer_kwargs(), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"#LINE# #TAB# if isinstance(secret, str): #LINE# #TAB# #TAB# secret = secret.encode('utf8') #LINE# #TAB# elif isinstance(secret, bytes): #LINE# #TAB# #TAB# secret = secret.decode('latin-1') #LINE# #TAB# try: #LINE# #TAB# #TAB# conv_key = hashlib.sha256(secret).digest() #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# conv_key = hashlib.sha256(secret).digest() #LINE# #TAB# fc = '%s%s' % (conv_key[:64], conv_key[64:]) #LINE# #TAB# return fc"
#LINE# #TAB# options = request.options.copy() #LINE# #TAB# if request.GET: #LINE# #TAB# #TAB# options['public'] = True #LINE# #TAB# quotes = get_quotes(request) #LINE# #TAB# if None in quotes: #LINE# #TAB# #TAB# context = {} #LINE# #TAB# #TAB# context['quotes'] = quotes[0] #LINE# #TAB# #TAB# context['request'] = request #LINE# #TAB# #TAB# return context #LINE# #TAB# else: #LINE# #TAB# #TAB# context['quotes'] = [] #LINE# #TAB# #TAB# context['request'] = request #LINE# #TAB# #TAB# return context
#LINE# #TAB# nsset_count = 0 #LINE# #TAB# for node in G: #LINE# #TAB# #TAB# if not G.has_node(node): #LINE# #TAB# #TAB# #TAB# return nsset_count #LINE# #TAB# for node in G.nodes(): #LINE# #TAB# #TAB# if not G.has_node(node): #LINE# #TAB# #TAB# #TAB# nsset_count += 1 #LINE# #TAB# return nsset_count
#LINE# #TAB# if line_or_func is not None: #LINE# #TAB# #TAB# return line_or_func.index #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# string_data = {} #LINE# #TAB# for key in data: #LINE# #TAB# #TAB# string_data[key] = str(data[key]) #LINE# #TAB# return string_data
"#LINE# #TAB# from hashlib import md5sum #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(src_file, 'r') as _file: #LINE# #TAB# #TAB# #TAB# text = _file.read() #LINE# #TAB# #TAB# #TAB# md5sum = md5sum.hexdigest() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# with open(src_file, 'r') as _file: #LINE# #TAB# #TAB# #TAB# text = _file.read() #LINE# #TAB# return text"
#LINE# #TAB# if key_func is not None: #LINE# #TAB# #TAB# if callable(key_func): #LINE# #TAB# #TAB# #TAB# return key_func #LINE# #TAB# return default_key_func
"#LINE# #TAB# with open(file_name, 'r') as fp: #LINE# #TAB# #TAB# if load_order: #LINE# #TAB# #TAB# #TAB# stages = json.load(fp, object_pairs_hook=OrderedDict) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# stages = json.load(fp, object_pairs_hook=OrderedDict) #LINE# #TAB# #TAB# for stage in stages: #LINE# #TAB# #TAB# #TAB# bdd.add_stage(stage) #LINE# #TAB# return bdd"
"#LINE# #TAB# t = t_backtickedvar_ENCAPSED_AND_WHITESPACE #LINE# #TAB# lang1, lang2 = path.split('-') #LINE# #TAB# if lang1 == '': #LINE# #TAB# #TAB# lang1 = 'default' #LINE# #TAB# else: #LINE# #TAB# #TAB# lang1 = lang1.split('.')[0] #LINE# #TAB# if lang2 == '': #LINE# #TAB# #TAB# lang2 = 'default' #LINE# #TAB# return t, lang1, lang2"
#LINE# #TAB# result = [] #LINE# #TAB# for id_item in ids: #LINE# #TAB# #TAB# item = cls.get(id_item) #LINE# #TAB# #TAB# if item is not None: #LINE# #TAB# #TAB# #TAB# result.append(item) #LINE# #TAB# return result
"#LINE# #TAB# DataFrame) ->None: #LINE# #TAB# for row_index, row in interactions.iterrows(): #LINE# #TAB# #TAB# if row_index not in complexes: #LINE# #TAB# #TAB# #TAB# columns = list(row.columns) #LINE# #TAB# #TAB# #TAB# del columns[row_index] #LINE# #TAB# if not interactions.empty: #LINE# #TAB# #TAB# return #LINE# #TAB# for row_index, row in complexes.iterrows(): #LINE# #TAB# #TAB# if row_index not in interactions.index: #LINE# #TAB# #TAB# #TAB# columns.remove(row_index) #LINE# #TAB# del interactions[~interactions.index.isin(complexes.index)]"
"#LINE# #TAB# tensor = encode_plugin(v) #LINE# #TAB# index = encode_plugin(v.index) #LINE# #TAB# size = encode_plugin(v.size) #LINE# #TAB# index_shape = encode_plugin(v.index_shape) #LINE# #TAB# index_size = encode_plugin(v.index_size) #LINE# #TAB# total_shape = encode_plugin(v.total_shape) #LINE# #TAB# total_shape = encode_plugin(v.total_size) #LINE# #TAB# return tensor.shape, size, index_shape, index_size, total_shape"
#LINE# #TAB# output = {} #LINE# #TAB# for key in inputs: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# output[key] = inputs[key] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return output
"#LINE# #TAB# abbrevs = {} #LINE# #TAB# for name, path in os.walk(os.getcwd()): #LINE# #TAB# #TAB# for f in os.listdir(path): #LINE# #TAB# #TAB# #TAB# s = os.path.join(path, f) #LINE# #TAB# #TAB# #TAB# if os.path.isfile(s): #LINE# #TAB# #TAB# #TAB# #TAB# abbrevs[f] = name #LINE# #TAB# return abbrevs"
#LINE# #TAB# script_id = nexus_client.scripts.delete(name) #LINE# #TAB# return script_id
"#LINE# #TAB# #TAB# logging.basicConfig(level=logging.INFO, format=""%(message)s"") #LINE# #TAB# #TAB# logging.getLogger().setLevel(logging.DEBUG) #LINE# #TAB# #TAB# if opts.quiet: #LINE# #TAB# #TAB# #TAB# logging.basicConfig(level=logging.WARNING) #LINE# #TAB# #TAB# #TAB# logging.getLogger().propagate = True"
"#LINE# #TAB# options = options or {} #LINE# #TAB# environment = environment or os.environ #LINE# #TAB# compose_version = options.get(COMPOSE_COMPATIBILITY, None) #LINE# #TAB# if compose_version: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# output = subprocess.check_output(['compose', '--version'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdin=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stderr=subprocess.STDOUT) #LINE# #TAB# #TAB# except subprocess.CalledProcessError: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return compose_version"
"#LINE# #TAB# data_iter = iter(data) #LINE# #TAB# for i in range(0, len(data_iter), size): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data_item = next(data_iter) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# yield data_item #LINE# #TAB# #TAB# data_iter = None #LINE# #TAB# if data_item: #LINE# #TAB# #TAB# yield data_item"
"#LINE# #TAB# node_dict = {} #LINE# #TAB# try: #LINE# #TAB# #TAB# for key, value in connection.keys().items(): #LINE# #TAB# #TAB# #TAB# if not key.startswith('rethink_'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if not isinstance(value, bytes): #LINE# #TAB# #TAB# #TAB# #TAB# raise Exception('rethink://{}: {}'.format(key, value)) #LINE# #TAB# #TAB# #TAB# node_dict[key] = value #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# raise RethinkException('rethink://{}: {}'.format(key, value)) #LINE# #TAB# return node_dict"
#LINE# #TAB# result = {} #LINE# #TAB# server_defaults = {} #LINE# #TAB# for env_name in server_defaults: #LINE# #TAB# #TAB# value = os.getenv(env_name) #LINE# #TAB# #TAB# if value is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if value == 'true': #LINE# #TAB# #TAB# #TAB# server_defaults[env_name] = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result[env_name] = value #LINE# #TAB# return result
"#LINE# #TAB# package_name = os.path.basename(content_path) #LINE# #TAB# scoped_name = os.path.join( #LINE# #TAB# #TAB# get_package_dir(), #LINE# #TAB# #TAB# package_name #LINE# #TAB# ) #LINE# #TAB# return scoped_name"
"#LINE# #TAB# pathname = str(pathname).replace('&', '&amp;') #LINE# #TAB# pathname = pathname.replace('<', '&lt;') #LINE# #TAB# pathname = pathname.replace('>', '&gt;') #LINE# #TAB# pathname = pathname.replace('""', '&quot;') #LINE# #TAB# return pathname"
"#LINE# #TAB# response.status_code = 204 #LINE# #TAB# response.headers['Access-Control-Allow-Origin'] = '*' #LINE# #TAB# response.headers['Access-Control-Allow-Headers'] = \ #LINE# #TAB# #TAB# 'Origin, X-Requested-With, Content-Type, Accept, Authorization' #LINE# #TAB# return response"
"#LINE# #TAB# if isinstance(q, astropy.units.Quantity): #LINE# #TAB# #TAB# q = to_zhuyin_units(q) #LINE# #TAB# elif isinstance(q, astropy.units.Moment): #LINE# #TAB# #TAB# q = to_zhuyin_moment(q) #LINE# #TAB# elif isinstance(q, np.ndarray): #LINE# #TAB# #TAB# q = np.array(q) #LINE# #TAB# f[key] = q"
"#LINE# #TAB# data = urlretrieve(url, params) #LINE# #TAB# if'scales' in data: #LINE# #TAB# #TAB# del data['scales'] #LINE# #TAB# surface = {} #LINE# #TAB# for key in data.keys(): #LINE# #TAB# #TAB# value = data[key] #LINE# #TAB# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# #TAB# value = ','.join(value) #LINE# #TAB# #TAB# surface[key] = value #LINE# #TAB# return surface"
#LINE# #TAB# return [response_from_point(p) for p in polygon] if len(polygon #LINE# #TAB# #TAB# ) > 1 else [0]
"#LINE# #TAB# parser = _get_config(file) #LINE# #TAB# key_func = {} #LINE# #TAB# for key, val in parser.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# key_func[str(key)] = val #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return key_func"
#LINE# #TAB# root = section #LINE# #TAB# children = [] #LINE# #TAB# current = section #LINE# #TAB# while current and children: #LINE# #TAB# #TAB# if current.children: #LINE# #TAB# #TAB# #TAB# current = children.pop() #LINE# #TAB# #TAB# #TAB# children.append(current) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# root = take_closest(current) #LINE# #TAB# return root
"#LINE# #TAB# html = ET.tostring(elem, encoding='utf-8') #LINE# #TAB# if html.tag == 'html': #LINE# #TAB# #TAB# html = html.replace('&quot;', '""').replace('&apos;', ""'"") #LINE# #TAB# #TAB# return html #LINE# #TAB# if pretty_print: #LINE# #TAB# #TAB# html = html.replace('&amp;', '&') #LINE# #TAB# #TAB# return html #LINE# #TAB# return html"
"#LINE# #TAB# recommended_name = '%s-%s' % (table_name, objid) #LINE# #TAB# return recommended_name"
"#LINE# #TAB# hankel = [None] * len(points) #LINE# #TAB# for p in points: #LINE# #TAB# #TAB# x = p[0] #LINE# #TAB# #TAB# y = p[1] #LINE# #TAB# #TAB# for i in range(len(hankel)): #LINE# #TAB# #TAB# #TAB# x = hankel[i] + x #LINE# #TAB# #TAB# #TAB# y = hankel[i] + y #LINE# #TAB# #TAB# hankel[i] = x, y #LINE# #TAB# return hankel"
#LINE# #TAB# if friendly: #LINE# #TAB# #TAB# return 'Alignak live state history' #LINE# #TAB# return 'livesynthesisretention'
"#LINE# #TAB# lines = string.splitlines(True) #LINE# #TAB# current_line = line_offset + lines[0] #LINE# #TAB# current_column = index #LINE# #TAB# while current_line < len(lines): #LINE# #TAB# #TAB# lines.append(current_line) #LINE# #TAB# #TAB# current_line += 1 #LINE# #TAB# if current_column > len(lines): #LINE# #TAB# #TAB# column = len(lines) - current_column - 1 #LINE# #TAB# return lines, column"
#LINE# #TAB# cur_msg = None #LINE# #TAB# for m in messages: #LINE# #TAB# #TAB# if cur_msg is None: #LINE# #TAB# #TAB# #TAB# cur_msg = m #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# msg = m #LINE# #TAB# #TAB# #TAB# cur_msg = cls.serialize(msg) #LINE# #TAB# #TAB# yield cur_msg
#LINE# #TAB# cov = skil.data.cov #LINE# #TAB# resource_list = [] #LINE# #TAB# for resource in cov: #LINE# #TAB# #TAB# if resource.resource_type == resource_type: #LINE# #TAB# #TAB# #TAB# resource_list.append(resource) #LINE# #TAB# return resource_list
"#LINE# #TAB# _, shape, _ = impact_function.shape #LINE# #TAB# contour_numbers = [] #LINE# #TAB# for i in range(shape[0]): #LINE# #TAB# #TAB# summation = 0 #LINE# #TAB# #TAB# for j in range(shape[1]): #LINE# #TAB# #TAB# #TAB# contour_numbers.append(i) #LINE# #TAB# #TAB# #TAB# summation += summation #LINE# #TAB# contour_summary = np.array(contour_numbers) #LINE# #TAB# return contour_summary"
"#LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# result = ''.join(obj) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = obj #LINE# #TAB# return result.startswith('\\\\') if result.endswith('\\' #LINE# #TAB# #TAB# ) else result"
#LINE# #TAB# if enabled: #LINE# #TAB# #TAB# gbp.log.debug('Setting file extension to'+ enabled) #LINE# #TAB# else: #LINE# #TAB# #TAB# gbp.log.debug('Setting file extension to'+ enabled) #LINE# #TAB# global plugin_enabled #LINE# #TAB# plugin_enabled = enabled
#LINE# #TAB# word = word.lower() #LINE# #TAB# if word[0].isupper(): #LINE# #TAB# #TAB# return '-'.join(word[1:]) #LINE# #TAB# if word[0] in '+-' and not word[0].islower(): #LINE# #TAB# #TAB# return word[0].capitalize() #LINE# #TAB# if word[0] == '-' and not word[1].islower(): #LINE# #TAB# #TAB# return word[1].capitalize() #LINE# #TAB# return ''
"#LINE# #TAB# if not isinstance(obj, color): #LINE# #TAB# #TAB# return color #LINE# #TAB# return obj"
#LINE# #TAB# names = [] #LINE# #TAB# for key in get_fields(config): #LINE# #TAB# #TAB# if key not in names: #LINE# #TAB# #TAB# #TAB# names.append(key) #LINE# #TAB# names.sort() #LINE# #TAB# return names
#LINE# #TAB# address_family = 'unknown' #LINE# #TAB# try: #LINE# #TAB# #TAB# parts = table.split('_') #LINE# #TAB# #TAB# if len(parts) == 3: #LINE# #TAB# #TAB# #TAB# address_family = parts[0] #LINE# #TAB# #TAB# elif len(parts) == 2: #LINE# #TAB# #TAB# #TAB# address_family = parts[1] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return address_family
"#LINE# #TAB# n = min(10000, n) #LINE# #TAB# r = '' #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# r = 'A' #LINE# #TAB# elif n < 3: #LINE# #TAB# #TAB# r = 'B' #LINE# #TAB# elif n < 6: #LINE# #TAB# #TAB# r = 'C' #LINE# #TAB# elif n < 7: #LINE# #TAB# #TAB# r = 'D' #LINE# #TAB# elif n < 8: #LINE# #TAB# #TAB# r = 'E' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('""n"" must be an integer between 0 and 7.') #LINE# #TAB# return r"
"#LINE# #TAB# body = json.dumps(obj, cls=DjangoJSONEncoder) #LINE# #TAB# return body"
"#LINE# #TAB# for key in sorted(nested_dict.keys()): #LINE# #TAB# #TAB# if isinstance(nested_dict[key], dict): #LINE# #TAB# #TAB# #TAB# new_keys = handle_fdid_aliases(nested_dict[key]) #LINE# #TAB# #TAB# #TAB# sub_dict = nested_dict[key] #LINE# #TAB# #TAB# #TAB# for new_key in new_keys: #LINE# #TAB# #TAB# #TAB# #TAB# sub_dict[new_key] = sub_dict #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return nested_dict[key] #LINE# #TAB# return nested_dict"
#LINE# #TAB# deps = [] #LINE# #TAB# for line in description.splitlines(): #LINE# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# if line.startswith(':'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# deps.append(field_to_deps(line)) #LINE# #TAB# return deps
#LINE# #TAB# if SerialPortDescriptor.__singleton_instance is None: #LINE# #TAB# #TAB# SerialPortDescriptor() #LINE# #TAB# return SerialPortDescriptor.__singleton_instance
#LINE# #TAB# assert i >= 0 #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return 'zero_byte' #LINE# #TAB# elif i == 1: #LINE# #TAB# #TAB# return 'one_byte' #LINE# #TAB# elif i == 2: #LINE# #TAB# #TAB# return 'two_byte' #LINE# #TAB# elif i == 3: #LINE# #TAB# #TAB# return 'larger_than_one' #LINE# #TAB# elif i == 4: #LINE# #TAB# #TAB# return 'larger_than_one' #LINE# #TAB# elif i == 5: #LINE# #TAB# #TAB# return 'larger_than_one' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'unknown'
"#LINE# #TAB# path = cls.task_dir_path(task_id) #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# if create: #LINE# #TAB# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# #TAB# with open(path, 'w') as f: #LINE# #TAB# #TAB# #TAB# f.write('') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# f.write('') #LINE# #TAB# return path"
"#LINE# #TAB# authuser = None #LINE# #TAB# try: #LINE# #TAB# #TAB# authuser = conn.get_authuser(filtr, dn, attr) #LINE# #TAB# except ldap.LDAPError: #LINE# #TAB# #TAB# with conn.begin(): #LINE# #TAB# #TAB# #TAB# if authuser is None: #LINE# #TAB# #TAB# #TAB# #TAB# authuser = dn #LINE# #TAB# #TAB# #TAB# #TAB# conn.add_authuser(authuser) #LINE# #TAB# return authuser"
"#LINE# #TAB# model = None #LINE# #TAB# events = state.events #LINE# #TAB# if tap_stream_id in events: #LINE# #TAB# #TAB# base_polymorphic_model = events[tap_stream_id]['polymorphic_model'] #LINE# #TAB# #TAB# if base_polymorphic_model: #LINE# #TAB# #TAB# #TAB# model = getattr(base_polymorphic_model, 'tap_stream_id') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# model = tap_stream_id #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unknown tap stream identifier: {0}'. #LINE# #TAB# #TAB# #TAB# format(tap_stream_id)) #LINE# #TAB# return model"
"#LINE# #TAB# root = raw.getroot() #LINE# #TAB# qname = root.tag.split('}')[-1] #LINE# #TAB# attributes = root.attrib #LINE# #TAB# if len(attributes) == 1: #LINE# #TAB# #TAB# return qname, attributes[0] #LINE# #TAB# return qname, attributes"
#LINE# #TAB# try: #LINE# #TAB# #TAB# float(val) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# graph = graph.copy() #LINE# #TAB# if edges is None: #LINE# #TAB# #TAB# edges = graph.edges() #LINE# #TAB# for u, v, k in graph.edges(data=True): #LINE# #TAB# #TAB# edge = u, v, k #LINE# #TAB# #TAB# if edge in edges: #LINE# #TAB# #TAB# #TAB# graph.delete_edge(edge, u, v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# graph.delete_edge(edge, v, k) #LINE# #TAB# return graph"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# size = os.stat(filename).st_size #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if size > 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# open(filename, 'w').truncate() #LINE# #TAB# #TAB# return True"
#LINE# #TAB# if dir == 0: #LINE# #TAB# #TAB# return Fore.GREEN #LINE# #TAB# elif dir == 1: #LINE# #TAB# #TAB# return Fore.BLUE #LINE# #TAB# elif dir == 2: #LINE# #TAB# #TAB# return Fore.RED #LINE# #TAB# else: #LINE# #TAB# #TAB# return Fore.BLUE
"#LINE# #TAB# cmd ='ssh -f '+ server #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# #TAB# out, err = p.communicate() #LINE# #TAB# #TAB# if p.returncode!= 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
#LINE# #TAB# for num in numbers: #LINE# #TAB# #TAB# if num >= 9: #LINE# #TAB# #TAB# #TAB# return 50 #LINE# #TAB# #TAB# elif num <= 11: #LINE# #TAB# #TAB# #TAB# return 100 #LINE# #TAB# return 0
"#LINE# #TAB# if span.sentence.is_visual(): #LINE# #TAB# #TAB# yield 'Visual-related', 1 #LINE# #TAB# for keyword in span.keywords: #LINE# #TAB# #TAB# if keyword in ('R', 'T'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if keyword in ('J', 'Q'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for other_keyword in (('Q', 'X'), ('Q', 'Y')): #LINE# #TAB# #TAB# #TAB# if keyword in other_keyword: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield '%s.%s' % (keyword, other_keyword[0]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield '%s.%s' % (keyword, 1), 0"
"#LINE# #TAB# ret = int(integer_string) #LINE# #TAB# if ret < 0 or ret == 0 and strict: #LINE# #TAB# #TAB# raise ValueError() #LINE# #TAB# if cutoff: #LINE# #TAB# #TAB# return min(ret, cutoff) #LINE# #TAB# return ret"
"#LINE# #TAB# python_version = 'python%d' % sys.version_info[0] #LINE# #TAB# path = os.path.dirname(sys.executable) #LINE# #TAB# new_path = os.path.join(path, python_version + '.py') #LINE# #TAB# if not os.path.exists(new_path): #LINE# #TAB# #TAB# os.makedirs(new_path) #LINE# #TAB# command = [p, '--version'] #LINE# #TAB# command.append(new_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# subprocess.check_call(command) #LINE# #TAB# except subprocess.CalledProcessError as e: #LINE# #TAB# #TAB# sys.stderr.write( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# raise e"
"#LINE# #TAB# assert isinstance(x, tuple) #LINE# #TAB# assert isinstance(y, tuple) #LINE# #TAB# return {'x': x, 'y': y}"
"#LINE# #TAB# strainT = tuple(strainT[1:]) #LINE# #TAB# geneT = cloneGene(geneT, genesO) #LINE# #TAB# strainT = cloneGene(strainT, genesO) #LINE# #TAB# return geneT, strainT"
"#LINE# #TAB# if isinstance(obj, Reference): #LINE# #TAB# #TAB# return obj.to_dict() #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# return [parse_input(v) for v in obj] #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# return {k: parse_input(v) for k, v in obj.items()} #LINE# #TAB# return {}"
"#LINE# #TAB# from lxml.etree import ElementTree as ET #LINE# #TAB# ents = [] #LINE# #TAB# for ent in doc: #LINE# #TAB# #TAB# if isinstance(ent, ET.Element): #LINE# #TAB# #TAB# #TAB# ents.append(ent.text) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ent = ent.text #LINE# #TAB# #TAB# ents.append(ent) #LINE# #TAB# ents.sort(key=lambda x: x[0], reverse=True) #LINE# #TAB# try: #LINE# #TAB# #TAB# return next(iter(ents)) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return doc"
"#LINE# #TAB# if day is None: #LINE# #TAB# #TAB# day = date.today() #LINE# #TAB# filename = get_inputs_filename() #LINE# #TAB# with open(filename, 'w') as handle: #LINE# #TAB# #TAB# handle.write( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return day"
#LINE# #TAB# if'receiver' in config: #LINE# #TAB# #TAB# return config['receiver'] #LINE# #TAB# elif 'urllist' in config: #LINE# #TAB# #TAB# return config['urllist'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# enum_list = DB.execute('SELECT * FROM enumeration WHERE address =?', (address,) #LINE# #TAB# #TAB# ).fetchall() #LINE# #TAB# return enum_list"
#LINE# #TAB# by_data = {} #LINE# #TAB# for data in params['bins'].values(): #LINE# #TAB# #TAB# if data['infromation'] == 'false': #LINE# #TAB# #TAB# #TAB# by_data[data['id']] = [] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# by_data[data['id']].append(data) #LINE# #TAB# return by_data
"#LINE# #TAB# order_dict = {'A': 0.0, 'C': 0.0, 'G': 0.0, 'T': 0.0} #LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# warnings.simplefilter('ignore') #LINE# #TAB# #TAB# order_dict['A'] = np.random.uniform(0.0, 1.0) #LINE# #TAB# #TAB# order_dict['C'] = np.random.uniform(0.0, 2.0 * np.pi) #LINE# #TAB# #TAB# order_dict['G'] = np.random.uniform(0.0, 2.0) #LINE# #TAB# return order_dict"
"#LINE# #TAB# if hasattr(fnode, 'func'): #LINE# #TAB# #TAB# func = fnode.func #LINE# #TAB# #TAB# args = fnode.args #LINE# #TAB# elif hasattr(fnode, 'keywords'): #LINE# #TAB# #TAB# args = fnode.keywords #LINE# #TAB# else: #LINE# #TAB# #TAB# return [] #LINE# #TAB# result = [] #LINE# #TAB# for arg in fnode.args: #LINE# #TAB# #TAB# if isinstance(arg, astroid.Name): #LINE# #TAB# #TAB# #TAB# result.append(arg.id) #LINE# #TAB# #TAB# elif isinstance(arg, astroid.Attribute): #LINE# #TAB# #TAB# #TAB# result.append(arg.id) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append('') #LINE# #TAB# return result"
"#LINE# #TAB# fin_txt =fin_txt.replace('\r', '').replace('\n', '') #LINE# #TAB# aliases = [] #LINE# #TAB# for line in fin_txt.splitlines(): #LINE# #TAB# #TAB# if line.startswith('#') or line == '' or line.startswith('['): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# aliases.append(line.replace('#', '')) #LINE# #TAB# return aliases"
#LINE# #TAB# host = request.get_host() #LINE# #TAB# if ':' in host: #LINE# #TAB# #TAB# host = host.split(':')[0] #LINE# #TAB# try: #LINE# #TAB# #TAB# user = User.objects.get(username=host) #LINE# #TAB# except User.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# password = User.objects.get(password=host) #LINE# #TAB# except User.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return User.objects.get(password=password) #LINE# #TAB# except User.DoesNotExist: #LINE# #TAB# #TAB# return None #LINE# #TAB# return None
"#LINE# #TAB# dev_template = InputTemplate(dev_name=dev_name, dev_type=dev_type) #LINE# #TAB# for favorite_id in dev_template.favorites: #LINE# #TAB# #TAB# dev_input = dev_template.get_favorite_action_id(favorite_id) #LINE# #TAB# #TAB# if not dev_input: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return dev_input #LINE# #TAB# return None"
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# if 'name' in obj: #LINE# #TAB# #TAB# #TAB# return obj['name'].title() #LINE# #TAB# #TAB# if 'protocol' in obj: #LINE# #TAB# #TAB# #TAB# return obj['protocol'] #LINE# #TAB# return None"
"#LINE# #TAB# with open(path, 'rb') as f: #LINE# #TAB# #TAB# f_type = f.read(4) #LINE# #TAB# return f_type"
"#LINE# #TAB# with open(device.last_saved_config_path, 'r') as f: #LINE# #TAB# #TAB# config = json.load(f) #LINE# #TAB# return config['execution_obj_type']"
#LINE# #TAB# if year!= _CURRENT_YEAR: #LINE# #TAB# #TAB# year = _CURRENT_YEAR #LINE# #TAB# if month!= _CURRENT_MONTH: #LINE# #TAB# #TAB# month = _CURRENT_MONTH #LINE# #TAB# if weekday!= _CURRENT_WEEK: #LINE# #TAB# #TAB# week = _CURRENT_WEEK #LINE# #TAB# else: #LINE# #TAB# #TAB# week = _CURRENT_WEEK #LINE# #TAB# return month == _CURRENT_MONTH and week == week
#LINE# #TAB# url_lower = url.lower() #LINE# #TAB# for method in BLACKLIST_URLS: #LINE# #TAB# #TAB# if url_lower in method.lower(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if not val: #LINE# #TAB# #TAB# return False #LINE# #TAB# if str(val).lower() == 'true': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if len(results) > 1: #LINE# #TAB# #TAB# return results[0][1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if np.isscalar(data) or len(data)!= 1: #LINE# #TAB# #TAB# return data #LINE# #TAB# key = list(data.keys())[0] #LINE# #TAB# if len(data[key]) == 1 and key in dataset.vdims: #LINE# #TAB# #TAB# return data[key][0]
"#LINE# #TAB# x = np.asarray(x) #LINE# #TAB# if ndim is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# x = x.reshape(-1, 1) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# x = x.reshape(-1, 1) #LINE# #TAB# x = x[..., np.newaxis] #LINE# #TAB# shape = x.shape #LINE# #TAB# if ndim is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# x = x[(np.newaxis), :] #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return x, shape"
#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == errno.EEXIST and os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise
"#LINE# #TAB# if remove_brackets_content: #LINE# #TAB# #TAB# trans = pangloss.remove_content_in_brackets(trans, ""[]"") #LINE# #TAB# trans = fr_nlp("" "".join(trans.split()[:])) #LINE# #TAB# trans = "" "".join([token.lower_ for token in trans if not token.is_punct]) #LINE# #TAB# return trans"
#LINE# #TAB# a = name.count('.') #LINE# #TAB# if a: #LINE# #TAB# #TAB# ext = name.split('.')[-1] #LINE# #TAB# #TAB# if ext in VALID_EXTENSIONS: #LINE# #TAB# #TAB# #TAB# return name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# content = '' #LINE# #TAB# content += '\n' #LINE# #TAB# for field in entry.fields: #LINE# #TAB# #TAB# content += '{} = {}'.format(field, entry[field]) #LINE# #TAB# #TAB# if field.db_field_type == 'date': #LINE# #TAB# #TAB# #TAB# content += '{} {}'.format(entry[field.name], entry[field.db_field_type]) #LINE# #TAB# #TAB# elif field.name == 'author': #LINE# #TAB# #TAB# #TAB# content += '{} {}'.format(entry[field.name], entry.get('author')) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# content += '{} {}'.format(entry[field.name], entry[field.db_field_type]) #LINE# #TAB# content += '\n' #LINE# #TAB# return content"
#LINE# #TAB# if '*' in value: #LINE# #TAB# #TAB# return value.split('*')[0] #LINE# #TAB# return value
"#LINE# #TAB# template_templates = {} #LINE# #TAB# power_state = nova.compute.power_state(pvm_state) #LINE# #TAB# for k, v in template_templates.items(): #LINE# #TAB# #TAB# if k =='system': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if power_state == v: #LINE# #TAB# #TAB# #TAB# template_templates[k] = v #LINE# #TAB# return template_templates"
#LINE# #TAB# slope = (y2 - y1 * x2 - x1) / (x2 - x1) #LINE# #TAB# return slope
"#LINE# #TAB# if isinstance(data, dict): #LINE# #TAB# #TAB# return set(data.values()) #LINE# #TAB# elif isinstance(data, list): #LINE# #TAB# #TAB# return set(path2_font_id(c) for c in data) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# keys = [k for k, _ in iter_public_keys(iso_year)] #LINE# #TAB# sum_of_public_keys = 0 #LINE# #TAB# for k in keys: #LINE# #TAB# #TAB# if k[-1] == 'T': #LINE# #TAB# #TAB# #TAB# sum_of_public_keys += 1 #LINE# #TAB# return sum_of_public_keys"
#LINE# #TAB# t.value = t.value[1:].upper() #LINE# #TAB# return t
"#LINE# #TAB# sa = mac2str(source) #LINE# #TAB# sa += b'\x00' * (4 - len(sa)) #LINE# #TAB# mac = mac2str(dest) #LINE# #TAB# mic = MIC(mic_key, sa, sa + b'\x00' * (4 - len(sa)) + b'\x00' * (4 - len(sa)) + data) #LINE# #TAB# return mic.decrypt(data)[:4]"
"#LINE# #TAB# a = a.astype(float) #LINE# #TAB# log2 = np.sum(np.abs(a), axis=1) #LINE# #TAB# a /= log2 #LINE# #TAB# color_map = 0.5 * (a[(0), :] ** 2 + a[(1), :] ** 2 + a[(2), :] ** 2) - 0.5 * (a[(0), : #LINE# #TAB# #TAB# ] ** 2 + a[(1), :] ** 2) #LINE# #TAB# return color_map"
"#LINE# #TAB# symbol_map = {} #LINE# #TAB# for index, blob_name in enumerate(ssa): #LINE# #TAB# #TAB# blob_name, version = blob_name.split('/', 1) #LINE# #TAB# #TAB# if version == 1: #LINE# #TAB# #TAB# #TAB# symbol_map[index] = blob_name #LINE# #TAB# #TAB# elif version == 2: #LINE# #TAB# #TAB# #TAB# symbol_map[index + 1] = blob_name, 1 #LINE# #TAB# return symbol_map"
"#LINE# #TAB# if n < 0: #LINE# #TAB# #TAB# n = -n #LINE# #TAB# return x1, y1 #LINE# #TAB# return x2, y2"
#LINE# #TAB# if pObj.__class__.__name__ == 'ClusterNode': #LINE# #TAB# #TAB# return 'ClusterNode' #LINE# #TAB# elif pObj.__class__.__name__ == 'Hypothesis_Node': #LINE# #TAB# #TAB# return 'Hypothesis_Node' #LINE# #TAB# return False
"#LINE# #TAB# module = __import__('lz4f') #LINE# #TAB# val = None #LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# val = f.read() #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# module.close() #LINE# #TAB# return val"
#LINE# #TAB# _validate_dependencies_met() #LINE# #TAB# util.SSLContext = PyOpenSSLContext #LINE# #TAB# util.ssl_.SSLContext = PyOpenSSLContext #LINE# #TAB# util.HAS_SNI = HAS_SNI #LINE# #TAB# util.ssl_.HAS_SNI = HAS_SNI #LINE# #TAB# util.IS_PYOPENSSL = True #LINE# #TAB# util.ssl_.IS_PYOPENSSL = True
"#LINE# #TAB# if not isinstance(value, str): #LINE# #TAB# #TAB# return False #LINE# #TAB# id_re = re.compile('\\W+') #LINE# #TAB# value = id_re.sub('', value) #LINE# #TAB# try: #LINE# #TAB# #TAB# imdbid = int(value) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# if imdbid >= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# imdbsize = int(imdbid) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# if imdbsize <= 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
#LINE# #TAB# try: #LINE# #TAB# #TAB# yield next(gen) #LINE# #TAB# except StopIteration as e: #LINE# #TAB# #TAB# if logger: #LINE# #TAB# #TAB# #TAB# logger.info('Continuing without raising KeyboardInterrupt') #LINE# #TAB# #TAB# raise
#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return float(value) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return value
"#LINE# #TAB# text = node.doc_content #LINE# #TAB# if text and isinstance(text, Text): #LINE# #TAB# #TAB# text = text.split('\n') #LINE# #TAB# #TAB# logging.debug('load_operations_from_docstring: node:\n%s', text) #LINE# #TAB# #TAB# operations = [] #LINE# #TAB# #TAB# for child in text: #LINE# #TAB# #TAB# #TAB# children = load_operations_from_docstring(child) #LINE# #TAB# #TAB# #TAB# if children is not None and isinstance(children[0], Operator): #LINE# #TAB# #TAB# #TAB# #TAB# operations.append(children[0]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# logging.debug('load_operations_from_docstring: node:\n%s', text) #LINE# #TAB# return operations"
"#LINE# #TAB# _path = Path(AbstractSample._full_path('attribute_is_list.csv')) #LINE# #TAB# df = pd.read_csv(_path, encoding='latin1') #LINE# #TAB# return df.iloc[:size]"
"#LINE# #TAB# prop_instances = {} #LINE# #TAB# for prop in source['properties']: #LINE# #TAB# #TAB# eid = getattr(source['properties'], prop['eid'], None) #LINE# #TAB# #TAB# if eid: #LINE# #TAB# #TAB# #TAB# prop_instances[eid] = { #LINE# #TAB# #TAB# #TAB# #TAB#'source_id': eid, #LINE# #TAB# #TAB# #TAB# #TAB# 'properties': [], #LINE# #TAB# #TAB# #TAB# } #LINE# #TAB# return prop_instances"
"#LINE# #TAB# if not isinstance(place, Place): #LINE# #TAB# #TAB# raise TypeError('Place must be of type Place') #LINE# #TAB# del net[place.id] #LINE# #TAB# return net"
"#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = cdf.name #LINE# #TAB# s = '\n'.join([ #LINE# #TAB# #TAB# timedelta(seconds=cdf.duration) #LINE# #TAB# #TAB# for val, unit in cdf.units.items() #LINE# #TAB# #TAB# if unit in ['s','m', 'h', 'd'] #LINE# #TAB# ]) #LINE# #TAB# s = '\n'.join([ #LINE# #TAB# #TAB# symbol + s #LINE# #TAB# #TAB# for symbol, duration in cdf.units.items() #LINE# #TAB# ]) #LINE# #TAB# s = '\n'.join([ #LINE# #TAB# #TAB# symbol + s #LINE# #TAB# #TAB# for s, unit in s.units.items() #LINE# #TAB# ]) #LINE# #TAB# return s"
"#LINE# #TAB# f = open(filename, 'rb') #LINE# #TAB# f.seek(2048) #LINE# #TAB# posbyte = 0 #LINE# #TAB# allsentences = '' #LINE# #TAB# for _ in list(range(32)): #LINE# #TAB# #TAB# tt = f.read(32) #LINE# #TAB# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# #TAB# if s1!= '': #LINE# #TAB# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# #TAB# posbyte += 32 #LINE# #TAB# tt = f.read(1024) #LINE# #TAB# s1 = tt.strip('\x00') #LINE# #TAB# if s1!= '': #LINE# #TAB# #TAB# allsentences += s1 + '\n' #LINE# #TAB# f.close() #LINE# #TAB# return allsentences"
"#LINE# #TAB# with open(json_fn) as f: #LINE# #TAB# #TAB# json_data = json.load(f) #LINE# #TAB# _flattened_data = [] #LINE# #TAB# for key, val in json_data.items(): #LINE# #TAB# #TAB# _flattened_data.append((key, val)) #LINE# #TAB# return _flattened_data"
"#LINE# #TAB# if num_bins is None: #LINE# #TAB# #TAB# num_bins=len(G.nodes()) #LINE# #TAB# bin_labels = range(num_bins) #LINE# #TAB# attr_values = pd.Series([data[attr] for u, v, key, data in G.nodes(data=True)]) #LINE# #TAB# cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels) #LINE# #TAB# colors = get_colors(num_bins, cmap, start, stop) #LINE# #TAB# return [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats]"
#LINE# #TAB# index_settings = {'schema': schema} #LINE# #TAB# if schema.startswith('.'): #LINE# #TAB# #TAB# index_settings['schema'] = schema[1:] #LINE# #TAB# if schema.startswith('/'): #LINE# #TAB# #TAB# index_settings['schema'] = '/' + schema[1:] #LINE# #TAB# return index_settings
#LINE# #TAB# try: #LINE# #TAB# #TAB# return array.get_least_indented(module.params['name']) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
#LINE# #TAB# name = None #LINE# #TAB# try: #LINE# #TAB# #TAB# name = sender.name() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# sha256 = hashlib.sha256(name).hexdigest() #LINE# #TAB# try: #LINE# #TAB# #TAB# return _hash_functions[name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return sha256
"#LINE# #TAB# if e.payload.file_name == cache.file_name: #LINE# #TAB# #TAB# cache.json = e.payload.json #LINE# #TAB# #TAB# cache.dict = {} #LINE# #TAB# #TAB# cache.post_fifo(Event(signal=signals.cache_file_write, payload={'times': #LINE# #TAB# #TAB# #TAB# 0, 'dict': cache.dict})) #LINE# #TAB# #TAB# return return_status.HANDLED #LINE# #TAB# elif e.payload.file_path == cache.file_path: #LINE# #TAB# #TAB# cache.post_fifo(Event(signal=signals.file_read, payload={'times': #LINE# #TAB# #TAB# #TAB# 0, 'dict': cache.dict})) #LINE# #TAB# #TAB# return return_status.HANDLED #LINE# #TAB# else: #LINE# #TAB# #TAB# return_status.HANDLED"
"#LINE# #TAB# b0 = 0.0215 #LINE# #TAB# b1 = 0.2122 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.00084 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['In'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"#LINE# #TAB# elements = find_elements(browser, selector) #LINE# #TAB# return [ #LINE# #TAB# #TAB# element #LINE# #TAB# #TAB# for element in elements #LINE# #TAB# #TAB# if element.get('type') =='month' #LINE# #TAB# ]"
"#LINE# #TAB# if sort: #LINE# #TAB# #TAB# ar = ar.sort(key=lambda x: x[1], reverse=True) #LINE# #TAB# ar = ar[ar.isin(ar.unique())] #LINE# #TAB# return ar"
"#LINE# #TAB# cmd = None #LINE# #TAB# cr.execute('SELECT * FROM xmls WHERE xmlid =?', (xmlid,)) #LINE# #TAB# try: #LINE# #TAB# #TAB# if noupdate: #LINE# #TAB# #TAB# #TAB# cmd = cr.fetchone()[0] #LINE# #TAB# #TAB# #TAB# if warn: #LINE# #TAB# #TAB# #TAB# #TAB# print(cmd) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cmd = cr.fetchone()[0] #LINE# #TAB# finally: #LINE# #TAB# #TAB# if cr.last_response and cr.last_response.text == '': #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return cmd"
#LINE# #TAB# messages = [] #LINE# #TAB# r = 0 #LINE# #TAB# for ch in string: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if len(ch) > width: #LINE# #TAB# #TAB# #TAB# #TAB# r += 1 #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# if r > width: #LINE# #TAB# #TAB# #TAB# messages.append(ch) #LINE# #TAB# return messages
"#LINE# #TAB# A = sp.zeros(len(sample)) #LINE# #TAB# n = len(sample) #LINE# #TAB# for k in range(n - 1): #LINE# #TAB# #TAB# ret = gearbar_simulation_A_k(sample, k, interval) #LINE# #TAB# #TAB# for j in range(k + 1): #LINE# #TAB# #TAB# #TAB# A[j, k] = ret[j, k] #LINE# #TAB# return A"
#LINE# #TAB# hazard_tags = impact_function.hazard.keywords.get('hazard') #LINE# #TAB# if hazard_tags: #LINE# #TAB# #TAB# return hazard_tags #LINE# #TAB# n_tags = 0 #LINE# #TAB# tags_to_skip = [] #LINE# #TAB# for key in hazard_tags: #LINE# #TAB# #TAB# tags_to_skip.append(key) #LINE# #TAB# if not tags_to_skip: #LINE# #TAB# #TAB# return None #LINE# #TAB# for tag in tags_to_skip: #LINE# #TAB# #TAB# if tag not in hazard_tags: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return tags_to_skip
#LINE# #TAB# locked = False #LINE# #TAB# while not locked: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# locked = os.stat(filepath).st_mtime #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if locked: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return True
"#LINE# #TAB# _, fov = cv2.bounding_box(contour) #LINE# #TAB# max_dist = 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# for _ in range(fov): #LINE# #TAB# #TAB# #TAB# dist = cv2.distance_transform_edt(fov, contour) #LINE# #TAB# #TAB# #TAB# if dist > max_dist: #LINE# #TAB# #TAB# #TAB# #TAB# max_dist = dist #LINE# #TAB# #TAB# #TAB# #TAB# fov = fov #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return max_dist"
#LINE# #TAB# if len(data_dicts) > 0: #LINE# #TAB# #TAB# return format_value_for_spreadsheet(data_dicts) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
#LINE# #TAB# if key not in rundata: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# rundata[key] = {} #LINE# #TAB# data = rundata[key]['data'] #LINE# #TAB# return data[str(len(data))]
"#LINE# #TAB# try: #LINE# #TAB# #TAB# response = requests.get(url, headers={'User-Agent': #LINE# #TAB# #TAB# #TAB# 'Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10 (.NET CLR 4.0.20506)'}) #LINE# #TAB# #TAB# response.raise_for_status() #LINE# #TAB# #TAB# return response.text #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# raise"
"#LINE# #TAB# X_train = scipy.sparse.dia_matrix((X_test, 0), shape=(n_components, #LINE# #TAB# #TAB# X_test.shape[1]), dtype=float) #LINE# #TAB# X_history = scipy.sparse.dia_matrix((X_history, 0), shape=(n_components, #LINE# #TAB# #TAB# n_components)) #LINE# #TAB# sst_train = scipy.sparse.linalg.svd(X_train) #LINE# #TAB# sst_history = scipy.sparse.dia_matrix((X_history, 0), shape=(n_components, #LINE# #TAB# #TAB# n_components)) #LINE# #TAB# W = sst_train.dot(X_history) #LINE# #TAB# K = np.argmax(W.T, axis=1) #LINE# #TAB# return K"
"#LINE# #TAB# settings = {} #LINE# #TAB# for sample_idx, sample_value in enumerate(x_samples): #LINE# #TAB# #TAB# if not np.any(sample_value): #LINE# #TAB# #TAB# #TAB# settings[sample_idx] = np.arange(len(geno_samples)) #LINE# #TAB# #TAB# for sample_idx, sample_value in enumerate(sample_value): #LINE# #TAB# #TAB# #TAB# if not np.any(sample_value): #LINE# #TAB# #TAB# #TAB# #TAB# settings[sample_idx] = sample_value #LINE# #TAB# return settings"
#LINE# #TAB# if dirpath: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shutil.rmtree(dirpath) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass
"#LINE# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.settimeout(timeout) #LINE# #TAB# buf = sock.recv(4096) #LINE# #TAB# buf = buf.decode('utf-8') #LINE# #TAB# sock.setblocking(0) #LINE# #TAB# return buf"
"#LINE# #TAB# if object_dict.get('value') in ('true', 'True'): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif object_dict.get('value') in ('false', 'False'): #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"#LINE# #TAB# out = subprocess.Popen(['wc', '-W', kernel, gui], stdout=subprocess.PIPE #LINE# #TAB# #TAB# ).communicate()[0] #LINE# #TAB# return out"
"#LINE# #TAB# lv = np.tensordot(lv, lt1.conj(), axes=(0, 2)) #LINE# #TAB# lv = np.tensordot(lv, lt2, axes=(0, 2)) #LINE# #TAB# return lv"
"#LINE# #TAB# obs = xw.sum(axis=1) #LINE# #TAB# Nw = weights.shape[0] #LINE# #TAB# s = np.zeros((Nw, Nw), dtype=float) #LINE# #TAB# for ii in range(Nw): #LINE# #TAB# #TAB# for jj in range(Nw): #LINE# #TAB# #TAB# #TAB# iif = ii[jj] #LINE# #TAB# #TAB# #TAB# s[iif, jj] = 1.0 / weights[iif] * (xw[iii] - xw[jj]) #LINE# #TAB# return s"
#LINE# #TAB# if len(ops) == 1: #LINE# #TAB# #TAB# return ops[0] #LINE# #TAB# new_ops = [] #LINE# #TAB# for op in ops: #LINE# #TAB# #TAB# new_op = pauli_term_copy(op) #LINE# #TAB# #TAB# new_ops.append(new_op) #LINE# #TAB# return new_ops
"#LINE# #TAB# return {'starttime': d.get('starttime'), 'endtime': d.get('endtime'), #LINE# #TAB# #TAB# 'weekday': d.get('weekday'), 'year': d.get('year'), 'week': d.get( #LINE# #TAB# #TAB# 'weekday'),'month': d.get('month'), 'day': d.get('day'), 'year': d.get( #LINE# #TAB# #TAB# 'year'), 'weekday_start': d.get('weekday_start'), 'weekday_end': #LINE# #TAB# #TAB# d.get('weekday_end'), 'year_start': d.get('year'), 'weekday_end': #LINE# #TAB# #TAB# d.get('weekday_end'), 'year_end': d.get('year')}"
"#LINE# #TAB# return get_blockchain_overview(coin_symbol=coin_symbol, api_key=api_key)[ #LINE# #TAB# #TAB# 'blocks']"
"#LINE# #TAB# md5 = hashlib.md5() #LINE# #TAB# with open(image_file, 'rb') as stream: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# chunk = stream.read(8) #LINE# #TAB# #TAB# #TAB# if not chunk: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# md5.update(chunk) #LINE# #TAB# image_hash = md5.hexdigest() #LINE# #TAB# return image_hash"
"#LINE# #TAB# if isinstance(file_path_or_generator, str): #LINE# #TAB# #TAB# for line in file_path_or_generator: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# if isinstance(line, dict): #LINE# #TAB# #TAB# #TAB# #TAB# for k, v in line.items(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield k, v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for k, v in file_path_or_generator: #LINE# #TAB# #TAB# #TAB# #TAB# yield k, v"
"#LINE# #TAB# print('get_active_window called') #LINE# #TAB# is_new_window = False #LINE# #TAB# if isinstance(emails, str): #LINE# #TAB# #TAB# emails = [emails] #LINE# #TAB# for email in emails: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# is_new_window = email in get_window(email) #LINE# #TAB# #TAB# except InvalidEmail: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if not is_new_window: #LINE# #TAB# #TAB# #TAB# #TAB# print('window %s does not exist' % email) #LINE# #TAB# if not is_new_window: #LINE# #TAB# #TAB# print('window %s does not exist' % email) #LINE# #TAB# return is_new_window"
"#LINE# #TAB# d = data[0].copy() #LINE# #TAB# for i in range(1, len(d)): #LINE# #TAB# #TAB# if d[i] == 0: #LINE# #TAB# #TAB# #TAB# d[i] = 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d[i] = d[i] / np.sqrt(1 - d[i] ** 2) #LINE# #TAB# return d"
#LINE# #TAB# if config['search engine'].lower() == 'contaminants': #LINE# #TAB# #TAB# df = df[df['leading_protein'].isin(config['contaminant_tag']).any( #LINE# #TAB# #TAB# #TAB# )] #LINE# #TAB# elif config['search engine'].lower() == 'protein': #LINE# #TAB# #TAB# df = df[df['leading_protein'].isin(config['contaminant_tag'] #LINE# #TAB# #TAB# #TAB# ).any(1) #LINE# #TAB# return df
"#LINE# #TAB# if overwrite: #LINE# #TAB# #TAB# barcodes_to_keep = [barcode.filename for barcode in barcodes] #LINE# #TAB# else: #LINE# #TAB# #TAB# barcodes_to_keep = [barcode.filename for barcode in barcodes] #LINE# #TAB# total = sum(barcodes_to_keep) #LINE# #TAB# filename = os.path.join(output_dir, prefix + filename) #LINE# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# open(filename, 'w').close() #LINE# #TAB# else: #LINE# #TAB# #TAB# os.remove(filename) #LINE# #TAB# return total"
"#LINE# #TAB# szsec, shsec = datetime.timedelta() #LINE# #TAB# szstart = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# shstart = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') #LINE# #TAB# filename = 'duration.py' #LINE# #TAB# with open(filename, 'r') as file: #LINE# #TAB# #TAB# content = file.read() #LINE# #TAB# content = content.replace('\n', '') #LINE# #TAB# for row in content.split('\n'): #LINE# #TAB# #TAB# rows = row.split(' ') #LINE# #TAB# #TAB# szsec += rows[0] + ',' + rows[2] + '\n' #LINE# #TAB# #TAB# shsec += rows[1] + ',' + rows[2] #LINE# #TAB# with open(filename, 'w') as file: #LINE# #TAB# #TAB# file.write(szsec + '\"
#LINE# #TAB# plan = [] #LINE# #TAB# for i1 in range(len(input_list)): #LINE# #TAB# #TAB# for i2 in range(len(input_list) - 1): #LINE# #TAB# #TAB# #TAB# if input_list[i1].count(input_list[i2])!= 1: #LINE# #TAB# #TAB# #TAB# #TAB# plan.append(input_list[i1]) #LINE# #TAB# return plan
"#LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# ax = plt.gca() #LINE# #TAB# n = len(net_exposures[0]) #LINE# #TAB# for t in range(n): #LINE# #TAB# #TAB# net_exposures = net_exposures[t] #LINE# #TAB# #TAB# lower_bound, upper_bound = find_unconstrained_reactions(net_exposures) #LINE# #TAB# #TAB# ax.plot(x=net_exposures[t][0], y=net_exposures[t][1], c= #LINE# #TAB# #TAB# #TAB# net_exposures[t][2], label=str(t)) #LINE# #TAB# return ax"
"#LINE# #TAB# pinned_mem = array.array('B', []) #LINE# #TAB# for i in range(num_bytes): #LINE# #TAB# #TAB# pinned_mem.insert(0, 0) #LINE# #TAB# return pinned_mem"
"#LINE# #TAB# path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'rfc-index.txt') #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# return [line.strip() for line in f.readlines()]"
"#LINE# #TAB# qfont = QFont() #LINE# #TAB# for key, value in font.properties().items(): #LINE# #TAB# #TAB# if value is not None: #LINE# #TAB# #TAB# #TAB# qfont.setAttribute(key, value) #LINE# #TAB# return qfont"
#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# data = receiver.recv() #LINE# #TAB# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# yield data
"#LINE# #TAB# foreign_keys = {} #LINE# #TAB# for person in fake.persons(): #LINE# #TAB# #TAB# if person.foreign_key: #LINE# #TAB# #TAB# #TAB# foreign_keys[person.foreign_key] = {} #LINE# #TAB# #TAB# #TAB# for k, v in person.foreign_keys.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if v: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# foreign_keys[person.foreign_key][k] = v #LINE# #TAB# return {'foreign_keys': foreign_keys}"
#LINE# #TAB# module_path = os.path.dirname(__file__) #LINE# #TAB# return module_path
"#LINE# #TAB# column_names = [col[0] for col in n] #LINE# #TAB# orm.engine.connect() #LINE# #TAB# with tempfile.NamedTemporaryFile(delete=False) as f: #LINE# #TAB# #TAB# for col_name in column_names: #LINE# #TAB# #TAB# #TAB# f.write('SELECT {} FROM `{}`;'.format(col_name, n[col_name])) #LINE# #TAB# #TAB# f.flush() #LINE# #TAB# return f"
"#LINE# #TAB# diff = abs(f2 - f1) #LINE# #TAB# if diff > 0: #LINE# #TAB# #TAB# return max(1, diff) #LINE# #TAB# if diff < 0: #LINE# #TAB# #TAB# return -max(1, diff) #LINE# #TAB# return 0"
"#LINE# #TAB# if item is None: #LINE# #TAB# #TAB# yield 0 #LINE# #TAB# elif isinstance(item, (list, tuple, set)): #LINE# #TAB# #TAB# return item #LINE# #TAB# else: #LINE# #TAB# #TAB# return [item]"
"#LINE# #TAB# cache = {} #LINE# #TAB# session = app.session #LINE# #TAB# policy = session.get(SENDER_NAME) #LINE# #TAB# if policy is None: #LINE# #TAB# #TAB# policy = session[SENDER_NAME] #LINE# #TAB# #TAB# if not policy: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# d = {} #LINE# #TAB# for rule in policy: #LINE# #TAB# #TAB# d.setdefault(rule[0], []).append(rule) #LINE# #TAB# return d"
"#LINE# #TAB# char_list = list(s) #LINE# #TAB# char_list.sort() #LINE# #TAB# n = len(char_list) #LINE# #TAB# opts = [] #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# code1 = ord(char_list.pop()) #LINE# #TAB# #TAB# code2 = ord(char_list.pop()) #LINE# #TAB# #TAB# opts.append((code1, code2)) #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# while n > 0: #LINE# #TAB# #TAB# code1 = ord(char_list.pop()) #LINE# #TAB# #TAB# code2 = ord(char_list.pop()) #LINE# #TAB# #TAB# opts.append((code1, code2)) #LINE# #TAB# #TAB# n -= 1 #LINE# #TAB# return opts"
"#LINE# #TAB# messages = {} #LINE# #TAB# for field in cls._meta.fields: #LINE# #TAB# #TAB# message = {'type': field.type, 'prefix': type_prefix + '.' + field.name} #LINE# #TAB# #TAB# if field.verbose_name: #LINE# #TAB# #TAB# #TAB# message['verbose_name'] = field.title #LINE# #TAB# #TAB# if field.related_model: #LINE# #TAB# #TAB# #TAB# message['related_model'] = field.related_model #LINE# #TAB# #TAB# if field.default_value is not None: #LINE# #TAB# #TAB# #TAB# message['default_value'] = field.default_value #LINE# #TAB# #TAB# messages[field.name] = message #LINE# #TAB# return messages"
#LINE# #TAB# arr = np.asarray(arr) #LINE# #TAB# if diff is not None: #LINE# #TAB# #TAB# dim = diff.shape #LINE# #TAB# #TAB# if dim!= arr.shape[0]: #LINE# #TAB# #TAB# #TAB# raise ValueError('Shape of diff must be equal to arr shape.') #LINE# #TAB# #TAB# arr = np.asarray(arr) #LINE# #TAB# #TAB# if dim == 1: #LINE# #TAB# #TAB# #TAB# diff = _to_blob_proto(diff) #LINE# #TAB# #TAB# elif dim == 2: #LINE# #TAB# #TAB# #TAB# if diff[0] is None: #LINE# #TAB# #TAB# #TAB# #TAB# diff[0] = _to_blob_proto(diff[0]) #LINE# #TAB# #TAB# #TAB# diff[1] = _to_blob_proto(diff[1]) #LINE# #TAB# return diff
#LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if node.tag == 'nav_extender': #LINE# #TAB# #TAB# #TAB# yield node
#LINE# #TAB# if incremental_state is not None: #LINE# #TAB# #TAB# ctx = incremental_state[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# ctx = ssl.create_default_context(key) #LINE# #TAB# return ctx
"#LINE# #TAB# rv = [] #LINE# #TAB# if 'CHAINLIST' in os.environ: #LINE# #TAB# #TAB# for chain in os.environ['CHAINLIST'].split(','): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# rv.append(chain.split(';')) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# for chain in os.environ['CHAINLIST']: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# rv.append(chain.split(';')[0]) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# return rv"
"#LINE# #TAB# if isinstance(uri, Namespace): #LINE# #TAB# #TAB# return uri.uri #LINE# #TAB# return uri"
#LINE# #TAB# if user is None: #LINE# #TAB# #TAB# user = '' #LINE# #TAB# if password is None: #LINE# #TAB# #TAB# password = '' #LINE# #TAB# if user is not None: #LINE# #TAB# #TAB# if not password: #LINE# #TAB# #TAB# #TAB# password = getpass.getpass('') #LINE# #TAB# #TAB# return user + ':' + password #LINE# #TAB# return user + ':' + password
"#LINE# #TAB# if not data: #LINE# #TAB# #TAB# return None #LINE# #TAB# most_recent_payment = None #LINE# #TAB# for chain in data.get('chains', []): #LINE# #TAB# #TAB# if len(chain['chains']) > most_recent_payment: #LINE# #TAB# #TAB# #TAB# most_recent_payment = chain #LINE# #TAB# if most_recent_payment is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return most_recent_payment"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# urls = t['raw_dict'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None #LINE# #TAB# next_url = urls[0] #LINE# #TAB# if type(next_url) == list: #LINE# #TAB# #TAB# for l in next_url: #LINE# #TAB# #TAB# #TAB# if not isinstance(l, str): #LINE# #TAB# #TAB# #TAB# #TAB# return l #LINE# #TAB# #TAB# #TAB# next_url = urlunparse(l) #LINE# #TAB# return next_url"
#LINE# #TAB# if flags & os.O_RDONLY: #LINE# #TAB# #TAB# return 'RDONLY' #LINE# #TAB# elif flags & os.O_RDWR: #LINE# #TAB# #TAB# return 'WRONLY' #LINE# #TAB# elif flags & os.O_CREAT: #LINE# #TAB# #TAB# return 'CREAT' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'UNKNOWN'
"#LINE# #TAB# penn_tree = re.sub('\\s+','', penn_tree) #LINE# #TAB# penn_tree = re.sub('\\s+','', penn_tree) #LINE# #TAB# return penn_tree"
"#LINE# #TAB# if index.dtype!= 'int64': #LINE# #TAB# #TAB# return index #LINE# #TAB# elif index.dtype == 'float64': #LINE# #TAB# #TAB# return index #LINE# #TAB# else: #LINE# #TAB# #TAB# dir_name = os.path.dirname(__file__) #LINE# #TAB# #TAB# file_name = '{}.{}'.format(dir_name, index.name) #LINE# #TAB# #TAB# if os.path.isfile(file_name): #LINE# #TAB# #TAB# #TAB# return recursive_repr(index) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return file_name"
"#LINE# #TAB# if ch =='sleep': #LINE# #TAB# #TAB# delta = config['sleep'] #LINE# #TAB# elif ch =='sleep-ms': #LINE# #TAB# #TAB# delta = config['sleep_ms'] #LINE# #TAB# elif ch =='sleep': #LINE# #TAB# #TAB# delta = config['sleep_ms'] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotImplementedError( #LINE# #TAB# #TAB# #TAB# 'Cannot determine timedelta offset for ch %r and service %r' % (ch, #LINE# #TAB# #TAB# #TAB# service_name)) #LINE# #TAB# duration = int(delta.total_seconds() * 1000) #LINE# #TAB# return duration"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return vi_mode.remove() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# shorty_conf = _default_config() #LINE# #TAB# if file is None: #LINE# #TAB# #TAB# return shorty_conf #LINE# #TAB# with open(file, 'rt') as f: #LINE# #TAB# #TAB# shorty_conf.update(json.load(f)) #LINE# #TAB# return shorty_conf"
"#LINE# #TAB# so_name = 'youtube-dl' #LINE# #TAB# if os.path.exists(so_name): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p = subprocess.Popen(['youtube-dl', '--version'], stdout= #LINE# #TAB# #TAB# #TAB# #TAB# subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# #TAB# #TAB# so_name = p.communicate()[0].decode('utf-8') #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return so_name"
"#LINE# #TAB# n_off_diag = int((n_features ** 2 - n_features) / 2) #LINE# #TAB# M = np.zeros((n_features, n_off_diag)) #LINE# #TAB# for i in range(n_features): #LINE# #TAB# #TAB# for j in range(n_features): #LINE# #TAB# #TAB# #TAB# M[(i), :] = 0.1 * lam_scale * prng.randn( #LINE# #TAB# #TAB# #TAB# #TAB# ) + lam_scale * M[(j), :] #LINE# #TAB# return M"
"#LINE# #TAB# datamean = data.mean(axis=2).imag #LINE# #TAB# datameanmin, datameanmax = rtlib.sigma_clip(datamean.flatten()) #LINE# #TAB# good = n.where((datamean > datameanmin) & (datamean < datameanmax)) #LINE# #TAB# data = data[good] #LINE# #TAB# return data"
#LINE# #TAB# fields = [] #LINE# #TAB# for rea in fvaMinmax: #LINE# #TAB# #TAB# if rea not in fields: #LINE# #TAB# #TAB# #TAB# fields.append(rea) #LINE# #TAB# return fields
#LINE# #TAB# if value.startswith('vehicle'): #LINE# #TAB# #TAB# return value[3:] #LINE# #TAB# return value
#LINE# #TAB# if not is_tdt(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# if isinstance(n, (float, Decimal)): #LINE# #TAB# #TAB# if n.is_integer(): #LINE# #TAB# #TAB# #TAB# return int(n) #LINE# #TAB# #TAB# elif n == 1.0: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# f""Cannot convert '{n}' to an integer."") #LINE# #TAB# return n"
"#LINE# #TAB# if not slug: #LINE# #TAB# #TAB# return '' #LINE# #TAB# objects = Post.objects.filter(slug=slug, on_touch_up=True) #LINE# #TAB# if limit: #LINE# #TAB# #TAB# content_type = ContentType.objects.get_for_model(context['object_cls']) #LINE# #TAB# else: #LINE# #TAB# #TAB# content_type = ContentType.objects.get_for_model(context['object_cls']) #LINE# #TAB# response = HttpResponse(content_type=content_type, content=content) #LINE# #TAB# if title: #LINE# #TAB# #TAB# response['title'] = title #LINE# #TAB# if text: #LINE# #TAB# #TAB# response['text'] = text #LINE# #TAB# return response"
"#LINE# #TAB# assert_type_or_raise(array, dict, parameter_name='array') #LINE# #TAB# data = Result.validate_array(array) #LINE# #TAB# data['level_set'] = int(array.get('level_set')) #LINE# #TAB# data['first_name'] = u(array.get('first_name')) #LINE# #TAB# data['last_name'] = u(array.get('last_name')) if array.get('last_name' #LINE# #TAB# #TAB# ) is not None else None #LINE# #TAB# data['level'] = int(array.get('level')) if array.get('level') is not None else None #LINE# #TAB# return data"
"#LINE# #TAB# a = startyear - 1 #LINE# #TAB# b = startyear + 1 #LINE# #TAB# c = startyear #LINE# #TAB# while 1: #LINE# #TAB# #TAB# d = bC_K_Cl_SP73(startyear, sep) #LINE# #TAB# #TAB# if d < 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# a += 1 #LINE# #TAB# #TAB# b -= 1 #LINE# #TAB# return a, b"
#LINE# #TAB# image = Image.open(image_bytes) #LINE# #TAB# return image
"#LINE# #TAB# d = g #LINE# #TAB# for k in d.keys(): #LINE# #TAB# #TAB# v = d[k] #LINE# #TAB# #TAB# if isinstance(v, string_types): #LINE# #TAB# #TAB# #TAB# if v == 'path': #LINE# #TAB# #TAB# #TAB# #TAB# d[k] = rt #LINE# #TAB# #TAB# #TAB# elif isinstance(v, int): #LINE# #TAB# #TAB# #TAB# #TAB# d[k] = int(v) #LINE# #TAB# return d"
"#LINE# #TAB# import matplotlib.pyplot as plt #LINE# #TAB# from matplotlib.path import dirname, join #LINE# #TAB# tlist = [] #LINE# #TAB# dirpath = os.path.dirname(os.path.abspath(__file__)) #LINE# #TAB# p = os.path.join(dirpath, 'bin') #LINE# #TAB# if os.path.isdir(p): #LINE# #TAB# #TAB# tlist.append('pyplot') #LINE# #TAB# #TAB# if'matplotlib' in p: #LINE# #TAB# #TAB# #TAB# tlist.append('matplotlib') #LINE# #TAB# #TAB# if 'bokeh' in p: #LINE# #TAB# #TAB# #TAB# tlist.append('bokeh') #LINE# #TAB# #TAB# if'matplotlib' not in tlist: #LINE# #TAB# #TAB# #TAB# tlist.append('matplotlib') #LINE# #TAB# return tlist"
#LINE# #TAB# for child in root.iter('softwareReleaseMetadata'): #LINE# #TAB# #TAB# software_release = child.get('softwareReleaseVersion') #LINE# #TAB# #TAB# if software_release is None: #LINE# #TAB# #TAB# #TAB# log.debug('Skipping nosa release') #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"#LINE# #TAB# #TAB# if not host: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if ':' in host: #LINE# #TAB# #TAB# #TAB# host, port = host.rsplit(':', 1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# host, port = host, port #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(host), port #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return host, port"
#LINE# #TAB# nonce = MapConverter.to_nullable_map(value) #LINE# #TAB# return nonce
#LINE# #TAB# url = urlparse(content) #LINE# #TAB# try: #LINE# #TAB# #TAB# return url.netloc #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return 500 #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 500
"#LINE# #TAB# n_samples = X.shape[1] #LINE# #TAB# covariance_out = np.zeros((n_samples, num_folds)) #LINE# #TAB# for i in range(num_folds): #LINE# #TAB# #TAB# l = GraphLassoCV(n_samples=n_samples, covariance_type='full') #LINE# #TAB# #TAB# l.fit(X) #LINE# #TAB# #TAB# covariance_out[i] = l.covariance_ #LINE# #TAB# return covariance_out"
"#LINE# #TAB# dist_matrix = ossos_dist_matrix(img, ncomp, norm_type=norm_type) #LINE# #TAB# out_img = dist_matrix[0:ncomp, :] #LINE# #TAB# if normalise_input_image: #LINE# #TAB# #TAB# out_img = dist_transform(out_img) #LINE# #TAB# return out_img"
"#LINE# #TAB# for field in model._meta.get_fields(): #LINE# #TAB# #TAB# if user.has_perm(field.content_type_id): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# reference = Reference.objects.get(user=user, model=model, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# content_type=field.content_type_id) #LINE# #TAB# #TAB# #TAB# except Reference.DoesNotExist: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# response = {} #LINE# #TAB# if 'txt' in params: #LINE# #TAB# #TAB# txt = params['txt'] #LINE# #TAB# else: #LINE# #TAB# #TAB# txt = params['txt'] #LINE# #TAB# for col in txt.split('_'): #LINE# #TAB# #TAB# if 'txt' in col: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# col = re.sub('[\\W_]+', '_', col) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# col = re.sub('[\\W_]+', '_', col) #LINE# #TAB# #TAB# #TAB# response[col] = camelcase_to_underscore(col) #LINE# #TAB# return response"
#LINE# #TAB# C = 1 / (-xm / (1 - a) - xm / a + math.exp(-a)) #LINE# #TAB# return C
"#LINE# #TAB# size = None #LINE# #TAB# for f in find_featured_artist_fields(artist, albumartist): #LINE# #TAB# #TAB# if f: #LINE# #TAB# #TAB# #TAB# size = os.path.getsize(f) #LINE# #TAB# return size"
"#LINE# #TAB# repo = cls() #LINE# #TAB# for key, value in dic.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# bar = cls.build_bar(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# bar = value #LINE# #TAB# #TAB# if bar.type == 'Bar': #LINE# #TAB# #TAB# #TAB# repo.append(bar) #LINE# #TAB# #TAB# elif bar.type == 'List': #LINE# #TAB# #TAB# #TAB# repo.list = [bar] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# repo.append(bar) #LINE# #TAB# return repo"
"#LINE# #TAB# epilog = parser.epilog #LINE# #TAB# if hasattr(parser, 'epilog'): #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# epilog = epilog.splitlines() #LINE# #TAB# #TAB# #TAB# if len(epilog) < 2: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# for line in epilog: #LINE# #TAB# #TAB# #TAB# if '=' not in line: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# epilog += line #LINE# #TAB# return epilog"
#LINE# #TAB# neuron.h.load_file('morphology.hoc') #LINE# #TAB# neuron.h.load_file('biophysics.hoc') #LINE# #TAB# neuron.h.load_file('template.hoc') #LINE# #TAB# print('Loading cell bAC217_L5_NBC_2db880c523') #LINE# #TAB# cell = neuron.h.bAC217_L5_NBC_2db880c523(1 if add_synapses else 0) #LINE# #TAB# return cell
#LINE# #TAB# times_needed = np.array(times_needed) #LINE# #TAB# offset = times_needed.mean() - times_needed.mean(axis=0) #LINE# #TAB# n = 0 #LINE# #TAB# for i in range(times_needed.shape[0]): #LINE# #TAB# #TAB# for j in range(times_needed.shape[1]): #LINE# #TAB# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# offset = times_needed[j] - offset #LINE# #TAB# #TAB# #TAB# #TAB# n = n + offset #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# offset = 0 #LINE# #TAB# #TAB# #TAB# if n < offset: #LINE# #TAB# #TAB# #TAB# #TAB# n = n + offset
#LINE# #TAB# if name.startswith(prefix): #LINE# #TAB# #TAB# return name[len(prefix):] #LINE# #TAB# if prefix.endswith('_'): #LINE# #TAB# #TAB# return name #LINE# #TAB# return name
"#LINE# #TAB# if dst.name!='scans': #LINE# #TAB# #TAB# dst.mkdir(parents=True, exist_ok=True) #LINE# #TAB# for root, _, files in os.walk(dst, topdown=True): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# path = Path(root, f) #LINE# #TAB# #TAB# #TAB# if not path.is_dir(): #LINE# #TAB# #TAB# #TAB# #TAB# mkdir_p(path) #LINE# #TAB# #TAB# #TAB# #TAB# os.chmod(path, 511) #LINE# #TAB# return"
"#LINE# #TAB# if not hasattr(cls, '_upper'): #LINE# #TAB# cls._upper = timestamp #LINE# #TAB# return #LINE# #TAB# if timestamp > cls._upper: #LINE# #TAB# cls._upper = timestamp"
"#LINE# #TAB# with open(file_name, 'rb') as f: #LINE# #TAB# #TAB# image = Image.open(f) #LINE# #TAB# #TAB# texture_w, texture_h = image.size #LINE# #TAB# #TAB# texture = resolver.resolve(texture_w, texture_h) #LINE# #TAB# #TAB# image.paste(texture, (0, 0)) #LINE# #TAB# #TAB# return image"
"#LINE# #TAB# typing.Any) ->tf.Tensor: #LINE# #TAB# mask_shape = attention_mask.shape[0] #LINE# #TAB# attention_input_shape = tf.reshape(attention_input, [mask_shape, 1]) #LINE# #TAB# attention_output = tf.nn.softmax(attention_input_shape, keep_dims=True) #LINE# #TAB# return attention_output"
#LINE# #TAB# r = session.get(url) #LINE# #TAB# while r.status_code!= 200: #LINE# #TAB# #TAB# headers = r.headers #LINE# #TAB# #TAB# final_url = r.url #LINE# #TAB# #TAB# if final_url.endswith('/'): #LINE# #TAB# #TAB# #TAB# final_url = final_url[:-1] #LINE# #TAB# return final_url
#LINE# #TAB# #TAB# check = 0 #LINE# #TAB# #TAB# for byte in line: #LINE# #TAB# #TAB# #TAB# check = (check << 5) + ord(byte) #LINE# #TAB# #TAB# return check
#LINE# #TAB# product = 1 #LINE# #TAB# while max_prime > 2: #LINE# #TAB# #TAB# if max_prime % 2 == 0: #LINE# #TAB# #TAB# #TAB# product *= 2 #LINE# #TAB# #TAB# elif max_prime % 3 == 0: #LINE# #TAB# #TAB# #TAB# product += 3 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return product
"#LINE# #TAB# obj = cls._unit_objects.get(name, None) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# myokit_unit = cls._unit_objects.get(name, None) #LINE# #TAB# #TAB# if myokit_unit is None: #LINE# #TAB# #TAB# #TAB# raise CellMLError('Unknown units name ""' + str(name) + '"".') #LINE# #TAB# #TAB# obj = cls(name, myokit_unit, predefined=True) #LINE# #TAB# #TAB# cls._unit_objects[name] = obj #LINE# #TAB# return obj"
"#LINE# #TAB# matching = [] #LINE# #TAB# non_matching = [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if cond(item): #LINE# #TAB# #TAB# #TAB# matching.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# non_matching.append(item) #LINE# #TAB# return matching, non_matching"
#LINE# #TAB# config = _DEFAULT_CONFIG.copy() #LINE# #TAB# if 'config' in req: #LINE# #TAB# #TAB# config['config'] = req.get('config') #LINE# #TAB# if'service' in req and req.get('service'): #LINE# #TAB# #TAB# config['service'] = req.get('service') #LINE# #TAB# return config
#LINE# #TAB# s = pt1.area #LINE# #TAB# s += pt2.area #LINE# #TAB# s += pt3.area #LINE# #TAB# return s
"#LINE# #TAB# branch_name_map = {} #LINE# #TAB# with open(os.path.join(os.getcwd(), '.branch_name_map')) as f: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# branch_name = f.read().strip() #LINE# #TAB# #TAB# #TAB# if not branch_name: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# branch_name_map[branch_name] = callback(branch_name) #LINE# #TAB# return branch_name_map"
#LINE# #TAB# if warningstuple[2] is ProxyWarning: #LINE# #TAB# #TAB# return warningstuple #LINE# #TAB# else: #LINE# #TAB# #TAB# return warningstuple
#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.NoPayload(BaseMsgCode.GetAvailableCart)) #LINE# #TAB# return packet
"#LINE# #TAB# assert kind in (""linear"", ""quadratic"", ""linear"") #LINE# #TAB# k = 0 #LINE# #TAB# for i in range(len(h)): #LINE# #TAB# #TAB# k += 1 #LINE# #TAB# #TAB# t = p_new_if_mask(h, lon, lat, dx, kind) #LINE# #TAB# #TAB# if plot: #LINE# #TAB# #TAB# #TAB# plt.plot(t) #LINE# #TAB# return t"
"#LINE# #TAB# for task_instance in task_instances: #LINE# #TAB# #TAB# if isinstance(task_instance, types.ModuleType): #LINE# #TAB# #TAB# #TAB# return task_instance.location_identifier"
#LINE# #TAB# conn = _get_conn(service) #LINE# #TAB# all_regions = conn.list_regions() #LINE# #TAB# service_regions = {x['RegionName']: x for x in all_regions} #LINE# #TAB# for region_name in service_regions: #LINE# #TAB# #TAB# region = conn.get_region(region_name) #LINE# #TAB# #TAB# if not region: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# all_regions.remove(region_name) #LINE# #TAB# #TAB# conn.delete_region(region_name) #LINE# #TAB# return True
#LINE# #TAB# group.scheduled_at = now() #LINE# #TAB# group.host = host #LINE# #TAB# group.cluster_name = cluster_name #LINE# #TAB# return group
"#LINE# #TAB# theta = 0.06 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
#LINE# #TAB# size_of_op = len(op.props) #LINE# #TAB# if size_of_op > 4: #LINE# #TAB# #TAB# size_of_op -= 4 #LINE# #TAB# size_of_op += 4 #LINE# #TAB# if op == 1: #LINE# #TAB# #TAB# if size_of_op >= 2: #LINE# #TAB# #TAB# #TAB# size_of_op -= 2 #LINE# #TAB# #TAB# return size_of_op #LINE# #TAB# elif op == 2: #LINE# #TAB# #TAB# if size_of_op >= 3: #LINE# #TAB# #TAB# #TAB# size_of_op -= 3 #LINE# #TAB# #TAB# return size_of_op #LINE# #TAB# return size_of_op
"#LINE# #TAB# motifs = [] #LINE# #TAB# for motor in iterator: #LINE# #TAB# #TAB# if test is None: #LINE# #TAB# #TAB# #TAB# motifs.append(load_jaspar_motif(motif)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# test = motif_to_test(motif, test) #LINE# #TAB# #TAB# #TAB# motifs.append(motif) #LINE# #TAB# return motifs"
"#LINE# #TAB# active_window_images = [_rotate_window(image, rotate) for image in images] #LINE# #TAB# active_window_images = [_flip_window(image, flip) for image in active_window_images] #LINE# #TAB# return active_window_images"
"#LINE# #TAB# songToSplit = songToSplit.replace(songToSplit[0], '').replace(songToSplit[1], #LINE# #TAB# #TAB# '').replace(songToSplit[2], '').replace(songToSplit[3], '').replace( #LINE# #TAB# #TAB# songToSplit[4], '').replace(songToSplit[5], '').replace(songToSplit[6], #LINE# #TAB# #TAB# '0') #LINE# #TAB# return [start1, start2] + songToSplit"
"#LINE# #TAB# import pandas as pd #LINE# #TAB# if n_gram is None: #LINE# #TAB# #TAB# n_gram = get_ngram_features() #LINE# #TAB# X = pd.read_csv(os.path.join(os.path.dirname(__file__), ""data"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""stacktrace"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""n-gram-features/{0}"".format(n_gram)), header=0, sep=""\t"") #LINE# #TAB# else: #LINE# #TAB# #TAB# X = pd.read_csv(os.path.join(os.getcwd(), ""data"", ""stacktrace"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""n-gram-features/{0}"".format(n_gram)) #LINE# #TAB# return X"
#LINE# #TAB# if cls._nccl_version is None: #LINE# #TAB# #TAB# from.dial import Cccl #LINE# #TAB# #TAB# cls._nccl_version = Cccl.CclGetVersion() #LINE# #TAB# return cls._nccl_version
#LINE# #TAB# config = load_config() #LINE# #TAB# app = create_app(config) #LINE# #TAB# peers_id = None #LINE# #TAB# while peers_id is not None: #LINE# #TAB# #TAB# peers_id = app.peers_ids.get(None) #LINE# #TAB# #TAB# if peers_id is None: #LINE# #TAB# #TAB# #TAB# peers_id = None #LINE# #TAB# return peers_id
#LINE# #TAB# #TAB# axis_idx = [] #LINE# #TAB# #TAB# for var in vars_list: #LINE# #TAB# #TAB# #TAB# if var.startswith('_'): #LINE# #TAB# #TAB# #TAB# #TAB# axis_idx.append(fluent.scope.indices[var].idx) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# axis_idx.append(fluent.scope.indices[var]) #LINE# #TAB# #TAB# return axis_idx
"#LINE# #TAB# config_path = find_config_file(config_dir) #LINE# #TAB# if config_path is None: #LINE# #TAB# #TAB# print('Unable to find configuration. Creating default one in', #LINE# #TAB# #TAB# #TAB# config_dir) #LINE# #TAB# #TAB# config_path = create_default_plot_format(config_dir, detect_location) #LINE# #TAB# return config_path"
"#LINE# #TAB# return { #LINE# #TAB# #TAB#'method': 'isnonzero_u32', #LINE# #TAB# #TAB# 'enable_lru': True, #LINE# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# 'username': spec.become_user(), #LINE# #TAB# #TAB# #TAB# 'password': spec.become_pass(), #LINE# #TAB# #TAB# #TAB# 'python_path': spec.python_path(), #LINE# #TAB# #TAB# #TAB#'su_path': spec.become_exe(), #LINE# #TAB# #TAB# #TAB# 'connect_timeout': spec.timeout(), #LINE# #TAB# #TAB# #TAB#'remote_name': get_remote_name(spec), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"#LINE# #TAB# read_bytes = f.read(4) #LINE# #TAB# return struct.unpack('>I', read_bytes)[0]"
"#LINE# #TAB# ses, auto_close = ensure_session(engine_or_session) #LINE# #TAB# obj = ses.query(cls).get(_id) #LINE# #TAB# if auto_close: #LINE# #TAB# #TAB# ses.close() #LINE# #TAB# return obj"
"#LINE# #TAB# ngram_list = [] #LINE# #TAB# with open(path, 'r') as fp: #LINE# #TAB# #TAB# for line in fp: #LINE# #TAB# #TAB# #TAB# ngram_list.append(line.rstrip()) #LINE# #TAB# return ngram_list"
"#LINE# #TAB# if 'package_name' in details: #LINE# #TAB# #TAB# return details['package_name'] #LINE# #TAB# if 'install_directory' in details: #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# 'name': details['install_directory'], #LINE# #TAB# #TAB# #TAB#'version': details['version'], #LINE# #TAB# #TAB# #TAB# 'homepage': details['homepage'], #LINE# #TAB# #TAB# } #LINE# #TAB# return {}"
"#LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# if filename.endswith('.phe'): #LINE# #TAB# #TAB# #TAB# os.remove(os.path.join(path, filename)) #LINE# #TAB# logger.info('Removing %d elemental materials in %s' % (len(os.listdir(path)) - 1, #LINE# #TAB# #TAB# path)) #LINE# #TAB# return"
#LINE# #TAB# from.dist import LPNORM_DISTANCE #LINE# #TAB# return LPNORM_DISTANCE
"#LINE# #TAB# for doc_type in page.DOC_TYPES: #LINE# #TAB# #TAB# return '%s\n%s' % (doc_type.upper(), doc_type) #LINE# #TAB# return ''"
"#LINE# #TAB# vertices = np.asarray(vertices, dtype=np.float64) #LINE# #TAB# faces = np.asarray(faces, dtype=np.float64) #LINE# #TAB# n = len(vertices) #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return vertices #LINE# #TAB# normals = np.zeros(n) #LINE# #TAB# for i, face in enumerate(faces): #LINE# #TAB# #TAB# v = face[:, (0)] #LINE# #TAB# #TAB# normal = np.cross(v, v[1:]) #LINE# #TAB# #TAB# normals[i] = normal #LINE# #TAB# return normals"
#LINE# #TAB# w = np.sqrt(qx ** 2 + qy ** 2 + qz ** 2) #LINE# #TAB# return w
"#LINE# #TAB# result = [] #LINE# #TAB# while size > 0: #LINE# #TAB# #TAB# result.append(range(size, chunk_size)) #LINE# #TAB# #TAB# size -= chunk_size #LINE# #TAB# return result"
"#LINE# #TAB# reqs = set() #LINE# #TAB# requirements_file = os.path.join('Pipfile','requirements.in') #LINE# #TAB# with open(requirements_file) as requirements_file: #LINE# #TAB# #TAB# for line in requirements_file: #LINE# #TAB# #TAB# #TAB# req = line.strip() #LINE# #TAB# #TAB# #TAB# if not req: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# reqs.add(req) #LINE# #TAB# #TAB# pipfile_lock = PipfileLock(lockfile=requirements_file) #LINE# #TAB# #TAB# pipfile_lock.generate_lock() #LINE# #TAB# return reqs"
#LINE# #TAB# for a in l: #LINE# #TAB# #TAB# if a: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# if path.is_dir(): #LINE# #TAB# #TAB# paths = [path] #LINE# #TAB# else: #LINE# #TAB# #TAB# paths = [path] #LINE# #TAB# for p in paths: #LINE# #TAB# #TAB# if p.name == name and not p.parent.exists(): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# if p.parent.exists(): #LINE# #TAB# #TAB# #TAB# return p #LINE# #TAB# for p in paths: #LINE# #TAB# #TAB# if p.name == name and not p.parent.exists(): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return paths[0] if len(paths) == 1 else None
"#LINE# #TAB# total = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise SpacegroupNotFoundError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid spacegroup %s, setting %i not found in data base' % #LINE# #TAB# #TAB# #TAB# #TAB# ( spacegroup, setting)) #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# raise SpacegroupNotFoundError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid spacegroup %s, setting %i not found in data base' % #LINE# #TAB# #TAB# #TAB# #TAB# (spacegroup, setting)) #LINE# #TAB# #TAB# total += line.strip() #LINE# #TAB# return total"
"#LINE# #TAB# if isinstance(obj, dict): #LINE# #TAB# #TAB# for var in obj.keys(): #LINE# #TAB# #TAB# #TAB# if var in _PC_VAR_EXPLAINED: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False #LINE# #TAB# if isinstance(obj, tuple): #LINE# #TAB# #TAB# for var in obj: #LINE# #TAB# #TAB# #TAB# PC_var_explained(var) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# return predicate(value) #LINE# #TAB# except (KeyError, IndexError): #LINE# #TAB# #TAB# return False"
#LINE# #TAB# with cls._ft_cache_lock: #LINE# #TAB# #TAB# if domain not in cls._ft_cache[name]: #LINE# #TAB# #TAB# #TAB# cls._ft_cache[domain][name] = {} #LINE# #TAB# #TAB# if version not in cls._ft_cache[name]: #LINE# #TAB# #TAB# #TAB# del cls._ft_cache[name][version] #LINE# #TAB# return cls._ft_cache[domain][name][version]
#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(cmds_file) as f: #LINE# #TAB# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# #TAB# if line[which] in which_list: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return len(line) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return 0
#LINE# #TAB# t_dec = float(time) / mem #LINE# #TAB# if type(t_dec) is int: #LINE# #TAB# #TAB# t_dec = int(t_dec) #LINE# #TAB# elif type(t_dec) is str: #LINE# #TAB# #TAB# t_dec = str(t_dec) #LINE# #TAB# return t_dec
"#LINE# #TAB# size = ffi.new('size_t *') #LINE# #TAB# _load_params(handle, key, size) #LINE# #TAB# return size[0]"
"#LINE# #TAB# cards_total = int(mana_div.text) #LINE# #TAB# raw_links = '' #LINE# #TAB# for i in range(0, cards_total): #LINE# #TAB# #TAB# raw_links += str(mana_div.text[i]) + '\n' #LINE# #TAB# #TAB# if mana_div.text[i] == 'O': #LINE# #TAB# #TAB# #TAB# raw_links += str(mana_div.text[i]) + '\n' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raw_links += str(mana_div.text[i]) + '\n' #LINE# #TAB# return raw_links"
"#LINE# #TAB# s = asarray_ndim(s, 1) #LINE# #TAB# if s.shape[0] <= n: #LINE# #TAB# #TAB# fname = s.reshape((n,)) #LINE# #TAB# elif s.shape[0] > n: #LINE# #TAB# #TAB# fname = s.reshape((n,)) #LINE# #TAB# else: #LINE# #TAB# #TAB# fname = s.reshape((n,)) #LINE# #TAB# return fname"
"#LINE# #TAB# return {'filters': {'main': {'path': os.path.join(config_dir, 'filter'), #LINE# #TAB# #TAB# 'python_path': sys.path.join(config_dir, 'bin', 'python.exe')}, #LINE# #TAB# #TAB# 'hypervisor': {'path': os.path.join(config_dir, 'hypervisor'), #LINE# #TAB# #TAB# 'build_local': {'path': os.path.join(config_dir, 'build', #LINE# #TAB# #TAB# 'build.py')}}}"
"#LINE# #TAB# ps_script = ( #LINE# #TAB# #TAB# 'Get-VM -VMName ""{}"" | Select VMName, Status, MetaData | ConvertTo-Json' #LINE# #TAB# #TAB#.format(vm_name)) #LINE# #TAB# rs = run_ps(ps_script) #LINE# #TAB# return rs"
"#LINE# #TAB# ret = [] #LINE# #TAB# for c in broker.cleating_contexts: #LINE# #TAB# #TAB# if isinstance(c, CleavingContext): #LINE# #TAB# #TAB# #TAB# ret.append((c, None)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# timestamp = c.timestamp #LINE# #TAB# #TAB# #TAB# if timestamp is None: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# ret.append((c, timestamp)) #LINE# #TAB# return ret"
#LINE# #TAB# service = _get_services() #LINE# #TAB# try: #LINE# #TAB# #TAB# if file_path.exists(): #LINE# #TAB# #TAB# #TAB# service.search(file_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return service #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return False #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# return service
#LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# soup = _get_soup_from_url(url) #LINE# #TAB# lutostring = _get_lwostring_from_soup(soup) #LINE# #TAB# if not lutostring: #LINE# #TAB# #TAB# msg = _('unable to find favicon under %s' % url) #LINE# #TAB# #TAB# logger.warning(msg) #LINE# #TAB# #TAB# return None #LINE# #TAB# return lutostring
"#LINE# #TAB# matches = re.match('^[0-9]+$', str(number)) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# raise ValueError('Invalid bank account') #LINE# #TAB# masses = 0 #LINE# #TAB# for match in matches: #LINE# #TAB# #TAB# if match.group(1)!= number: #LINE# #TAB# #TAB# #TAB# raise ValueError('Invalid bank account') #LINE# #TAB# #TAB# masses += int(match.group(2)) #LINE# #TAB# return masses"
"#LINE# #TAB# username = click.prompt('Please enter your One Codex (email)') #LINE# #TAB# password = click.prompt( #LINE# #TAB# #TAB# 'Please enter your password (typing will be hidden)', hide_input=True) #LINE# #TAB# if api_key is not None: #LINE# #TAB# #TAB# return [user_id for user_id in api_key.split('/') if user_id.strip()] #LINE# #TAB# user_id = click.prompt('Please enter your username') #LINE# #TAB# password = click.prompt('Please enter your password', hide_input=True) #LINE# #TAB# api_key = user_id.split('/')[-1] if api_key else user_id #LINE# #TAB# return [user_id]"
#LINE# #TAB# if value == b'': #LINE# #TAB# #TAB# return node.value #LINE# #TAB# else: #LINE# #TAB# #TAB# decoded = base64.b64decode(value) #LINE# #TAB# #TAB# if decoded == b'': #LINE# #TAB# #TAB# #TAB# node.value = None #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# decoded = decoded.decode() #LINE# #TAB# #TAB# #TAB# node.value = decoded #LINE# #TAB# #TAB# #TAB# return decoded
"#LINE# #TAB# is_valid = False #LINE# #TAB# for i in range(len(data) - 1, -1, -1): #LINE# #TAB# #TAB# value = data[i] #LINE# #TAB# #TAB# if not is_valid: #LINE# #TAB# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# #TAB# is_valid = True #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# crc = ccITT_polynomial(value) #LINE# #TAB# #TAB# #TAB# #TAB# data[i] = crc ^ value #LINE# #TAB# return is_valid"
#LINE# #TAB# global _pydot_check #LINE# #TAB# _pydot_check = fun #LINE# #TAB# return _pydot_check
"#LINE# #TAB# if start_time is None: #LINE# #TAB# #TAB# start_time = time.time() #LINE# #TAB# logger.debug('Running portray_pdagent with start_time: {0}'.format(start_time)) #LINE# #TAB# df = pd.DataFrame(source) #LINE# #TAB# df.index = pd.to_datetime(start_time, format='%Y-%m-%dT%H:%M:%S') #LINE# #TAB# df.index = pd.to_datetime(df.index, format='%Y-%m-%dT%H:%M:%S') #LINE# #TAB# return df"
#LINE# #TAB# lomb_signif_ratio = org_client.get_lomb_signif_ratio(spec) #LINE# #TAB# return lomb_signif_ratio
"#LINE# #TAB# m = Manifold() #LINE# #TAB# data = cPickle.load(open(fn, 'rb')) #LINE# #TAB# if data[0]: #LINE# #TAB# #TAB# m.name = data[0].decode('utf-8') #LINE# #TAB# return m"
"#LINE# #TAB# jid = '' #LINE# #TAB# url = config.backups_url.format(backupID) #LINE# #TAB# if url: #LINE# #TAB# #TAB# r = requests.get(url) #LINE# #TAB# #TAB# if r.status_code == 200: #LINE# #TAB# #TAB# #TAB# data = r.json() #LINE# #TAB# #TAB# #TAB# jid = data['jid'] #LINE# #TAB# #TAB# #TAB# log.debug('Found remote JID %s for block %d', backupID, blockNum) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# jid = data['jid'] #LINE# #TAB# return jid"
"#LINE# #TAB# resp = make_response(json_dumps({'data': data}), code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
"#LINE# #TAB# res = '' #LINE# #TAB# first = True #LINE# #TAB# for c in buff: #LINE# #TAB# #TAB# if first: #LINE# #TAB# #TAB# #TAB# if c == '<': #LINE# #TAB# #TAB# #TAB# #TAB# res +='' #LINE# #TAB# #TAB# #TAB# elif c == '>': #LINE# #TAB# #TAB# #TAB# #TAB# res += '\\' #LINE# #TAB# #TAB# #TAB# elif c == ""'"": #LINE# #TAB# #TAB# #TAB# #TAB# res += c #LINE# #TAB# #TAB# #TAB# elif c == ""'"": #LINE# #TAB# #TAB# #TAB# #TAB# res += ""'"" #LINE# #TAB# #TAB# #TAB# first = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res += c #LINE# #TAB# return res"
"#LINE# #TAB# if val is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not isinstance(val, bool): #LINE# #TAB# #TAB# return False #LINE# #TAB# return val"
#LINE# #TAB# output = sys.stdout #LINE# #TAB# old_format = sys.stdout #LINE# #TAB# sys.stdout = output #LINE# #TAB# try: #LINE# #TAB# #TAB# yield output #LINE# #TAB# finally: #LINE# #TAB# #TAB# sys.stdout = old_format
"#LINE# #TAB# paths = set() #LINE# #TAB# for f in maya.cmds.ls(l=True, type='file'): #LINE# #TAB# #TAB# if maya.cmds.referenceQuery(f, isNodeReferenced=True): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# texture_path = maya.cmds.getAttr(os.path.normpath('.'.join([f, #LINE# #TAB# #TAB# #TAB# 'fileTextureName']))) #LINE# #TAB# #TAB# if texture_path: #LINE# #TAB# #TAB# #TAB# paths.add(texture_path) #LINE# #TAB# return paths"
"#LINE# #TAB# file_path, format_found = None, None #LINE# #TAB# if os.path.isfile(file_name): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# file_path = open(file_name, 'r').read() #LINE# #TAB# #TAB# #TAB# format_found = True #LINE# #TAB# #TAB# except IOError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# if file_path is not None and file_path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# file_path = file_name[:-3] #LINE# #TAB# #TAB# #TAB# format_found = True #LINE# #TAB# return file_path, format_found"
"#LINE# #TAB# fname = 'xml_format_detector.txt' #LINE# #TAB# df = pd.read_csv(fname, index_col=0) #LINE# #TAB# df.columns = ['xml_format_detector_name', 'xml_format_detector_version'] #LINE# #TAB# return df"
"#LINE# #TAB# differences = OrderedDict() #LINE# #TAB# for key, value in response.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for delta in value.get('delta', []): #LINE# #TAB# #TAB# #TAB# #TAB# differences[key].append(delta) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# differences[key] = [value] #LINE# #TAB# return differences"
"#LINE# #TAB# signature = sign_request_signature(req, cano_req, scope) #LINE# #TAB# if 'X-Amz-Credential' not in signature: #LINE# #TAB# #TAB# sig_string = sign_request_signature(req, scope) #LINE# #TAB# else: #LINE# #TAB# #TAB# sig_string = sign_request_signature(req, scope) #LINE# #TAB# return sig_string"
"#LINE# #TAB# pos = cio.tell() #LINE# #TAB# fig_data = cio.read(1024) #LINE# #TAB# lines = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# line = cio.readline() #LINE# #TAB# #TAB# if line == '\n': #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# lines.append(line) #LINE# #TAB# position = cio.tell() #LINE# #TAB# cio.seek(pos) #LINE# #TAB# return fig_data, position"
"#LINE# #TAB# bb = np.asarray(bb, dtype=np.float64) #LINE# #TAB# rows = bb.shape[0] // factor #LINE# #TAB# cols = bb.shape[1] // factor #LINE# #TAB# return bb[0:rows * factor, 0:cols * factor]"
#LINE# #TAB# if chrom_str.startswith('chr'): #LINE# #TAB# #TAB# return 'chr' #LINE# #TAB# elif chrom_str.startswith('chr'): #LINE# #TAB# #TAB# return 'chr' #LINE# #TAB# elif chrom_str.startswith('_'): #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# if urlconf is None: #LINE# #TAB# #TAB# urlconf = resolve_default_urlconf() #LINE# #TAB# if not urlconf: #LINE# #TAB# #TAB# return False #LINE# #TAB# parsed = urlparse(path) #LINE# #TAB# return parsed.scheme in ('http', 'https' #LINE# #TAB# #TAB# ) and parsed.netloc == urlconf.netloc"
#LINE# #TAB# if 'IPython' not in sys.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'IPython' not in sys.modules: #LINE# #TAB# #TAB# return False #LINE# #TAB# update_flag = np.getenv('MSUPDATE_STATUS') #LINE# #TAB# if update_flag is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
#LINE# #TAB# try: #LINE# #TAB# #TAB# t = [s] #LINE# #TAB# #TAB# n = len(t) #LINE# #TAB# #TAB# for i in range(n): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# t.append(float(t[i])) #LINE# #TAB# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# #TAB# #TAB# #TAB# raise
"#LINE# #TAB# LOG.debug('check_rgb_value() called') #LINE# #TAB# session = bc.get_writer_session() #LINE# #TAB# binding = session.query(nexus_models_v2.NexusNVEBinding).filter_by(vni #LINE# #TAB# #TAB# =vni, switch_ip=switch_ip, device_id=device_id).one() #LINE# #TAB# if binding: #LINE# #TAB# #TAB# session.delete(binding) #LINE# #TAB# #TAB# session.flush() #LINE# #TAB# #TAB# return binding.rgb_value #LINE# #TAB# return None"
#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
#LINE# #TAB# r = [] #LINE# #TAB# for i in range(len(lst)): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# r.append(lst[i][key]) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# r.append(lst[i][key]) #LINE# #TAB# return r
"#LINE# #TAB# return [{'id': path.get('id'), 'name': path.get('name'), 'type': path.get( #LINE# #TAB# #TAB# 'type'), 'url': path.get('url')} for path in request.db.all()]"
"#LINE# #TAB# fscores = [] #LINE# #TAB# for root, dirs, files in os.walk(restore_dir): #LINE# #TAB# #TAB# for name in files: #LINE# #TAB# #TAB# #TAB# if name == 'operative_config': #LINE# #TAB# #TAB# #TAB# #TAB# fscores.append(os.path.relpath(root, name)) #LINE# #TAB# fscores.sort(key=lambda x: x[1]) #LINE# #TAB# return fscores"
#LINE# #TAB# if len(trace) < 2: #LINE# #TAB# #TAB# raise ValueError('Invalid trace') #LINE# #TAB# data_home = {} #LINE# #TAB# for activity in trace: #LINE# #TAB# #TAB# if activity not in data_home: #LINE# #TAB# #TAB# #TAB# data_home[activity] = 0 #LINE# #TAB# #TAB# data_home[activity] += 1 #LINE# #TAB# return data_home
"#LINE# #TAB# assert len(version) == 5 #LINE# #TAB# assert version[3] in ('alpha', 'beta', 'rc', 'final') #LINE# #TAB# parts = 3 #LINE# #TAB# main = '.'.join(str(x) for x in version[:parts]) #LINE# #TAB# sub = '' #LINE# #TAB# if version[3] == 'alpha' and version[4] == 0: #LINE# #TAB# #TAB# git_changeset = _get_git_changeset() #LINE# #TAB# #TAB# if git_changeset: #LINE# #TAB# #TAB# #TAB# sub = '.dev%s' % git_changeset #LINE# #TAB# elif version[3]!= 'final': #LINE# #TAB# #TAB# mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'} #LINE# #TAB# #TAB# sub = mapping[version[3]] + str(version[4]) #LINE# #TAB# return main + sub"
#LINE# #TAB# api.print_credentials() #LINE# #TAB# return 0
#LINE# #TAB# lines = [] #LINE# #TAB# for lib in pkg_libraries: #LINE# #TAB# #TAB# lines.extend(measure_report_impl(lib)) #LINE# #TAB# return lines
"#LINE# #TAB# with open(path, 'r') as file: #LINE# #TAB# #TAB# lines = file.readlines() #LINE# #TAB# #TAB# if len(lines) < 3: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if lines[0][0] == '#': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if lines[0][1] == '#': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False"
#LINE# #TAB# for i in props.keys(): #LINE# #TAB# #TAB# props[i]['unsampled'] = False #LINE# #TAB# #TAB# if'sample_id' in props[i]: #LINE# #TAB# #TAB# #TAB# for j in props[i]['sample_id']: #LINE# #TAB# #TAB# #TAB# #TAB# props[i]['unsampled'] = False #LINE# #TAB# #TAB# if'sample_type' in props[i]: #LINE# #TAB# #TAB# #TAB# for j in props[i]['sample_type']: #LINE# #TAB# #TAB# #TAB# #TAB# props[i]['sample_type'] = j
#LINE# #TAB# maps_to_drop = [] #LINE# #TAB# for map_name in escher_maps: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# map_dict = json.loads(escher_maps[map_name]) #LINE# #TAB# #TAB# #TAB# map_dict['_id'] = map_name #LINE# #TAB# #TAB# #TAB# maps_to_drop.append(map_dict['_id']) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# logger.warning('Dropping non-signature field {} from escher map'. #LINE# #TAB# #TAB# #TAB# #TAB# format(map_name)) #LINE# #TAB# return maps_to_drop
"#LINE# #TAB# database_name = settings.INDY_CONFIG['database_name'] #LINE# #TAB# wallet_config = get_wallet_config(database_name, wallet_name) #LINE# #TAB# execution_engine = create_execution_engine(wallet_config['name']) #LINE# #TAB# return execution_engine"
#LINE# #TAB# try: #LINE# #TAB# #TAB# v = int(s) #LINE# #TAB# #TAB# if v >= 1 and v <= 18: #LINE# #TAB# #TAB# #TAB# return v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return v #LINE# #TAB# except: #LINE# #TAB# #TAB# logger.error(traceback.format_exc()) #LINE# #TAB# #TAB# return 0
"#LINE# #TAB# flux_path = os.path.join(flux_path, 'flux') #LINE# #TAB# cmd = 'flux -f'+ flux_path #LINE# #TAB# if not no_errors: #LINE# #TAB# #TAB# cmd +='-v' #LINE# #TAB# return cmd"
"#LINE# #TAB# server_groups = [] #LINE# #TAB# for server_group in settings.SERVER_GROUPS: #LINE# #TAB# #TAB# if server_group.get('validation_digit', '') == 'yes': #LINE# #TAB# #TAB# #TAB# server_groups.append(server_group) #LINE# #TAB# if len(server_groups) > 0: #LINE# #TAB# #TAB# return max(server_groups, key=lambda x: x[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"#LINE# #TAB# data = {} #LINE# #TAB# fname = os.path.join(os.getcwd(), file_name) #LINE# #TAB# if os.path.exists(fname) and not force: #LINE# #TAB# #TAB# with open(fname, 'r') as f: #LINE# #TAB# #TAB# #TAB# data = json.load(f) #LINE# #TAB# hostname = data #LINE# #TAB# return hostname"
"#LINE# #TAB# if not path: #LINE# #TAB# #TAB# path = [] #LINE# #TAB# for key in path: #LINE# #TAB# #TAB# if key in a: #LINE# #TAB# #TAB# #TAB# if isinstance(a[key], dict) and isinstance(b[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# get_completed(a[key], b[key], path + [str(key)]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# a.update(b) #LINE# #TAB# return a"
"#LINE# #TAB# for item in delta: #LINE# #TAB# #TAB# entry = item[3] #LINE# #TAB# #TAB# if isinstance(entry, dict): #LINE# #TAB# #TAB# #TAB# entry = entry[1] #LINE# #TAB# #TAB# #TAB# if entry and entry['file_id'] is not None: #LINE# #TAB# #TAB# #TAB# #TAB# raise errors.InconsistentTopic(entry['file_id'], entry) #LINE# #TAB# #TAB# elif entry and entry['file_id']!= entry['file_id']: #LINE# #TAB# #TAB# #TAB# raise errors.InconsistentTopic(entry['file_id'], entry) #LINE# #TAB# #TAB# yield item"
"#LINE# #TAB# filter_fn = filter_fn or _default_filter_fn #LINE# #TAB# matches = {} #LINE# #TAB# for k, v in obs.items(): #LINE# #TAB# #TAB# name = filter_fn(k) #LINE# #TAB# #TAB# if name not in filter_fn: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# match = _is_complex_format_str(v, name, owner, unit_type, tag) #LINE# #TAB# #TAB# if not match: #LINE# #TAB# #TAB# #TAB# return matches #LINE# #TAB# #TAB# matches[name] = match #LINE# #TAB# return matches"
"#LINE# #TAB# val = isinstance(pe, _bp('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('Entity')) #LINE# #TAB# return val"
#LINE# #TAB# face_descriptions = {} #LINE# #TAB# for face in table.faces: #LINE# #TAB# #TAB# face_descriptions.update(get_face_descriptions(face)) #LINE# #TAB# faces = list(face_descriptions.keys()) #LINE# #TAB# for face in table.faces: #LINE# #TAB# #TAB# if face.is_unknown: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if face.name not in faces: #LINE# #TAB# #TAB# #TAB# faces.append(face) #LINE# #TAB# return face_descriptions
#LINE# #TAB# if not os.path.splitext(file)[1]: #LINE# #TAB# #TAB# return 'unknown' #LINE# #TAB# return file
"#LINE# #TAB# if not isinstance(text, six.text_type): #LINE# #TAB# #TAB# text = unicodedata.normalize('NFC', text) #LINE# #TAB# if isinstance(text, six.text_type): #LINE# #TAB# #TAB# text = unicodedata.normalize('NFC', text) #LINE# #TAB# return text"
"#LINE# #TAB# #TAB# EXPIRED_MESSAGE = 'Expired oauth2 access token' #LINE# #TAB# #TAB# INVALID_MESSAGE = 'Invalid oauth2 access token' #LINE# #TAB# #TAB# if response.status_code == 400: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# body = response.json() #LINE# #TAB# #TAB# #TAB# #TAB# if str(body.get('error_description')) in [EXPIRED_MESSAGE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# INVALID_MESSAGE]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return False"
"#LINE# #TAB# try: #LINE# #TAB# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# #TAB# return s.encode('utf-8') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return s #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(s) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return s"
"#LINE# #TAB# if not regex: #LINE# #TAB# #TAB# return urls #LINE# #TAB# matches = [] #LINE# #TAB# for url in urls: #LINE# #TAB# #TAB# match = re.search(regex, url) #LINE# #TAB# #TAB# if not match: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if matches: #LINE# #TAB# #TAB# #TAB# yield url #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# matches.append(url) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# yield url"
"#LINE# #TAB# if not bs_terms and not bp_terms: #LINE# #TAB# #TAB# return {} #LINE# #TAB# dvs_link_discovery_protocol = {} #LINE# #TAB# for u, v in bs_terms.items(): #LINE# #TAB# #TAB# if u in bp_terms: #LINE# #TAB# #TAB# #TAB# if not v: #LINE# #TAB# #TAB# #TAB# #TAB# dvs_link_discovery_protocol[u] = {v} #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dvs_link_discovery_protocol[u] = {v} #LINE# #TAB# return dvs_link_discovery_protocol"
"#LINE# #TAB# config_map = {} #LINE# #TAB# for name, value in cls.DEFAULTS.items(): #LINE# #TAB# #TAB# if name not in config_map: #LINE# #TAB# #TAB# #TAB# config_map[name] = value #LINE# #TAB# return config_map"
"#LINE# #TAB# if not AIR_boundaries_are_interior: #LINE# #TAB# #TAB# return False #LINE# #TAB# g = [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [-1.0, 0.0, #LINE# #TAB# #TAB# 0.0], [0.0, 0.0, 1.0]] #LINE# #TAB# for other in AIR_boundaries_are_interior: #LINE# #TAB# #TAB# diff = np.diff(other) #LINE# #TAB# #TAB# g[other] += diff #LINE# #TAB# #TAB# if g[other] < 0: #LINE# #TAB# #TAB# #TAB# g[other] += diff #LINE# #TAB# return True"
"#LINE# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# assess_vencode_one_0(path, ignore=ignore) #LINE# #TAB# #TAB# return None #LINE# #TAB# for root, _, files in os.walk(path, topdown=False): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# if _is_vencode_one_zero(os.path.join(root, f)): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return None"
"#LINE# #TAB# with open(input_toml, 'r') as f: #LINE# #TAB# #TAB# _ = toml.load(f) #LINE# #TAB# return _"
"#LINE# #TAB# buffer, name = index #LINE# #TAB# while buffer.endswith('}'): #LINE# #TAB# #TAB# buffer = buffer[:-1] #LINE# #TAB# #TAB# name = name + '\\' #LINE# #TAB# return name"
"#LINE# #TAB# perm = numpy.zeros((image.shape[0], image.shape[1]), dtype=numpy.float32) #LINE# #TAB# for i in range(image.shape[0]): #LINE# #TAB# #TAB# for j in range(image.shape[1]): #LINE# #TAB# #TAB# #TAB# perm[i, j] = assign(image[i, j]) #LINE# #TAB# return perm"
#LINE# #TAB# match = _xlog_re.search(os.path.basename(path)) #LINE# #TAB# if match and match.group(0).endswith('.history'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
#LINE# #TAB# try: #LINE# #TAB# #TAB# return list(exp.branches())[0] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return exp
#LINE# #TAB# try: #LINE# #TAB# #TAB# lock = Lock.objects.get(login=login_or_id) #LINE# #TAB# except Lock.DoesNotExist: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# return lock.label
"#LINE# #TAB# result = np.empty(feature.shape) #LINE# #TAB# for i in range(feature.shape[0]): #LINE# #TAB# #TAB# for j in range(feature.shape[1]): #LINE# #TAB# #TAB# #TAB# result[i, j] = _to_0d_object_array(feature[i, j]) #LINE# #TAB# return result"
"#LINE# #TAB# with salt.utils.files.fopen(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# line = in_handle.readline() #LINE# #TAB# #TAB# field_map = {""top"": ""left"", ""bottom"": ""top""} #LINE# #TAB# #TAB# for field in line.split(): #LINE# #TAB# #TAB# #TAB# if field not in field_map: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# field_map[field] = float(field) #LINE# #TAB# #TAB# return field_map[field]"
#LINE# #TAB# if type(_yaml_bool_value) is bool: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"#LINE# #TAB# req.router.add_route('/GetYHeight', {'collector_addr': collector_addr, #LINE# #TAB# #TAB# 'prefix': prefix}) #LINE# #TAB# return req"
#LINE# #TAB# try: #LINE# #TAB# #TAB# return zk.get(six.text_type(topic)) #LINE# #TAB# except kazoo.client.NoNodeError: #LINE# #TAB# #TAB# return None
"#LINE# #TAB# max_value = np.max(values) #LINE# #TAB# min_value = np.min(values) #LINE# #TAB# if np.isinf(min_value): #LINE# #TAB# #TAB# min_value = np.inf #LINE# #TAB# if np.isnan(min_value): #LINE# #TAB# #TAB# min_value = -np.inf #LINE# #TAB# if np.isinf(max_value): #LINE# #TAB# #TAB# max_value = np.inf #LINE# #TAB# return min_value, max_value"
