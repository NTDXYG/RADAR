nl,code
see if a character is printable <code> def is_printable(c): ,"#LINE# #TAB# global have_ascii #LINE# #TAB# if have_ascii: #LINE# #TAB# #TAB# return ascii.isprint(c) #LINE# #TAB# if isinstance(c, int): #LINE# #TAB# #TAB# ic = c #LINE# #TAB# else: #LINE# #TAB# #TAB# ic = ord(c) #LINE# #TAB# return ic >= 32 and ic <= 126"
"Get the bounds of the system in global coordinates . Returns two arrays of shape ( 3 ) containing minimum and maximum bounds respectively  <code> def get_bounds(p_state, idx_image=-1, idx_chain=-1): ","#LINE# #TAB# _min = (3 * ctypes.c_float)() #LINE# #TAB# _max = (3 * ctypes.c_float)() #LINE# #TAB# _Get_Bounds(ctypes.c_void_p(p_state), _min, _max, ctypes.c_int( #LINE# #TAB# #TAB# idx_image), ctypes.c_int(idx_chain)) #LINE# #TAB# return [_min[i] for i in range(3)], [_max[i] for i in range(3)]"
Extract the command name and arguments to pass to docopt . Args : argv : The argument list being used to run the command . Returns : A tuple containing the name of the command and the arguments to pass to docopt  <code> def get_command_and_argv(argv): ,"#LINE# #TAB# command_name = argv[0] #LINE# #TAB# if not command_name: #LINE# #TAB# #TAB# argv = argv[1:] #LINE# #TAB# elif command_name == settings.command: #LINE# #TAB# #TAB# argv.remove(command_name) #LINE# #TAB# return command_name, argv"
Get the next slip packet from raw data  <code> def get_next_slip(raw): ,"#LINE# #TAB# if not is_slip(raw): #LINE# #TAB# #TAB# return None, raw #LINE# #TAB# length = raw[1:].index(SLIP_END) #LINE# #TAB# slip_packet = decode(raw[1:length+1]) #LINE# #TAB# new_raw = raw[length+2:] #LINE# #TAB# return slip_packet, new_raw"
"Encode ReplayGain gain / peak values as a Sound Check string  <code> def sc_encode(gain, peak): ","#LINE# #TAB# peak *= 32768.0 #LINE# #TAB# g1 = int(min(round(10 ** (gain / -10) * 1000), 65534)) #LINE# #TAB# g2 = int(min(round(10 ** (gain / -10) * 2500), 65534)) #LINE# #TAB# uk = 0 #LINE# #TAB# values = g1, g1, g2, g2, uk, uk, int(peak), int(peak), uk, uk #LINE# #TAB# return u' %08X' * 10 % values"
Determine if an error encountered during an HTTP download is likely to go away if we try again  <code> def retryable_http_error( e ): ,"#LINE# #TAB# if isinstance( e, urllib.error.HTTPError ) and e.code in ('503', '408', '500'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance( e, BadStatusLine ): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Extracts the Content - Type header from the headers returned by page  <code> def get_mime_type(page): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# doc_type = str(page.headers['content-type']) #LINE# #TAB# #TAB# return doc_type #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return ''
"Returns back a float from the given value <code> def floating_number(value, minimum=None, maximum=None, cut=False, pad=False): ","#LINE# #TAB# value = float(value) #LINE# #TAB# if minimum is not None and value < minimum: #LINE# #TAB# #TAB# if pad: #LINE# #TAB# #TAB# #TAB# return minimum #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Provided value of {} is below specified minimum of {}'.format( #LINE# #TAB# #TAB# #TAB# value, minimum)) #LINE# #TAB# if maximum is not None and value > maximum: #LINE# #TAB# #TAB# if cut: #LINE# #TAB# #TAB# #TAB# return maximum #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Provided value of {} is above specified maximum of {}'.format( #LINE# #TAB# #TAB# #TAB# value, maximum)) #LINE# #TAB# return value"
"Yield dependant user packages for a given package name  <code> def get_dependants(cls, dist): ",#LINE# #TAB# for package in cls.installed_distributions: #LINE# #TAB# #TAB# for requirement_package in package.requires(): #LINE# #TAB# #TAB# #TAB# requirement_name = requirement_package.project_name #LINE# #TAB# #TAB# #TAB# if requirement_name.lower() == dist.lower(): #LINE# #TAB# #TAB# #TAB# #TAB# yield package
Estimate the center and full - width half max of a signal using moments <code> def center_fwhm_bymoments(signal): ,"#LINE# #TAB# x = np.arange(signal.size) #LINE# #TAB# m0 = signal.sum() #LINE# #TAB# m1 = (x * signal).sum() #LINE# #TAB# m2 = (x ** 2.0 * signal).sum() #LINE# #TAB# mu = m1 / m0 #LINE# #TAB# sigma = np.sqrt(np.abs(m2) - mu ** 2) #LINE# #TAB# return mu, sigma * 2.3548"
Returns out object from context : param om : out_message : return : out_object <code> def get_out_object(ctx): ,"#LINE# #TAB# om = ctx.descriptor.out_message #LINE# #TAB# oo = None #LINE# #TAB# if ctx.descriptor.is_out_bare(): #LINE# #TAB# #TAB# oo = next(iter(ctx.out_object)) #LINE# #TAB# elif len(om._type_info) == 0: #LINE# #TAB# #TAB# oo, = ctx.out_object #LINE# #TAB# elif len(om._type_info) == 1: #LINE# #TAB# #TAB# om, = om._type_info.values() #LINE# #TAB# #TAB# oo, = ctx.out_object #LINE# #TAB# else: #LINE# #TAB# #TAB# oo = om.get_serialization_instance(ctx.out_object) #LINE# #TAB# return oo"
Get project keystone client using admin credential  <code> def get_trust_client(trust_id): ,"#LINE# #TAB# client = ks_client.Client(username=CONF.keystone_authtoken.username, #LINE# #TAB# #TAB# password=CONF.keystone_authtoken.password, auth_url=CONF. #LINE# #TAB# #TAB# keystone_authtoken.www_authenticate_uri, trust_id=trust_id) #LINE# #TAB# return client"
"split by multiple delimiters <code> def multi_split(txt, delims): ","#LINE# #TAB# res = [txt] #LINE# #TAB# for delimChar in delims: #LINE# #TAB# #TAB# txt, res = res, [] #LINE# #TAB# #TAB# for word in txt: #LINE# #TAB# #TAB# #TAB# if len(word) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# res += word.split(delimChar) #LINE# #TAB# return res"
Prompt a user for a boolean expression and repeat until a valid boolean has been entered . ` ` prompt ` ` is the text to prompt the user with  <code> def get_bool(prompt): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# bool_str = str_2_bool(raw_input(prompt)) #LINE# #TAB# except ToyzError: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ""'{0}' did not match a boolean expression (true/false, yes/no, t/f, y/n)"" #LINE# #TAB# #TAB# #TAB# .format(bool_str)) #LINE# #TAB# #TAB# return get_bool(prompt) #LINE# #TAB# return bool_str"
"Extract a LZH archive  <code> def extract_lzh(archive, compression, cmd, verbosity, interactive, outdir): ","#LINE# #TAB# opts = 'x' #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# opts += 'v' #LINE# #TAB# opts += 'w=%s' % outdir #LINE# #TAB# return [cmd, opts, archive]"
Returns the user s information <code> def get_user_information(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# import pwd #LINE# #TAB# #TAB# _username = pwd.getpwuid(os.getuid())[0] #LINE# #TAB# #TAB# _userid = os.getuid() #LINE# #TAB# #TAB# _uname = os.uname()[1] #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# import getpass #LINE# #TAB# #TAB# _username = getpass.getuser() #LINE# #TAB# #TAB# _userid = 0 #LINE# #TAB# #TAB# import platform #LINE# #TAB# #TAB# _uname = platform.node() #LINE# #TAB# return _username, _userid, _uname"
Calculates the minimum spanning forest and returns a list of trees as subgraphs  <code> def find_minimum_spanning_forest_as_subgraphs(graph): ,"#LINE# #TAB# forest = find_minimum_spanning_forest(graph) #LINE# #TAB# list_of_subgraphs = map(lambda edge_list: get_subgraph_from_edge_list( #LINE# #TAB# #TAB# graph, edge_list), forest) #LINE# #TAB# return list_of_subgraphs"
Get list of instances in current project <code> def list_instances(name_only=True): ,#LINE# #TAB# vms = run('compute instances list') #LINE# #TAB# return [str(vm['name']) for vm in vms] if name_only else vms
"Verify that objects in list are of the allowed type or raise TypeError  <code> def check_list_type(objects, allowed_type, name, allow_none=True): ","#LINE# if objects is None: #LINE# #TAB# if not allow_none: #LINE# #TAB# raise TypeError('%s is None, which is not allowed.' % name) #LINE# #TAB# return objects #LINE# if not isinstance(objects, (tuple, list)): #LINE# #TAB# raise TypeError('%s is not a list.' % name) #LINE# if not all(isinstance(i, allowed_type) for i in objects): #LINE# #TAB# type_list = sorted(list(set(type(obj) for obj in objects))) #LINE# #TAB# raise TypeError('%s contains types that don\'t match %s: %s' % #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# (name, allowed_type.__name__, type_list)) #LINE# return objects"
"Converts a foundation stress to an equivalent mass  <code> def convert_stress_to_mass(q, width, length, gravity): ",#LINE# #TAB# mass = q * width * length / gravity #LINE# #TAB# return mass
parse dever_config.py and return list of apps <code> def parse_py_config(): ,"#LINE# #TAB# spec = importlib.util.spec_from_file_location('', CONFIG_PY) #LINE# #TAB# module = importlib.util.module_from_spec(spec) #LINE# #TAB# spec.loader.exec_module(module) #LINE# #TAB# return module.apps"
Load json file content Parameters ---------- file_path : path to the file Raises ------ TypeError Error with the file path <code> def get_json_content(file_path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file_path, 'r') as file: #LINE# #TAB# #TAB# #TAB# return json.load(file) #LINE# #TAB# except TypeError as err: #LINE# #TAB# #TAB# print('Error: ', err) #LINE# #TAB# #TAB# return None"
"Returns True if ' distinct ( ) ' should be used to query the given lookup path  <code> def lookup_needs_distinct(opts, lookup_path): ","#LINE# #TAB# field_name = lookup_path.split('__', 1)[0] #LINE# #TAB# field = opts.get_field(field_name) #LINE# #TAB# if hasattr(field, 'remote_field') and isinstance(field.remote_field, #LINE# #TAB# #TAB# models.ManyToManyRel) or is_related_field(field #LINE# #TAB# #TAB# ) and not field.field.unique: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
unpatch_app will remove tracing from a celery app <code> def unpatch_app(app): ,"#LINE# #TAB# patched_methods = ['task'] #LINE# #TAB# for method_name in patched_methods: #LINE# #TAB# #TAB# wrapper = getattr(app, method_name, None) #LINE# #TAB# #TAB# if wrapper is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if not isinstance(wrapper, wrapt.ObjectProxy): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# setattr(app, method_name, wrapper.__wrapped__) #LINE# #TAB# setattr(app, 'Task', unpatch_task(app.Task)) #LINE# #TAB# return app"
"For each country , get the days of the spread since 500 cases <code> def get_day_counts(d, country): ","#LINE# #TAB# data = d.copy() #LINE# #TAB# result_df = pd.DataFrame([]) #LINE# #TAB# result_df = data.groupby(['file_date']).agg({'confirmed': 'sum', #LINE# #TAB# #TAB# 'recovered': 'sum', 'deaths': 'sum'}) #LINE# #TAB# result_df['date'] = data['file_date'].unique() #LINE# #TAB# result_df['country'] = country #LINE# #TAB# result_df = result_df[result_df.confirmed >= 500] #LINE# #TAB# result_df.insert(loc=0, column='day', value=np.arange(len(result_df))) #LINE# #TAB# return result_df"
"Clean lookup fields and check for errors  <code> def clean_fields(allowed_fields: dict, fields: FieldsParam) ->Iterable[str]: ",#LINE# #TAB# if fields == ALL: #LINE# #TAB# #TAB# fields = allowed_fields.keys() #LINE# #TAB# else: #LINE# #TAB# #TAB# fields = tuple(fields) #LINE# #TAB# #TAB# unknown_fields = set(fields) - allowed_fields.keys() #LINE# #TAB# #TAB# if unknown_fields: #LINE# #TAB# #TAB# #TAB# raise ValueError('Unknown fields: {}'.format(unknown_fields)) #LINE# #TAB# return fields
True if the version is specified and is n't exact False otherwise <code> def should_update_package(version): ,"#LINE# #TAB# if version: #LINE# #TAB# #TAB# if not isinstance(version, SpecifierSet): #LINE# #TAB# #TAB# #TAB# version_specifier = SpecifierSet(version) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# version_specifier = version #LINE# #TAB# #TAB# for spec in version_specifier._specs: #LINE# #TAB# #TAB# #TAB# if hasattr(spec, 'operator'): #LINE# #TAB# #TAB# #TAB# #TAB# if spec.operator not in ('==', '==='): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# elif spec._spec[0] not in ('==', '==='): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Parse first row of NEM file <code> def parse_header_row(row: List[Any], file_name=None) ->HeaderRecord: ","#LINE# #TAB# record_indicator = int(row[0]) #LINE# #TAB# if record_indicator != 100: #LINE# #TAB# #TAB# raise ValueError('NEM Files must start with a 100 row') #LINE# #TAB# header = parse_100_row(row, file_name) #LINE# #TAB# if header.version_header not in ['NEM12', 'NEM13']: #LINE# #TAB# #TAB# raise ValueError('Invalid NEM version {}'.format(header.version_header) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# log.debug('Parsing %s file %s ...', header.version_header, file_name) #LINE# #TAB# return header"
"Parses row and gets columns matching tag <code> def _get_row_tag(row, tag): ",#LINE# #TAB# #TAB# is_empty = True #LINE# #TAB# #TAB# data = [] #LINE# #TAB# #TAB# for column_label in row.find_all(tag): #LINE# #TAB# #TAB# #TAB# data.append( #LINE# #TAB# #TAB# #TAB# #TAB# String(column_label.text).strip_bad_html() #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# if data[-1]: #LINE# #TAB# #TAB# #TAB# #TAB# is_empty = False #LINE# #TAB# #TAB# if not is_empty: #LINE# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# return None
Lazily load the McQuillan rotation periods table <code> def get_rotationtable(): ,"#LINE# #TAB# import astropy.io.ascii as ascii #LINE# #TAB# import pkgutil #LINE# #TAB# global rotation_table #LINE# #TAB# if rotation_table is None: #LINE# #TAB# #TAB# logger.info('Reading McQuillan rotation table') #LINE# #TAB# #TAB# rotation_table = ascii.read(pkgutil.get_data(__name__, #LINE# #TAB# #TAB# #TAB# 'data/rotation_McQuillan.txt')) #LINE# #TAB# return rotation_table"
"r Create new trajectories that are subsampled at lag but shifted <code> def lag_observations(observations, lag, stride=1): ","#LINE# #TAB# obsnew = [] #LINE# #TAB# for obs in observations: #LINE# #TAB# #TAB# for shift in range(0, lag, stride): #LINE# #TAB# #TAB# #TAB# obs_lagged = (obs[shift:][::lag]) #LINE# #TAB# #TAB# #TAB# if len(obs_lagged) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# obsnew.append(obs_lagged) #LINE# #TAB# return obsnew"
"Get PV arguments for a VM <code> def get_pv_args(name, session=None, call=None): ","#LINE# #TAB# if call == 'function': #LINE# #TAB# #TAB# raise SaltCloudException( #LINE# #TAB# #TAB# #TAB# 'This function must be called with -a or --action.' #LINE# #TAB# #TAB# ) #LINE# #TAB# if session is None: #LINE# #TAB# #TAB# log.debug('New session being created') #LINE# #TAB# #TAB# session = _get_session() #LINE# #TAB# vm = _get_vm(name, session=session) #LINE# #TAB# pv_args = session.xenapi.VM.get_PV_args(vm) #LINE# #TAB# if pv_args: #LINE# #TAB# #TAB# return pv_args #LINE# #TAB# return None"
"Authenticate a username and password against our database : param username : : param password : : return : authenticated username <code> def get_api_key(username, password): ","#LINE# #TAB# global db #LINE# #TAB# if db is None: #LINE# #TAB# #TAB# init_db() #LINE# #TAB# user_model = Query() #LINE# #TAB# user = db.get(user_model.username == username) #LINE# #TAB# if not user: #LINE# #TAB# #TAB# LOGGER.warning('User %s not found', username) #LINE# #TAB# #TAB# return False #LINE# #TAB# if user['password'] == hash_password(password, user.get('salt')): #LINE# #TAB# #TAB# return user['api_key'] #LINE# #TAB# return False"
"r Check time and signal parameters  <code> def check_time_only(time, signal, verb): ","#LINE# #TAB# global _min_time #LINE# #TAB# if int(signal) not in [-1, 0, 1]: #LINE# #TAB# #TAB# print(""* ERROR :: <signal> must be one of: [None, -1, 0, 1]; "" + #LINE# #TAB# #TAB# #TAB# ""<signal> provided: ""+str(signal)) #LINE# #TAB# #TAB# raise ValueError('signal') #LINE# #TAB# time = _check_var(time, float, 1, 'time') #LINE# #TAB# time = _check_min(time, _min_time, 'Times', 's', verb) #LINE# #TAB# if verb > 2: #LINE# #TAB# #TAB# _prnt_min_max_val(time, "" time#TAB# #TAB# [s] : "", verb) #LINE# #TAB# return time"
"Returns an instance of lxml . etree . _Element for the given doc input  <code> def get_etree_root(doc, encoding=None): ","#LINE# #TAB# tree = get_etree(doc, encoding) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# return root"
"Rolling sum of data  <code> def window_sum(data, window_len): ","#LINE# #TAB# window_sum = np.cumsum(data) #LINE# #TAB# np.subtract(window_sum[window_len:], window_sum[:-window_len], out= #LINE# #TAB# #TAB# window_sum[:-window_len]) #LINE# #TAB# return window_sum[:-window_len]"
"Calculate the overlap of two lines in average percent overlap  <code> def calculate_overlap(min1, max1, min2, max2): ","#LINE# #TAB# dist = max(0, min(max1, max2) - max(min1, min2)) #LINE# #TAB# len1 = max1 - min1 #LINE# #TAB# len2 = max2 - min2 #LINE# #TAB# return (dist / len1 if len1 else 0 + dist / len2 if len2 else 0) / 2"
"Check if cache is invalig and regeneration required <code> def need_regeneration(directory, cache_file_path, cache_mtime): ","#LINE# #TAB# return not isfile(cache_file_path) or getmtime(config.__file__ #LINE# #TAB# #TAB# ) > cache_mtime or any(True for f in listdir(directory) if getmtime #LINE# #TAB# #TAB# (path_join(directory, f)) > cache_mtime and not f.startswith('.') #LINE# #TAB# #TAB# ) or getmtime(get_config_path(directory)) > cache_mtime"
Get the available operator name . Parameters ---------- name : str The optional name to use . Returns ------- str The operator name  <code> def get_operator_name(name=None): ,"#LINE# #TAB# op_idx = GetOperatorIdx() #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return op_idx, 'Op_' + str(op_idx) #LINE# #TAB# else: #LINE# #TAB# #TAB# return op_idx, name"
Use the name field in the configuration or p + count : param partition_dicts : : return : list of Partition objects <code> def generate_gpt_partitions(partition_dicts): ,"#LINE# #TAB# count = 0 #LINE# #TAB# partitions = list() #LINE# #TAB# for partition in partition_dicts: #LINE# #TAB# #TAB# partitions.append(generate_partition(partition.get('name', 'p' + #LINE# #TAB# #TAB# #TAB# str(count)), partition)) #LINE# #TAB# #TAB# count += 1 #LINE# #TAB# return partitions"
"Get first no NaN value in column : param array : : param idx_col : : return : <code> def get_no_nan(array, idx_col): ","#LINE# #TAB# for value in array[:, (idx_col)]: #LINE# #TAB# #TAB# if ~np.isnan(value): #LINE# #TAB# #TAB# #TAB# return value"
"Checks necessity of clearing the folder . : param paths : list of directories : return : True if at least one directory exists , False otherwise <code> def check_existance(paths): ",#LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# if os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
Restart the traffic_manager process and the traffic_server process on all the nodes in a cluster  <code> def restart_cluster(name): ,"#LINE# #TAB# ret = {'name': name, #LINE# #TAB# #TAB# 'changes': {}, #LINE# #TAB# #TAB# 'result': None, #LINE# #TAB# #TAB# 'comment': ''} #LINE# #TAB# if __opts__['test']: #LINE# #TAB# #TAB# ret['comment'] = 'Restarting cluster' #LINE# #TAB# #TAB# return ret #LINE# #TAB# __salt__['trafficserver.restart_cluster']() #LINE# #TAB# ret['result'] = True #LINE# #TAB# ret['comment'] = 'Restarted cluster' #LINE# #TAB# return ret"
Imports the module for a DOAC handler based on the string representation of the module path that is provided  <code> def get_handler(handler_name): ,"#LINE# #TAB# from .conf import options #LINE# #TAB# handlers = options.handlers #LINE# #TAB# for handler in handlers: #LINE# #TAB# #TAB# handler_path = handler.split(""."") #LINE# #TAB# #TAB# name = handler_path[-2] #LINE# #TAB# #TAB# if handler_name == name: #LINE# #TAB# #TAB# #TAB# handler_module = __import__(""."".join(handler_path[:-1]), {}, {}, str(handler_path[-1])) #LINE# #TAB# #TAB# #TAB# return getattr(handler_module, handler_path[-1])() #LINE# #TAB# return None"
Retrieves the TidyPy tools that are available in the current Python environment . The returned dictionary has keys that are the tool names and values are the tool classes . : rtype : dict <code> def get_tools(): ,"#LINE# #TAB# if not hasattr(get_tools, '_CACHE'): #LINE# #TAB# #TAB# get_tools._CACHE = dict() #LINE# #TAB# #TAB# for entry in pkg_resources.iter_entry_points('tidypy.tools'): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# get_tools._CACHE[entry.name] = entry.load() #LINE# #TAB# #TAB# #TAB# except ImportError as exc: #LINE# #TAB# #TAB# #TAB# #TAB# output_error('Could not load tool ""%s"" defined by ""%s"": %s' % #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# (entry, entry.dist, exc)) #LINE# #TAB# return get_tools._CACHE"
Returns the store being used on the editor * field * is attached <code> def get_store_for_field(field): ,"#LINE# #TAB# if not isinstance(field, Field): #LINE# #TAB# #TAB# raise TypeError('field must be a Field subclass, not %s' % (field,)) #LINE# #TAB# toplevel = field.toplevel #LINE# #TAB# if not toplevel: #LINE# #TAB# #TAB# raise TypeError('Field %r must be attached to a form' % (field,)) #LINE# #TAB# if not isinstance(toplevel, BaseEditorSlave): #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# 'Field %r must be attached to a BaseEditorSlave subclass, not %r' % #LINE# #TAB# #TAB# #TAB# (field, toplevel.__class__.__name__)) #LINE# #TAB# return toplevel.store"
Get the current set of waypoints active from boatd . : returns : The current waypoints : rtype : List of Points <code> def get_current_waypoints(boatd=None): ,#LINE# #TAB# if boatd is None: #LINE# #TAB# #TAB# boatd = Boatd() #LINE# #TAB# content = boatd.get('/waypoints') #LINE# #TAB# return [Point(*coords) for coords in content.get('waypoints')]
Function to construct default path to OutputData folder This function constructs the default path to the OutputData folder <code> def get_default_path(): ,"#LINE# #TAB# home_path = os.path.expanduser('~') #LINE# #TAB# teaser_default_path = os.path.join(home_path, 'TEASEROutput') #LINE# #TAB# return teaser_default_path"
Returns dictionary of sequence fragment lengths keyed by fragment ID  <code> def get_fragment_lengths(fastafile): ,"#LINE# #TAB# fraglengths = {} #LINE# #TAB# for seq in SeqIO.parse(fastafile, ""fasta""): #LINE# #TAB# #TAB# fraglengths[seq.id] = len(seq) #LINE# #TAB# return fraglengths"
Gets the devices count . : return : The devices count  <code> def get_device_count(): ,"#LINE# #TAB# device_count = (ctypes.c_int * 1)(*[0]) #LINE# #TAB# error_code = ctypes.c_int(0) #LINE# #TAB# error_message = ctypes.create_string_buffer(KHIVA_ERROR_LENGTH) #LINE# #TAB# KhivaLibrary().c_khiva_library.get_device_count(ctypes.pointer( #LINE# #TAB# #TAB# device_count), ctypes.pointer(error_code), error_message) #LINE# #TAB# if error_code.value != 0: #LINE# #TAB# #TAB# raise Exception(str(error_message.value.decode())) #LINE# #TAB# return device_count[0]"
"c - a - a ' : magnesium - hydroxide hydroxide sulfate [ HMW84 ]  <code> def psi_mgoh_oh_so4_hmw84(T, P): ","#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
Convert a dictionary into a CORBA namevalue list  <code> def dict_to_nvlist(dict): ,"#LINE# #TAB# result = [] #LINE# #TAB# for item in list(dict.keys()): #LINE# #TAB# #TAB# result.append(SDOPackage.NameValue(item, omniORB.any.to_any(dict[item]))) #LINE# #TAB# return result"
Check if a file is a Python source file  <code> def is_python_file(filename): ,#LINE# #TAB# if filename == '-' or filename.endswith('.py' #LINE# #TAB# #TAB# ) or SUPPORTS_IPYNB and filename.endswith('.ipynb'): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename) as fobj: #LINE# #TAB# #TAB# #TAB# first_line = fobj.readline() #LINE# #TAB# #TAB# #TAB# if first_line.startswith('#!') and 'python' in first_line: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False #LINE# #TAB# return False
"Non - byline authors for group author members <code> def authors_non_byline(soup, detail=""full""): ","#LINE# #TAB# contrib_type = ""author non-byline"" #LINE# #TAB# contributors_ = contributors(soup, detail) #LINE# #TAB# non_byline_authors = [author for author in contributors_ if author.get('type', None) == contrib_type] #LINE# #TAB# position = 1 #LINE# #TAB# for author in non_byline_authors: #LINE# #TAB# #TAB# author[""position""] = position #LINE# #TAB# #TAB# position = position + 1 #LINE# #TAB# return non_byline_authors"
"Browserify a single javascript entry point plus non - external dependencies into a single javascript file . Generates source maps in debug mode . Minifies the output in release mode  <code> def browserify_file(entry_point, output_file, babelify=False, export_as=None): ","#LINE# #TAB# from .modules import browserify #LINE# #TAB# if not isinstance(entry_point, str): #LINE# #TAB# #TAB# raise RuntimeError('Browserify File compiler takes a single entry point as input.') #LINE# #TAB# return { #LINE# #TAB# #TAB# 'dependencies_fn': browserify.browserify_deps_file, #LINE# #TAB# #TAB# 'compiler_fn': browserify.browserify_compile_file, #LINE# #TAB# #TAB# 'input': entry_point, #LINE# #TAB# #TAB# 'output': output_file, #LINE# #TAB# #TAB# 'kwargs': { #LINE# #TAB# #TAB# #TAB# 'babelify': babelify, #LINE# #TAB# #TAB# #TAB# 'export_as': export_as, #LINE# #TAB# #TAB# }, #LINE# #TAB# }"
returns a list of all possible combinations of a barcode with one mismatch <code> def all_possible_combinations(barcode): ,#LINE# #TAB# combinations = list() #LINE# #TAB# for i in range(len(barcode)): #LINE# #TAB# #TAB# newbarcode = list(barcode) #LINE# #TAB# #TAB# newbarcode[i] = 'N' #LINE# #TAB# #TAB# combinations.append(''.join(newbarcode)) #LINE# #TAB# return combinations
read flowMatrix and construct MarkovMatrix <code> def get_markov_matrix(m): ,"#LINE# #TAB# n = len(m) #LINE# #TAB# mm = np.zeros((n, n), np.float) #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# for j in range(n): #LINE# #TAB# #TAB# #TAB# if m[i, j] > 0: #LINE# #TAB# #TAB# #TAB# #TAB# mm[i, j] = float(m[i, j]) / float(m[(i), 0:].sum()) #LINE# #TAB# return mm"
"sql - code to search in party <code> def search_sa_partyname(cls, name, clause): ","#LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# tab_sql = cls.get_sql_table() #LINE# #TAB# qu1 = tab_sql.select(tab_sql.id_line, where=Operator(tab_sql.partyname, #LINE# #TAB# #TAB# clause[2])) #LINE# #TAB# return [('id', 'in', qu1)]"
Runs mafft on the given fasta format file . Returns the filename produced . : param clustered_fn : file to align . : return : filename of aligned file - if its created . None if something fails  <code> def align_with_mafft(file_to_align): ,"#LINE# #TAB# base_path = os.path.split(file_to_align)[0] #LINE# #TAB# filenamenoext = os.path.splitext(os.path.split(file_to_align)[1])[0] #LINE# #TAB# outfn = os.path.join(base_path, filenamenoext + '_aligned.fasta') #LINE# #TAB# cmd = 'mafft {} > {}'.format(file_to_align, outfn) #LINE# #TAB# try: #LINE# #TAB# #TAB# print('Going to call cmd:\n' + cmd) #LINE# #TAB# #TAB# subprocess.call(cmd, shell=True) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# print('Something went wrong while running mafft to make an alignment') #LINE# #TAB# #TAB# return None #LINE# #TAB# return outfn"
Encode all ' : ' substrings in a string ( also encodes ' $ ' so that it can be used to mark encoded characters ) . This way we can use : : to separate the two halves of an aside key  <code> def encode_v2(value): ,"#LINE# #TAB# simple = value.replace('$', '$$').replace(':', '$:') #LINE# #TAB# return simple"
This filter return info about few topics missing for moderate . Args : forum ( object ) : Forum object . Returns : int : If it is 0 there are no topics to moderate  <code> def get_tot_topics_moderate(forum): ,"#LINE# #TAB# topics_count = forum.topics_count #LINE# #TAB# idforum = forum.idforum #LINE# #TAB# moderates = Topic.objects.filter(forum_id=idforum, moderate=True).count() #LINE# #TAB# return topics_count - moderates"
Returns if array is a square matrix <code> def is_square_matrix(array): ,#LINE# #TAB# return array.shape.ndims == 2 and array.shape[0].value == array.shape[1 #LINE# #TAB# #TAB# ].value
Specializes : func:`encode ` for invocations where ` ` v ` ` is an instance of the : class:`~BaseModel ` class  <code> def encode_pydantic_model(v: BaseModel) ->Any: ,"#LINE# #TAB# return {'__kind__': kind_inst, 'class': fqname_for(v.__class__), #LINE# #TAB# #TAB# 'kwargs': encode(v.__dict__)}"
"A skeleton JSON API document , containing the basic elements but no data  <code> def json_api_document(): ","#LINE# #TAB# document = {'data': None, 'jsonapi': {'version': JSONAPI_VERSION}, #LINE# #TAB# #TAB# 'links': {}, 'meta': {}, 'included': []} #LINE# #TAB# return document"
Get the currently checked - out branch <code> def get_current_branch(repo): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# branch = repo.get_branch() #LINE# #TAB# except GitRepositoryError: #LINE# #TAB# #TAB# branch = None #LINE# #TAB# return branch
"Creates a dictionary representing the edges in the graph and formats it in such a way that it can be encoded into JSON for comparing the graph objects between versions of GOcats  <code> def json_format_graph(graph_object, graph_identifier): ",#LINE# #TAB# json_dict = dict() #LINE# #TAB# for edge in graph_object.edge_list: #LINE# #TAB# #TAB# json_dict[str(graph_identifier) + '_' + edge.json_edge[0] #LINE# #TAB# #TAB# #TAB# ] = edge.json_edge[1] #LINE# #TAB# return json_dict
"Attempts to accept ( ) on the descriptor , returns a client , address tuple if it succeeds ; returns None if it needs to trampoline , and raises any exceptions  <code> def socket_accept(descriptor): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return descriptor.accept() #LINE# #TAB# except socket.error as e: #LINE# #TAB# #TAB# if get_errno(e) == errno.EWOULDBLOCK: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# raise
ROS geometry_msgs.msg . Transform instance <code> def ros_transform(dual_quat): ,"#LINE# #TAB# transform_msg = geometry_msgs.msg.Transform() #LINE# #TAB# quat_pose_arr = dual_quat.quat_pose_array() #LINE# #TAB# rot = quat_pose_arr[:4] #LINE# #TAB# tra = quat_pose_arr[4:] #LINE# #TAB# transform_msg.translation = geometry_msgs.msg.Vector3(*tra) #LINE# #TAB# transform_msg.rotation = geometry_msgs.msg.Quaternion(rot[1], rot[2], #LINE# #TAB# #TAB# rot[3], rot[0]) #LINE# #TAB# return transform_msg"
Helper function that merges the _ blocks attribute of a ds - array into a single ndarray / sparse matrix  <code> def merge_blocks(blocks): ,"#LINE# #TAB# sparse = None #LINE# #TAB# b0 = blocks[0][0] #LINE# #TAB# if sparse is None: #LINE# #TAB# #TAB# sparse = issparse(b0) #LINE# #TAB# if sparse: #LINE# #TAB# #TAB# ret = sp.bmat(blocks, format=b0.getformat(), dtype=b0.dtype) #LINE# #TAB# else: #LINE# #TAB# #TAB# ret = np.block(blocks) #LINE# #TAB# return ret"
Non recursive algorithm Based on pop from old and append elements to new list <code> def zart_flatten(a: Iterable) ->List: ,"#LINE# #TAB# queue, out = [a], [] #LINE# #TAB# while queue: #LINE# #TAB# #TAB# elem = queue.pop(-1) #LINE# #TAB# #TAB# if isinstance(elem, list): #LINE# #TAB# #TAB# #TAB# queue.extend(elem) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# out.append(elem) #LINE# #TAB# return out[::-1]"
"Attributes for resource creation  <code> def create_attributes(klass, attributes, previous_object=None): ",#LINE# #TAB# if 'fields' not in attributes: #LINE# #TAB# #TAB# if previous_object is None: #LINE# #TAB# #TAB# #TAB# attributes['fields'] = {} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# attributes['fields'] = previous_object.to_json()['fields'] #LINE# #TAB# return {'fields': attributes['fields']}
"Makes a Flask response with a JSON encoded body <code> def output_json_pretty(request, data, code, headers=None): ","#LINE# #TAB# current_app = request.app #LINE# #TAB# settings = current_app.config.get('RESTPLUS_JSON', {}) #LINE# #TAB# if current_app.debug: #LINE# #TAB# #TAB# settings.setdefault('indent', 4) #LINE# #TAB# dumped = dumps(data, **settings) + '\n' #LINE# #TAB# resp = text(dumped, code, headers, content_type='application/json') #LINE# #TAB# return resp"
Reindex subdomain dataframe and starts at indice 1  <code> def reindex_df(subdomain_df): ,"#LINE# #TAB# subdomain_df.drop_duplicates(inplace=True) #LINE# #TAB# subdomain_df.reset_index(drop=True, inplace=True) #LINE# #TAB# subdomain_df.index += 1 #LINE# #TAB# return subdomain_df"
"Web sites often gives the secret with spaces between 4-char words . This method normalizes the received key into one parseable by base64 , and handles padding . : param key : the original key . : return : a normalized b32 string  <code> def b32_normalize(key: str) ->str: ","#LINE# #TAB# key = key.upper().replace(' ', '') #LINE# #TAB# key += '=' * (-len(key) % 8) #LINE# #TAB# return key"
"c - a - a ' : potassium bicarbonate sulfate [ HMW84 ]  <code> def psi_k_hco3_so4_hmw84(T, P): ","#LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"Return a libcloud node for the named VM <code> def get_node(conn, name): ","#LINE# #TAB# nodes = conn.list_nodes() #LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if node.name == name: #LINE# #TAB# #TAB# #TAB# __utils__['cloud.cache_node'](salt.utils.data.simple_types_filter(node.__dict__), __active_provider_name__, __opts__) #LINE# #TAB# #TAB# #TAB# return node"
"Prepare an image for and ` ` date ` ` from an ImageCollection <code> def prep_collection_image(imgcol, collection, date, validate=False): ","#LINE# #TAB# date_end = date + dt.timedelta(days=1) #LINE# #TAB# imgcol_ = common.filter_collection_time(imgcol, date, date_end) #LINE# #TAB# if validate: #LINE# #TAB# #TAB# _ = common.get_collection_uniq_dates(imgcol_) #LINE# #TAB# #TAB# assert len(_) == 1 #LINE# #TAB# img = imgcol_.mosaic() #LINE# #TAB# keys = METADATA[collection] #LINE# #TAB# meta = _imgcol_metadata(imgcol_, keys) #LINE# #TAB# return img, meta"
if we have a valid ' search < source > ' statement <code> def line_search_subject_is_valid(line): ,#LINE# #TAB# if len(line.split()) == 2 and line.split()[1] in G.sources(): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
Given a sequence of lines for each line that contains a ANSI color escape sequence without a reset add a reset to the end of that line and copy all colors in effect at the end of it to the beginning of the next line  <code> def carry_over_color(lines): ,"#LINE# #TAB# lines2 = [] #LINE# #TAB# in_effect = '' #LINE# #TAB# for s in lines: #LINE# #TAB# #TAB# s = in_effect + s #LINE# #TAB# #TAB# in_effect = '' #LINE# #TAB# #TAB# m = re.search(COLOR_BEGIN_RGX + '(?:(?!' + COLOR_END_RGX + ').)*$', s) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# s += '\033[m' #LINE# #TAB# #TAB# #TAB# in_effect = ''.join(re.findall(COLOR_BEGIN_RGX, m.group(0))) #LINE# #TAB# #TAB# lines2.append(s) #LINE# #TAB# return lines2"
"Takes a relative plain - text day and time zone , returns a datetime object corresponding to the colloquial date in the specified timezone <code> def relative_datetime(relPeriod='today', tz=None): ","#LINE# #TAB# tz = 'America/Los_Angeles' if tz is None else tz #LINE# #TAB# cal = parsedatetime.Calendar() #LINE# #TAB# datetimeObj, _ = cal.parseDT(datetimeString=relPeriod, tzinfo=timezone(tz)) #LINE# #TAB# return datetimeObj"
Load an existing NER model ( with weights ) from HDF5 file . : param path : String . The path to the pre - defined model . : return : NER  <code> def load_model(path): ,"#LINE# #TAB# labor = ner_model.NERCRF(use_cudnn=False) #LINE# #TAB# model = TextKerasModel._load_model(labor, path) #LINE# #TAB# model.__class__ = NER #LINE# #TAB# return model"
Return ` ` True ` ` if colored output is possible / requested and not running in GUI . Colored output can be explicitly requested by setting : envvar:`COCOTB_ANSI_OUTPUT ` to ` ` 1 ` `  <code> def want_color_output(): ,"#LINE# #TAB# want_color = sys.stdout.isatty() #LINE# #TAB# if os.getenv('COCOTB_ANSI_OUTPUT', default='0') == '1': #LINE# #TAB# #TAB# want_color = True #LINE# #TAB# if os.getenv('GUI', default='0') == '1': #LINE# #TAB# #TAB# want_color = False #LINE# #TAB# return want_color"
Test if a string represents a float  <code> def is_float(s): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# float(s) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"Convert a color to an RGB tuple . This accepts the same input formats as convert_color , but raises an exception if the alpha value is not 1  <code> def convert_color_rgb(c: Union[str, tuple]) ->Tuple[float, float, float]: ",#LINE# #TAB# c = convert_color(c) #LINE# #TAB# if abs(c[3] - 1.0) > 0.0001: #LINE# #TAB# #TAB# raise ValueError('Color may not have an alpha component.') #LINE# #TAB# return c[:3]
"Class numbers are assumed to be between 0 and k-1 <code> def threshold_predict(X, w, theta): ","#LINE# #TAB# tmp = theta[:, (None)] - np.asarray(X.dot(w)) #LINE# #TAB# pred = np.sum(tmp < 0, axis=0).astype(np.int) #LINE# #TAB# return pred"
Supply references to initial variantcalls if run in addition to batching  <code> def fix_orig_vcf_refs(data): ,"#LINE# #TAB# variantcaller = tz.get_in((""config"", ""algorithm"", ""variantcaller""), data) #LINE# #TAB# if variantcaller: #LINE# #TAB# #TAB# data[""vrn_file_orig""] = data[""vrn_file""] #LINE# #TAB# for i, sub in enumerate(data.get(""group_orig"", [])): #LINE# #TAB# #TAB# sub_vrn = sub.pop(""vrn_file"", None) #LINE# #TAB# #TAB# if sub_vrn: #LINE# #TAB# #TAB# #TAB# sub[""vrn_file_orig""] = sub_vrn #LINE# #TAB# #TAB# #TAB# data[""group_orig""][i] = sub #LINE# #TAB# return data"
"draws building heat balance graph : param analysis_fields : : param data_frame : : return : <code> def calc_graph(analysis_fields, data_frame): ","#LINE# #TAB# graph = [] #LINE# #TAB# for field in analysis_fields: #LINE# #TAB# #TAB# y = data_frame[field] #LINE# #TAB# #TAB# trace = go.Bar(x=data_frame.index, y=y, name=field.split('_kWh', 1) #LINE# #TAB# #TAB# #TAB# [0], marker=dict(color=COLOR[field])) #LINE# #TAB# #TAB# graph.append(trace) #LINE# #TAB# return graph"
"Handler function for radius keyword with smart conversion to a degrees value <code> def handle_radius(url, key, radius): ","#LINE# #TAB# assert key == 'radius' #LINE# #TAB# if isinstance(radius, (int, float)): #LINE# #TAB# #TAB# radius = radius * units.deg #LINE# #TAB# radius = astropy.coordinates.Angle(radius) #LINE# #TAB# if radius is not None: #LINE# #TAB# #TAB# return '%s/sr=%fd' % (url, radius.deg) #LINE# #TAB# return url"
"Page lacking interwikis generator . @param total : Maximum number of pages to retrieve in total @param site : Site for generator results . @type site : L{pywikibot.site . BaseSite } <code> def without_interwiki_page_generator(total=None, site=None): ",#LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page in site.withoutinterwiki(total=total): #LINE# #TAB# #TAB# yield page
Retrieves the login credentials from the computer you are on . Good for command line tools <code> def localsys_get_login(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# sid = local_credentials.get_password('minervaclient_sid', 'minerva') #LINE# #TAB# #TAB# pin = local_credentials.get_password('minervalcient_pin', 'minerva') #LINE# #TAB# #TAB# if sid is None or pin is None: #LINE# #TAB# #TAB# #TAB# raise Exception('No credentials detected') #LINE# #TAB# #TAB# return sid, pin #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(str(e)) #LINE# #TAB# #TAB# return '', ''"
Create a tissue target space from a given image . The image is assumed to contain a black - colored tissue space in white background . image -- the location of the image on the disk  <code> def create_target_space_from_image(image): ,"#LINE# #TAB# img = imread(image) #LINE# #TAB# img_width = img.shape[1] #LINE# #TAB# img_height = img.shape[0] #LINE# #TAB# locations = np.array([(x, y) for x in range(img_width) for y in range( #LINE# #TAB# #TAB# img_height) if sum(img[(y), (x), :] == np.array([0, 0, 0]))]) #LINE# #TAB# return locations"
Choose a node of the ` Expression ` such that no property leading to a data has to be reversed ( with ! )  <code> def choose_start_node(e): ,#LINE# #TAB# for node in e.iter_nodes(): #LINE# #TAB# #TAB# if list(e.iter_edges(node)): #LINE# #TAB# #TAB# #TAB# return node #LINE# #TAB# return node
"Set valid properties for the artist dropping the others  <code> def set_valid_props(artist, kwargs): ","#LINE# #TAB# artist.set(**{k: kwargs[k] for k in kwargs if hasattr(artist, ""set_"" + k)}) #LINE# #TAB# return artist"
"Unique elements preserving order  <code> def unique_everseen(iterable, filterfalse_=itertools.filterfalse): ","#LINE# #TAB# seen = set() #LINE# #TAB# seen_add = seen.add #LINE# #TAB# for element in filterfalse_(seen.__contains__, iterable): #LINE# #TAB# #TAB# seen_add(element) #LINE# #TAB# #TAB# yield element"
"Substitute environment variables and split into list  <code> def subst_libs(env, libs): ","#LINE# #TAB# if SCons.Util.is_String(libs): #LINE# #TAB# #TAB# libs = env.subst(libs) #LINE# #TAB# #TAB# if SCons.Util.is_String(libs): #LINE# #TAB# #TAB# #TAB# libs = libs.split() #LINE# #TAB# elif SCons.Util.is_Sequence(libs): #LINE# #TAB# #TAB# _libs = [] #LINE# #TAB# #TAB# for l in libs: #LINE# #TAB# #TAB# #TAB# _libs += subst_libs(env, l) #LINE# #TAB# #TAB# libs = _libs #LINE# #TAB# else: #LINE# #TAB# #TAB# libs = [libs] #LINE# #TAB# return libs"
convert a boolean value to text <code> def ods_bool_value(value): ,#LINE# #TAB# if value is True: #LINE# #TAB# #TAB# return 'true' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'false'
Determine AuthservID if needed <code> def make_authserv_id(as_id): ,#LINE# #TAB# if as_id == 'HOSTNAME': #LINE# #TAB# #TAB# as_id = socket.gethostname() #LINE# #TAB# return as_id
"Applies all expansion mechanisms to the given path  <code> def expand_path(path, opts=None): ","#LINE# precondition.AssertType(path, Text) #LINE# for grouped_path in ExpandGroups(path): #LINE# #TAB# for globbed_path in ExpandGlobs(grouped_path, opts): #LINE# #TAB# yield globbed_path"
Set the maximum number of pending measurements allowed in the buffer before new measurements are discarded . : param int limit : The maximum number of measurements per batch <code> def set_max_buffer_size(limit): ,"#LINE# #TAB# global _max_buffer_size #LINE# #TAB# LOGGER.debug('Setting maximum buffer size to %i', limit) #LINE# #TAB# _max_buffer_size = limit"
Use the API to check for an existing fork <code> def check_forked(orgrepo): ,"#LINE# #TAB# repository = orgrepo.split('/', 1)[-1] #LINE# #TAB# if check_forked_direct(repository): #LINE# #TAB# #TAB# return True #LINE# #TAB# api = get_api() #LINE# #TAB# repo = api.get_repo(orgrepo) #LINE# #TAB# while repo.parent and not repo.parent.archived: #LINE# #TAB# #TAB# repo = repo.parent #LINE# #TAB# #TAB# orgrepo = repo.full_name #LINE# #TAB# #TAB# repository = orgrepo.split('/', 1)[-1] #LINE# #TAB# #TAB# if check_forked_direct(repository): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Returns an * RGBA * tuple of 4 ints from 0 - 255 <code> def to_rgba_255(colorname, alpha=255): ","#LINE# #TAB# rgb = to_rgb_255(colorname) #LINE# #TAB# return [rgb[0], rgb[1], rgb[2], alpha]"
"Parses a whitespace - delimited string of "" [ user@]host[:port ] "" entries . Returns a list of ( host , port , user ) triples  <code> def parse_host_string(host_string, default_user=None, default_port=None): ","#LINE# #TAB# hosts = [] #LINE# #TAB# entries = host_string.split() #LINE# #TAB# for entry in entries: #LINE# #TAB# #TAB# hosts.append(parse_host(entry, default_user, default_port)) #LINE# #TAB# return hosts"
return JUST the comment lines from a csv file <code> def get_comments(f): ,#LINE# #TAB# for l in f: #LINE# #TAB# #TAB# line = l.rstrip() #LINE# #TAB# #TAB# if line and line.startswith('#'): #LINE# #TAB# #TAB# #TAB# yield line
"Converts a float X.Y into ( X , Y )  <code> def float_version(f): ","#LINE# #TAB# assert not f < 0 #LINE# #TAB# major = floor(f) #LINE# #TAB# minor = int((f - major) * 10) #LINE# #TAB# return major, minor"
Check this is a valid 2D pixel list <code> def pixel_2d_check(value): ,#LINE# #TAB# if len(value) != 2: #LINE# #TAB# #TAB# raise ValidationError('must have 2 elements') #LINE# #TAB# return value
"Given a date in YYYY - MM - DD , returns a Python date object . Throws a ValueError if the date is invalid  <code> def date_string_to_date(p_date): ","#LINE# #TAB# result = None #LINE# #TAB# if p_date: #LINE# #TAB# #TAB# parsed_date = re.match('(\\d{4})-(\\d{2})-(\\d{2})', p_date) #LINE# #TAB# #TAB# if parsed_date: #LINE# #TAB# #TAB# #TAB# result = date(int(parsed_date.group(1)), int(parsed_date.group( #LINE# #TAB# #TAB# #TAB# #TAB# 2)), int(parsed_date.group(3))) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# return result"
"filePropertiesDictionary(sql_raw_list ) transforms a row gotten via SQL request ( list ) , to a dictionary <code> def file_properties_dictionary(sql_raw_list): ","#LINE# #TAB# if sql_raw_list[5] == 0: #LINE# #TAB# #TAB# is_deployed = False #LINE# #TAB# else: #LINE# #TAB# #TAB# is_deployed = True #LINE# #TAB# properties_dictionary = {'id': sql_raw_list[0], 'model_version_id': #LINE# #TAB# #TAB# sql_raw_list[1], 'absolute_path': sql_raw_list[2], #LINE# #TAB# #TAB# 'file_commit_state': sql_raw_list[3], 'last_modified_time': #LINE# #TAB# #TAB# sql_raw_list[4], 'is_deployed': is_deployed} #LINE# #TAB# return properties_dictionary"
"Finds the area under a polynomial between the specified bounds using a rectangle - sum ( of width 1 ) approximation  <code> def area_under_curve(poly, bounds, algorithm): ","#LINE# #TAB# LOGGER.info(poly) #LINE# #TAB# LOGGER.info(bounds) #LINE# #TAB# LOGGER.info(f'Algorithm: {algorithm.__name__}') #LINE# #TAB# range_upper_index = len(bounds.full_range) - 1 #LINE# #TAB# total_area = 0 #LINE# #TAB# for range_index, val in enumerate(bounds.full_range): #LINE# #TAB# #TAB# if range_index == range_upper_index: #LINE# #TAB# #TAB# #TAB# return total_area #LINE# #TAB# #TAB# total_area += algorithm(poly, val, bounds.full_range[range_index + 1]) #LINE# #TAB# return total_area"
"This function contains the logic used to check if the thread should be restarted . Beware , this function * must not * call any hooks  <code> def should_start_runner_thread(): ",#LINE# #TAB# global RUNNER_THREAD #LINE# #TAB# global RUNNER_THREAD_PID #LINE# #TAB# runner_stopped = False #LINE# #TAB# if (RUNNER_THREAD is not None and RUNNER_THREAD.runner is not None and #LINE# #TAB# #TAB# RUNNER_THREAD.runner.stop is True): #LINE# #TAB# #TAB# runner_stopped = True #LINE# #TAB# return RUNNER_THREAD_PID != os.getpid() and not runner_stopped
Yield valid expressions from text  <code> def get_expressions(text): ,"#LINE# #TAB# for line in text.splitlines(): #LINE# #TAB# #TAB# line_text = line.strip() #LINE# #TAB# #TAB# if not line_text or line_text.startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# expr_text = line_text.split('#', 1)[0].strip() #LINE# #TAB# #TAB# yield expr_text"
Metadata for notebook bundlerextension <code> def jupyter_bundlerextension_paths(): ,"#LINE# #TAB# return [{'name': 'tarball_bundler', 'module_name': #LINE# #TAB# #TAB# 'notebook.bundler.tarball_bundler', 'label': #LINE# #TAB# #TAB# 'Notebook Tarball (tar.gz)', 'group': 'download'}]"
"Projects along rowspace of B onto rowspace of C <code> def project_oblique(B, C): ",#LINE# #TAB# proj_B_perp = project_perp(B) #LINE# #TAB# return proj_B_perp * (C * proj_B_perp).I * C
"Number of basis functions in the basis set . Returns ------- int The number of basis functions  <code> def calculate_nbf(cls, atom_map, center_data) ->int: ","#LINE# #TAB# center_count = {} #LINE# #TAB# for k, center in center_data.items(): #LINE# #TAB# #TAB# center_count[k] = sum(x.nfunctions() for x in center.electron_shells) #LINE# #TAB# ret = 0 #LINE# #TAB# for center in atom_map: #LINE# #TAB# #TAB# ret += center_count[center] #LINE# #TAB# return ret"
Flatten the tree into a list of properties adding parents as prefixes  <code> def flat_model(tree): ,"#LINE# #TAB# names = [] #LINE# #TAB# for columns in viewvalues(tree): #LINE# #TAB# #TAB# for col in columns: #LINE# #TAB# #TAB# #TAB# if isinstance(col, dict): #LINE# #TAB# #TAB# #TAB# #TAB# col_name = list(col)[0] #LINE# #TAB# #TAB# #TAB# #TAB# names += [(col_name + '__' + c) for c in flat_model(col)] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# names.append(col) #LINE# #TAB# return names"
"Parse a tsv header : param line : the header text : return : ( column_names list , column_indices dict ) <code> def header_from_line(line): ","#LINE# #TAB# if len(line) == 0: #LINE# #TAB# #TAB# return [], {} #LINE# #TAB# column_names = line.rstrip('\n').split('\t') #LINE# #TAB# column_indices = dict([(name, index) for index, name in enumerate( #LINE# #TAB# #TAB# column_names)]) #LINE# #TAB# return column_names, column_indices"
"Comments must be converted to < xsl : comment > to be output , doing it early allows them to get an xml : id so they can be matched in the theme . The theme selector needs rewriting to replace comment ( ) with xsl : comment <code> def fixup_theme_comment_selectors(rules): ","#LINE# #TAB# for element in rules.xpath(""//@theme[contains(., 'comment()')]/..""): #LINE# #TAB# #TAB# element.attrib['theme'] = element.attrib['theme'].replace('comment()', #LINE# #TAB# #TAB# #TAB# 'xsl:comment') #LINE# #TAB# return rules"
"Find files corresponding to urls  <code> def find_files(dl_paths, publisher, url_dict): ","#LINE# if publisher == 'cnn': #LINE# #TAB# top_dir = os.path.join(dl_paths['cnn_stories'], 'cnn', 'stories') #LINE# elif publisher == 'dm': #LINE# #TAB# top_dir = os.path.join(dl_paths['dm_stories'], 'dailymail', 'stories') #LINE# else: #LINE# #TAB# logging.fatal('Unsupported publisher: %s', publisher) #LINE# files = tf.io.gfile.listdir(top_dir) #LINE# ret_files = [] #LINE# for p in files: #LINE# #TAB# basename = os.path.basename(p) #LINE# #TAB# if basename[0:basename.find('.story')] in url_dict: #LINE# #TAB# ret_files.append(os.path.join(top_dir, p)) #LINE# return ret_files"
"Read dataset from single sheet as dataframe ( or list of dataframe ) <code> def data_from_sheet(df, station_name, as_df=True): ","#LINE# #TAB# n_years = int(df.iloc[0, 1]) #LINE# #TAB# frames = [] #LINE# #TAB# for i in range(2, n_years * 33, 33): #LINE# #TAB# #TAB# year = int(df.iloc[i, 1]) #LINE# #TAB# #TAB# pivot = df.iloc[i:i + 31, 4:16] #LINE# #TAB# #TAB# data = _yearly_df(pivot, year, station_name) #LINE# #TAB# #TAB# frames.append(data) #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return pd.concat(frames, sort=True) #LINE# #TAB# else: #LINE# #TAB# #TAB# return frames"
"Hack to get relevant arguments for lineage computation . We need a better way to determine the relevant arguments of an expression  <code> def get_args(op, name): ","#LINE# #TAB# if isinstance(op, ops.Selection): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# result = op.selections #LINE# #TAB# #TAB# return [col for col in result if col._name == name] #LINE# #TAB# elif isinstance(op, ops.Aggregation): #LINE# #TAB# #TAB# assert name is not None, 'name is None' #LINE# #TAB# #TAB# return [col for col in itertools.chain(op.by, op.metrics) if col. #LINE# #TAB# #TAB# #TAB# _name == name] #LINE# #TAB# else: #LINE# #TAB# #TAB# return op.args"
Import the specified modules and provide status  <code> def check_import(module_names): ,#LINE# #TAB# diagnostics = {} #LINE# #TAB# for module_name in module_names: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# __import__(module_name) #LINE# #TAB# #TAB# #TAB# res = 'ok' #LINE# #TAB# #TAB# except ImportError as err: #LINE# #TAB# #TAB# #TAB# res = str(err) #LINE# #TAB# #TAB# diagnostics[module_name] = res #LINE# #TAB# return diagnostics
Write MULTI block data  <code> def write_multi(parameters): ,"#LINE# #TAB# from ._common import eos #LINE# #TAB# out = list(eos[parameters['eos']]) if parameters['eos'] else [0, 0, 0, 6] #LINE# #TAB# out[0] = parameters['n_component'] if parameters['n_component'] else out[0] #LINE# #TAB# out[1] = out[0] if parameters['isothermal'] else out[0] + 1 #LINE# #TAB# out[2] = parameters['n_phase'] if parameters['n_phase'] else out[2] #LINE# #TAB# if parameters['diffusion']: #LINE# #TAB# #TAB# out[3] = 8 #LINE# #TAB# #TAB# parameters['n_phase'] = out[2] #LINE# #TAB# if parameters['n_component_mass']: #LINE# #TAB# #TAB# out.append(parameters['n_component_mass']) #LINE# #TAB# return [('{:>5d}' * len(out) + '\n').format(*out)]"
"Detect all configurations / channels that this interface could currently connect with . : rtype : Iterator[dict ] : return : an iterable of dicts , each being a configuration suitable for usage in the interface 's bus constructor  <code> def detect_available_configs(): ","#LINE# #TAB# if ics is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# try: #LINE# #TAB# #TAB# devices = ics.find_devices() #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logger.debug('Failed to detect configs: %s', e) #LINE# #TAB# #TAB# return [] #LINE# #TAB# return [{'interface': 'neovi', 'serial': NeoViBus.get_serial_number( #LINE# #TAB# #TAB# device)} for device in devices]"
"Read the member in struct Index in nnet3 / nnet - common.h Return a tuple ( n , t , x ) <code> def read_index_tuple(fd): ","#LINE# #TAB# n = read_int32(fd) #LINE# #TAB# t = read_int32(fd) #LINE# #TAB# x = read_int32(fd) #LINE# #TAB# return n, t, x"
Extracts currency from value  <code> def get_currency(value): ,"#LINE# #TAB# if isinstance(value, MONEY_CLASSES): #LINE# #TAB# #TAB# return smart_str(value.currency) #LINE# #TAB# elif isinstance(value, (list, tuple)): #LINE# #TAB# #TAB# return value[1]"
Get a plotting backend  <code> def get_backend(name: str = None): ,"#LINE# #TAB# if not backends: #LINE# #TAB# #TAB# raise RuntimeError(""No plotting backend available. Please, install matplotlib (preferred) or bokeh (limited)."") #LINE# #TAB# if not name: #LINE# #TAB# #TAB# name = _default_backend #LINE# #TAB# if name == ""bokeh"": #LINE# #TAB# #TAB# raise RuntimeError(""Support for bokeh has been discontinued. At some point, we may return to support holoviews."") #LINE# #TAB# backend = backends.get(name) #LINE# #TAB# if not backend: #LINE# #TAB# #TAB# raise RuntimeError(""Backend {0} does not exist. Use one of the following: {1}"".format(name, "", "".join(backends.keys()))) #LINE# #TAB# return name, backends[name]"
"Prepare group by list item . Convert replaces any special "" dimensions # "" , "" meta # "" or "" value_meta # "" occurrences into spark sql syntax to retrieve data from extra_data_map column  <code> def prepare_group_by_item(item): ","#LINE# #TAB# if item.startswith('dimensions#') or item.startswith('meta#' #LINE# #TAB# #TAB# ) or item.startswith('value_meta#'): #LINE# #TAB# #TAB# return '.'.join(('extra_data_map', item)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return item"
Get celery schedule  <code> def get_schedule(_sched): ,"#LINE# #TAB# dyn_schedule = dict() #LINE# #TAB# for source in get_sources(): #LINE# #TAB# #TAB# dyn_schedule[source['name']] = {'task': 'openkongqi.tasks.scrape', #LINE# #TAB# #TAB# #TAB# 'schedule': _sched, 'args': (source['name'],)} #LINE# #TAB# return dyn_schedule"
"Decodes int value from RLP format : param rlp : RLP bytes : return : tuple ( value , length ) <code> def decode_int(rlp, start): ","#LINE# #TAB# _, value_len, value_start = consume_length_prefix(rlp, start) #LINE# #TAB# value_bytes = rlp[value_start:value_start + value_len] #LINE# #TAB# if len(value_bytes) == 0: #LINE# #TAB# #TAB# value = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# value = big_endian_to_int(value_bytes) #LINE# #TAB# return value, value_start - start + value_len"
"Resolves the IP for a given hostname or returns the input if it is already an IP  <code> def get_host_ip(hostname, fallback=None): ","#LINE# #TAB# if is_ip(hostname): #LINE# #TAB# #TAB# return hostname #LINE# #TAB# ip_addr = ns_query(hostname) #LINE# #TAB# if not ip_addr: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ip_addr = socket.gethostbyname(hostname) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# log(""Failed to resolve hostname '%s'"" % (hostname), #LINE# #TAB# #TAB# #TAB# #TAB# level=WARNING) #LINE# #TAB# #TAB# #TAB# return fallback #LINE# #TAB# return ip_addr"
"Read configuration from INI file . Parameters ---------- file_path path of the configuration file force force reading of the file , fail if not found <code> def read_configfile(file_path: str, force: bool=True) ->ConfigParser: ","#LINE# #TAB# c = ConfigParser() #LINE# #TAB# c.optionxform = str #LINE# #TAB# if force: #LINE# #TAB# #TAB# with open(file_path, 'r') as f: #LINE# #TAB# #TAB# #TAB# c.read_file(f) #LINE# #TAB# else: #LINE# #TAB# #TAB# c.read(file_path) #LINE# #TAB# return c"
"compute gradient of biomass_per_flux function <code> def neg_grad_biomass_per_flux(flux_vec, biomass_index): ",#LINE# #TAB# denominator = (flux_vec ** 2).sum() #LINE# #TAB# sqdenom = denominator * denominator #LINE# #TAB# if sqdenom < CUTOFF: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# factor = 2.0 * flux_vec[biomass_index] / sqdenom #LINE# #TAB# solution = flux_vec * factor #LINE# #TAB# solution[biomass_index] -= 1.0 / denominator #LINE# #TAB# return solution
"Decorrelate colors of image . : param image : Input image . : param color_correlation_svd_sqrt : Color correlation matrix . : return : Decorrelated image  <code> def decorrelate_colors(image, color_correlation_svd_sqrt): ","#LINE# #TAB# max_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, #LINE# #TAB# #TAB# axis=0)) #LINE# #TAB# image_flat = tf.reshape(image, [-1, 3]) #LINE# #TAB# color_correlation_normalized = (color_correlation_svd_sqrt / #LINE# #TAB# #TAB# max_norm_svd_sqrt) #LINE# #TAB# image_flat = tf.matmul(image_flat, color_correlation_normalized.T) #LINE# #TAB# image = tf.reshape(image_flat, tf.shape(image)) #LINE# #TAB# return image"
"Return True if the store contains an array at the given logical path  <code> def contains_array(store, path=None): ",#LINE# #TAB# path = normalize_storage_path(path) #LINE# #TAB# prefix = _path_to_prefix(path) #LINE# #TAB# key = prefix + array_meta_key #LINE# #TAB# return key in store
Cast a arrays to a shared dtype using xarray s type promotion rules  <code> def as_shared_dtype(scalars_or_arrays): ,"#LINE# #TAB# arrays = [asarray(x) for x in scalars_or_arrays] #LINE# #TAB# out_type = dtypes.result_type(*arrays) #LINE# #TAB# return [x.astype(out_type, copy=False) for x in arrays]"
Return information about server health  <code> def get_heartbeat(request): ,"#LINE# #TAB# status = {} #LINE# #TAB# heartbeats = request.registry.heartbeats #LINE# #TAB# for name, callable in heartbeats.items(): #LINE# #TAB# #TAB# status[name] = callable(request) #LINE# #TAB# has_error = not all([(v or v is None) for v in status.values()]) #LINE# #TAB# if has_error: #LINE# #TAB# #TAB# request.response.status = 503 #LINE# #TAB# return status"
"Normalize a string into multiple lines  <code> def normalize_string(string, length): ","#LINE# #TAB# lines = normalize([''], string, length) #LINE# #TAB# s = lines[0] #LINE# #TAB# for line in lines[1:]: #LINE# #TAB# #TAB# s += '\n' + line #LINE# #TAB# return s"
Allocate new Netlink socket . Does not yet actually open a socket  <code> def nl_socket_alloc(cb=None): ,"#LINE# #TAB# cb = cb or nl_cb_alloc(default_cb) #LINE# #TAB# if not cb: #LINE# #TAB# #TAB# return None #LINE# #TAB# sk = nl_sock() #LINE# #TAB# sk.s_cb = cb #LINE# #TAB# sk.s_local.nl_family = getattr(socket, 'AF_NETLINK', -1) #LINE# #TAB# sk.s_peer.nl_family = getattr(socket, 'AF_NETLINK', -1) #LINE# #TAB# sk.s_seq_expect = sk.s_seq_next = int(time.time()) #LINE# #TAB# sk.s_flags = NL_OWN_PORT #LINE# #TAB# nl_socket_get_local_port(sk) #LINE# #TAB# return sk"
"Default WSGI 404 app  <code> def not_found(environ, start_response): ","#LINE# #TAB# start_response('404 Not Found', [('Content-Type', 'text/plain')]) #LINE# #TAB# return [ #LINE# #TAB# #TAB# '404 Not Found\n\nThe server has not found anything matching the Request-URI.' #LINE# #TAB# #TAB# ]"
Read metadata from a HDF5 file Parameters ---------- filename : path <code> def read_metadata(filename): ,"#LINE# #TAB# metadata = MetaData() #LINE# #TAB# with open_file(filename) as file: #LINE# #TAB# #TAB# for k in metadata.keys(): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# metadata[k] = file.root._v_attrs[k] #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# print('Metadata {} does not exist in file {}'.format(k, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# filename)) #LINE# #TAB# return metadata"
"Compute a random base of the given trace set . Returns unit L2-norm base of shape ` ( dims , tot_actions ) ` . Optionally weights the actions by weight_vec <code> def common_random_base(cls, traces, weight_vec=None, dims=2, seed=None): ","#LINE# #TAB# base = np.random.normal(size=(dims, len(traces[0].d_strat[0]))) #LINE# #TAB# if weight_vec is not None: #LINE# #TAB# #TAB# base *= [weight_vec] #LINE# #TAB# base = base / sp.linalg.norm(base, axis=1).reshape((-1, 1)) #LINE# #TAB# assert base.shape[0] == dims #LINE# #TAB# return base"
"Set the uids in a PIF , explicitly if the list of UIDs is passed in : param pifs : to set UIDs in : param uids : to set ; defaults to a hash of the object : return : <code> def set_uids(pifs, uids=None): ","#LINE# #TAB# if not uids: #LINE# #TAB# #TAB# uids = [str(hash(dumps(x))) for x in pifs] #LINE# #TAB# for pif, uid in zip(pifs, uids): #LINE# #TAB# #TAB# pif.uid = uid #LINE# #TAB# return pifs"
"Helper function to create the structure for absolute values  <code> def create_absolute_values_structure(layer, fields): ","#LINE# #TAB# source_fields = layer.keywords['inasafe_fields'] #LINE# #TAB# absolute_fields = [field['key'] for field in count_fields] #LINE# #TAB# summaries = {} #LINE# #TAB# for field in source_fields: #LINE# #TAB# #TAB# if field in absolute_fields: #LINE# #TAB# #TAB# #TAB# field_name = source_fields[field] #LINE# #TAB# #TAB# #TAB# index = layer.fields().lookupField(field_name) #LINE# #TAB# #TAB# #TAB# flat_table = FlatTable(*fields) #LINE# #TAB# #TAB# #TAB# summaries[index] = (flat_table, field) #LINE# #TAB# return summaries"
Execute a terminal command and return the stdout  <code> def execute_command(cmd: str) ->str: ,"#LINE# #TAB# p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE) #LINE# #TAB# stdout, _ = p.communicate() #LINE# #TAB# return stdout.decode()[:-1]"
"Compare and return value <code> def greater_than(data, loperand, roperand, is_not=False, extras=None): ","#LINE# #TAB# value = get_field_value(data, loperand) #LINE# #TAB# gt = False #LINE# #TAB# if value: #LINE# #TAB# #TAB# if extras: #LINE# #TAB# #TAB# #TAB# value = apply_extras(value, extras) #LINE# #TAB# #TAB# if type(value) == type(roperand) and value > roperand: #LINE# #TAB# #TAB# #TAB# gt = True #LINE# #TAB# if is_not: #LINE# #TAB# #TAB# gt = not gt #LINE# #TAB# return gt"
Filter the string so Gold does n't have heart failure  <code> def filter_string(value): ,"#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# value = '' #LINE# #TAB# value = value.replace('\n', ' ') #LINE# #TAB# value = value.replace('\t', ' ') #LINE# #TAB# value = value.replace('|', ' ') #LINE# #TAB# value = value.strip() #LINE# #TAB# value = ''.join(c for c in value if 31 < ord(c) < 127) #LINE# #TAB# return value"
"In case of buffer size not aligned to sample_width pad it with 0s <code> def align_buf(buf, sample_width): ",#LINE# #TAB# remainder = len(buf) % sample_width #LINE# #TAB# if remainder != 0: #LINE# #TAB# #TAB# buf += b'\0' * (sample_width - remainder) #LINE# #TAB# return buf
"Determines if the given python value ` ` arg ` ` can be encoded as a value of abi type ` ` typ ` `  <code> def is_encodable(typ, arg): ","#LINE# #TAB# if isinstance(typ, str): #LINE# #TAB# #TAB# type_str = typ #LINE# #TAB# else: #LINE# #TAB# #TAB# type_str = collapse_type(*typ) #LINE# #TAB# encoder = registry.get_encoder(type_str) #LINE# #TAB# try: #LINE# #TAB# #TAB# encoder.validate_value(arg) #LINE# #TAB# except EncodingError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
Convert an object 's attributes to a dict  <code> def to_dict(obj: object) ->dict: ,"#LINE# #TAB# obj_dict = obj.__dict__ #LINE# #TAB# if '__values__' in obj_dict: #LINE# #TAB# #TAB# obj_dict = obj.__dict__.get('__values__') #LINE# #TAB# return {k: v for k, v in _unpack(obj_dict)}"
"- file uris are converted to file names - if also netcdf wrap in NETCDF:""${filename}"":${layer } - All other protocols go through unmodified <code> def rio_uri(band: BandInfo) ->str: ","#LINE# #TAB# if band.uri_scheme == 'file': #LINE# #TAB# #TAB# fname = str(uri_to_local_path(band.uri)) #LINE# #TAB# #TAB# if _is_netcdf(band.format): #LINE# #TAB# #TAB# #TAB# fname = 'NETCDF:""{}"":{}'.format(fname, band.layer) #LINE# #TAB# #TAB# return fname #LINE# #TAB# return band.uri"
Extracts the date from a file with a specific filename . : param filename : Name of the HDF image file . : type filename : str : return : Date the image with a given * filename * was taken . : rtype : datetime.datetime <code> def get_image_date(filename): ,"#LINE# #TAB# identifier = filename.split('-')[0] #LINE# #TAB# year = int(identifier[:4]) #LINE# #TAB# month = int(identifier[4:6]) #LINE# #TAB# day = int(identifier[6:8]) #LINE# #TAB# hour = int(identifier[8:]) #LINE# #TAB# date = dt(year, month, day, hour) #LINE# #TAB# return date"
"Decorator that registers ` exc_type ` as deserializable back into an instance , rather than a : class:`RemoteError ` . See : func:`deserialize `  <code> def deserialize_to_instance(exc_type): ",#LINE# #TAB# key = get_module_path(exc_type) #LINE# #TAB# registry[key] = exc_type #LINE# #TAB# return exc_type
"Convert Gerrit account properties to Buildbot format Take into account missing values <code> def gerrit_user_to_author(props, username='unknown'): ","#LINE# #TAB# username = props.get('username', username) #LINE# #TAB# username = props.get('name', username) #LINE# #TAB# if 'email' in props: #LINE# #TAB# #TAB# username += ' <%(email)s>' % props #LINE# #TAB# return username"
"Check if given index exists . : param engine : sqlalchemy engine : param table_name : name of the table : param index_name : name of the index <code> def index_exists(engine, table_name, index_name): ","#LINE# #TAB# indexes = get_indexes(engine, table_name) #LINE# #TAB# index_names = [index['name'] for index in indexes] #LINE# #TAB# return index_name in index_names"
Unpack a udp datagram into a log record <code> def convert_datagram(chunk): ,"#LINE# #TAB# slen = struct.unpack('>L', chunk[:4])[0] #LINE# #TAB# chunk = chunk[4:] #LINE# #TAB# assert slen == len(chunk) #LINE# #TAB# obj = pickle.loads(chunk) #LINE# #TAB# record = logging.makeLogRecord(obj) #LINE# #TAB# return record"
"Check if package is installed . : param package : : param installed_packages : : type package : str : type installed_packages : iterable : return : : rtype : bool <code> def check_if_installed(package, installed_packages=None): ",#LINE# #TAB# if installed_packages is None: #LINE# #TAB# #TAB# installed_packages = get_installed_packages(with_versions=False) #LINE# #TAB# return package in installed_packages
"Convert time period to a number of seconds . Raise ValueError for invalid time , otherwise return a number of seconds  <code> def time_to_sec(s): ",#LINE# #TAB# s = s.lower() #LINE# #TAB# m = _TPAT.match(s) #LINE# #TAB# if m is None: #LINE# #TAB# #TAB# raise ValueError('invalid time') #LINE# #TAB# g = m.groups() #LINE# #TAB# num = int(g[0]) #LINE# #TAB# factor = _TFAC[g[1]] #LINE# #TAB# return num * factor
"Returns int pointing to an OpenGL texture <code> def create_opengl_object(gl_gen_function, n=1): ","#LINE# #TAB# handle = gl.GLuint(1) #LINE# #TAB# gl_gen_function(n, byref(handle)) #LINE# #TAB# if n > 1: #LINE# #TAB# #TAB# return [(handle.value + el) for el in range(n)] #LINE# #TAB# else: #LINE# #TAB# #TAB# return handle.value"
Return True if the text in < sentence > contains brackets : param sentence : : return : <code> def is_containing_bracket(sentence): ,"#LINE# #TAB# pattern = '{{|}}' #LINE# #TAB# if not isinstance(sentence, six.text_type): #LINE# #TAB# #TAB# sentence = str(sentence) #LINE# #TAB# check_bool = re.search(pattern, sentence) #LINE# #TAB# if check_bool is not None: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Return a list of resource names found in the html metadata section  <code> def parse_resources(html): ,"#LINE# #TAB# xpath = '//*[@data-type=""resources""]//xhtml:li/xhtml:a' #LINE# #TAB# for resource in html.xpath(xpath, namespaces=HTML_DOCUMENT_NAMESPACES): #LINE# #TAB# #TAB# yield { #LINE# #TAB# #TAB# #TAB# 'id': resource.get('href'), #LINE# #TAB# #TAB# #TAB# 'filename': resource.text.strip(), #LINE# #TAB# #TAB# #TAB# }"
"Return the bounds of wavenumber values for the specified grid  <code> def get_k_bounds(data, spacing, packed=True): ","#LINE# #TAB# nx, ny, nz = transform.expanded_shape(data, packed=packed) #LINE# #TAB# k0 = 2 * np.pi / spacing #LINE# #TAB# k_min = k0 / max(nx, ny, nz) #LINE# #TAB# k_max = k0 * np.sqrt(3) / 2 #LINE# #TAB# return k_min, k_max"
Get the torch device we are using  <code> def get_device(): ,#LINE# #TAB# global device #LINE# #TAB# return device
"Flushes cache for the stage identified by stageName from API identified by restApiId <code> def flush_api_stage_cache(restApiId, stageName, region=None, key=None, keyid=None, profile=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# conn.flush_stage_cache(restApiId=restApiId, stageName=stageName) #LINE# #TAB# #TAB# return {'flushed': True} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'flushed': False, 'error': __utils__['boto3.get_error'](e)}"
Return the Shannon Entropy of the sample data  <code> def shannon_entropy(time_series): ,"#LINE# #TAB# if not isinstance(time_series, str): #LINE# #TAB# #TAB# time_series = list(time_series) #LINE# #TAB# data_set = list(set(time_series)) #LINE# #TAB# freq_list = [] #LINE# #TAB# for entry in data_set: #LINE# #TAB# #TAB# counter = 0. #LINE# #TAB# #TAB# for i in time_series: #LINE# #TAB# #TAB# #TAB# if i == entry: #LINE# #TAB# #TAB# #TAB# #TAB# counter += 1 #LINE# #TAB# #TAB# freq_list.append(float(counter) / len(time_series)) #LINE# #TAB# ent = 0.0 #LINE# #TAB# for freq in freq_list: #LINE# #TAB# #TAB# ent += freq * np.log2(freq) #LINE# #TAB# ent = -ent #LINE# #TAB# return ent"
Return the set of all child classes of cls  <code> def flatten_subclass_tree(cls): ,"#LINE# #TAB# subclasses = frozenset(cls.__subclasses__()) #LINE# #TAB# children = frozenset(toolz.concat(map(flatten_subclass_tree, subclasses))) #LINE# #TAB# return frozenset({cls}) | subclasses | children"
"Calculate the gelman_rubin statistic ( R_hat ) for every stochastic in the model . ( Gelman at al 2004 , 11.4 ) Input : models - list of models <code> def gelman_rubin(models): ","#LINE# #TAB# stochastics = models[0].get_stochastics() #LINE# #TAB# R_hat_dict = {} #LINE# #TAB# num_samples = stochastics.node[0].trace().shape[0] #LINE# #TAB# num_chains = len(models) #LINE# #TAB# for name, stochastic in stochastics.iterrows(): #LINE# #TAB# #TAB# samples = np.empty((num_chains, num_samples)) #LINE# #TAB# #TAB# for i, model in enumerate(models): #LINE# #TAB# #TAB# #TAB# samples[(i), :] = model.nodes_db.ix[name, 'node'].trace() #LINE# #TAB# #TAB# R_hat_dict[name] = pm.diagnostics.gelman_rubin(samples) #LINE# #TAB# return R_hat_dict"
"Convert angular velocity to DCM dot Omega - angular velocity defined in body frame <code> def ang_veltodcmdot(R, Omega): ",#LINE# #TAB# Rdot = R.dot(hat_map(Omega)) #LINE# #TAB# return Rdot
"Finds the separator that gives the longest array <code> def get_best_sep(string, sep): ","#LINE# #TAB# if not sep is None: #LINE# #TAB# #TAB# return sep, string.split(sep) #LINE# #TAB# sep = '' #LINE# #TAB# maxlen = 0 #LINE# #TAB# for i in [';', ',', ' ', '\t']: #LINE# #TAB# #TAB# b = head_split(string, i) #LINE# #TAB# #TAB# if len(b) > maxlen: #LINE# #TAB# #TAB# #TAB# maxlen = len(b) #LINE# #TAB# #TAB# #TAB# sep = i #LINE# #TAB# #TAB# #TAB# c = b #LINE# #TAB# return sep, c, maxlen"
"Like equiv_url , but forces an insecure connection <code> def equiv_url_is(url_parts): ",#LINE# #TAB# eq_url = equiv_url(url_parts) #LINE# #TAB# if eq_url.startswith('https'): #LINE# #TAB# #TAB# return eq_url[:4] + eq_url[5:] #LINE# #TAB# return eq_url
Return random triangles as a Trimesh Parameters ----------- face_count : int Number of faces desired in mesh Returns ----------- soup : trimesh . Trimesh Geometry with face_count random faces <code> def random_soup(face_count=100): ,"#LINE# #TAB# vertices = np.random.random((face_count * 3, 3)) - 0.5 #LINE# #TAB# faces = np.arange(face_count * 3).reshape((-1, 3)) #LINE# #TAB# soup = Trimesh(vertices=vertices, faces=faces) #LINE# #TAB# return soup"
"Gets frame resolution for a movie . Returns ------- tuple : ( x , y ) <code> def get_movie_size_pix(movie_path): ","#LINE# #TAB# video = cv2.VideoCapture(movie_path) #LINE# #TAB# _, frame = video.read() #LINE# #TAB# shape = frame.shape #LINE# #TAB# video.release() #LINE# #TAB# return shape[1], shape[0]"
Check the target URL will lead to the same host server . Parameters ---------- target : str The target redirect URL . Returns ------- bool True if the target URL is safe ; False otherwise  <code> def is_safe_url(target): ,"#LINE# #TAB# ref_url = urlparse(request.host_url) #LINE# #TAB# test_url = urlparse(urljoin(request.host_url, target)) #LINE# #TAB# return test_url.scheme in ('http', 'https' #LINE# #TAB# #TAB# ) and ref_url.netloc == test_url.netloc"
Backwards compatibility wrapper for CPU count  <code> def enum_cpus(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# from os import cpu_count #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# from multiprocessing import cpu_count #LINE# #TAB# finally: #LINE# #TAB# #TAB# cpus = cpu_count() #LINE# #TAB# return cpus
gitignored file that contains specific SNOWAV version and path Input : - none Output : - path to base SNOWAV directory <code> def get_snowav_path(): ,#LINE# #TAB# snowav_path = os.path.abspath(__gitPath__) #LINE# #TAB# return snowav_path
Extract country code from addresses : param profile : contact profile : return : two - letter country code or None if not unambiguous <code> def get_countrycode(profile): ,"#LINE# #TAB# countries_found = [] #LINE# #TAB# for addr in profile.get('addresses', []): #LINE# #TAB# #TAB# if addr.get('country', False) and addr['country'].strip().lower( #LINE# #TAB# #TAB# #TAB# ) not in countries_found: #LINE# #TAB# #TAB# #TAB# countries_found.append(addr['country'].strip().lower()) #LINE# #TAB# logging.debug('countries found %s', countries_found) #LINE# #TAB# if len(countries_found) == 1: #LINE# #TAB# #TAB# return countries.search_fuzzy(countries_found[0])[0].alpha_2 #LINE# #TAB# return None"
"Returns True if child is an eventual child node of parent <code> def is_child_node(child, parent): ",#LINE# #TAB# node = child #LINE# #TAB# while node is not None: #LINE# #TAB# #TAB# if node == parent: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# node = node.parent #LINE# #TAB# return False
"Yield all direct child nodes of * node * , that is , all fields that are nodes and all items of fields that are lists of nodes  <code> def iter_child_nodes(node): ","#LINE# #TAB# for name, field in iter_fields(node): #LINE# #TAB# #TAB# if isinstance(field, AST): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, AST): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item"
Returns True if manufacturer builds a limited number of vehicles a year . The limit varies globally  <code> def small_manuf(vin): ,#LINE# #TAB# if vin[2] == '9': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Parse preferences to the dictionary  <code> def parse_preferences(file, preferences): ","#LINE# #TAB# for line in open(file, 'r').readlines(): #LINE# #TAB# #TAB# line = line.lower() #LINE# #TAB# #TAB# if line[0] == '!' or line[0] == '#' or not line.split(): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key = line.split(':')[0].strip() #LINE# #TAB# #TAB# value = line.split(':')[1].strip() #LINE# #TAB# #TAB# value = check(key, value) #LINE# #TAB# #TAB# add_preference(key, value) #LINE# #TAB# return preferences"
Removes HTML comments from the value  <code> def remove_html_comments(text): ,"#LINE# #TAB# assert isinstance(text, str) #LINE# #TAB# text = text.strip() #LINE# #TAB# text = re.sub('(?s)<!--.*?-->', '', text) #LINE# #TAB# return text"
Create a dictionary with the relevant information from the request <code> def format_request(request): ,"#LINE# #TAB# if not request: #LINE# #TAB# #TAB# return None #LINE# #TAB# data = {'content_type': str(request.content_type), 'headers': str( #LINE# #TAB# #TAB# request.content_params)} #LINE# #TAB# if type(request.body) is dict: #LINE# #TAB# #TAB# data.update({'body': json.loads(request.body).decode('utf-8')}) #LINE# #TAB# if type(request.body) is bytes: #LINE# #TAB# #TAB# data.update({'body': request.body.decode('utf-8')}) #LINE# #TAB# return data"
"Function to get an issue 's metadata from Metron and the write that information to a tag in the comic archive <code> def get_issue_metadata(filename, issue_id, talker): ",#LINE# #TAB# success = False #LINE# #TAB# metron_md = talker.fetch_issue_data_by_issue_id(issue_id) #LINE# #TAB# if metron_md: #LINE# #TAB# #TAB# comic_archive = ComicArchive(filename) #LINE# #TAB# #TAB# meta_data = create_pagelist_metadata(comic_archive) #LINE# #TAB# #TAB# meta_data.overlay(metron_md) #LINE# #TAB# #TAB# comic_archive.write_metadata(meta_data) #LINE# #TAB# #TAB# success = True #LINE# #TAB# return success
Format invariants as reST  <code> def format_invariants(invariants: List[icontract._Contract]) -> List[str]: ,"#LINE# #TAB# if not invariants: #LINE# #TAB# #TAB# return [] #LINE# #TAB# result = ["":establishes:""] #LINE# #TAB# for invariant in invariants: #LINE# #TAB# #TAB# result.append(""#TAB# * {}"".format(_format_contract(contract=invariant))) #LINE# #TAB# return result"
Convert newlines to unix  <code> def fix_newlines(lines): ,"#LINE# #TAB# for i, line in enumerate(lines): #LINE# #TAB# #TAB# if line.endswith('\r\n'): #LINE# #TAB# #TAB# #TAB# lines[i] = line[:-2] + '\n' #LINE# #TAB# #TAB# elif line.endswith('\r'): #LINE# #TAB# #TAB# #TAB# lines[i] = line[:-1] + '\n'"
Nulls make more sense for some columns than zeros  <code> def zero_to_none(value): ,"#LINE# #TAB# if value is None or value == 0 or isinstance(value, basestring #LINE# #TAB# #TAB# ) and value.strip() == '0': #LINE# #TAB# #TAB# return None #LINE# #TAB# return value"
Recursively iterate over the items of a dictionary  <code> def iterate_flattened(d): ,"#LINE# #TAB# for key in sorted(d.keys()): #LINE# #TAB# #TAB# value = d[key] #LINE# #TAB# #TAB# if isinstance(value, dict) and value: #LINE# #TAB# #TAB# #TAB# for k, v in iterate_flattened(d[key]): #LINE# #TAB# #TAB# #TAB# #TAB# yield join_paths(key, k), v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield key, value"
"Compute Bit - Error - Rate ( BER ) by comparing 2 binary audio arrays  <code> def ber_audio(original_audio_bin, decoded_audio_bin): ","#LINE# #TAB# if not original_audio_bin.shape == decoded_audio_bin.shape: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# length, k = original_audio_bin.shape #LINE# #TAB# total_bits = np.prod(original_audio_bin.shape) #LINE# #TAB# errors_bits = abs(original_audio_bin - decoded_audio_bin).flatten().sum() #LINE# #TAB# ber = errors_bits / total_bits #LINE# #TAB# return ber"
"Returns a sorted list of prefixes  <code> def sort_prefixes(orig, prefixes='@+'): ",#LINE# #TAB# new = '' #LINE# #TAB# for prefix in prefixes: #LINE# #TAB# #TAB# if prefix in orig: #LINE# #TAB# #TAB# #TAB# new += prefix #LINE# #TAB# return new
"Given a set of sequences append a suffix for each sequence s name  <code> def name_append_suffix(records, suffix): ","#LINE# #TAB# logging.info('Applying _name_append_suffix generator: ' #LINE# #TAB# #TAB# #TAB# #TAB# 'Appending suffix ' + suffix + ' to all ' #LINE# #TAB# #TAB# #TAB# #TAB# 'sequence IDs.') #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# new_id = record.id + suffix #LINE# #TAB# #TAB# _update_id(record, new_id) #LINE# #TAB# #TAB# yield record"
"Performs a force - clean of a project , removing all files instead of politely calling the clean function of the underlying build system  <code> def force_clean(ws, proj): ","#LINE# #TAB# build_dir = get_build_dir(ws, proj) #LINE# #TAB# log('removing %s' % build_dir) #LINE# #TAB# if dry_run(): #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# rmtree(build_dir) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# log('%s already removed' % build_dir) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# config = get_ws_config(ws) #LINE# #TAB# config['projects'][proj]['taint'] = False"
"Find the topics in a list of texts with Latent Dirichlet Allocation  <code> def find_topics(token_lists, num_topics=10): ","#LINE# #TAB# dictionary = Dictionary(token_lists) #LINE# #TAB# print('Number of unique words in original documents:', len(dictionary)) #LINE# #TAB# dictionary.filter_extremes(no_below=2, no_above=0.7) #LINE# #TAB# print('Number of unique words after removing rare and common words:', len(dictionary)) #LINE# #TAB# corpus = [dictionary.doc2bow(tokens) for tokens in token_lists] #LINE# #TAB# model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, chunksize=100, passes=5, random_state=1) #LINE# #TAB# print_topics(model) #LINE# #TAB# return model, dictionary"
Returns the names of all positional arguments to the given function  <code> def get_all_positional_parameter_names(fn): ,#LINE# #TAB# arg_spec = _get_cached_arg_spec(fn) #LINE# #TAB# args = arg_spec.args #LINE# #TAB# if arg_spec.defaults: #LINE# #TAB# #TAB# args = args[:-len(arg_spec.defaults)] #LINE# #TAB# return args
"Returns the CUDA device i d <code> def get_cuda_device_id(gpu_id, job_name, using_cpu_only): ","#LINE# #TAB# if using_cpu_only or job_name != 'worker': #LINE# #TAB# #TAB# return '' #LINE# #TAB# cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None) #LINE# #TAB# if len(cuda_visible_devices.split(',')) > gpu_id: #LINE# #TAB# #TAB# return cuda_visible_devices.split(',')[gpu_id] #LINE# #TAB# return cuda_visible_devices"
Get path to local executable . : param executable : Name of executable in the $ PATH variable : type executable : str : return : path to executable : rtype : str <code> def get_path_to_executable(executable): ,"#LINE# #TAB# path = shutil.which(executable) #LINE# #TAB# if path is None: #LINE# #TAB# #TAB# raise ValueError(""'{}' executable not found in PATH."".format( #LINE# #TAB# #TAB# #TAB# executable)) #LINE# #TAB# return path"
"Check Intan recording directory to determine type of recording and thus extraction method to use . Asks user to confirm , and manually correct if incorrect Parameters ---------- file_dir : str , recording directory to check Returns ------- str : file_type of recording <code> def get_recording_filetype(file_dir): ","#LINE# #TAB# file_list = os.listdir(file_dir) #LINE# #TAB# file_type = None #LINE# #TAB# for k, v in support_rec_types.items(): #LINE# #TAB# #TAB# regex = re.compile(v) #LINE# #TAB# #TAB# if any([(True) for x in file_list if regex.match(x) is not None]): #LINE# #TAB# #TAB# #TAB# file_type = k #LINE# #TAB# if file_type is None: #LINE# #TAB# #TAB# msg = '\n '.join([ #LINE# #TAB# #TAB# #TAB# 'unsupported recording type. Supported types are:', *list( #LINE# #TAB# #TAB# #TAB# support_rec_types.keys())]) #LINE# #TAB# else: #LINE# #TAB# #TAB# msg = '""' + file_type + '""' #LINE# #TAB# return file_type"
Returns unquoted value of given string <code> def get_unquoted_value(value: str): ,#LINE# #TAB# if value and is_quote(value[0]): #LINE# #TAB# #TAB# value = value[1:] #LINE# #TAB# if is_quote(value[-1]): #LINE# #TAB# #TAB# value = value[0:-1] #LINE# #TAB# return value
Check if is Google App Engine <code> def is_gae(): ,"#LINE# #TAB# if 'CURRENT_VERSION_ID' not in os.environ: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'AUTH_DOMAIN' not in os.environ: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'SERVER_SOFTWARE' not in os.environ: #LINE# #TAB# #TAB# return False #LINE# #TAB# server_software = os.environ.get('SERVER_SOFTWARE') #LINE# #TAB# if not re.match('^Development\\/', server_software) and not re.match( #LINE# #TAB# #TAB# '^Google App Engine\\/', server_software): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Write a cython include file ( .pxi ) , ` filename ` , with the definitions in the ` definitions ` mapping  <code> def write_pxi(filename, definitions): ","#LINE# #TAB# with io.open(filename, mode='w', encoding='utf-8') as pxi_file: #LINE# #TAB# #TAB# for name, val in definitions.items(): #LINE# #TAB# #TAB# #TAB# pxi_file.write(u'DEF {name} = {val}\n'.format(name=name, val=val)) #LINE# #TAB# return filename"
"Get union of all community solutions Args : grounding ( bytes ) : grounded model optimum ( str ) : optimal score Returns : TermSet : union <code> def get_union_communities_from_g(grounding, optimum): ","#LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode=brave --opt-mode=optN,' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# models = clyngor.solve_from_grounded(grounding, options=options) #LINE# #TAB# best_model = None #LINE# #TAB# for model in models.by_arity.discard_quotes.with_optimization: #LINE# #TAB# #TAB# best_model = model #LINE# #TAB# return best_model"
"Takes the given form text , parses it and returns a WTForms form class ( not an instance of it )  <code> def parse_form(text, base_class=Form): ","#LINE# #TAB# builder = WTFormsClassBuilder(base_class) #LINE# #TAB# for fieldset in parse_formcode(text): #LINE# #TAB# #TAB# builder.set_current_fieldset(fieldset.label) #LINE# #TAB# #TAB# for field in fieldset.fields: #LINE# #TAB# #TAB# #TAB# handle_field(builder, field) #LINE# #TAB# form_class = builder.form_class #LINE# #TAB# form_class._source = text #LINE# #TAB# return form_class"
"r iterates through iterable with a window size generalizeation of itertwo <code> def iter_window(iterable, size=2, step=1, wrap=False): ","#LINE# #TAB# iter_list = it.tee(iterable, size) #LINE# #TAB# if wrap: #LINE# #TAB# #TAB# iter_list = [iter_list[0]] + list(map(it.cycle, iter_list[1:])) #LINE# #TAB# try: #LINE# #TAB# #TAB# for count, iter_ in enumerate(iter_list[1:], start=1): #LINE# #TAB# #TAB# #TAB# for _ in range(count): #LINE# #TAB# #TAB# #TAB# #TAB# six.next(iter_) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return iter(()) #LINE# #TAB# else: #LINE# #TAB# #TAB# _window_iter = zip(*iter_list) #LINE# #TAB# #TAB# window_iter = it.islice(_window_iter, 0, None, step) #LINE# #TAB# #TAB# return window_iter"
"Start updating rolled metrics in daemon thread <code> def start_update_daemon(updater=None, roller_registry=ROLLER_REGISTRY): ",#LINE# #TAB# if updater is None: #LINE# #TAB# #TAB# updater = PrometheusRollingMetricsUpdater() #LINE# #TAB# #TAB# for roller in roller_registry.values(): #LINE# #TAB# #TAB# #TAB# updater.add(roller) #LINE# #TAB# updater.daemon = True #LINE# #TAB# updater.start() #LINE# #TAB# return updater
"Fit a 2D 2nd order polynomial to ` data[mask ] ` <code> def profile_poly2o(data, mask): ","#LINE# #TAB# params = lmfit.Parameters() #LINE# #TAB# params.add(name='mx', value=0) #LINE# #TAB# params.add(name='my', value=0) #LINE# #TAB# params.add(name='mxy', value=0) #LINE# #TAB# params.add(name='ax', value=0) #LINE# #TAB# params.add(name='ay', value=0) #LINE# #TAB# params.add(name='off', value=np.average(data[mask])) #LINE# #TAB# fr = lmfit.minimize(poly2o_residual, params, args=(data, mask)) #LINE# #TAB# bg = poly2o_model(fr.params, data.shape) #LINE# #TAB# return bg"
Return fyda configuration file ( ' .fydarc ' ) using YAML  <code> def load_config(filepath=None): ,"#LINE# #TAB# if filepath is None: #LINE# #TAB# #TAB# filepath = _get_conf() #LINE# #TAB# with open(filepath, 'r') as stream: #LINE# #TAB# #TAB# conf = yaml.safe_load(stream) #LINE# #TAB# return conf"
Generate a CamelCase string from an underscore_string <code> def camel_case_from_underscores(string): ,#LINE# #TAB# components = string.split('_') #LINE# #TAB# string = '' #LINE# #TAB# for component in components: #LINE# #TAB# #TAB# if component in abbreviations: #LINE# #TAB# #TAB# #TAB# string += component #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# string += component[0].upper() + component[1:].lower() #LINE# #TAB# return string
"Walk a directory and produce the cards found there , one by one  <code> def cards_from_dir(dirname): ","#LINE# #TAB# for parent_dir, _, files in os.walk(dirname): #LINE# #TAB# #TAB# for fn in files: #LINE# #TAB# #TAB# #TAB# if fn.endswith('.md') or fn.endswith('.markdown'): #LINE# #TAB# #TAB# #TAB# #TAB# for card in produce_cards(os.path.join(parent_dir, fn)): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield card"
stick list of blocks together <code> def im_from_blocks(blocks): ,"#LINE# #TAB# im = [] #LINE# #TAB# im = np.hstack(blocks[0]) #LINE# #TAB# i = 0 #LINE# #TAB# for row in blocks[1:]: #LINE# #TAB# #TAB# row = np.hstack(row) #LINE# #TAB# #TAB# if i == 7: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# im = np.vstack([im, row]) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# return im"
"Iter the immediate contents of ` data ` and yield any dictionaries . Does not recurse . Parameters ---------- data : Mapping Returns ------- Iterator[Mapping ] <code> def iter_contents(cls, data): ","#LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if isinstance(v, Mapping): #LINE# #TAB# #TAB# #TAB# yield v #LINE# #TAB# #TAB# elif isinstance(v, (list, tuple, set)): #LINE# #TAB# #TAB# #TAB# for l in v: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(l, Mapping): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield l"
"Returns ` Content - Type ` based on an array of content_types provided . : param content_types : List of content - types . : return : Content - Type ( e.g. application / json )  <code> def select_header_content_type(B, content_types): ",#LINE# #TAB# A = content_types #LINE# #TAB# if not A: #LINE# #TAB# #TAB# return _B #LINE# #TAB# A = [B.lower() for B in A] #LINE# #TAB# if _B in A or '*/*' in A: #LINE# #TAB# #TAB# return _B #LINE# #TAB# else: #LINE# #TAB# #TAB# return A[0]
Return a list of slot names for slots defined in class_ and its superclasses  <code> def get_all_slots(class_): ,"#LINE# #TAB# all_slots = [] #LINE# #TAB# parent_param_classes = [c for c in classlist(class_)[1::]] #LINE# #TAB# for c in parent_param_classes: #LINE# #TAB# #TAB# if hasattr(c,'__slots__'): #LINE# #TAB# #TAB# #TAB# all_slots+=c.__slots__ #LINE# #TAB# return all_slots"
Cuts the score into colors using the PREA colors : param score : : return : <code> def get_colors(score: List[T]) ->List[Tuple]: ,"#LINE# #TAB# colors = cut(score, 5, labels=[(49, 252, 3), (252, 223, 3), (252, 169, #LINE# #TAB# #TAB# 3), (252, 98, 3), (252, 3, 3)]) #LINE# #TAB# return colors"
"Render REST path from * args  <code> def render_path(path, args): ","#LINE# #TAB# LOG.debug('RENDERING PATH FROM: %s, %s', path, args) #LINE# #TAB# result = path #LINE# #TAB# matches = re.search('{([^}.]*)}', result) #LINE# #TAB# while matches: #LINE# #TAB# #TAB# path_token = matches.group(1) #LINE# #TAB# #TAB# if path_token not in args: #LINE# #TAB# #TAB# #TAB# raise ValueError('Missing argument %s in REST call' % path_token) #LINE# #TAB# #TAB# result = re.sub('{%s}' % path_token, str(args[path_token]), result) #LINE# #TAB# #TAB# matches = re.search('{([^}.]*)}', result) #LINE# #TAB# return result"
"read local dataset and return the DDF object  <code> def read_local_ddf(ddf_id, base_dir='./'): ","#LINE# #TAB# if os.path.isabs(ddf_id): #LINE# #TAB# #TAB# return DDFcsv.from_path(ddf_id).ddf #LINE# #TAB# else: #LINE# #TAB# #TAB# path = os.path.join(base_dir, ddf_id) #LINE# #TAB# #TAB# return DDFcsv.from_path(path).ddf"
exec functions ( e.g. subprocess . Popen ) by default force the child process to inherit all file handles which can result in stuck connections and unacknowledged messages . Setting FD_CLOEXEC forces the handles to be closed first  <code> def set_close_exec(fd): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# import fcntl #LINE# #TAB# #TAB# flags = fcntl.fcntl(fd, fcntl.F_GETFD) #LINE# #TAB# #TAB# fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass"
"Tries to check if an object has a variable  <code> def has_variable(obj, variable): ","#LINE# #TAB# if hasattr(obj, 'has_variable'): #LINE# #TAB# #TAB# return obj.has_variable(variable) #LINE# #TAB# return False"
"gets the table fullname that this entity represents in database . fullname is ` schema.table_name ` if schema is available , otherwise it defaults to ` table_name ` . : rtype : str <code> def table_fullname(cls): ","#LINE# #TAB# schema = cls.table_schema() #LINE# #TAB# name = cls.table_name() #LINE# #TAB# if schema is not None: #LINE# #TAB# #TAB# return '{schema}.{name}'.format(schema=schema, name=name) #LINE# #TAB# else: #LINE# #TAB# #TAB# return name"
"If the range of characters from code1 to code2 - 1 includes any upper case letters , return the corresponding lower case range  <code> def lowercase_range(code1, code2): ","#LINE# #TAB# code3 = max(code1, ord('A')) #LINE# #TAB# code4 = min(code2, ord('Z') + 1) #LINE# #TAB# if code3 < code4: #LINE# #TAB# #TAB# d = ord('a') - ord('A') #LINE# #TAB# #TAB# return code3 + d, code4 + d #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
takes list of Field or tuple . returns list of Field ( using tuples as constructor arguments ) <code> def cast_fields(list_): ,"#LINE# #TAB# return [(field if isinstance(field, Field) else Field(*field)) for #LINE# #TAB# #TAB# field in list_]"
Split the given sql statement into a list of individual sql statements  <code> def parse_multiple_statements(statement): ,"#LINE# #TAB# statements_list = [] #LINE# #TAB# if isinstance(statement, SQLALCHEMY_BASES): #LINE# #TAB# #TAB# statements_list.append(statement) #LINE# #TAB# #TAB# return statements_list #LINE# #TAB# statement = _preprocess(statement) #LINE# #TAB# statements_list = [str(statement) for statement in sqlparse.split( #LINE# #TAB# #TAB# statement)] #LINE# #TAB# return statements_list"
A canned scenario for display in the workbench  <code> def workbench_scenarios(): ,"#LINE# #TAB# return [('ScormXBlock', #LINE# #TAB# #TAB# )]"
Check if an instance with given name exists <code> def has_instance(name): ,#LINE# #TAB# vms = list_instances() #LINE# #TAB# return name in vms
"A helper method to get the correct address family for a given address . : param address : The address to get the af for : returns : AF_INET for ipv4 and AF_INET6 for ipv6 <code> def get_address_family(cls, address): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# _ = socket.inet_pton(socket.AF_INET6, address) #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return socket.AF_INET #LINE# #TAB# return socket.AF_INET6"
"Get a hashable object given a string and a dictionary  <code> def cache_key(method, args=None): ","#LINE# #TAB# if args is None: #LINE# #TAB# #TAB# args = {} #LINE# #TAB# hash_args = frozenset(deepcopy(args).items()) #LINE# #TAB# return method, hash_args"
Parse docx and odf files  <code> def doc_reader(infile): ,"#LINE# #TAB# if infile.endswith('.docx'): #LINE# #TAB# #TAB# docid = 'word/document.xml' #LINE# #TAB# else: #LINE# #TAB# #TAB# docid = 'content.xml' #LINE# #TAB# try: #LINE# #TAB# #TAB# zfile = zipfile.ZipFile(infile) #LINE# #TAB# except: #LINE# #TAB# #TAB# print('Sorry, can\'t open {}.'.format(infile)) #LINE# #TAB# #TAB# return #LINE# #TAB# body = ET.fromstring(zfile.read(docid)) #LINE# #TAB# text = '\n'.join([et.text.strip() for et in body.iter() if et.text]) #LINE# #TAB# return text"
Retrieves download location for FEH data zip file from hosted json configuration file  <code> def retrieve_download_url(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with urlopen(config['nrfa']['oh_json_url'], timeout=10) as f: #LINE# #TAB# #TAB# #TAB# remote_config = json.loads(f.read().decode('utf-8')) #LINE# #TAB# #TAB# if remote_config['nrfa_url'].startswith('.'): #LINE# #TAB# #TAB# #TAB# remote_config['nrfa_url'] = 'file:' + pathname2url(os.path.abspath(remote_config['nrfa_url'])) #LINE# #TAB# #TAB# _update_nrfa_metadata(remote_config) #LINE# #TAB# #TAB# return remote_config['nrfa_url'] #LINE# #TAB# except URLError: #LINE# #TAB# #TAB# return config['nrfa']['url']"
( ? : \d+\.\d*)|(?:\.\d+ ) <code> def t_floatlit(t): ,#LINE# #TAB# t.value = float(t.value) #LINE# #TAB# return t
"Convert str s to unicode : param str s : string : return : "" unicode "" version of s ( unicode in py2 , str in py3 ) <code> def to_unicode(s): ",#LINE# #TAB# if PY_VERSION == 2: #LINE# #TAB# #TAB# s = unicode(s) #LINE# #TAB# return s
"Force a value to an integer . If the value can not be converted return the default value  <code> def force_int(num, default=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(num) #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# return default"
Convert float to int : param val : value to convert : type val : float : return : int(val ) : rtype : int <code> def from_float_to_int(val): ,#LINE# #TAB# val = int(val) #LINE# #TAB# return val
"Generate the SVG file ai_filename from the figure ai_figure ( list of format B outline ) <code> def write_figure_in_svg(ai_figure, ai_filename): ","#LINE# #TAB# print('Generate with mozman svgwrite the SVG file {:s}'.format(ai_filename) #LINE# #TAB# #TAB# ) #LINE# #TAB# object_svg = svgwrite.Drawing(filename=ai_filename) #LINE# #TAB# for i_ol in ai_figure: #LINE# #TAB# #TAB# svg_outline = outline_arc_line(i_ol, 'svgwrite') #LINE# #TAB# #TAB# for one_line_or_arc in svg_outline: #LINE# #TAB# #TAB# #TAB# object_svg.add(one_line_or_arc) #LINE# #TAB# object_svg.save() #LINE# #TAB# return 0"
"Returns whether the class has the required info to generate code for a specified code template string  <code> def template_generatable(cls, template: str) ->bool: ","#LINE# #TAB# get_set_value_requires = (cls._name_value_type is not None and cls. #LINE# #TAB# #TAB# _name_get_value_func is not None and cls._name_set_value_func is not #LINE# #TAB# #TAB# None) #LINE# #TAB# get_set_enable_requires = (cls._name_get_enabled_func is not None and #LINE# #TAB# #TAB# cls._name_set_enabled_func) #LINE# #TAB# requirements = {cls._template_get_value: get_set_value_requires, cls. #LINE# #TAB# #TAB# _template_set_value: get_set_value_requires, cls. #LINE# #TAB# #TAB# _template_get_enabled: get_set_enable_requires, cls. #LINE# #TAB# #TAB# _template_set_enabled: get_set_enable_requires} #LINE# #TAB# return requirements[template]"
"Unlink all links in dst that are in filenames . Args : src : The directory where the source files exist . dst : The directory where the links should be made . filenames : A list of filenames in src  <code> def unlink_all_files(_, dst, filenames): ","#LINE# #TAB# for fname in filenames: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.remove(os.path.join(dst, fname)) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# os.rmdir(dst) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass"
Create a binary image of a circle radius r <code> def image_circle(r): ,"#LINE# #TAB# im = np.zeros([2 * r - 1, 2 * r - 1], dtype=np.int) #LINE# #TAB# r2 = r ** 2 #LINE# #TAB# for i in range(2 * r - 1): #LINE# #TAB# #TAB# for j in range(2 * r - 1): #LINE# #TAB# #TAB# #TAB# if (i - r + 1) ** 2 + (j - r + 1) ** 2 < r2: #LINE# #TAB# #TAB# #TAB# #TAB# im[i, j] = 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# im[i, j] = 0 #LINE# #TAB# return im"
Gets the BCML internal working directory <code> def get_work_dir() ->Path: ,"#LINE# #TAB# work_dir = get_data_dir() / 'work_dir' #LINE# #TAB# if not work_dir.exists(): #LINE# #TAB# #TAB# work_dir.mkdir(parents=True, exist_ok=True) #LINE# #TAB# return work_dir"
"Returns a FrequencySeries of ones above the low_frequency_cutoff  <code> def flat_unity(length, delta_f, low_freq_cutoff): ","#LINE# #TAB# fseries = FrequencySeries(numpy.ones(length), delta_f=delta_f) #LINE# #TAB# kmin = int(low_freq_cutoff / fseries.delta_f) #LINE# #TAB# fseries.data[:kmin] = 0 #LINE# #TAB# return fseries"
"Given a framerate , makes a ` ` rate ` ` xml tree . : param fps : The framerate . : return : The fcp xml ` ` rate ` ` tree  <code> def build_rate(fps): ","#LINE# #TAB# rate = math.ceil(fps) #LINE# #TAB# rate_e = cElementTree.Element('rate') #LINE# #TAB# _append_new_sub_element(rate_e, 'timebase', text=str(int(rate))) #LINE# #TAB# _append_new_sub_element(rate_e, 'ntsc', text='FALSE' if rate == fps else #LINE# #TAB# #TAB# 'TRUE') #LINE# #TAB# return rate_e"
Marks this field as belonging to a group <code> def group_tag(group_name): ,#LINE# #TAB# tag = 'group:%s' % group_name #LINE# #TAB# return tag
"Make sure the string has no double underscores , also convert from camelcase  <code> def sanitize_order_by(string): ",#LINE# #TAB# if string and string.find('__') == -1 and string.find('?') == -1: #LINE# #TAB# #TAB# return camel_case_to_underscore(string) #LINE# #TAB# return ''
"Returns the geometry type of the first shape under the given geometry object : param geometry : str , geometry object to query : return : str <code> def geometry_type(geometry): ","#LINE# #TAB# if not maya.cmds.objExists(geometry): #LINE# #TAB# #TAB# raise exceptions.GeometryExistsException(geometry) #LINE# #TAB# shapes_list = shape.get_shapes(node=geometry, intermediates=False) #LINE# #TAB# if not shapes_list: #LINE# #TAB# #TAB# shapes_list = shape.get_shapes(node=geometry, intermediates=True) #LINE# #TAB# if not shapes_list: #LINE# #TAB# #TAB# raise exceptions.NoShapeChildren(geometry) #LINE# #TAB# geometry_type = maya.cmds.objectType(shapes_list[0]) #LINE# #TAB# return geometry_type"
"Generates a batch iterator for a dataset  <code> def batch_iter(data, batch_size, num_epochs): ","#LINE# #TAB# data = np.array(data) #LINE# #TAB# data_size = len(data) #LINE# #TAB# num_batches_per_epoch = int(len(data)/batch_size) + 1 #LINE# #TAB# for epoch in range(num_epochs): #LINE# #TAB# #TAB# shuffle_indices = np.random.permutation(np.arange(data_size)) #LINE# #TAB# #TAB# shuffled_data = data[shuffle_indices] #LINE# #TAB# #TAB# for batch_num in range(num_batches_per_epoch): #LINE# #TAB# #TAB# #TAB# start_index = batch_num * batch_size #LINE# #TAB# #TAB# #TAB# end_index = min((batch_num + 1) * batch_size, data_size) #LINE# #TAB# #TAB# #TAB# yield shuffled_data[start_index:end_index]"
"split string into list of strings by specified number  <code> def split_str(s, n): ","#LINE# #TAB# length = len(s) #LINE# #TAB# return [s[i:i + n] for i in range(0, length, n)]"
"Return the possible configs , args , params for a Pig job  <code> def get_possible_pig_config_from(file_name): ","#LINE# #TAB# config = {'configs': utils.load_hadoop_xml_defaults(file_name, #LINE# #TAB# #TAB# 'sahara_plugin_cdh'), 'args': [], 'params': {}} #LINE# #TAB# return config"
"Verify if a latin letter is accentuated , unicode point of view . : param str letter : Letter to check : return : True if accentuated , else False : rtype : bool <code> def is_accentuated(letter): ",#LINE# #TAB# if len(letter) != 1: #LINE# #TAB# #TAB# raise IOError( #LINE# #TAB# #TAB# #TAB# 'Trying to determine accentuated state of multiple char <{}>'. #LINE# #TAB# #TAB# #TAB# format(letter)) #LINE# #TAB# return 192 <= ord(letter) <= 383
"Clone a sql object avoiding the properties in the skip parameter The skip parameter is optional but you probably want to always pass the primary key <code> def sql_clone(obj, skip={}, session=None): ","#LINE# #TAB# skip.add('_sa_instance_state') #LINE# #TAB# for column in obj.__table__.columns: #LINE# #TAB# #TAB# if column.key not in skip: #LINE# #TAB# #TAB# #TAB# getattr(obj, column.key, None) #LINE# #TAB# params = {key: value for key, value in obj.__dict__.iteritems() if key #LINE# #TAB# #TAB# not in skip} #LINE# #TAB# clone = obj.__class__(**params) #LINE# #TAB# if session: #LINE# #TAB# #TAB# session.add(clone) #LINE# #TAB# return clone"
Build lists of incoming edges to each vertex in a linearized graph  <code> def extract_incoming_edges(G): ,"#LINE# #TAB# V, E = G #LINE# #TAB# n = len(V) #LINE# #TAB# Ein = lists(n) #LINE# #TAB# for i, e in enumerate(E): #LINE# #TAB# #TAB# Ein[e[1]].append(i) #LINE# #TAB# return Ein"
"Given a dictionary - > { start : quantity , start : quantity , ... } Return a list of tuples [ ( start , end , quantity ) , ( start , end , quantity ) , ... ] <code> def map_dict_to_list(d, width): ","#LINE# #TAB# assert width >= 0 #LINE# #TAB# for startTimeMillis in d: #LINE# #TAB# #TAB# yield int(startTimeMillis), int(startTimeMillis) + width, d[ #LINE# #TAB# #TAB# #TAB# startTimeMillis]"
"Display the issue template directory <code> def show_path(reponame, reposave, path): ",#LINE# #TAB# print('Opening editor') #LINE# #TAB# editor = local[get_editor()] #LINE# #TAB# repodir = reposave['repodir'] #LINE# #TAB# with local.cwd(repodir): #LINE# #TAB# #TAB# _ = editor[str(path)] & FG
Converts a hex - formatted ( e.g. # FFFFFF ) string to a Gdk color object <code> def parse_color_string(value): ,#LINE# #TAB# color = Gdk.RGBA() #LINE# #TAB# color.parse(value) #LINE# #TAB# return color
Return a container for milestone properties if it does not yet exist one is created <code> def ticket_properties_of_holder(holder): ,#LINE# #TAB# if not 'ticket_properties' in holder: #LINE# #TAB# #TAB# holder['ticket_properties'] = TicketProperties() #LINE# #TAB# return holder['ticket_properties']
Iterate over all fields of a node only yielding existing fields  <code> def iter_fields(node): ,"#LINE# #TAB# if not hasattr(node, '_fields') or not node._fields: #LINE# #TAB# #TAB# return #LINE# #TAB# for field in node._fields: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield field, getattr(node, field) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass"
Check if truecolor is supported by the current tty . Note : this currently only checks to see if COLORTERM contains one of the following enumerated case - sensitive values : - truecolor - 24bit <code> def true_color_supported() ->bool: ,"#LINE# #TAB# color_term = os.getenv('COLORTERM', '') #LINE# #TAB# return True if any(check in color_term for check in ['truecolor', '24bit'] #LINE# #TAB# #TAB# ) else False"
"Class Factory Method that builds the connector handlers an Amazon AWS s3 remote store  <code> def from_remote_redis(cls) ->(str, str): ","#LINE# #TAB# _module_name = 'ds_connectors.handlers.redis_handlers' #LINE# #TAB# _handler = 'RedisPersistHandler' #LINE# #TAB# return _module_name, _handler"
"Imports the contents of filepath as a Python module  <code> def import_pyfile(filepath, mod_name=None): ","#LINE# #TAB# import sys #LINE# #TAB# if sys.version_info.major == 3: #LINE# #TAB# #TAB# import importlib.machinery #LINE# #TAB# #TAB# loader = importlib.machinery.SourceFileLoader('', filepath) #LINE# #TAB# #TAB# mod = loader.load_module(mod_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# import imp #LINE# #TAB# #TAB# mod = imp.load_source(mod_name, filepath) #LINE# #TAB# return mod"
Convert top level keys from bytes to strings if possible . This is necessary because Python 3 makes a distinction between these types  <code> def decode_dict_keys_to_str(src): ,"#LINE# #TAB# if not six.PY3 or not isinstance(src, dict): #LINE# #TAB# #TAB# return src #LINE# #TAB# output = {} #LINE# #TAB# for key, val in six.iteritems(src): #LINE# #TAB# #TAB# if isinstance(key, bytes): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# key = key.decode() #LINE# #TAB# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# output[key] = val #LINE# #TAB# return output"
Sets up a subparser to download audio of YouTube videos  <code> def fill_subparser(subparser): ,"#LINE# #TAB# subparser.add_argument( #LINE# #TAB# #TAB# '--youtube-id', type=str, required=True, #LINE# #TAB# #TAB# help=(""The YouTube ID of the video from which to extract audio, "" #LINE# #TAB# #TAB# #TAB# ""usually an 11-character string."") #LINE# #TAB# ) #LINE# #TAB# return download"
return a list of article - id data <code> def article_id_list(soup): ,"#LINE# #TAB# id_list = [] #LINE# #TAB# for article_id_tag in raw_parser.article_id(soup): #LINE# #TAB# #TAB# id_details = OrderedDict() #LINE# #TAB# #TAB# set_if_value(id_details, ""type"", article_id_tag.get(""pub-id-type"")) #LINE# #TAB# #TAB# set_if_value(id_details, ""value"", article_id_tag.text) #LINE# #TAB# #TAB# set_if_value(id_details, ""assigning-authority"", article_id_tag.get(""assigning-authority"")) #LINE# #TAB# #TAB# id_list.append(id_details) #LINE# #TAB# return id_list"
Get default languages . used in case the model and the app does not have languages configured : return : <code> def get_languages_default(): ,#LINE# #TAB# languages = [it.code for it in TransLanguage.objects.all()] #LINE# #TAB# return languages
Print all registry values registered under key : param key : : return : <code> def show_win(key): ,"#LINE# #TAB# for i in range(1024): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# n, v, t = EnumValue(key, i) #LINE# #TAB# #TAB# #TAB# print('%s=%s' % (n, v)) #LINE# #TAB# #TAB# except EnvironmentError: #LINE# #TAB# #TAB# #TAB# break"
Extract token from header of query parameter . : param request : The request to look for an access token in . : type request : twisted.web.server . Request : return : The token or None if not found : rtype : unicode or None <code> def token_from_request(request): ,"#LINE# #TAB# token = None #LINE# #TAB# authHeader = request.getHeader('Authorization') #LINE# #TAB# if authHeader is not None and authHeader.startswith('Bearer '): #LINE# #TAB# #TAB# token = authHeader[len('Bearer '):] #LINE# #TAB# if token is None and 'access_token' in request.args: #LINE# #TAB# #TAB# token = request.args['access_token'][0] #LINE# #TAB# if token and isinstance(token, bytes): #LINE# #TAB# #TAB# token = token.decode('UTF-8') #LINE# #TAB# return token"
"For each top - level object in a dictionary , replace it with a Link , if appropriate  <code> def replace_objects_with_links(json: dict) ->dict: ","#LINE# #TAB# for key, value in list(json.items()): #LINE# #TAB# #TAB# json[key] = object_to_link(value) #LINE# #TAB# return json"
Function that verify if the header parameter is a essential header : param header : A string represented a header : returns : A boolean value that represent if the header is required <code> def required_header(header): ,#LINE# #TAB# if header in IGNORE_HEADERS: #LINE# #TAB# #TAB# return False #LINE# #TAB# if header.startswith('HTTP_') or header == 'CONTENT_TYPE': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Check if variable is numeric  <code> def pass_number(inp, err=error.TYPE_MISMATCH): ","#LINE# #TAB# if not isinstance(inp, numbers.Number): #LINE# #TAB# #TAB# check_value(inp) #LINE# #TAB# #TAB# raise error.BASICError(err) #LINE# #TAB# return inp"
Get the compose label for a compose by ID . Only completed production composes have discoverable labels ; will return the empty string for other composes  <code> def label_from_cid(cid): ,"#LINE# #TAB# typ = parse_cid(cid, dic=True)['compose_type'] #LINE# #TAB# if typ != 'production': #LINE# #TAB# #TAB# return '' #LINE# #TAB# params = {'compose_id': cid} #LINE# #TAB# res = pdc_query('composes', params) #LINE# #TAB# if res: #LINE# #TAB# #TAB# return res[0]['compose_label'] #LINE# #TAB# return ''"
Find and returns the path to the nearest big datasets . Parameters ---------- envkey : str Environment variable name . It must contain a string defining the root Artifactory URL or path to local big data storage  <code> def get_bigdata_root(envkey='TEST_BIGDATA'): ,#LINE# #TAB# if envkey not in os.environ: #LINE# #TAB# #TAB# raise BigdataError('Environment variable {} is undefined'.format( #LINE# #TAB# #TAB# #TAB# envkey)) #LINE# #TAB# path = os.environ[envkey] #LINE# #TAB# if os.path.exists(path) or check_url(path): #LINE# #TAB# #TAB# return path #LINE# #TAB# return None
Resolver for ner and sequence classification label config  <code> def resolve_label(cfg: NLPConfig) ->NLPConfig: ,#LINE# #TAB# ner = cfg.pipeline[TRANSFORMERS_NER] #LINE# #TAB# if ner: #LINE# #TAB# #TAB# ner[LABELS] = get_ner_labels(ner[LABELS]) #LINE# #TAB# seq = cfg.pipeline[TRANSFORMERS_SEQ_CLASSIFIER] #LINE# #TAB# if seq: #LINE# #TAB# #TAB# seq[LABELS] = get_labels(seq[LABELS]) #LINE# #TAB# multiseq = cfg.pipeline[TRANSFORMERS_MULTILABEL_SEQ_CLASSIFIER] #LINE# #TAB# if multiseq: #LINE# #TAB# #TAB# multiseq[LABELS] = get_labels(multiseq[LABELS]) #LINE# #TAB# return cfg
"Attempts to compile the given source first as an expression and then as a statement if the first approach fails  <code> def try_compile(source, name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# c = compile(source, name, 'eval') #LINE# #TAB# except SyntaxError: #LINE# #TAB# #TAB# c = compile(source, name, 'exec') #LINE# #TAB# return c"
Get Num Spellcheck Errors <code> def get_num_spellcheck_errors(build_dir): ,"#LINE# #TAB# output_txt = os.path.join(build_dir, 'spellcheck', 'output.txt') #LINE# #TAB# res = 0 #LINE# #TAB# if not os.path.exists(output_txt): #LINE# #TAB# #TAB# return 1 #LINE# #TAB# with open(output_txt, 'r') as fp: #LINE# #TAB# #TAB# lines = fp.readlines() #LINE# #TAB# #TAB# res = len(lines) #LINE# #TAB# if res != 0: #LINE# #TAB# #TAB# ui.error('Found %i spelling error(s). See %s for the details' % ( #LINE# #TAB# #TAB# #TAB# res, output_txt)) #LINE# #TAB# return res"
"@return : An HTML string that encodes the given plaintext string . In particular , special characters ( such as C { ' < ' } and C { ' & ' } ) are escaped . @rtype : C{string } <code> def plaintext_to_html(s): ","#LINE# #TAB# s = s.replace('&', '&amp;').replace('""', '&quot;') #LINE# #TAB# s = s.replace('<', '&lt;').replace('>', '&gt;') #LINE# #TAB# return s"
"> > > nextWeek((2015,53 ) ) ( 2016 , 1 ) <code> def next_week(yw): ","#LINE# #TAB# return (iso_to_gregorian((*yw, 7)) + pendulum.duration(days=1) #LINE# #TAB# #TAB# ).isocalendar()[:2]"
Finds the location of the current Steam installation on Windows machines . Returns None for any non - Windows machines or for Windows machines where Steam is not installed  <code> def find_steam_location(): ,"#LINE# if registry is None: #LINE# #TAB# return None #LINE# key = registry.CreateKey(registry.HKEY_CURRENT_USER,""Software\Valve\Steam"") #LINE# return registry.QueryValueEx(key,""SteamPath"")[0]"
"Set content of given textarea element el to value  <code> def _set_textarea(el, value): ","#LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# el.text = value[""val""] #LINE# #TAB# #TAB# elif type(value) in [list, tuple]: #LINE# #TAB# #TAB# #TAB# el.text = ""\n\n"".join( #LINE# #TAB# #TAB# #TAB# #TAB# ""-- %s --\n%s"" % (item[""source""], item[""val""]) #LINE# #TAB# #TAB# #TAB# #TAB# for item in value #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# el.text = value"
"Return the variable i d by finding the line in the program AST and gettings its argument <code> def get_variable_id(line, program_ast): ","#LINE# #TAB# parsed_line = ast.dump(ast.parse(line).body[0]) #LINE# #TAB# node = get_node(parsed_line, program_ast) #LINE# #TAB# return node.value.args[0].id"
"Generate the consensus hash from the hash over the current ops and all previous required consensus hashes  <code> def make_snapshot_from_ops_hash( cls, record_root_hash, prev_consensus_hashes ): ",#LINE# #TAB# #TAB# all_hashes = prev_consensus_hashes[:] + [record_root_hash] #LINE# #TAB# #TAB# all_hashes.sort() #LINE# #TAB# #TAB# all_hashes_merkle_tree = MerkleTree( all_hashes ) #LINE# #TAB# #TAB# root_hash = all_hashes_merkle_tree.root() #LINE# #TAB# #TAB# consensus_hash = StateEngine.calculate_consensus_hash( root_hash ) #LINE# #TAB# #TAB# return consensus_hash
"Check whether a arginfo and b arginfo are the same signature . Actual names of arguments may differ . Default arguments may be different  <code> def same_signature(a, b): ",#LINE# #TAB# a_args = set(a.args) #LINE# #TAB# b_args = set(b.args) #LINE# #TAB# return len(a_args) == len(b_args #LINE# #TAB# #TAB# ) and a.varargs == b.varargs and a.varkw == b.varkw
Opens the json file and returns the data as dict object <code> def open_store(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(STORE) as d: #LINE# #TAB# #TAB# #TAB# data = json.load(d) #LINE# #TAB# except: #LINE# #TAB# #TAB# raise SniperError('Cannot load data.') #LINE# #TAB# return data
Renders an action button <code> def action_button(button: mara_page.response.ActionButton): ,"#LINE# #TAB# return [_.a(class_='btn', href=button.action, title=button.title)[_. #LINE# #TAB# #TAB# span(class_='fa fa-' + button.icon)[''], ' ', button.label]]"
"Add app to local registry by name <code> def add_usable_app(name, app): ",#LINE# #TAB# name = slugify(name) #LINE# #TAB# global usable_apps #LINE# #TAB# usable_apps[name] = app #LINE# #TAB# return name
Creates a socket suitable for SSDP searches  <code> def make_socket(): ,"#LINE# #TAB# mreq = struct.pack(""4sl"", socket.inet_aton(MCAST_IP), socket.INADDR_ANY) #LINE# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) #LINE# #TAB# sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# sock.bind(('', MCAST_PORT)) #LINE# #TAB# sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq) #LINE# #TAB# sock.settimeout(0.2) #LINE# #TAB# return sock"
Create whole genome contigs we want to process only non - alts  <code> def create_genome_regions(data): ,"#LINE# #TAB# work_dir = utils.safe_makedir(os.path.join(dd.get_work_dir(data), ""coverage"", dd.get_sample_name(data))) #LINE# #TAB# variant_regions = os.path.join(work_dir, ""target-genome.bed"") #LINE# #TAB# with file_transaction(data, variant_regions) as tx_variant_regions: #LINE# #TAB# #TAB# with open(tx_variant_regions, ""w"") as out_handle: #LINE# #TAB# #TAB# #TAB# for c in shared.get_noalt_contigs(data): #LINE# #TAB# #TAB# #TAB# #TAB# out_handle.write(""%s\t%s\t%s\n"" % (c.name, 0, c.size)) #LINE# #TAB# return variant_regions"
Check if NumPy 's correlate function reveals old behaviour <code> def numpy_has_correlate_flip_bug(): ,"#LINE# #TAB# if _globals._numpy_has_correlate_flip_bug is None: #LINE# #TAB# #TAB# a = num.array([0, 0, 1, 0, 0, 0, 0]) #LINE# #TAB# #TAB# b = num.array([0, 0, 0, 0, 1, 0, 0, 0]) #LINE# #TAB# #TAB# ab = num.correlate(a, b, mode='same') #LINE# #TAB# #TAB# ba = num.correlate(b, a, mode='same') #LINE# #TAB# #TAB# _globals._numpy_has_correlate_flip_bug = num.all(ab == ba) #LINE# #TAB# return _globals._numpy_has_correlate_flip_bug"
Get the SHA1 of the current repo <code> def get_repo_hexsha(git_folder): ,"#LINE# #TAB# repo = Repo(str(git_folder)) #LINE# #TAB# if repo.bare: #LINE# #TAB# #TAB# not_git_hexsha = ""notgitrepo"" #LINE# #TAB# #TAB# _LOGGER.warning(""Not a git repo, SHA1 used will be: %s"", not_git_hexsha) #LINE# #TAB# #TAB# return not_git_hexsha #LINE# #TAB# hexsha = repo.head.commit.hexsha #LINE# #TAB# _LOGGER.info(""Found REST API repo SHA1: %s"", hexsha) #LINE# #TAB# return hexsha"
Try to convert x to int  <code> def to_int(x): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(x) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return x
Takes the binary data stored in the binary string provided and extracts the data for each channel that was saved along with the sample rate and length of the data array  <code> def read_data_from_bytes(fileContent): ,"#LINE# #TAB# TotalDataLen = struct.unpack('Q', fileContent[:8])[0] #LINE# #TAB# NumOfChannels = struct.unpack('I', fileContent[8:12])[0] #LINE# #TAB# SampleTime = struct.unpack('d', fileContent[12:20])[0] #LINE# #TAB# AllChannelData = struct.unpack(""f"" * ((len(fileContent) -20) // 4), fileContent[20:]) #LINE# #TAB# LenOf1Channel = int(TotalDataLen/NumOfChannels) #LINE# #TAB# ChannelData = list(get_chunks(AllChannelData, LenOf1Channel)) #LINE# #TAB# return ChannelData, LenOf1Channel, NumOfChannels, SampleTime"
Load EDS points in the glue interface <code> def load_points(): ,#LINE# #TAB# dialog = LoadEDSPointsData() #LINE# #TAB# dialog_result = dialog.exec_() #LINE# #TAB# if dialog_result != qtw.QDialog.Accepted: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return dialog.results
"Search for a class string in module : param class_string : the class name to search : param module : the module : return : the class if found else None <code> def get_deep_class_from_string(class_string, module): ","#LINE# #TAB# for member in inspect.getmembers(module): #LINE# #TAB# #TAB# if member[0] == class_string: #LINE# #TAB# #TAB# #TAB# return getattr(module, class_string) #LINE# #TAB# return None"
"Utility function to load an image from disk  <code> def load_img(path, grayscale=False, target_size=None): ","#LINE# #TAB# img = io.imread(path, grayscale) #LINE# #TAB# if target_size: #LINE# #TAB# #TAB# img = transform.resize(img, target_size, preserve_range=True).astype('uint8') #LINE# #TAB# return img"
Parse yaml file of configuration parameters  <code> def parse_yaml(input_file): ,"#LINE# #TAB# with open(input_file, 'r') as yaml_file: #LINE# #TAB# #TAB# params = yaml.safe_load(yaml_file) #LINE# #TAB# return params"
Return list of all database locales available . : returns : dictionary of Language objects language_code : Language : rtype : dict <code> def database_locales(request): ,#LINE# #TAB# locales = {} #LINE# #TAB# for language in pyramid_basemodel.Session.query(Language).all(): #LINE# #TAB# #TAB# locales[language.language_code] = language #LINE# #TAB# return locales
Returns a user from the DCOS Enterprise . It returns None if none exists  <code> def get_user(uid): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# acl_url = urljoin(_acl_url(), 'users/{}'.format(uid)) #LINE# #TAB# #TAB# r = http.get(acl_url) #LINE# #TAB# #TAB# return r.json() #LINE# #TAB# except DCOSHTTPException as e: #LINE# #TAB# #TAB# if e.response.status_code == 400: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise"
"Best guess of intended case . manchester = > manchester chilton = > Chilton AAvTech = > AAvTech THe = > The imho = > IMHO <code> def get_case(word, correction): ",#LINE# #TAB# if word.istitle(): #LINE# #TAB# #TAB# return correction.title() #LINE# #TAB# if word.isupper(): #LINE# #TAB# #TAB# return correction.upper() #LINE# #TAB# if correction == word and not word.islower(): #LINE# #TAB# #TAB# return word #LINE# #TAB# if len(word) > 2 and word[:2].isupper(): #LINE# #TAB# #TAB# return correction.title() #LINE# #TAB# if not known_as_lower([correction]): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return CASE_MAPPED[correction] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return correction
Given a list of file names return those names that should be copied  <code> def filter_names(names): ,"#LINE# #TAB# names = [n for n in names #LINE# #TAB# #TAB# #TAB# if n not in EXCLUDE_NAMES] #LINE# #TAB# for pattern in EXCLUDE_PATTERNS: #LINE# #TAB# #TAB# names = [n for n in names #LINE# #TAB# #TAB# #TAB# #TAB# if (not fnmatch.fnmatch(n, pattern)) #LINE# #TAB# #TAB# #TAB# #TAB# and (not n.endswith('.py'))] #LINE# #TAB# return names"
Look for a dictionary that can not be pickled  <code> def find_unpickable_doc(dict_of_dict): ,"#LINE# #TAB# for name, collection in dict_of_dict.items(): #LINE# #TAB# #TAB# documents = collection.find() #LINE# #TAB# #TAB# for doc in documents: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# pickle.dumps(doc) #LINE# #TAB# #TAB# #TAB# except (PicklingError, AttributeError): #LINE# #TAB# #TAB# #TAB# #TAB# return name, doc #LINE# #TAB# return None, None"
Test if all sweeps in radar are sequentially ordered  <code> def is_radar_sequential(radar): ,"#LINE# #TAB# for i in range(radar.nsweeps): #LINE# #TAB# #TAB# if not _is_sweep_sequential(radar, i): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Create a C array of doubles from a list . Args : mylist : ( list of numerical values ) will be converted to a C array <code> def double_array(mylist): ,"#LINE# #TAB# c = doubleArray(len(mylist)) #LINE# #TAB# for i, v in enumerate(mylist): #LINE# #TAB# #TAB# c[i] = v #LINE# #TAB# return c"
Checks ( recursively ) if the directory contains .py or .pyc files <code> def contains_python_files_or_subdirs(folder): ,"#LINE# #TAB# for root, dirs, files in os.walk(folder): #LINE# #TAB# #TAB# if [filename for filename in files if filename.endswith('.py') or #LINE# #TAB# #TAB# #TAB# filename.endswith('.pyc')]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# for d in dirs: #LINE# #TAB# #TAB# #TAB# for _, subdirs, subfiles in os.walk(d): #LINE# #TAB# #TAB# #TAB# #TAB# if [filename for filename in subfiles if filename.endswith( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# '.py') or filename.endswith('.pyc')]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Assemble a block of relevant help text to be returned with an invalid request  <code> def compose_help_block_message(): ,"#LINE# #TAB# message = 'You must supply a numeric transaction id ' #LINE# #TAB# message += '(like ""/transactioninfo/<transaction_id>"")' #LINE# #TAB# return message"
An implementation of computing the analytic signal  <code> def analytic_signal(s): ,"#LINE# #TAB# sfft = fft(s) #LINE# #TAB# freq = fftfreq(len(s)) #LINE# #TAB# sfft[freq < 0.0] = np.zeros(np.sum(freq < 0.0), dtype='complex') #LINE# #TAB# sfft[freq >= 0.0] *= 2.0 #LINE# #TAB# z = ifft(sfft) #LINE# #TAB# return z"
hash a file with the sha256 algo <code> def create_hash(file): ,"#LINE# #TAB# hasher = sha256() #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file, 'rb') as f: #LINE# #TAB# #TAB# #TAB# hasher.update(f.read()) #LINE# #TAB# #TAB# #TAB# return hasher.hexdigest() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None"
"Build an output table parameter <code> def build_output_table(cls, name='inputTableName', output_name='output'): ",#LINE# #TAB# #TAB# obj = cls(name) #LINE# #TAB# #TAB# obj.exporter = 'get_output_table_name' #LINE# #TAB# #TAB# obj.output_name = output_name #LINE# #TAB# #TAB# return obj
Iterate through fasta file <code> def fasta_iter(fasta_name): ,"#LINE# #TAB# fh = open(fasta_name) #LINE# #TAB# faiter = (x[1] for x in groupby(fh, lambda line: line[0] == '>')) #LINE# #TAB# for header in faiter: #LINE# #TAB# #TAB# header = header.__next__()[1:].strip() #LINE# #TAB# #TAB# seq = ''.join(s.strip() for s in faiter.__next__()) #LINE# #TAB# #TAB# yield header, seq"
"Returns an open named graph  <code> def get_named_graph(identifier, store_id=DEFAULT_STORE, create=True): ","#LINE# #TAB# if not isinstance(identifier, URIRef): #LINE# #TAB# #TAB# identifier = URIRef(identifier) #LINE# #TAB# store = DjangoStore(store_id) #LINE# #TAB# graph = Graph(store, identifier=identifier) #LINE# #TAB# if graph.open(None, create=create) != VALID_STORE: #LINE# #TAB# #TAB# raise ValueError('The store identified by {} is not a valid store'. #LINE# #TAB# #TAB# #TAB# format(store_id)) #LINE# #TAB# return graph"
"Returns three sets of triples : "" in both "" , "" in first "" and "" in second ""  <code> def graph_diff(g1, g2): ","#LINE# #TAB# cg1 = to_canonical_graph(g1) #LINE# #TAB# cg2 = to_canonical_graph(g2) #LINE# #TAB# in_both = cg1 * cg2 #LINE# #TAB# in_first = cg1 - cg2 #LINE# #TAB# in_second = cg2 - cg1 #LINE# #TAB# return in_both, in_first, in_second"
"Merge properties if the keys on old and new are disjoint . Return a mapping containing the values from both old and new properties , but only if there is no key that exists in both  <code> def simple_merge_handler(previous_props, next_props): ","#LINE# #TAB# for name, value in previous_props.items(): #LINE# #TAB# #TAB# if name in next_props and value != next_props[name]: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Cannot merge - {0} changed (old value: {1}, new value: {2})' #LINE# #TAB# #TAB# #TAB# #TAB# .format(name, value, next_props[name])) #LINE# #TAB# #TAB# if name not in next_props: #LINE# #TAB# #TAB# #TAB# next_props[name] = value #LINE# #TAB# return next_props"
Return the MDL-12345 number from a commit message <code> def get_mdlfrom_commit_message(message): ,"#LINE# #TAB# mdl = None #LINE# #TAB# match = re.match('MDL(-|_)([0-9]+)', message, re.I) #LINE# #TAB# if match: #LINE# #TAB# #TAB# mdl = 'MDL-%s' % match.group(2) #LINE# #TAB# return mdl"
Filter all supported languages by given SEARCH_TEXT . : param search_text : String to look for in supported languages : return : ALl languages that contain the search_text  <code> def filter_languages(search_text: str) ->iter: ,#LINE# #TAB# for language in sorted(supported_languages): #LINE# #TAB# #TAB# if search_text is None or search_text in language: #LINE# #TAB# #TAB# #TAB# yield language
Escapes certain characters in a given string such that haproxy will parse the string as a single value <code> def escape_haproxy_config_string(value): ,"#LINE# #TAB# value = re.sub('\\\\', '\\\\\\\\', value) #LINE# #TAB# value = re.sub(' ', '\\ ', value) #LINE# #TAB# return value"
Automatically highlight interesting parts in the text : param text : : return : <code> def auto_text_highlight(text): ,"#LINE# #TAB# text = text.replace('In class', ""<span class='bg-dark chip'>Class</span>"") #LINE# #TAB# text = text.replace('In method', ""<span class='bg-dark chip'>Method</span>"" #LINE# #TAB# #TAB# ) #LINE# #TAB# text = text.replace('At ', ""<span class='bg-dark chip'>File</span>"") #LINE# #TAB# text = text.replace('line ', ""<span class='bg-dark chip'>line</span>"") #LINE# #TAB# text = text.replace('\n', '<br/>') #LINE# #TAB# return text"
Shallow clone of module ` m `  <code> def clone_module(m): ,"#LINE# #TAB# if isinstance(m, _module_wrapper.TFModuleWrapper): #LINE# #TAB# #TAB# return _module_wrapper.TFModuleWrapper(wrapped=clone_module(m. #LINE# #TAB# #TAB# #TAB# _tfmw_wrapped_module), module_name=m._tfmw_module_name, #LINE# #TAB# #TAB# #TAB# public_apis=m._tfmw_public_apis, deprecation=m. #LINE# #TAB# #TAB# #TAB# _tfmw_print_deprecation_warnings, has_lite=m._tfmw_has_lite) #LINE# #TAB# out = type(m)(m.__name__, m.__doc__) #LINE# #TAB# out.__dict__.update(m.__dict__) #LINE# #TAB# return out"
Conditional import of matplotlib <code> def import_matplotlib(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# from matplotlib import pyplot as plt #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# print('matplotlib not found. Please consider installing it to proceed.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# sys.exit(0) #LINE# #TAB# return plt
"Create a mesh from mesh data  <code> def from_data( name, coors, ngroups, conns, mat_ids, descs, igs = None ): ","#LINE# #TAB# #TAB# if igs is None: #LINE# #TAB# #TAB# #TAB# igs = range( len( conns ) ) #LINE# #TAB# #TAB# mesh = Mesh(name) #LINE# #TAB# #TAB# mesh._set_data(coors = coors, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ngroups = ngroups, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# conns = [conns[ig] for ig in igs], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# mat_ids = [mat_ids[ig] for ig in igs], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# descs = [descs[ig] for ig in igs]) #LINE# #TAB# #TAB# mesh._set_shape_info() #LINE# #TAB# #TAB# return mesh"
Get the first toplevel widget in a gtk . Builder hierarchy  <code> def get_first_builder_window(builder): ,"#LINE# #TAB# for obj in builder.get_objects(): #LINE# #TAB# #TAB# if isinstance(obj, gtk.Window): #LINE# #TAB# #TAB# #TAB# return obj"
"Add object to cache <code> def add_object(objects, summary): ","#LINE# #TAB# last_modified = int(summary.last_modified.timestamp()) #LINE# #TAB# logger.debug('Adding object ""%s"", size %i, modified %i', summary.key, #LINE# #TAB# #TAB# summary.size, last_modified) #LINE# #TAB# objects[summary.key] = summary.size, last_modified"
Load the time machine data set ( available in the English book )  <code> def load_data_time_machine(): ,"#LINE# #TAB# with open('../data/timemachine.txt') as f: #LINE# #TAB# #TAB# corpus_chars = f.read() #LINE# #TAB# corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ').lower() #LINE# #TAB# corpus_chars = corpus_chars[0:10000] #LINE# #TAB# idx_to_char = list(set(corpus_chars)) #LINE# #TAB# char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)]) #LINE# #TAB# vocab_size = len(char_to_idx) #LINE# #TAB# corpus_indices = [char_to_idx[char] for char in corpus_chars] #LINE# #TAB# return corpus_indices, char_to_idx, idx_to_char, vocab_size"
"Returns a tkinter . Image with background color bgcolor and img superimposed  <code> def color_transparency(img, bgcolor): ","#LINE# #TAB# if img.mode == 'RBGA': #LINE# #TAB# #TAB# img_w, img_h = img.size #LINE# #TAB# #TAB# red, green, blue = colstr2tuple(bgcolor) #LINE# #TAB# #TAB# background = Image.new('RGBA', (img_w, img_h), (red, green, blue, 255)) #LINE# #TAB# #TAB# background.paste(img, (0, 0), img) #LINE# #TAB# #TAB# return background #LINE# #TAB# else: #LINE# #TAB# #TAB# return img"
"Helper to get most frequent value for one field of a groupby <code> def get_most_common_field_value_for_group(df, groupby, fld): ","#LINE# #TAB# cts = df.groupby([groupby, fld]).size().to_frame('n_rows') #LINE# #TAB# pairs = cts.sort_values('n_rows', ascending=False).reset_index( #LINE# #TAB# #TAB# ).drop_duplicates([groupby], keep='first') #LINE# #TAB# ser = pairs.set_index(groupby)[[fld]] #LINE# #TAB# return ser"
setup refunc 's logger with given level and streaming to stderr <code> def config_loggers(): ,#LINE# #TAB# global __configured #LINE# #TAB# if __configured: #LINE# #TAB# #TAB# return #LINE# #TAB# sch = logging.StreamHandler(sys.stderr) #LINE# #TAB# sch.setFormatter(logging.Formatter('[fn %(levelname).1s] %(message)s')) #LINE# #TAB# sys_logger.handlers = [] #LINE# #TAB# sys_logger.addHandler(sch) #LINE# #TAB# sys_logger.propagate = False #LINE# #TAB# rch = logging.StreamHandler(sys.stderr) #LINE# #TAB# rch.setFormatter(logging.Formatter('[ >>] %(message)s')) #LINE# #TAB# remote_logger.handlers = [] #LINE# #TAB# remote_logger.addHandler(rch) #LINE# #TAB# remote_logger.propagate = False #LINE# #TAB# __configured = True
This function will return the expected path for an init script corresponding to the service name supplied . : arg name : name or path of the service to test for <code> def get_sysv_script(name): ,#LINE# #TAB# if name.startswith('/'): #LINE# #TAB# #TAB# result = name #LINE# #TAB# else: #LINE# #TAB# #TAB# result = '/etc/init.d/%s' % name #LINE# #TAB# return result
"Determine if two revisions have actually changed  <code> def default_diff(latest_config, current_config): ","#LINE# #TAB# pop_no_diff_fields(latest_config, current_config) #LINE# #TAB# diff = DeepDiff(latest_config, current_config, ignore_order=True) #LINE# #TAB# return diff"
Returns the requested content back in unicode . : param r : Response object to get unicode content from . Tried : 1 . charset from content - type 2 . fall back and replace all unicode characters : rtype : str <code> def get_unicode_from_response(r): ,"#LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB# 'In requests 3.0, get_unicode_from_response will be removed. For more information, please see the discussion on issue #2266. (This warning should only appear once.)' #LINE# #TAB# #TAB# , DeprecationWarning) #LINE# #TAB# tried_encodings = [] #LINE# #TAB# encoding = get_encoding_from_headers(r.headers) #LINE# #TAB# if encoding: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return str(r.content, encoding) #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# tried_encodings.append(encoding) #LINE# #TAB# try: #LINE# #TAB# #TAB# return str(r.content, encoding, errors='replace') #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return r.content"
"Sort by increasing amount but ensure that negative amounts are sorted first . max_amount is the maximum amount found in the stock  <code> def stock_sort_key(item, max_amount): ",#LINE# #TAB# amount = item['amount'] #LINE# #TAB# if amount < 0: #LINE# #TAB# #TAB# amount = max_amount - amount #LINE# #TAB# return -amount
Check if bam file is valid . Raises a ValueError if pysam cannot read the file  <code> def is_bam_valid(bam_file): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# f = pysam.AlignmentFile(bam_file) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise #LINE# #TAB# except: #LINE# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# f.close() #LINE# #TAB# return True
"Helper function to parse function arguments . This takes a logic or a string or None , and returns a logic or None  <code> def convert_logic_from_string(name): ","#LINE# #TAB# if name is not None and isinstance(name, six.string_types): #LINE# #TAB# #TAB# name = get_logic_by_name(name) #LINE# #TAB# return name"
"Get suggestions in case of OBJECT_HAS_NO_FUNC  <code> def suggest_obj_has_no(value, frame, groups): ","#LINE# #TAB# del value #LINE# #TAB# type_str, feature = groups #LINE# #TAB# if feature in ('length', 'len'): #LINE# #TAB# #TAB# return suggest_feature_not_supported('__len__', type_str, frame) #LINE# #TAB# return []"
"Reads the node to return a magnitude frequency distribution <code> def node_to_mfd(node, taglist): ","#LINE# #TAB# if ""incrementalMFD"" in taglist: #LINE# #TAB# #TAB# mfd = node_to_evenly_discretized( #LINE# #TAB# #TAB# #TAB# node.nodes[taglist.index(""incrementalMFD"")]) #LINE# #TAB# elif ""truncGutenbergRichterMFD"" in taglist: #LINE# #TAB# #TAB# mfd = node_to_truncated_gr( #LINE# #TAB# #TAB# #TAB# node.nodes[taglist.index(""truncGutenbergRichterMFD"")]) #LINE# #TAB# else: #LINE# #TAB# #TAB# mfd = None #LINE# #TAB# return mfd"
Return all digits that the given string starts with  <code> def digits_only(string): ,"#LINE# #TAB# match = re.match('\\D*(?P<digits>\\d+)', string) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return int(match.group('digits')) #LINE# #TAB# return 0"
"Process all headers to produce a set of combined headers that follows the rules defined by each instrument  <code> def get_templates(fnames, blend=True): ","#LINE# #TAB# if not blend: #LINE# #TAB# #TAB# newhdrs = blendheaders.getSingleTemplate(fnames[0]) #LINE# #TAB# #TAB# newtab = None #LINE# #TAB# else: #LINE# #TAB# #TAB# newhdrs, newtab = blendheaders.get_blended_headers(inputs=fnames) #LINE# #TAB# cleanTemplates(newhdrs[1],newhdrs[2],newhdrs[3]) #LINE# #TAB# return newhdrs, newtab"
Return a list of valid ecc names  <code> def ecc_map_names(): ,#LINE# #TAB# global __eccmap_names #LINE# #TAB# return __eccmap_names
"Normalizes the model MAE score , given the baseline score <code> def normalized_mae_score(model_mae, naive_mae): ",#LINE# #TAB# if model_mae > naive_mae: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1 - model_mae / naive_mae
Find XML files in the directory <code> def find_files(directory): ,"#LINE# #TAB# pattern = ""{directory}/*.xml"".format( #LINE# #TAB# #TAB# directory=directory, #LINE# #TAB# ) #LINE# #TAB# files = glob(pattern) #LINE# #TAB# return files"
"For an entity with width and height attributes , determine the bounding box if were positioned at ` ` ( x , y ) ` `  <code> def calc_bounds(xy, entity): ","#LINE# #TAB# left, top = xy #LINE# #TAB# right, bottom = left + entity.width, top + entity.height #LINE# #TAB# return [left, top, right, bottom]"
Eval a string without eval expressions  <code> def no_expression_eval(obj): ,"#LINE# #TAB# if isinstance(obj, str): #LINE# #TAB# #TAB# return obj #LINE# #TAB# result = ez.Eval(obj) #LINE# #TAB# string = str(result) #LINE# #TAB# if string == obj: #LINE# #TAB# #TAB# return result #LINE# #TAB# if isNumeric(result): #LINE# #TAB# #TAB# return obj #LINE# #TAB# if isinstance(result, list): #LINE# #TAB# #TAB# return result #LINE# #TAB# return result"
Return a restclients . models . uwnetid . UwEmailForwarding object on the given uwnetid <code> def get_email_forwarding(netid): ,"#LINE# #TAB# subscriptions = get_netid_subscriptions(netid, Subscription.SUBS_CODE_U_FORWARDING) #LINE# #TAB# for subscription in subscriptions: #LINE# #TAB# #TAB# if subscription.subscription_code == Subscription.SUBS_CODE_U_FORWARDING: #LINE# #TAB# #TAB# #TAB# return_obj = UwEmailForwarding() #LINE# #TAB# #TAB# #TAB# if subscription.data_value: #LINE# #TAB# #TAB# #TAB# #TAB# return_obj.fwd = subscription.data_value #LINE# #TAB# #TAB# #TAB# return_obj.permitted = subscription.permitted #LINE# #TAB# #TAB# #TAB# return_obj.status = subscription.status_name #LINE# #TAB# #TAB# #TAB# return return_obj #LINE# #TAB# return None"
"Takes in a Series which contains lists of the same structure and returns a Dataframe With a column for everey List Entry <code> def columns_from_list(df, series): ","#LINE# #TAB# name = series.name #LINE# #TAB# df1 = series.apply(lambda x: pd.Series(x)) #LINE# #TAB# df1 = df1.rename(columns=lambda x: name + '_' + str(x + 1)) #LINE# #TAB# df = df.drop(name, axis=1) #LINE# #TAB# df = pd.concat([df, df1], axis=1) #LINE# #TAB# return df"
"Intializes the document from an exisiting native StochKit XML file read from disk  <code> def from_string(cls, string): ",#LINE# #TAB# root = eTree.fromString(string) #LINE# #TAB# md = cls() #LINE# #TAB# md.document = root #LINE# #TAB# return md
"Scale a polygon . Parameters ---------- polygon : list of lists A list of polygon point coordinates . Returns ------- list of lists Reordered polygon point coordinates  <code> def scale_polygon(polygon, scale): ","#LINE# #TAB# center = centroid_points(polygon) #LINE# #TAB# scaled_polygon = [] #LINE# #TAB# for pt in polygon: #LINE# #TAB# #TAB# vector = subtract_vectors(pt, center) #LINE# #TAB# #TAB# new_pt = add_vectors(center, scale_vector(vector, scale)) #LINE# #TAB# #TAB# scaled_polygon.append(new_pt) #LINE# #TAB# return scaled_polygon"
wrap h5py.highlevel . Dataset objects into a more numpy like object <code> def wrap_data(data): ,#LINE# #TAB# if len(data.shape) == 1: #LINE# #TAB# #TAB# return data #LINE# #TAB# elif len(data.shape) == 2: #LINE# #TAB# #TAB# return glue_2d(data) #LINE# #TAB# elif len(data.shape) == 3: #LINE# #TAB# #TAB# return glue_3d(data) #LINE# #TAB# else: #LINE# #TAB# #TAB# return data
Build a dict of AAIndexObjects hashed by ID <code> def aaindex_lookup(records): ,#LINE# #TAB# result = {} #LINE# #TAB# for r in records: #LINE# #TAB# #TAB# result[r.ID] = r #LINE# #TAB# return result
Fix invalid symbols in unicode string  <code> def sanitize_unicode(u): ,"#LINE# #TAB# global _urc #LINE# #TAB# if not isinstance(u, unicode): #LINE# #TAB# #TAB# raise TypeError('Need unicode string') #LINE# #TAB# if not _urc: #LINE# #TAB# #TAB# rx = u'[\ud800-\udbff] [\udc00-\udfff]? | [\x00\udc00-\udfff]' #LINE# #TAB# #TAB# _urc = re.compile(rx, re.X) #LINE# #TAB# m = _urc.search(u) #LINE# #TAB# if m: #LINE# #TAB# #TAB# u = _urc.sub(_fix_utf8, u) #LINE# #TAB# return u"
Converts an SDK object to a dictionary . Fixes any SDK imposed object oddities . : param object : SDK object : returns : Dictionary <code> def get_sdk_object_dict(object): ,#LINE# #TAB# item_dict = object.to_dict() #LINE# #TAB# if 'is_admin_state_up' in item_dict: #LINE# #TAB# #TAB# item_dict['admin_state_up'] = item_dict['is_admin_state_up'] #LINE# #TAB# return item_dict
"Shift and scale matrix so its minimum value is placed at shift and its maximum value is scaled to scale <code> def shift_and_scale(matrix, shift, scale): ",#LINE# #TAB# zeroed = matrix - matrix.min() #LINE# #TAB# scaled = (scale - shift) * (zeroed / zeroed.max()) #LINE# #TAB# return scaled + shift
Validate the mimetype string  <code> def validate_mime_type(mimetype): ,"#LINE# #TAB# valid_prefixes = ['application', 'audio', 'font', 'example', 'image', #LINE# #TAB# #TAB# 'message', 'model', 'mulitpart', 'text', 'video'] #LINE# #TAB# validated = False #LINE# #TAB# for prefix in valid_prefixes: #LINE# #TAB# #TAB# if prefix + '/' == mimetype[:len(prefix) + 1]: #LINE# #TAB# #TAB# #TAB# validated = True #LINE# #TAB# return validated"
Return the file system type for mypath  <code> def get_fs_type(mypath): ,#LINE# #TAB# root_type = '' #LINE# #TAB# for part in psutil.disk_partitions(): #LINE# #TAB# #TAB# if part.mountpoint == os.path.sep: #LINE# #TAB# #TAB# #TAB# root_type = part.fstype #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if mypath.startswith(part.mountpoint): #LINE# #TAB# #TAB# #TAB# return part.fstype #LINE# #TAB# return root_type
Handle a response from the newton API <code> def handle_response (response): ,"#LINE# #TAB# response = json.loads(response.read()) #LINE# #TAB# if 'error' in response: #LINE# #TAB# #TAB# raise ValueError(response['error']) #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return json.loads(response['result']) #LINE# #TAB# #TAB# except (TypeError, json.decoder.JSONDecodeError): #LINE# #TAB# #TAB# #TAB# if response['result'] == 'NaN': #LINE# #TAB# #TAB# #TAB# #TAB# return float('nan') #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return response['result']"
Helper for loading a manifest yaml doc  <code> def load_manifest(data): ,"#LINE# #TAB# if isinstance(data, dict): #LINE# #TAB# #TAB# return data #LINE# #TAB# doc = yaml.safe_load(data) #LINE# #TAB# if not isinstance(doc, dict): #LINE# #TAB# #TAB# raise Exception(""Manifest didn't result in dict."") #LINE# #TAB# return doc"
"Called in an exception handler to ignore any exception , isssue a warning , and continue  <code> def warn_and_continue(exn): ",#LINE# #TAB# warnings.warn(repr(exn)) #LINE# #TAB# time.sleep(0.5) #LINE# #TAB# return True
"Alternate constructor to generate a curve instance by its name . Raises NotImplementedError if the name can not be mapped to a known supported curve NID  <code> def from_name(cls, name: str) ->'Curve': ","#LINE# #TAB# name = name.casefold() #LINE# #TAB# for supported_nid, supported_name in cls._supported_curves.items(): #LINE# #TAB# #TAB# if name == supported_name: #LINE# #TAB# #TAB# #TAB# instance = cls(nid=supported_nid) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# message = '{} is not supported curve name.'.format(name) #LINE# #TAB# #TAB# raise NotImplementedError(message) #LINE# #TAB# return instance"
Opens a stream and reads 8192 bytes from it . This is useful to check if a stream actually has data before opening the output  <code> def open_stream(stream): ,"#LINE# #TAB# global stream_fd #LINE# #TAB# try: #LINE# #TAB# #TAB# stream_fd = stream.open() #LINE# #TAB# except StreamError as err: #LINE# #TAB# #TAB# raise StreamError('Could not open stream: {0}'.format(err)) #LINE# #TAB# try: #LINE# #TAB# #TAB# log.debug('Pre-buffering 8192 bytes') #LINE# #TAB# #TAB# prebuffer = stream_fd.read(8192) #LINE# #TAB# except IOError as err: #LINE# #TAB# #TAB# stream_fd.close() #LINE# #TAB# #TAB# raise StreamError('Failed to read data from stream: {0}'.format(err)) #LINE# #TAB# if not prebuffer: #LINE# #TAB# #TAB# stream_fd.close() #LINE# #TAB# #TAB# raise StreamError('No data returned from stream') #LINE# #TAB# return stream_fd, prebuffer"
"Generate all bindings for the given predicate which are not true in the given model  <code> def compute_complementary_atoms(model, predicate): ",#LINE# #TAB# extension = model.get_extension(predicate) #LINE# #TAB# for binding in compute_signature_bindings(predicate.sort): #LINE# #TAB# #TAB# refd = tuple(symref(x) for x in binding) #LINE# #TAB# #TAB# if refd not in extension: #LINE# #TAB# #TAB# #TAB# yield binding
"Kullback leibler divergence between two covariance matrices A and B. : param A : First covariance matrix : param B : Second covariance matrix : returns : Kullback leibler divergence between A and B <code> def distance_kullback(A, B): ","#LINE# #TAB# dim = A.shape[0] #LINE# #TAB# logdet = numpy.log(numpy.linalg.det(B) / numpy.linalg.det(A)) #LINE# #TAB# kl = numpy.trace(numpy.dot(numpy.linalg.inv(B), A)) - dim + logdet #LINE# #TAB# return 0.5 * kl"
Returns a copy of an iterable object ( also copying all embedded iterables )  <code> def iter_copy(structure): ,"#LINE# #TAB# l = [] #LINE# #TAB# for i in structure: #LINE# #TAB# #TAB# if hasattr(i, '__iter__'): #LINE# #TAB# #TAB# #TAB# l.append(iter_copy(i)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# l.append(i) #LINE# #TAB# return l"
"Adapter for rendering an object of : class : ` crabpy.gateway.capakey . Perceel ` to json  <code> def item_perceel_adapter(obj, request): ","#LINE# #TAB# return {'id': obj.id, 'sectie': {'id': obj.sectie.id, 'afdeling': {'id': #LINE# #TAB# #TAB# obj.sectie.afdeling.id, 'naam': obj.sectie.afdeling.naam, #LINE# #TAB# #TAB# 'gemeente': {'id': obj.sectie.afdeling.gemeente.id, 'naam': obj. #LINE# #TAB# #TAB# sectie.afdeling.gemeente.naam}}}, 'capakey': obj.capakey, 'percid': #LINE# #TAB# #TAB# obj.percid, 'centroid': obj.centroid, 'bounding_box': obj.bounding_box}"
Read time from a path . Returns None if unreadable  <code> def read_time(path): ,#LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# return None #LINE# #TAB# with open(path) as fd: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return int(fd.read()) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None
"Convert duration to a string : n days , HH : MM : SS . : param duration_in_seconds : Duration in seconds . max : 67767976233532799  <code> def duration_to_date_and_time(duration_in_seconds: int) ->str: ","#LINE# #TAB# __check_if_negative(duration_in_seconds) #LINE# #TAB# dur_in_days = duration_to_int_days(duration_in_seconds) #LINE# #TAB# dur_to_time = duration_to_time(duration_in_seconds) #LINE# #TAB# if dur_in_days > 1: #LINE# #TAB# #TAB# return f'{dur_in_days} days, {dur_to_time}' #LINE# #TAB# elif dur_in_days > 0: #LINE# #TAB# #TAB# return f'{dur_in_days} day, {dur_to_time}' #LINE# #TAB# else: #LINE# #TAB# #TAB# return f'{duration_to_time(duration_in_seconds)}'"
"Create , populate and return the VersioneerConfig ( ) object  <code> def get_config(): ",#LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = 'v' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source = 'pak/_version.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"get the correct names of the columns holding the coordinates @param header Header of the CSV @returns ( lon , lat , crs ) where lon , lat , crs are the column names <code> def get_lon_lat_crs(searchList): ","#LINE# #TAB# lng = None #LINE# #TAB# lat = None #LINE# #TAB# crs = None #LINE# #TAB# for t in searchList: #LINE# #TAB# #TAB# if t.lower() in ('longitude', 'lon', 'long', 'lng'): #LINE# #TAB# #TAB# #TAB# lng = t #LINE# #TAB# #TAB# if t.lower() in ('latitude', 'lat'): #LINE# #TAB# #TAB# #TAB# lat = t #LINE# #TAB# #TAB# if t.lower() in ('crs', 'srs', 'coordinate reference systems', #LINE# #TAB# #TAB# #TAB# 'reference systems', 'spatial reference system'): #LINE# #TAB# #TAB# #TAB# crs = t #LINE# #TAB# return lng, lat, crs"
"Tag the repo using an annotated tag  <code> def git_tag(tag_name, push=False): ","#LINE# #TAB# with chdir(get_root()): #LINE# #TAB# #TAB# result = run_command('git tag -a {} -m ""{}""'.format(tag_name, tag_name), capture=True) #LINE# #TAB# #TAB# if push: #LINE# #TAB# #TAB# #TAB# if result.code != 0: #LINE# #TAB# #TAB# #TAB# #TAB# return result #LINE# #TAB# #TAB# #TAB# return run_command('git push origin {}'.format(tag_name), capture=True) #LINE# #TAB# #TAB# return result"
Create ` Keymap ` object from ` raw_keymaps `  <code> def create_keymaps(raw_keymaps): ,"#LINE# #TAB# keymap_objs = [] #LINE# #TAB# for raw_keymap in raw_keymaps: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# keymap_obj = KeymapCreater.create(raw_keymap) #LINE# #TAB# #TAB# #TAB# keymap_objs.append(keymap_obj) #LINE# #TAB# #TAB# except exception.InvalidKeymapException as e: #LINE# #TAB# #TAB# #TAB# exception.ExceptionRegister.record_by(raw_keymap, e) #LINE# #TAB# return keymap_objs"
Processes a CMOR style file path  <code> def get_cmor_fp_meta(fp): ,"#LINE# #TAB# directory_meta = list(CMIP5_FP_ATTS) #LINE# #TAB# meta = get_dir_meta(fp, directory_meta) #LINE# #TAB# meta.update(get_cmor_fname_meta(fp)) #LINE# #TAB# return meta"
Check whether we should enable DHCP at all . We wo n't even open our DHCP if no nodes are on introspection and node_not_found_hook is not set  <code> def should_enable_dhcp(): ,#LINE# #TAB# return node_cache.introspection_active( #LINE# #TAB# #TAB# ) or CONF.processing.node_not_found_hook is not None
parse module file name into author / name <code> def parse_module_name(string): ,"#LINE# #TAB# ModuleName = namedtuple('ModuleName', ['module', 'author']) #LINE# #TAB# module_name = string.strip(' \'""').rsplit('/', 1) #LINE# #TAB# try: #LINE# #TAB# #TAB# module = ModuleName(module=module_name[1], author=module_name[0]) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# module = ModuleName(module=module_name[0], author=None) #LINE# #TAB# LOG.debug('%s parsed into %s', string, module) #LINE# #TAB# return module"
"Run the given list of ` regexps ` which are expected to be ` ( regexp , replace ) ` tuples on the given ` content `  <code> def format_text(content, regexps): ","#LINE# #TAB# for search, replace in regexps: #LINE# #TAB# #TAB# content = re.sub(search, replace, content) #LINE# #TAB# return content"
"Remove rows with invalid ids <code> def step_remove_rows_with_missing_ids(df, id_cols=None): ",#LINE# #TAB# if not id_cols: #LINE# #TAB# #TAB# id_cols = list(meta.entity_definition[meta.entity_definition. #LINE# #TAB# #TAB# #TAB# is_unique_id == True]['name_clean'].values) #LINE# #TAB# df = df.dropna(subset=id_cols) #LINE# #TAB# return df
Stripped - down version of ofxget.mk_server_cfg ( ) <code> def mk_server_cfg(args: ofxget.ArgsType) ->configparser.SectionProxy: ,"#LINE# #TAB# server = args['server'] #LINE# #TAB# assert server #LINE# #TAB# if not LibraryConfig.has_section(server): #LINE# #TAB# #TAB# LibraryConfig[server] = {} #LINE# #TAB# cfg = LibraryConfig[server] #LINE# #TAB# for opt, opt_type in ofxget.CONFIGURABLE_SRVR.items(): #LINE# #TAB# #TAB# if opt in args: #LINE# #TAB# #TAB# #TAB# value = args[opt] #LINE# #TAB# #TAB# #TAB# default_value = ofxget.DEFAULTS[opt] #LINE# #TAB# #TAB# #TAB# if value != default_value and value not in ofxget.NULL_ARGS: #LINE# #TAB# #TAB# #TAB# #TAB# cfg[opt] = ofxget.arg2config(opt, opt_type, value) #LINE# #TAB# return cfg"
read text files in directory and returns them as array <code> def read_folder(directory): ,"#LINE# #TAB# res = [] #LINE# #TAB# for filename in os.listdir(directory): #LINE# #TAB# #TAB# with io.open(os.path.join(directory, filename), encoding=""utf-8"") as f: #LINE# #TAB# #TAB# #TAB# content = f.read() #LINE# #TAB# #TAB# #TAB# res.append(content) #LINE# #TAB# return res"
"Utility function to create date(time ) partitioned tables <code> def create_table_date_partitioning(field, expiration_ms=None): ","#LINE# #TAB# return {'type': 'time_partitioning', 'definition': _bigquery. #LINE# #TAB# #TAB# TimePartitioning(type_=_bigquery.TimePartitioningType.DAY, field= #LINE# #TAB# #TAB# field, expiration_ms=expiration_ms)}"
"Filter out vtodos from an iterator over calendars . : param calendars : Iterator over ( href , calendar ) tuples : return : Iterator over Calendar subcomponents <code> def extract_vtodos(calendars): ","#LINE# #TAB# for href, calendar in calendars: #LINE# #TAB# #TAB# for component in calendar.subcomponents: #LINE# #TAB# #TAB# #TAB# if component.name == 'VTODO': #LINE# #TAB# #TAB# #TAB# #TAB# yield component"
"Returns the list of completions generated by _ completer . Also stores start and end globally for later retrieval by get_begidx ( ) and get_endidx ( )  <code> def get_completions(text, start, end): ","#LINE# #TAB# global _begidx, _endidx #LINE# #TAB# _begidx, _endidx = start, end #LINE# #TAB# pygnurl.readline.completion.append_character = '\x00' #LINE# #TAB# pygnurl.readline.completion.suppress_append = False #LINE# #TAB# completions = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# completion = _completer(text, len(completions)) #LINE# #TAB# #TAB# if completion is None: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# completions.append(completion) #LINE# #TAB# return completions"
Ask the user to provide the API key <code> def request_key(): ,#LINE# #TAB# log('Please enter your API key (from ' + WEBAPP_URL + ' ):') #LINE# #TAB# api_key = getpass() #LINE# #TAB# return api_key
"Converts cartesian x y to polar r theta  <code> def to_polar(x, y, theta_units=""radians""): ","#LINE# #TAB# assert theta_units in [ #LINE# #TAB# #TAB# ""radians"", #LINE# #TAB# #TAB# ""degrees"", #LINE# #TAB# ], ""kwarg theta_units must specified in radians or degrees"" #LINE# #TAB# theta = atan2(y, x) #LINE# #TAB# r = sqrt(x ** 2 + y ** 2) #LINE# #TAB# if theta_units == ""degrees"": #LINE# #TAB# #TAB# theta = to_degrees(theta) #LINE# #TAB# return r, theta"
"Handles a GET request on a resource <code> def handle_resource_get(resource, request, parent_resource): ","#LINE# #TAB# if resource is None: #LINE# #TAB# #TAB# raise NotFoundException #LINE# #TAB# accepts = fp.prop('accept', request) #LINE# #TAB# for mime_pattern in accepts: #LINE# #TAB# #TAB# accepted = handle_resource_get_typed(mime_pattern, resource, request) #LINE# #TAB# #TAB# if accepted is not None: #LINE# #TAB# #TAB# #TAB# return accepted #LINE# #TAB# raise NotAcceptableException"
"Return matplotlib pyplot and FuncFormatter as a tuple : returns : a tuple of modules : rtype : tuple : param renderer : The renderer name . If renderer is None , it will not be set . : type renderer : str <code> def import_plot_and_formatter(renderer='agg'): ","#LINE# #TAB# from matplotlib import pyplot as plt #LINE# #TAB# from matplotlib.ticker import FuncFormatter #LINE# #TAB# return plt, FuncFormatter"
"Preprocesses a tensor encoding a batch of images . # Arguments x : input Numpy tensor , 4D. data_format : data format of the image tensor . # Returns Preprocessed tensor  <code> def preprocess_input(x): ",#LINE# #TAB# x *= 1.0 / 255 #LINE# #TAB# return x
"Given a graph G , makes sure that all of the pair betweennesses listed as attributes remain possible , and removes those that are not . Also adds new attributes where new edges have been added  <code> def fix_pair_betweennesses(G): ","#LINE# #TAB# for v in G.vs: #LINE# #TAB# #TAB# toDel = [] #LINE# #TAB# #TAB# neededPairs = {uw for uw in itertools.combinations(G.neighbors(v), 2)} #LINE# #TAB# #TAB# for pair in v['pb']: #LINE# #TAB# #TAB# #TAB# if pair not in neededPairs: #LINE# #TAB# #TAB# #TAB# #TAB# toDel.append(pair) #LINE# #TAB# #TAB# for d in toDel: #LINE# #TAB# #TAB# #TAB# del v['pb'][d] #LINE# #TAB# #TAB# for pair in neededPairs: #LINE# #TAB# #TAB# #TAB# if pair not in v['pb']: #LINE# #TAB# #TAB# #TAB# #TAB# v['pb'][pair] = 0"
Returns the duration of a wav file ( in seconds ) <code> def get_sound_file_duration(fn): ,"#LINE# #TAB# audiofile = wave.open(fn, 'r') #LINE# #TAB# params = audiofile.getparams() #LINE# #TAB# framerate = params[2] #LINE# #TAB# nframes = params[3] #LINE# #TAB# duration = float(nframes) / framerate #LINE# #TAB# return duration"
"Return a list of 2 - tuples - the argument name and its default value or a special value that indicates there is no default value  <code> def get_args_and_defaults(args, defaults): ","#LINE# #TAB# defaults = defaults or [] #LINE# #TAB# args_and_defaults = [(argument, default) for (argument, default) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# in zip_longest(args[::-1], defaults[::-1], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# fillvalue=NoDefault)] #LINE# #TAB# return args_and_defaults[::-1]"
"This extractor can be used if a blueprint can act as compound or leaf  <code> def hybrid_extractor(widget, data): ","#LINE# #TAB# if len(widget) and not attr_value('leaf', widget, data): #LINE# #TAB# #TAB# return compound_extractor(widget, data) #LINE# #TAB# return data.extracted"
"Get string representation of an object <code> def _get_repr(obj, pretty=False, indent=1): ","#LINE# #TAB# #TAB# if pretty: #LINE# #TAB# #TAB# #TAB# repr_value = pformat(obj, indent) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# repr_value = repr(obj) #LINE# #TAB# #TAB# if sys.version_info[0] == 2: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# repr_value = repr_value.decode('raw_unicode_escape') #LINE# #TAB# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# #TAB# repr_value = repr_value.decode('utf-8', 'replace') #LINE# #TAB# #TAB# return repr_value"
"Attempt to open each Elastic index in a list of indices : param elastic_client : Elastic client : param indices : List of indices to open : return : List of failed indices <code> def open_indices(elastic_client, indices): ","#LINE# #TAB# open_index_failures = [] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# open_index(elastic_client, index) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# open_index_failures.append(index) #LINE# #TAB# return open_index_failures"
"Dynamically creates a module with the given name  <code> def create_module(name, code=None): ","#LINE# #TAB# if name not in sys.modules: #LINE# #TAB# #TAB# sys.modules[name] = imp.new_module(name) #LINE# #TAB# module = sys.modules[name] #LINE# #TAB# if code: #LINE# #TAB# #TAB# print('executing code for %s: %s' % (name, code)) #LINE# #TAB# #TAB# exec(code in module.__dict__) #LINE# #TAB# #TAB# exec(""from %s import %s"" % (name, '*')) #LINE# #TAB# return module"
Converts a list of model objects into a list of dictionaries representing the data contained in those model objects . : param object_list : a list of model objects : return : a list of dictionaries <code> def list_to_json(object_list): ,#LINE# #TAB# result = [] #LINE# #TAB# for item in object_list: #LINE# #TAB# #TAB# result.append(item.to_json()) #LINE# #TAB# return result
"algle to 2x3 transform matrix theta : param angles : assume radian : return : outdim : [ angleshapes , 2 , 3 ] <code> def angle_to_theta(angles): ","#LINE# #TAB# c = tf.cos(angles) #LINE# #TAB# s = tf.sin(angles) #LINE# #TAB# z = tf.zeros_like(angles) #LINE# #TAB# t = tf.stack([[c, -s, z], [s, c, z]]) #LINE# #TAB# t = t.shiftdim(-2) #LINE# #TAB# return t"
Set the ID of reach data entry Set the order ID ( zero - based ) for each object to be plotted within the context of each panel Args : params ( dict ) : internal plotting parameter dictionary Returns : an updated parameter dictionary <code> def set_data_id(params): ,#LINE# #TAB# counter = dict() #LINE# #TAB# for p in params['data']: #LINE# #TAB# #TAB# panel_id = p['which_panel'] #LINE# #TAB# #TAB# if panel_id not in counter: #LINE# #TAB# #TAB# #TAB# counter[panel_id] = 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# counter[panel_id] += 1 #LINE# #TAB# #TAB# p['local_id'] = counter[panel_id] #LINE# #TAB# return params
Filters leaderboard columns to get the system column names . Args : columns(iterable ) : Iterable of leaderboard column names . Returns : list : A list of channel system names  <code> def get_system_columns(columns): ,"#LINE# #TAB# excluded_prefices = ['channel_', 'parameter_', 'property_'] #LINE# #TAB# return [col for col in columns if not any([col.startswith(prefix) for #LINE# #TAB# #TAB# prefix in excluded_prefices])]"
"Get learning rate based on schedule  <code> def get_lr(lr, epoch, steps, factor): ",#LINE# #TAB# for s in steps: #LINE# #TAB# #TAB# if epoch >= s: #LINE# #TAB# #TAB# #TAB# lr *= factor #LINE# #TAB# return lr
Infer the connectivity matrix associated with a state - by - node TPM in multidimensional form  <code> def infer_cm(tpm): ,"#LINE# #TAB# network_size = tpm.shape[-1] #LINE# #TAB# all_contexts = tuple(all_states(network_size - 1)) #LINE# #TAB# cm = np.empty((network_size, network_size), dtype=int) #LINE# #TAB# for a, b in np.ndindex(cm.shape): #LINE# #TAB# #TAB# cm[a][b] = infer_edge(tpm, a, b, all_contexts) #LINE# #TAB# return cm"
Get all the complex mode attributes in the network so that they can be used for mapping to resource scenarios later  <code> def get_all_attributes(network): ,#LINE# #TAB# attrs = network.attributes #LINE# #TAB# for n in network.nodes: #LINE# #TAB# #TAB# attrs.extend(n.attributes) #LINE# #TAB# for l in network.links: #LINE# #TAB# #TAB# attrs.extend(l.attributes) #LINE# #TAB# for g in network.resourcegroups: #LINE# #TAB# #TAB# attrs.extend(g.attributes) #LINE# #TAB# return attrs
Return a dictionary with file as keys and content as values  <code> def parse_file_dict(file_dict): ,#LINE# #TAB# result = {} #LINE# #TAB# for section in file_dict: #LINE# #TAB# #TAB# result.update(file_dict[section]) #LINE# #TAB# return result
Create a Metrics instance from a bigquery query or table  <code> def from_bigquery(sql): ,"#LINE# #TAB# if isinstance(sql, bq.Query): #LINE# #TAB# sql = sql._expanded_sql() #LINE# #TAB# parts = sql.split('.') #LINE# #TAB# if len(parts) == 1 or len(parts) > 3 or any(' ' in x for x in parts): #LINE# #TAB# sql = '(' + sql + ')' #LINE# #TAB# else: #LINE# #TAB# sql = '`' + sql + '`' #LINE# #TAB# metrics = Metrics(bigquery=sql) #LINE# #TAB# return metrics"
A helper function that returns ContentType object based on its slugified verbose_name_plural  <code> def get_content_type(ct_name): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# ct = CONTENT_TYPE_MAPPING[ct_name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# for model in models.get_models(): #LINE# #TAB# #TAB# #TAB# if ct_name == slugify(model._meta.verbose_name_plural): #LINE# #TAB# #TAB# #TAB# #TAB# ct = ContentType.objects.get_for_model(model) #LINE# #TAB# #TAB# #TAB# #TAB# CONTENT_TYPE_MAPPING[ct_name] = ct #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise Http404 #LINE# #TAB# return ct
This function links the user 's choice of transformation with its inverse <code> def itransform_define(transform): ,#LINE# #TAB# if transform == 'tanh': #LINE# #TAB# #TAB# return np.arctanh #LINE# #TAB# elif transform == 'exp': #LINE# #TAB# #TAB# return np.log #LINE# #TAB# elif transform == 'logit': #LINE# #TAB# #TAB# return Family.logit #LINE# #TAB# elif transform is None: #LINE# #TAB# #TAB# return np.array #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
"Attempt to evaluate promise , even if obj is not a promise  <code> def maybe_evaluate(obj: Any) ->Any: ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return obj.__maybe_evaluate__() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return obj
Return the list of paths for given Python module . Args : module_name ( str ) : Name of the Python module . Returns : : obj:`list ` of : obj:`str ` : List of source paths for the module  <code> def get_module_sources(module_name: str) ->typing.List[str]: ,#LINE# #TAB# module = importlib.__import__(module_name) #LINE# #TAB# return [f'{module_path}/**/*.py' for module_path in module.__path__]
Returns valid tags an extension module might have <code> def extension_module_tags(): ,#LINE# #TAB# import sysconfig #LINE# #TAB# tags = [] #LINE# #TAB# if six.PY2: #LINE# #TAB# #TAB# multiarch = sysconfig.get_config_var('MULTIARCH') #LINE# #TAB# #TAB# if multiarch is not None: #LINE# #TAB# #TAB# #TAB# tags.append(multiarch) #LINE# #TAB# else: #LINE# #TAB# #TAB# tags.append(sysconfig.get_config_var('SOABI')) #LINE# #TAB# #TAB# tags.append('abi3') #LINE# #TAB# tags = [t for t in tags if t] #LINE# #TAB# return tags
"Does a CGI form contain the key? <code> def cgi_parameter_exists(form: cgi.FieldStorage, key: str) -> bool: ","#LINE# #TAB# s = get_cgi_parameter_str(form, key) #LINE# #TAB# return s is not None"
Map an XML tree into a dict of dicts  <code> def recursive_dict(element): ,"#LINE# #TAB# if isinstance(element, (tuple, list)): #LINE# #TAB# #TAB# return tuple(recursive_dict(child) for child in element) #LINE# #TAB# return '{}{}'.format(element.tag, pprint.pformat(element.attrib, #LINE# #TAB# #TAB# compact=True, width=10000)), dict(map(recursive_dict, element) #LINE# #TAB# #TAB# ) or element.text"
Map SSMs to chromosomal locations  <code> def read_ssm_locs(in_file): ,"#LINE# #TAB# out = {} #LINE# #TAB# with open(in_file) as in_handle: #LINE# #TAB# #TAB# in_handle.readline() #LINE# #TAB# #TAB# for line in in_handle: #LINE# #TAB# #TAB# #TAB# sid, loc = line.split()[:2] #LINE# #TAB# #TAB# #TAB# chrom, pos = loc.split(""_"") #LINE# #TAB# #TAB# #TAB# out[sid] = (chrom, int(pos)) #LINE# #TAB# return out"
Return unique machine identifier . This is software - based but fairly standardized from the /etc / machine - id file . We can potentially rely on this for uniquely identifying a node  <code> def get_machine_id(): ,#LINE# #TAB# if os.path.isfile('/etc/machine-id'): #LINE# #TAB# #TAB# return readSysFile('/etc/machine-id') #LINE# #TAB# elif os.path.isfile('/var/lib/dbus/machine-id'): #LINE# #TAB# #TAB# return readSysFile('/var/lib/dbus/machine-id') #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
Indicates if the request context is a normal user  <code> def is_user_context(context): ,"#LINE# #TAB# if not context or not isinstance(context, RequestContext): #LINE# #TAB# #TAB# return False #LINE# #TAB# if context.is_admin: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
Retrieves AWS Account Number <code> def account_number(profile): ,#LINE# #TAB# session = boto3.Session(profile_name=profile) #LINE# #TAB# client = session.client('sts') #LINE# #TAB# return client.get_caller_identity()['Account']
Small hack function to eliminate double peak values from a timeserie Parameters ----------- data : pd . DataFrame <code> def getridof_double_peaks(data): ,#LINE# #TAB# diff = data.diff() #LINE# #TAB# data[diff == 0.0] = data[diff == 0.0] - 0.001 #LINE# #TAB# return data
"Returns the ` execute ( ) definition based on the arrays in ` operand_list ` <code> def gen_function_prototype(operand_list, operand_name_list=None): ","#LINE# #TAB# dtype_list = [dtype_to_c99(t.dtype) for t in operand_list] #LINE# #TAB# ret = '#include <stdint.h>\n#include <complex.h>\n' #LINE# #TAB# ret += 'void execute(' #LINE# #TAB# for i in range(len(dtype_list)): #LINE# #TAB# #TAB# ret += '%s *' % dtype_list[i] #LINE# #TAB# #TAB# if operand_name_list is None: #LINE# #TAB# #TAB# #TAB# ret += 'a%d, ' % i #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# ret += '%s, ' % operand_name_list[i] #LINE# #TAB# return '%s)\n' % ret[:-2]"
Function to get rest host <code> def get_rest_host(): ,"#LINE# #TAB# host = config_get('rest', 'host') #LINE# #TAB# url_prefix = get_rest_url_prefix() #LINE# #TAB# while host.endswith('/'): #LINE# #TAB# #TAB# host = host[:-1] #LINE# #TAB# if url_prefix: #LINE# #TAB# #TAB# host = ''.join([host, url_prefix]) #LINE# #TAB# return host"
Import Load Cell data from as a .txt file and creates a numpy array of values of sensed data Parameters ---------- filename : .txt file The first parameter . Returns ------- lc_input_data : numpy array numpy array of sensed load cell data <code> def get_lcdata(filename): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# lc_input_data = np.loadtxt(filename) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# print('Load Cell Input Data File Read Error', filename) #LINE# #TAB# return lc_input_data"
Strips typedef info from a function signature : param signature : function signature : return : string that can be used to construct function calls with the same variable names and ordering as in the function signature <code> def remove_typedefs(signature: str) ->str: ,"#LINE# #TAB# typedefs = ['const realtype *', 'const double *', 'const realtype ', #LINE# #TAB# #TAB# 'double *', 'realtype *', 'const int ', 'int ', #LINE# #TAB# #TAB# 'SUNMatrixContent_Sparse '] #LINE# #TAB# for typedef in typedefs: #LINE# #TAB# #TAB# signature = signature.replace(typedef, ' ') #LINE# #TAB# return signature"
"Remove collect 's static root folder from list  <code> def collect_staticroot_removal(app, blueprints): ",#LINE# #TAB# collect_root = app.extensions['collect'].static_root #LINE# #TAB# return [bp for bp in blueprints if bp.has_static_folder and bp. #LINE# #TAB# #TAB# static_folder != collect_root]
"Datapipeline API is throttling us as all the pipelines are started at the same time . We would like to uniformly distribute the startTime over a 60 minute window  <code> def next_datetime_with_utc_hour(table_name, utc_hour): ","#LINE# #TAB# today = datetime.date.today() #LINE# #TAB# start_date_time = datetime.datetime( #LINE# #TAB# #TAB# year=today.year, #LINE# #TAB# #TAB# month=today.month, #LINE# #TAB# #TAB# day=today.day, #LINE# #TAB# #TAB# hour=utc_hour, #LINE# #TAB# #TAB# minute=_get_deterministic_value_for_table_name(table_name, 60), #LINE# #TAB# #TAB# second=_get_deterministic_value_for_table_name(table_name, 60) #LINE# #TAB# ) #LINE# #TAB# if start_date_time < datetime.datetime.utcnow(): #LINE# #TAB# #TAB# one_day = datetime.timedelta(days=1) #LINE# #TAB# #TAB# start_date_time += one_day #LINE# #TAB# return start_date_time"
"Copy the field at name from fromDic to toDic only if it exists @param fromDic : Origin Dictionary @param toDic : Destination Dictionary @param keys : Array of keys of the elements to copy <code> def copy_field_if_exists(fromDic, toDic, keys): ",#LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if key in fromDic: #LINE# #TAB# #TAB# #TAB# toDic[key] = fromDic[key]
Turns a list of urls using arrow notation into a list of Source objects <code> def parse_arrow(sourcelist: Iterable[str]) ->List[Source]: ,"#LINE# #TAB# result: List[Source] = [] #LINE# #TAB# arrow = False #LINE# #TAB# for value in sourcelist: #LINE# #TAB# #TAB# if arrow: #LINE# #TAB# #TAB# #TAB# result[-1] = Source(result[-1].url, value) #LINE# #TAB# #TAB# #TAB# arrow = False #LINE# #TAB# #TAB# elif value == '->': #LINE# #TAB# #TAB# #TAB# arrow = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# url = urllib.parse.urlparse(value) #LINE# #TAB# #TAB# #TAB# result.append(Source(value, os.path.basename(url.path))) #LINE# #TAB# return result"
"Walk the package resources . Implementation from os.walk  <code> def pkg_walk(package, top): ","#LINE# #TAB# names = pkg_resources.resource_listdir(package, top) #LINE# #TAB# dirs, nondirs = [], [] #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# if pkg_resources.resource_isdir(package, posixpath.join(top, name)): #LINE# #TAB# #TAB# #TAB# dirs.append(name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# nondirs.append(name) #LINE# #TAB# yield top, dirs, nondirs #LINE# #TAB# for name in dirs: #LINE# #TAB# #TAB# new_path = posixpath.join(top, name) #LINE# #TAB# #TAB# for out in pkg_walk(package, new_path): #LINE# #TAB# #TAB# #TAB# yield out"
Returns a BiopaxProcessor for a BioPAX model object  <code> def process_model(model): ,#LINE# #TAB# bp = BiopaxProcessor(model) #LINE# #TAB# bp.get_modifications() #LINE# #TAB# bp.get_regulate_activities() #LINE# #TAB# bp.get_regulate_amounts() #LINE# #TAB# bp.get_activity_modification() #LINE# #TAB# bp.get_gef() #LINE# #TAB# bp.get_gap() #LINE# #TAB# bp.get_conversions() #LINE# #TAB# bp.eliminate_exact_duplicates() #LINE# #TAB# return bp
Register the streaming http handlers in the global urllib2 default opener object . Returns the created OpenerDirector object  <code> def register_openers(): ,#LINE# #TAB# opener = urllib2.build_opener(*get_handlers()) #LINE# #TAB# urllib2.install_opener(opener) #LINE# #TAB# return opener
"Return the average Jaccard similarity between a brand 's followers and the followers of each exemplar . We merge all exemplar followers into one big pseudo - account  <code> def jaccard_merge(brands, exemplars): ","#LINE# #TAB# scores = {} #LINE# #TAB# exemplar_followers = set() #LINE# #TAB# for followers in exemplars.values(): #LINE# #TAB# #TAB# exemplar_followers |= followers #LINE# #TAB# for brand, followers in brands: #LINE# #TAB# #TAB# scores[brand] = _jaccard(followers, exemplar_followers) #LINE# #TAB# return scores"
"Convert a Positivist date to Julian day count  <code> def to_jd(year, month, day): ","#LINE# #TAB# legal_date(year, month, day) #LINE# #TAB# gyear = year + YEAR_EPOCH - 1 #LINE# #TAB# return gregorian.EPOCH - 1 + 365 * (gyear - 1) + floor((gyear - 1) / 4 #LINE# #TAB# #TAB# ) + -floor((gyear - 1) / 100) + floor((gyear - 1) / 400) + (month - 1 #LINE# #TAB# #TAB# ) * 28 + day"
Attempt to extract an error message from response body <code> def get_error_message(response): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# data = response.json() #LINE# #TAB# #TAB# if 'error_description' in data: #LINE# #TAB# #TAB# #TAB# return data['error_description'] #LINE# #TAB# #TAB# if 'error' in data: #LINE# #TAB# #TAB# #TAB# return data['error'] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return 'Unknown error'
"Guess lexer for given text limited to lexers for this file s extension  <code> def guess_lexer_using_filename(file_name, text): ","#LINE# #TAB# lexer, accuracy = None, None #LINE# #TAB# try: #LINE# #TAB# #TAB# lexer = custom_pygments_guess_lexer_for_filename(file_name, text) #LINE# #TAB# except SkipHeartbeat as ex: #LINE# #TAB# #TAB# raise SkipHeartbeat(u(ex)) #LINE# #TAB# except: #LINE# #TAB# #TAB# log.traceback(logging.DEBUG) #LINE# #TAB# if lexer is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# accuracy = lexer.analyse_text(text) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# log.traceback(logging.DEBUG) #LINE# #TAB# return lexer, accuracy"
"Try to import ` module ` . Returns module 's object on success , None on failure <code> def try_import(module): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return importlib.import_module(module) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
"Zero - pad indices so that file names will sort properly  <code> def stringify_index(index: Union[int, str], count: int) ->str: ","#LINE# #TAB# if isinstance(index, str): #LINE# #TAB# #TAB# return index #LINE# #TAB# assert 0 <= index < count #LINE# #TAB# decimal_digits = len(str(count - 1)) #LINE# #TAB# padded_format = '%0' + str(decimal_digits) + 'd' #LINE# #TAB# return padded_format % index"
"Decrypt ` ` inp ` ` string with a Private Key provided as string in PEM format  <code> def decrypt_open_sshprivate_key(privkeystring, inp): ",#LINE# #TAB# priv_key = rsa_key.RSAKey() #LINE# #TAB# priv_key.fromString(privkeystring) #LINE# #TAB# result = priv_key.decrypt(inp) #LINE# #TAB# return result
"Parse version string into release major minor version  <code> def parse_version(version_string: str) -> Tuple[str, str, str]: ","#LINE# #TAB# release, major, minor = version_string.split(""."", 2) #LINE# #TAB# return release, major, minor"
Determine how many cores we are able to use . Return 1 if we are not able to make a queue via pprocess . Parameters ---------- cores : int The number of cores that are requested . Returns ------- cores : int The number of cores available  <code> def check_cores(cores): ,"#LINE# #TAB# cores = min(multiprocessing.cpu_count(), cores) #LINE# #TAB# try: #LINE# #TAB# #TAB# queue = pprocess.Queue(limit=cores, reuse=1) #LINE# #TAB# except: #LINE# #TAB# #TAB# cores = 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# _ = queue.manage(pprocess.MakeReusable(fix_shape)) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# cores = 1 #LINE# #TAB# return cores"
"Main entry point for trading mode <code> def start_trading(args: Dict[str, Any]) ->int: ","#LINE# #TAB# from freqtrade.worker import Worker #LINE# #TAB# worker = None #LINE# #TAB# try: #LINE# #TAB# #TAB# worker = Worker(args) #LINE# #TAB# #TAB# worker.run() #LINE# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# logger.info('SIGINT received, aborting ...') #LINE# #TAB# finally: #LINE# #TAB# #TAB# if worker: #LINE# #TAB# #TAB# #TAB# logger.info('worker found ... calling exit') #LINE# #TAB# #TAB# #TAB# worker.exit() #LINE# #TAB# return 0"
"Builds the update values clause of an update statement based on the dictionary representation of an instance <code> def get_update_clause_from_dict(dictionary, datetime_format='%Y-%m-%d %H:%M:%S'): ","#LINE# #TAB# #TAB# items = [] #LINE# #TAB# #TAB# CoyoteDb.escape_dictionary(dictionary, datetime_format=datetime_format) #LINE# #TAB# #TAB# for k,v in dictionary.iteritems(): #LINE# #TAB# #TAB# #TAB# item = '{k} = {v}'.format(k=k, v=v) #LINE# #TAB# #TAB# #TAB# items.append(item) #LINE# #TAB# #TAB# clause = ', '.join(item for item in items) #LINE# #TAB# #TAB# return clause"
Splits multiple commands in a single line separated by ' ; ' <code> def split_line(line): ,"#LINE# #TAB# split_pattern = re.compile('((?:[^;""\']|""[^""]*""|\'[^\']*\')+)') #LINE# #TAB# return split_pattern.split(line)[1::2]"
convert NMEA formatting latitude / longitude to decimal degrees <code> def convert_nmea_degrees(nmea_degrees): ,#LINE# #TAB# deg = int(nmea_degrees * 0.01) #LINE# #TAB# decimal = (nmea_degrees - deg * 100) / 60.0 #LINE# #TAB# return deg + decimal
"Return team information and picks for the given gameweek <code> def get_team_picks(team_id, gameweek, game='FPL'): ","#LINE# #TAB# team_object = team.download_team_picks(game, team_id, gameweek) #LINE# #TAB# return team_object"
"Due to some events in PloneFormGen , we need to use this event to end the signupsheet configuration <code> def form_initialized(obj, event): ","#LINE# #TAB# action_adapters = [] #LINE# #TAB# for id in ('registrants', 'manager_notification_mailer'): #LINE# #TAB# #TAB# action_adapters.append(id) #LINE# #TAB# obj.actionAdapter = action_adapters"
"Returns HTTP basic auth string sent with request , or None . If request includes basic auth , result is string ' username : password '  <code> def get_request_basic_auth(request): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# authtype, authdata = request.META['HTTP_AUTHORIZATION'].split() #LINE# #TAB# #TAB# if authtype.lower() == 'basic': #LINE# #TAB# #TAB# #TAB# return base64.b64decode(authdata).decode('utf-8') #LINE# #TAB# except (IndexError, KeyError, TypeError, ValueError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return None"
Parses weekday info from TransXChange VehicleJourney element <code> def get_weekday_info(vehicle_journey_element): ,#LINE# #TAB# j = vehicle_journey_element #LINE# #TAB# try: #LINE# #TAB# #TAB# reg_weekdays = (j.OperatingProfile.RegularDayType.DaysOfWeek. #LINE# #TAB# #TAB# #TAB# get_elements()) #LINE# #TAB# #TAB# weekdays = [] #LINE# #TAB# #TAB# for elem in reg_weekdays: #LINE# #TAB# #TAB# #TAB# weekdays.append(elem._name) #LINE# #TAB# #TAB# if len(weekdays) == 1: #LINE# #TAB# #TAB# #TAB# return weekdays[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return '|'.join(weekdays) #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
Builds a CompilationDatabase from the database found in buildDir <code> def from_directory(buildDir): ,"#LINE# #TAB# errorCode = c_uint() #LINE# #TAB# try: #LINE# #TAB# #TAB# cdb = conf.lib.clang_CompilationDatabase_from_directory(buildDir. #LINE# #TAB# #TAB# #TAB# encode('utf-8'), byref(errorCode)) #LINE# #TAB# except CompilationDatabaseError: #LINE# #TAB# #TAB# raise CompilationDatabaseError(int(errorCode.value), #LINE# #TAB# #TAB# #TAB# 'CompilationDatabase loading failed') #LINE# #TAB# return cdb"
Get the installation directory for Steam games : return List of Steam library folders where games can be installed <code> def get_steam_library_folders(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# steam_dir = _find_steam_directory() #LINE# #TAB# #TAB# vdf_file_location = (steam_dir + os.path.sep + 'steamapps' + os. #LINE# #TAB# #TAB# #TAB# path.sep + 'libraryfolders.vdf') #LINE# #TAB# #TAB# with open(vdf_file_location) as adf_file: #LINE# #TAB# #TAB# #TAB# paths = [l.split('""')[3] for l in adf_file.readlines()[1:] if #LINE# #TAB# #TAB# #TAB# #TAB# ':\\\\' in l] #LINE# #TAB# #TAB# #TAB# return paths #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# return []"
Return a list of all ( long and short ) names for the tag <code> def all_names_for_tag(tag): ,#LINE# #TAB# longname = keyword_for_tag(tag) #LINE# #TAB# shortname = short_name(longname) #LINE# #TAB# names = [longname] #LINE# #TAB# if shortname: #LINE# #TAB# #TAB# names.append(shortname) #LINE# #TAB# return names
"Register a namespace for use by ORDF : param prefix : the namespace prefix that should be used : param namespace : an instance of ordf.namespace . Namespace <code> def register_ns(prefix, ns): ","#LINE# #TAB# from ordf import namespace #LINE# #TAB# from logging import getLogger #LINE# #TAB# log = getLogger('ordf.namespace') #LINE# #TAB# ns = Namespace(str(ns)) #LINE# #TAB# if hasattr(namespace, prefix.upper()): #LINE# #TAB# #TAB# log.warning('Registering %s: <%s> prefix already exists' % (prefix, ns) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# log.debug('Registering %s: <%s>' % (prefix, ns)) #LINE# #TAB# setattr(namespace, prefix.upper().replace('-', '_'), ns) #LINE# #TAB# namespaces[prefix] = ns"
Verifies whether is cold start and return a string used for struct logging Returns ------- str lower case bool as a string aws_lambda_logging does n't support bool ; cast cold start value to string <code> def is_cold_start() ->str: ,#LINE# #TAB# cold_start = 'false' #LINE# #TAB# global is_cold_start #LINE# #TAB# if is_cold_start: #LINE# #TAB# #TAB# cold_start = str(is_cold_start).lower() #LINE# #TAB# #TAB# is_cold_start = False #LINE# #TAB# return cold_start
"This converts from galactic coords to equatorial coordinates  <code> def galactic_to_equatorial(gl, gb): ","#LINE# #TAB# gal = SkyCoord(gl*u.degree, gl*u.degree, frame='galactic') #LINE# #TAB# transformed = gal.transform_to('icrs') #LINE# #TAB# return transformed.ra.degree, transformed.dec.degree"
"Set logger level to ` level ` within a context block . Do n't use this except for debugging please , it 's gross  <code> def logger_level(logger, level): ",#LINE# #TAB# initial = logger.level #LINE# #TAB# logger.level = level #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# logger.level = initial
Infer start activities from a Directly - Follows Graph Parameters ---------- dfg Directly - Follows Graph Returns ---------- start_activities Start activities in the log <code> def infer_start_activities(dfg): ,#LINE# #TAB# ingoing = get_ingoing_edges(dfg) #LINE# #TAB# outgoing = get_outgoing_edges(dfg) #LINE# #TAB# start_activities = [] #LINE# #TAB# for act in outgoing: #LINE# #TAB# #TAB# if act not in ingoing: #LINE# #TAB# #TAB# #TAB# start_activities.append(act) #LINE# #TAB# return start_activities
given the path of a file returns it 's extension as string : param file_path : the path of an input file : return : the extension of the file <code> def get_file_extension(file_path): ,#LINE# #TAB# if not file_path: #LINE# #TAB# #TAB# raise ValueError('file path is None') #LINE# #TAB# else: #LINE# #TAB# #TAB# file_extension = file_path.split('.')[-1] #LINE# #TAB# #TAB# if file_extension == '': #LINE# #TAB# #TAB# #TAB# raise ValueError('expected .[extension] file') #LINE# #TAB# #TAB# elif file_extension not in constants.FILE_EXTENSION_TO_READ_ATTRIBUTE.keys( #LINE# #TAB# #TAB# #TAB# ): #LINE# #TAB# #TAB# #TAB# raise ValueError('supported file types are \n %s' % constants. #LINE# #TAB# #TAB# #TAB# #TAB# FILE_EXTENSION_TO_READ_ATTRIBUTE.keys()) #LINE# #TAB# return file_extension
Return a list of stdio command 's parameters by iterating through its annotations <code> def get_cli_params(func): ,"#LINE# #TAB# parameters = list(inspect.signature(func).parameters.values()) #LINE# #TAB# cli_params = [] #LINE# #TAB# for parameter in parameters: #LINE# #TAB# #TAB# if parameter.name == 'server': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# annotation = parameter.annotation #LINE# #TAB# #TAB# if not isinstance(annotation, BaseParam): #LINE# #TAB# #TAB# #TAB# annotation = annotation() #LINE# #TAB# #TAB# if isinstance(annotation, BaseParam): #LINE# #TAB# #TAB# #TAB# annotation.name = parameter.name #LINE# #TAB# #TAB# #TAB# cli_params.append(annotation) #LINE# #TAB# return cli_params"
Retrieve content as HTML using the ident - hash ( uuid <code> def get_content_html(request): ,"#LINE# #TAB# result = _get_content_json() #LINE# #TAB# media_type = result['mediaType'] #LINE# #TAB# if media_type == COLLECTION_MIMETYPE: #LINE# #TAB# #TAB# content = tree_to_html(result['tree']) #LINE# #TAB# else: #LINE# #TAB# #TAB# content = result['content'] #LINE# #TAB# resp = request.response #LINE# #TAB# resp.body = content #LINE# #TAB# resp.status = ""200 OK"" #LINE# #TAB# resp.content_type = 'application/xhtml+xml' #LINE# #TAB# return result, resp"
"If typ is a forward reference , return a resolved version of it . If it is not , return typ ' as is ' : param typ : : return : <code> def resolve_forward_ref(typ): ",#LINE# #TAB# if is_forward_ref(typ): #LINE# #TAB# #TAB# return eval_forward_ref(typ) #LINE# #TAB# else: #LINE# #TAB# #TAB# return typ
Return either a string or the doi part of a URL  <code> def extract_doi(value): ,#LINE# #TAB# value = str(value) #LINE# #TAB# if is_doi(value): #LINE# #TAB# #TAB# return extract_doi(value) #LINE# #TAB# return value
[ -+]?[0 - 9]*\.?[0 - 9]+([eE][-+]?[0 - 9]+ ) ? <code> def t_number(t): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# t.value = float(t.value) #LINE# #TAB# except: #LINE# #TAB# #TAB# t.lexer.log.info('[%d]: Number %s is not valid!' % (t.lineno, t.value)) #LINE# #TAB# #TAB# t.value = 0 #LINE# #TAB# return t"
Convert a list of Uint8 to Int32 Ported from javascript : param _ bytes : a bytearray <code> def convert_to_int32(_bytes: list) ->list: ,#LINE# #TAB# result = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < len(_bytes): #LINE# #TAB# #TAB# result.append(_bytes[i] << 24 | _bytes[i + 1] << 16 | _bytes[i + 2] << #LINE# #TAB# #TAB# #TAB# 8 | _bytes[i + 3]) #LINE# #TAB# #TAB# i += 4 #LINE# #TAB# return result
"SRM87 equation from Table III  <code> def srm87_eqtableiii(T, abc): ",#LINE# #TAB# return abc[0] + abc[1] * 0.001 * (T - 298.15) + abc[2] * 0.001 * (T - #LINE# #TAB# #TAB# 303.15) ** 2
Load crs object from esri code via spatialreference . org . Parses based on the proj4 representation  <code> def from_esri_code(code): ,"#LINE# #TAB# code = str(code) #LINE# #TAB# proj4 = utils.crscode_to_string(""esri"", code, ""proj4"") #LINE# #TAB# crs = from_proj4(proj4) #LINE# #TAB# return crs"
"Compute the angle between two vectors of equal length : param v1 : numpy array : param v2 : numpy array : return : angle between the two vectors v1 and v2 in degrees <code> def calculate_angle(v1, v2): ","#LINE# #TAB# v1_u = v1 / np.linalg.norm(v1) #LINE# #TAB# v2_u = v2 / np.linalg.norm(v2) #LINE# #TAB# return np.arccos(np.dot(v1_u, v2_u)) * 360 / 2 / np.pi"
| Given a module service try to import its models module  <code> def import_models(module): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# module = importlib.import_module('{0}.models'.format(module)) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# else: #LINE# #TAB# #TAB# clsmembers = inspect.getmembers(module, lambda member: inspect.isclass(member) and member.__module__ == module.__name__) #LINE# #TAB# #TAB# return [kls for name, kls in clsmembers]"
"Converts relative URLs into absolute URLs . Used for RSS feeds to provide more complete HTML for item descriptions , but could also be used as a general richtext filter  <code> def absolute_urls(html): ","#LINE# #TAB# from bs4 import BeautifulSoup #LINE# #TAB# from yacms.core.request import current_request #LINE# #TAB# request = current_request() #LINE# #TAB# if request is not None: #LINE# #TAB# #TAB# dom = BeautifulSoup(html, 'html.parser') #LINE# #TAB# #TAB# for tag, attr in ABSOLUTE_URL_TAGS.items(): #LINE# #TAB# #TAB# #TAB# for node in dom.findAll(tag): #LINE# #TAB# #TAB# #TAB# #TAB# url = node.get(attr, '') #LINE# #TAB# #TAB# #TAB# #TAB# if url: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# node[attr] = request.build_absolute_uri(url) #LINE# #TAB# #TAB# html = str(dom) #LINE# #TAB# return html"
"One - Vs - All mode handler  <code> def one_vs_all_func(classes, table, TP, TN, FP, FN, class_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# report_classes = [str(class_name), ""~""] #LINE# #TAB# #TAB# report_table = {str(class_name): {str(class_name): TP[class_name], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""~"": FN[class_name]}, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""~"": {str(class_name): FP[class_name], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""~"": TN[class_name]}} #LINE# #TAB# #TAB# return [report_classes, report_table] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return [classes, table]"
"Reduce a dictionary by selecting a set of keys <code> def reduce_by_keys(orig_dict, keys, default=None): ","#LINE# #TAB# ret = {} #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# ret[key] = orig_dict.get(key, default) #LINE# #TAB# return ret"
"Filter to select concepts with minimum / maximum number of spikes and occurrences and first spike in the first bin position <code> def fca_filter(concept, winlen, min_c, min_z, max_c, max_z, min_neu): ",#LINE# #TAB# intent = tuple(concept.intent) #LINE# #TAB# extent = tuple(concept.extent) #LINE# #TAB# keep_concepts = min_z <= len(intent) <= max_z and min_c <= len(extent #LINE# #TAB# #TAB# ) <= max_c and len(np.unique(np.array(intent) // winlen) #LINE# #TAB# #TAB# ) >= min_neu and min(np.array(intent) % winlen) == 0 #LINE# #TAB# return keep_concepts
"Get bins that contain approximately an equal number of data points  <code> def get_equal_bins(probs: List[float], num_bins: int=10) ->Bins: ","#LINE# #TAB# sorted_probs = sorted(probs) #LINE# #TAB# binned_data = split(sorted_probs, num_bins) #LINE# #TAB# bins: Bins = [] #LINE# #TAB# for i in range(len(binned_data) - 1): #LINE# #TAB# #TAB# last_prob = binned_data[i][-1] #LINE# #TAB# #TAB# next_first_prob = binned_data[i + 1][0] #LINE# #TAB# #TAB# bins.append((last_prob + next_first_prob) / 2.0) #LINE# #TAB# bins.append(1.0) #LINE# #TAB# return bins"
"Concatenate file piece hashes <code> def make_pieces(files, psize): ","#LINE# #TAB# pieces = b'' #LINE# #TAB# with fileListConcatenator(files, psize) as f: #LINE# #TAB# #TAB# for piece in f: #LINE# #TAB# #TAB# #TAB# pieces += hashlib.sha1(piece).digest() #LINE# #TAB# return pieces"
"Determines whether a string represents True . As the name suggests , any input which is not explicitly evaluated to True will cause this method to return False . Args : string : the string to evaluate <code> def is_true(string) ->bool: ","#LINE# #TAB# assert isinstance(string, str) #LINE# #TAB# return string in ['True', 'true', '1']"
Locale negotiator base on the Accept - Language header <code> def locale_negotiator(request): ,"#LINE# #TAB# locale = 'en' #LINE# #TAB# if request.accept_language: #LINE# #TAB# #TAB# locale = request.accept_language.best_match(LANGUAGES) #LINE# #TAB# #TAB# locale = LANGUAGES.get(locale, 'en') #LINE# #TAB# return locale"
"Computes the Wilcoxon ranksum statistic Parameters ---------- array_one , array_two : iterable Two arrays of values , possibly of different length . Returns ------- exp_diff : float scalar value from the Wilcoxon ranksum statistic <code> def ranksum_statistic(array_one, array_two): ","#LINE# #TAB# array_one = check_array(array_one) #LINE# #TAB# array_two = check_array(array_two) #LINE# #TAB# from scipy.stats import ranksums #LINE# #TAB# stat, p_val = ranksums(array_one, array_two) #LINE# #TAB# return stat"
"arg = grid and r : 3 * 3 median filter <code> def find_max_with_bin(grid, rectangle): ","#LINE# #TAB# r = rectangle #LINE# #TAB# cutted = grid[r[0]:r[1], r[2]:r[3]] #LINE# #TAB# cutted = median_filter(cutted, size=(3, 3)) #LINE# #TAB# coord = np.unravel_index(cutted.argmax(), cutted.shape) #LINE# #TAB# return coord[0] + r[0], coord[1] + r[2]"
"Generates rst raw block for given image filename and format <code> def get_imageblock(filename, format=''): ","#LINE# #TAB# _, extension = os.path.splitext(filename) #LINE# #TAB# with open(filename, ""rb"") as image_file: #LINE# #TAB# #TAB# encoded_string = base64.b64encode(image_file.read()).decode('utf-8') #LINE# #TAB# return '\n\n.. raw:: html\n\n\t<img src=""data:image/' + (format if format else extension[1:]) + ';base64,' + encoded_string +'"">\n'"
Return True if the annotation if for a Dict <code> def is_dict(annotation: Any) ->bool: ,"#LINE# #TAB# return get_origin(annotation) is dict and getattr(annotation, '_name', None #LINE# #TAB# #TAB# ) == 'Dict'"
"Make an event message ready to be published or stored . Returns a serialized model of Event containing an event message <code> def message_from_event(event, secret): ","#LINE# #TAB# msg = event.serialize() #LINE# #TAB# msg['message_signature'] = compute_signature(msg, secret) #LINE# #TAB# return msg"
Get password for rar archive from user input  <code> def get_rar_password(skipUserInput): ,"#LINE# goodlogging.Log.Info(""EXTRACT"", ""RAR file needs password to extract"") #LINE# if skipUserInput is False: #LINE# #TAB# prompt = ""Enter password, 'x' to skip this file or 'exit' to quit this program: "" #LINE# #TAB# response = goodlogging.Log.Input(""EXTRACT"", prompt) #LINE# #TAB# response = util.CheckEmptyResponse(response) #LINE# else: #LINE# #TAB# response = 'x' #LINE# if response.lower() == 'x': #LINE# #TAB# goodlogging.Log.Info(""EXTRACT"", ""File extraction skipped without password"") #LINE# #TAB# return False #LINE# elif response.lower() == 'exit': #LINE# #TAB# goodlogging.Log.Fatal(""EXTRACT"", ""Program terminated by user 'exit'"") #LINE# else: #LINE# #TAB# return response"
"Retrieve a string , error if a blank value is given <code> def get_str_errblank(row, name, errors, error_message=None): ","#LINE# #TAB# if error_message is None: #LINE# #TAB# #TAB# error_message = _('No value in column {}').format(name) #LINE# #TAB# val = row.get(name, '').strip() #LINE# #TAB# if val == '': #LINE# #TAB# #TAB# errors.append(error_message) #LINE# #TAB# return val"
Split ( if needed ) and obtain the list of commands that were sent into Skelebot <code> def get_commands(args): ,#LINE# #TAB# commands = [] #LINE# #TAB# command = [] #LINE# #TAB# for arg in args: #LINE# #TAB# #TAB# if arg == '+': #LINE# #TAB# #TAB# #TAB# commands.append(command) #LINE# #TAB# #TAB# #TAB# command = [] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# command.append(arg) #LINE# #TAB# commands.append(command) #LINE# #TAB# return commands
Get host name provide by operating system <code> def get_host_name(): ,#LINE# #TAB# if sys.platform == 'win32': #LINE# #TAB# #TAB# host = os.getenv('COMPUTERNAME') #LINE# #TAB# else: #LINE# #TAB# #TAB# host = os.uname()[1] #LINE# #TAB# return host
get CPU type of computer <code> def get_proctype(): ,"#LINE# #TAB# osname = platform.system() #LINE# #TAB# proctype = '' #LINE# #TAB# if osname == 'Darwin': #LINE# #TAB# #TAB# import subprocess #LINE# #TAB# #TAB# proc = subprocess.Popen([ #LINE# #TAB# #TAB# #TAB# 'system_profiler SPHardwareDataType | grep ""Processor Name""'], #LINE# #TAB# #TAB# #TAB# shell=True, stdout=subprocess.PIPE) #LINE# #TAB# #TAB# output = proc.communicate()[0] #LINE# #TAB# #TAB# proctype = output.split(':')[1].split('\n')[0].lstrip() #LINE# #TAB# if osname == ['Linux', 'Windows', 'Win32']: #LINE# #TAB# #TAB# proctype = platform.processor() #LINE# #TAB# return proctype"
"Return a list of intermediate shapes under a transform parent : param geo : str , transform to list intermediate shapes for <code> def list_intermediates(geo): ","#LINE# #TAB# if not maya.cmds.objExists(geo): #LINE# #TAB# #TAB# raise exceptions.NodeExistsException(geo) #LINE# #TAB# if is_shape(geo): #LINE# #TAB# #TAB# geo = maya.cmds.listRelatives(geo, parent=True, path=True)[0] #LINE# #TAB# shapes = maya.cmds.listRelatives(geo, shapes=True, noIntermediate=True, #LINE# #TAB# #TAB# path=True) #LINE# #TAB# all_shapes = maya.cmds.listRelatives(geo, shapes=True, path=True) #LINE# #TAB# if not all_shapes: #LINE# #TAB# #TAB# return [] #LINE# #TAB# if not shapes: #LINE# #TAB# #TAB# return all_shapes #LINE# #TAB# intermediate_shapes = list(set(all_shapes) - set(shapes)) #LINE# #TAB# return intermediate_shapes"
"Get content - type based on magic library as * bytes * As libmagic bindings are provided via several ' magic ' packages , we try them in order <code> def get_magic_content_type(input): ","#LINE# #TAB# if magic is not None: #LINE# #TAB# #TAB# if hasattr(input, 'seek'): #LINE# #TAB# #TAB# #TAB# input.seek(0) #LINE# #TAB# #TAB# if hasattr(input, 'read'): #LINE# #TAB# #TAB# #TAB# input = input.read() #LINE# #TAB# #TAB# if hasattr(magic, 'detect_from_content'): #LINE# #TAB# #TAB# #TAB# result = magic.detect_from_content(input) #LINE# #TAB# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# #TAB# return result.mime_type #LINE# #TAB# #TAB# elif hasattr(magic, 'from_buffer'): #LINE# #TAB# #TAB# #TAB# return magic.from_buffer(input, mime=True) #LINE# #TAB# return None"
Tries to parse a date into a DateTime instance Returns ` None ` if it can not be parsed <code> def convert_date(text): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return pendulum.from_format(text, 'DD/M/YYYY') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return pendulum.from_format(text, 'DD MMM').set(year=pendulum. #LINE# #TAB# #TAB# #TAB# #TAB# now().year) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None"
Creates a blueprint and register its routes <code> def setup_blueprint(): ,"#LINE# #TAB# blueprint = Blueprint('Management', __name__) #LINE# #TAB# blueprint.route('/management/app-info', methods=['GET'])(app_info) #LINE# #TAB# blueprint.route('/management/health-check', methods=['GET'])(health_check) #LINE# #TAB# return blueprint"
"Strip url from protocol and trailing slash , in order to use it as config key <code> def get_hostname(url): ",#LINE# #TAB# url = api._ensure_protocol(url) #LINE# #TAB# return urlparse(url).hostname
"Create ColumnDataSource for copy number and breakpoints given a specific solution <code> def create_cnv_brk_sources(store, solution, chromosome_plot_info): ","#LINE# #TAB# cnv = retrieve_cnv_data(store, solution) #LINE# #TAB# cnv_data = prepare_cnv_data(cnv, chromosome_plot_info) #LINE# #TAB# brk_data = retrieve_brk_data(store, solution, chromosome_plot_info) #LINE# #TAB# assert cnv_data.notnull().all().all() #LINE# #TAB# assert brk_data.notnull().all().all() #LINE# #TAB# cnv_source = bokeh.models.ColumnDataSource(cnv_data) #LINE# #TAB# brk_source = bokeh.models.ColumnDataSource(brk_data) #LINE# #TAB# return cnv_source, brk_source"
"Add query parameter  <code> def add_argument_parameter(path, kwargs, param, prameter_type): ",#LINE# #TAB# path_list = path.split(prameter_type + '/') #LINE# #TAB# arg = '-' #LINE# #TAB# if len(path_list) > 1: #LINE# #TAB# #TAB# arg = path_list[1].split('/')[0] #LINE# #TAB# kwargs['params'][param] = arg
"Utility method to get a unique path that can be used for caching a download  <code> def calculate_cache_location_for_url(url, postfix=None): ","#LINE# #TAB# REPL_CHARS = '[^_\\-A-Za-z0-9.]+' #LINE# #TAB# if postfix is not None: #LINE# #TAB# #TAB# temp = os.path.join(url, postfix) #LINE# #TAB# else: #LINE# #TAB# #TAB# temp = url #LINE# #TAB# path = re.sub(REPL_CHARS, os.sep, temp) #LINE# #TAB# return path"
"Prompt the user for a password on stdin  <code> def get_password(entry=None, username=None, prompt=None, always_ask=False): ","#LINE# #TAB# password = None #LINE# #TAB# if username is None: #LINE# #TAB# #TAB# username = get_username() #LINE# #TAB# has_keychain = initialize_keychain() #LINE# #TAB# unlock_keychain(username) #LINE# #TAB# if prompt is None: #LINE# #TAB# #TAB# prompt = ""Enter %s's password: "" % username #LINE# #TAB# if has_keychain and entry is not None and always_ask is False: #LINE# #TAB# #TAB# password = get_password_from_keyring(entry, username) #LINE# #TAB# if password is None: #LINE# #TAB# #TAB# password = getpass.getpass(prompt=prompt) #LINE# #TAB# return password"
Standard optimization routines as used in lmfit require real data . This function takes a real FID generated from the optimization routine and converts it back into a true complex form  <code> def real_to_complex(real_fid): ,"#LINE# #TAB# np = int(real_fid.shape[0] / 2) #LINE# #TAB# complex_fid = numpy.zeros(np, 'complex') #LINE# #TAB# complex_fid[:] = real_fid[:np] #LINE# #TAB# imag_fid = real_fid[np:] #LINE# #TAB# complex_fid += 1j * imag_fid[::-1] #LINE# #TAB# return complex_fid"
"For Python 2 , make sure the string is properly converted to unicode : param basestring value : : return : <code> def to_string(value): ","#LINE# #TAB# if six.PY2: #LINE# #TAB# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# #TAB# value.encode('utf8') #LINE# #TAB# #TAB# elif isinstance(value, str): #LINE# #TAB# #TAB# #TAB# value = value.decode('utf8') #LINE# #TAB# else: #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
Generates a GPS time set reply packet . Returns : packet : A reply packet  <code> def set_time_reply(): ,"#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.Ack(BaseMsgCode.SetTime, AckCode.OK)) #LINE# #TAB# return packet"
Select an available repository <code> def pick_repo_common(key): ,"#LINE# #TAB# repository_list = get_json_value(key, {}) #LINE# #TAB# if not repository_list: #LINE# #TAB# #TAB# print('No repositories available.', file=sys.stderr) #LINE# #TAB# #TAB# raise NoRepoException() #LINE# #TAB# option = make_simple_choice(repository_list, 'Which Repository?') #LINE# #TAB# if option is None: #LINE# #TAB# #TAB# raise NoRepoException() #LINE# #TAB# repo_data = repository_list[option] #LINE# #TAB# return option, repo_data"
"Strip tweet message  <code> def strip_tweet(text, remove_url=True): ","#LINE# #TAB# if remove_url: #LINE# #TAB# #TAB# text = url_pattern.sub('', text) #LINE# #TAB# else: #LINE# #TAB# #TAB# text = expand_url(text) #LINE# #TAB# text = mention_pattern.sub('', text) #LINE# #TAB# text = html_parser.unescape(text) #LINE# #TAB# text = text.strip() #LINE# #TAB# return text"
Return True if x is a valid email address ; otherwise return False  <code> def valid_email(x: str) -> bool: ,"#LINE# #TAB# if isinstance(x, str) and re.match(EMAIL_PATTERN, x): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Displays whether wake on network is on or off if supported <code> def get_wake_on_network(): ,#LINE# #TAB# ret = salt.utils.mac_utils.execute_return_result( #LINE# #TAB# #TAB# 'systemsetup -getwakeonnetworkaccess') #LINE# #TAB# return salt.utils.mac_utils.validate_enabled( #LINE# #TAB# #TAB# salt.utils.mac_utils.parse_return(ret)) == 'on'
Returns the ip address of the given hostname Note : Will return none if the hostname is not found <code> def get_ip_address(hostname): ,"#LINE# #TAB# if 'SLURM_JOB_NODELIST' not in os.environ and hostname == get_hostname(): #LINE# #TAB# #TAB# return 'localhost' #LINE# #TAB# try: #LINE# #TAB# #TAB# _, _, ip_list = socket.gethostbyaddr(hostname) #LINE# #TAB# #TAB# ip_address = None if not ip_list else ip_list[0] #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# ip_address = None #LINE# #TAB# return ip_address"
"Encrypt data using Damgard Jurik Cryptosystem : param m : plaintext : param n : modulus : param g : random public integer g : param s : order n^s : return : <code> def damgard_jurik_encrypt(m, n, g, s): ","#LINE# #TAB# m = ensure_long(m) #LINE# #TAB# s1 = s + 1 #LINE# #TAB# ns1 = n ** s1 #LINE# #TAB# r = random.randint(2, ns1) #LINE# #TAB# enc = pow(g, m, ns1) * pow(r, n ** s, ns1) % ns1 #LINE# #TAB# return enc"
Dimensionless negative reaction rate <code> def m_n(T): ,#LINE# #TAB# T_dim = Delta_T * T + T_ref #LINE# #TAB# return m_n_dimensional(T_dim) / m_n_ref_dimensional
"Use co - occurrences to compute scores  <code> def get_trans_co(x2ys, n_trans): ","#LINE# #TAB# x2ys_co = dict() #LINE# #TAB# for x, ys in x2ys.items(): #LINE# #TAB# #TAB# ys = [y for y, cnt in sorted(ys.items(), key=operator.itemgetter(1), #LINE# #TAB# #TAB# #TAB# reverse=True)[:n_trans]] #LINE# #TAB# #TAB# x2ys_co[x] = ys #LINE# #TAB# return x2ys_co"
"MLE estimator of linear model . Parameters ---------- X : Design matrix y : Response variable Returns Intercept ( = 0 if fit_intercept is False),coefficients of MLE ; the regularization parameter before which the variable is not selected <code> def linear_mle_tmle(X, y, group_index, sample_weight=None): ","#LINE# #TAB# intercept = np.mean(y) #LINE# #TAB# d_beta = safe_sparse_dot(X.T, intercept - y) / X.shape[0] #LINE# #TAB# G = len(np.unique(group_index)) #LINE# #TAB# tmle = 1 / np.max([np.linalg.norm(d_beta[np.where(group_index == g)[0]], #LINE# #TAB# #TAB# 2) for g in range(0, G)]) #LINE# #TAB# return intercept, d_beta, tmle"
node is either online or standby <code> def is_online(peer): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return get_node_state(peer)['online'] and not get_node_state(peer)[ #LINE# #TAB# #TAB# #TAB# 'standby'] #LINE# #TAB# except: #LINE# #TAB# #TAB# return False
Gets a dict from a Colour object <code> def get_c_dict(colour): ,"#LINE# #TAB# return {'code': colour.code, 'full_name': colour.name, 'name': camel( #LINE# #TAB# #TAB# clean(colour.name)), 'alpha': colour.alpha, 'rgb': colour.rgb, #LINE# #TAB# #TAB# 'colour_attributes': colour.colour_attributes}"
"Return True if the given item is available for loan , False otherwise . : param item_pid : a dict containing ` value ` and ` type ` fields to uniquely identify the item  <code> def is_item_available_for_checkout(item_pid): ","#LINE# #TAB# config = current_app.config #LINE# #TAB# cfg_item_can_circulate = config['CIRCULATION_POLICIES']['checkout'].get( #LINE# #TAB# #TAB# 'item_can_circulate') #LINE# #TAB# if not cfg_item_can_circulate(item_pid): #LINE# #TAB# #TAB# return False #LINE# #TAB# search = search_by_pid(item_pid=item_pid, filter_states=config.get( #LINE# #TAB# #TAB# 'CIRCULATION_STATES_LOAN_ACTIVE')) #LINE# #TAB# search_result = search.execute() #LINE# #TAB# if ES_VERSION[0] >= 7: #LINE# #TAB# #TAB# return search_result.hits.total.value == 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return search_result.hits.total == 0"
Get the mtime associated with a module . If this is a . pyc or . pyo file and a corresponding . py file exists the time of the . py file is returned  <code> def module_getmtime(filename): ,"#LINE# #TAB# if os.path.splitext(filename)[1].lower() in ("".pyc"", "".pyo"") and os.path.exists(filename[:-1]): #LINE# #TAB# #TAB# return os.path.getmtime(filename[:-1]) #LINE# #TAB# if os.path.exists(filename): #LINE# #TAB# #TAB# return os.path.getmtime(filename) #LINE# #TAB# return None"
"Require a file which may exist in a parent directory . Searches for the file in the nested directory , and failing that , in its parent(s ) , up to and including the current directory ( but no further )  <code> def erequire_ancestor(nested_path: str) ->str: ","#LINE# #TAB# require_path = nested_path = e(nested_path) #LINE# #TAB# nested_dir = os.path.dirname(nested_path) #LINE# #TAB# path = os.path.basename(nested_path) #LINE# #TAB# while True: #LINE# #TAB# #TAB# if Stat.exists(nested_path): #LINE# #TAB# #TAB# #TAB# require_path = nested_path #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# if not nested_dir: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# nested_dir = os.path.dirname(nested_dir) #LINE# #TAB# #TAB# nested_path = os.path.join(nested_dir, path) #LINE# #TAB# require(require_path) #LINE# #TAB# return require_path"
"Factory returning related storage resources for ` used_by `  <code> def related_used_by(remote, entry): ","#LINE# #TAB# collections = remote.containers, remote.images, remote.profiles #LINE# #TAB# for collection in collections: #LINE# #TAB# #TAB# if entry.startswith(collection.uri): #LINE# #TAB# #TAB# #TAB# return collection.get_resource(entry) #LINE# #TAB# return entry"
"Find rows in m filled with masking value . : param m : Numpy matrix to be searched for masked rows . : param masking_value : Value that corresponds to missing contacts , defaults to 0 : returns : Indices of masked rows  <code> def find_masked_rows(m, masking_value=0): ","#LINE# #TAB# s = np.sum(m, 0) #LINE# #TAB# n_bins = m.shape[0] #LINE# #TAB# cutoff = n_bins * masking_value #LINE# #TAB# idxs = np.where(s == cutoff)[0] #LINE# #TAB# return idxs"
"Return True when point ( x , y ) is inside rect  <code> def point_in_rect(p, rect): ","#LINE# #TAB# x, y = p #LINE# #TAB# xMin, yMin, xMax, yMax = rect #LINE# #TAB# return xMin <= x <= xMax and yMin <= y <= yMax"
"finds the ' vertices ' in the equatorial zone conjugate to ' pole ' with width half ' width ' degrees <code> def equatorial_zone_vertices(vertices, pole, width=5): ","#LINE# #TAB# return [i for i, v in enumerate(vertices) if np.abs(np.dot(v, pole)) < #LINE# #TAB# #TAB# np.abs(np.sin(np.pi * width / 180))]"
"Replaces words that may be problematic <code> def fix_reserved_word(word, is_module=False): ","#LINE# #TAB# if is_module: #LINE# #TAB# #TAB# if word == 'logging': #LINE# #TAB# #TAB# #TAB# word = 'logging_' #LINE# #TAB# else: #LINE# #TAB# #TAB# if keyword.iskeyword(word): #LINE# #TAB# #TAB# #TAB# word += '_' #LINE# #TAB# #TAB# elif word in ['id', 'type', 'str', 'max', 'input', 'license', 'copyright', 'credits', 'help']: #LINE# #TAB# #TAB# #TAB# word += '_' #LINE# #TAB# return word"
"Check if the names passed are accessed undeclared . The return value is a set of all the undeclared names from the sequence of names found  <code> def find_undeclared(nodes, names): ",#LINE# #TAB# visitor = UndeclaredNameVisitor(names) #LINE# #TAB# try: #LINE# #TAB# #TAB# for node in nodes: #LINE# #TAB# #TAB# #TAB# visitor.visit(node) #LINE# #TAB# except VisitorExit: #LINE# #TAB# #TAB# pass #LINE# #TAB# return visitor.undeclared
"Saturates the logging server 's max number of connections , ensuring it departs its .accept ( ) loop  <code> def close_server(port): ","#LINE# #TAB# conn_timeout = 0.1 #LINE# #TAB# log_server = '127.0.0.1', port #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# socket.create_connection(log_server, conn_timeout) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# break"
Try to resolve the given DNS name to IPv4 addresses and return empty set on ANY error  <code> def resolve_to_ip_addresses(dns_name: str) ->set: ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# answers = dns.resolver.query(dns_name, 'A') #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return set() #LINE# #TAB# else: #LINE# #TAB# #TAB# return {answer.address for answer in answers}"
Discover what directory the command should run in  <code> def get_cwd(options): ,"#LINE# #TAB# if not options.cwd: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not os.path.exists(options.cwd): #LINE# #TAB# #TAB# raise exceptions.InvalidCwd(""can't --cwd to invalid path {0!r}"". #LINE# #TAB# #TAB# #TAB# format(options.cwd)) #LINE# #TAB# return options.cwd"
parse website seeds . @params seeds example : - url1 - user1 : pwd1@url1 - user1 : pwd1@url1|url2|user3 : pwd3@url3 <code> def parse_seeds(seeds): ,"#LINE# #TAB# seeds = seeds.strip().split('|') #LINE# #TAB# website_list = [] #LINE# #TAB# for seed in seeds: #LINE# #TAB# #TAB# if '@' not in seed: #LINE# #TAB# #TAB# #TAB# website = {'url': seed, 'auth': None} #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# user_pwd, url = seed.split('@') #LINE# #TAB# #TAB# #TAB# username, password = user_pwd.split(':') #LINE# #TAB# #TAB# #TAB# website = {'url': url, 'auth': (username, password)} #LINE# #TAB# #TAB# website_list.append(website) #LINE# #TAB# return website_list"
calculate sha256 values from files found in a directory and map them to their filepath directory { str } -- path to directory to search return { dict } -- dictionary with filepath as key and sha256 as value <code> def calc_sha256_from_dir_map(directory: str) ->dict: ,"#LINE# #TAB# sha256_map: dict = {} #LINE# #TAB# if os.path.exists(directory) and os.path.isfile(directory): #LINE# #TAB# #TAB# files = [os.path.join(directory, path) for path in os.listdir( #LINE# #TAB# #TAB# #TAB# directory) if os.path.isfile(os.path.join(directory, path))] #LINE# #TAB# #TAB# for filepath in files: #LINE# #TAB# #TAB# #TAB# with open(filepath, 'rb') as fin: #LINE# #TAB# #TAB# #TAB# #TAB# sha256_map.update({filepath: sha256(fin.read()).hexdigest()}) #LINE# #TAB# return sha256_map"
Convert an RGB ANTsImage to a Vector ANTsImage <code> def rgb_to_vector(image): ,"#LINE# #TAB# if image.pixeltype != 'unsigned char': #LINE# #TAB# #TAB# image = image.clone('unsigned char') #LINE# #TAB# idim = image.dimension #LINE# #TAB# libfn = utils.get_lib_fn('RgbToVector%i' % idim) #LINE# #TAB# new_ptr = libfn(image.pointer) #LINE# #TAB# new_img = iio.ANTsImage(pixeltype=image.pixeltype, dimension=image.dimension, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# components=3, pointer=new_ptr, is_rgb=False) #LINE# #TAB# return new_img"
"Get NETCONF Credential that matches name  <code> def get_netconf_cred_by_name(context, name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# query = context.session.query(models.BNPNETCONFCredential) #LINE# #TAB# #TAB# netconf_creds = query.filter_by(name=name).all() #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# LOG.info(_LI('no netconf credential found with name: %s'), name) #LINE# #TAB# #TAB# return #LINE# #TAB# return netconf_creds"
"merge two dictionaries  <code> def merge_dictionaries(base_dict, extra_dict): ",#LINE# #TAB# new_dict = base_dict.copy() #LINE# #TAB# new_dict.update(extra_dict) #LINE# #TAB# return new_dict
"Will thaw into a temporary location <code> def auto_thaw(vault_client, opt): ","#LINE# #TAB# icefile = opt.thaw_from #LINE# #TAB# if not os.path.exists(icefile): #LINE# #TAB# #TAB# raise aomi.exceptions.IceFile(""%s missing"" % icefile) #LINE# #TAB# thaw(vault_client, icefile, opt) #LINE# #TAB# return opt"
Return the dictionary of metadata from the database . This data is keyed using the cnx - epub data structure  <code> def get_metadata(ident_hash): ,"#LINE# #TAB# id, version = get_id_n_version(ident_hash) #LINE# #TAB# stmt = _get_sql('get-metadata.sql') #LINE# #TAB# args = dict(id=id, version=version) #LINE# #TAB# with db_connect() as db_conn: #LINE# #TAB# #TAB# with db_conn.cursor() as cursor: #LINE# #TAB# #TAB# #TAB# cursor.execute(stmt, args) #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# metadata = cursor.fetchone()[0] #LINE# #TAB# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# #TAB# raise NotFound(ident_hash) #LINE# #TAB# return metadata"
"Alternative way to look up BLAST for a given PDB ID . This function is a wrapper for get_raw_blast and parse_blast <code> def get_blast2(pdb_id, chain_id='A', output_form='HTML'): ","#LINE# #TAB# raw_results = get_raw_blast(pdb_id, chain_id=chain_id, output_form=output_form) #LINE# #TAB# out = parse_blast(raw_results) #LINE# #TAB# return out"
"Set the default value of the ` ` guess ` ` parameter for read ( ) Parameters ---------- guess : bool New default ` ` guess ` ` value ( e.g. , True or False ) <code> def set_guess(guess): ",#LINE# #TAB# global _GUESS #LINE# #TAB# _GUESS = guess
"Skip past chunk data <code> def skip_past_data_dd(data_stream, start): ",#LINE# #TAB# if _ord(data_stream[start]) != 221: #LINE# #TAB# #TAB# raise TypeError('Unexpected block format for 0xDD at 0x%x.' % start) #LINE# #TAB# length = _ord(data_stream[start + 1]) #LINE# #TAB# return start + 2 + length
Serializes the given text batch update into JSON . : param MtBatchTextSmsUpdate batch : the batch update to serialize : return : dictionary suitable for JSON serialization : rtype : dict <code> def text_batch_update(batch): ,#LINE# #TAB# fields = _batch_update_helper(batch) #LINE# #TAB# fields['type'] = 'mt_text' #LINE# #TAB# if batch.body: #LINE# #TAB# #TAB# fields['body'] = batch.body #LINE# #TAB# if batch.parameters == RESET: #LINE# #TAB# #TAB# fields['parameters'] = None #LINE# #TAB# elif batch.parameters: #LINE# #TAB# #TAB# fields['parameters'] = batch.parameters #LINE# #TAB# return fields
"Return the weight corresponding to single power  <code> def compute_value(power, wg): ","#LINE# #TAB# if power not in wg: #LINE# #TAB# #TAB# p1, p2 = power #LINE# #TAB# #TAB# if p1 == 0: #LINE# #TAB# #TAB# #TAB# yy = wg[(0, -1)] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(yy, p2 / 2).sum() / len(yy) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# xx = wg[(-1, 0)] #LINE# #TAB# #TAB# #TAB# wg[power] = numpy.power(xx, p1 / 2).sum() / len(xx) #LINE# #TAB# return wg[power]"
"Retrieve a HTTP response json object from the cache . Parameters ---------- url : string the url of the request Returns ------- response_json : dict cached response for url if it exists in the cache , otherwise None <code> def get_from_cache(url): ","#LINE# #TAB# if settings.use_cache: #LINE# #TAB# #TAB# cache_filepath = url_in_cache(url) #LINE# #TAB# #TAB# if cache_filepath is not None: #LINE# #TAB# #TAB# #TAB# with io.open(cache_filepath, encoding='utf-8') as cache_file: #LINE# #TAB# #TAB# #TAB# #TAB# response_json = json.load(cache_file) #LINE# #TAB# #TAB# #TAB# log('Retrieved response from cache file ""{}"" for URL ""{}""'. #LINE# #TAB# #TAB# #TAB# #TAB# format(cache_filepath, url)) #LINE# #TAB# #TAB# #TAB# return response_json"
build params { ... } : param dic : dict : return : str <code> def build_params(dic): ,"#LINE# #TAB# s = '' #LINE# #TAB# if not dic: #LINE# #TAB# #TAB# return s #LINE# #TAB# if not isinstance(dic, dict): #LINE# #TAB# #TAB# raise AssertionError('params must be dict type.') #LINE# #TAB# for k, v in dic.items(): #LINE# #TAB# #TAB# if s: #LINE# #TAB# #TAB# #TAB# s += ', {}=""{}""'.format(k, v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# s = '{}=""{}""'.format(k, v) #LINE# #TAB# return '{%s}' % s"
"Find the slot with the given name . It 's possible to specify a module name then forwarded to loadConfig ( )  <code> def get_slot(name, module=None): ","#LINE# #TAB# slot = loadConfig(module).get(name) #LINE# #TAB# if slot: #LINE# #TAB# #TAB# __log__.debug('using slot {0.name} from {0._source}'.format(slot)) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise RuntimeError('cannot find slot {0}{1}'.format(name, ' in ' + #LINE# #TAB# #TAB# #TAB# module if module else '')) #LINE# #TAB# return slot"
"Obtains the polished filenames for the weights and the CSVLogger of the model . # Arguments model_name : String to declare the name of the model # Returns Two strings that will serve as the filenames for the weights and the CSVLogger respectively  <code> def obtain_weights_csvlogger_filenames(body_backbone_CNN, image_backbone_CNN): ","#LINE# #TAB# prefix = 'trained_models/emotic_vad_' #LINE# #TAB# suffix = '_weights_tf_dim_ordering_tf_kernels.h5' #LINE# #TAB# weights_filename = prefix + body_backbone_CNN + suffix #LINE# #TAB# CSVLogger_filename = 'emotic_vad_' + body_backbone_CNN + '_training.csv' #LINE# #TAB# return weights_filename, CSVLogger_filename"
Receive an integer from a ` socket ` using ` struct.unpack ` : param sock : ` socket ` to receive integer from : return : ` int ` <code> def recv_int(sock: socket.socket) ->int: ,"#LINE# #TAB# received_bytes = b'' #LINE# #TAB# while len(received_bytes) < _INT_RECV_LEN: #LINE# #TAB# #TAB# received = sock.recv(_INT_RECV_LEN - len(received_bytes)) #LINE# #TAB# #TAB# if not received: #LINE# #TAB# #TAB# #TAB# raise NoDataReceived #LINE# #TAB# #TAB# received_bytes += received #LINE# #TAB# return struct.unpack('i', received_bytes)[0]"
Normalises a covariance matrix into a cross - correlation matrix . Parameters ---------- covmatrix : ndarray of rank-2 A covariance matrix . Returns ------- corrmatrix : ndarray of rank-2 The cross - correlation matrix  <code> def covmat2_corrmat(covmatrix): ,"#LINE# #TAB# if covmatrix.dtype not in ['float32', 'float64', 'float']: #LINE# #TAB# #TAB# covmatrix = covmatrix.astype(np.float) #LINE# #TAB# diagvalues = np.diag(covmatrix).astype(np.float) #LINE# #TAB# normmatrix = np.sqrt(np.outer(diagvalues, diagvalues)) #LINE# #TAB# corrmatrix = covmatrix / normmatrix #LINE# #TAB# return corrmatrix"
Function for getting formatted print <code> def beauty_print(pr_list): ,"#LINE# #TAB# result = [] #LINE# #TAB# for pr in pr_list: #LINE# #TAB# #TAB# pr_print = '_' * 20, 'Title: %s' % pr['title'], 'Description: %s' % pr[ #LINE# #TAB# #TAB# #TAB# 'description'], 'Link: %s' % pr['links']['html']['href'] #LINE# #TAB# #TAB# result.append('\n'.join(pr_print)) #LINE# #TAB# result = '\n'.join(result) #LINE# #TAB# return result"
"Simple wrapper that gets an attribute from ldap3 dictionary , converting empty values to the default specified . This is primarily for output to JSON <code> def get_entry_property(entry, prop, default=None, raw=False): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# if raw: #LINE# #TAB# #TAB# #TAB# value = entry['raw_attributes'][prop] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# value = entry['attributes'][prop] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return default #LINE# #TAB# if value == []: #LINE# #TAB# #TAB# return default #LINE# #TAB# try: #LINE# #TAB# #TAB# if len(value) == 1 and default != []: #LINE# #TAB# #TAB# #TAB# return value[0] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return value
Converts any user specified datatype into dbrcode and dtype  <code> def datatype_to_dtype(datatype): ,"#LINE# #TAB# if datatype not in BasicDbrTypes: #LINE# #TAB# #TAB# datatype = _datatype_to_dbr(datatype) #LINE# #TAB# return datatype, DbrCodeToType[datatype].dtype"
"Generate a window for a given length . : param n : an integer for the length of the window . : param mono : True for a mono window , False for a stereo window . : return : an numpy array containing the window value  <code> def make_win(n, mono=False): ","#LINE# #TAB# if mono: #LINE# #TAB# #TAB# win = np.hanning(n) + 1e-05 #LINE# #TAB# else: #LINE# #TAB# #TAB# win = np.array([np.hanning(n) + 1e-05, np.hanning(n) + 1e-05]) #LINE# #TAB# win = np.transpose(win) #LINE# #TAB# return win"
Get whether or not Glean is allowed to record and upload data  <code> def get_upload_enabled(cls) ->bool: ,#LINE# #TAB# if cls.is_initialized(): #LINE# #TAB# #TAB# return bool(_ffi.lib.glean_is_upload_enabled()) #LINE# #TAB# else: #LINE# #TAB# #TAB# return cls._upload_enabled
Returns the NNTP argument parser  <code> def setup_cmd_parser(cls): ,"#LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, offset=True, archive #LINE# #TAB# #TAB# =True) #LINE# #TAB# parser.parser.add_argument('host', help='NNTP server host') #LINE# #TAB# parser.parser.add_argument('group', help='Name of the NNTP group') #LINE# #TAB# return parser"
"Extracts elements of the given sequence . : param sequence : The sequence to get elements from . : param indices : The indices of the elements to get . : return : The selected elements  <code> def extract_by_index(sequence: Sequence, indices: Iterable[int]) ->Iterator: ",#LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# yield sequence[index]
Gets the MOLGENIS version lazily  <code> def get_version(): ,#LINE# #TAB# global _version #LINE# #TAB# if not _version: #LINE# #TAB# #TAB# _get_version() #LINE# #TAB# return _version
"Return a numpy array mapping events to slots <code> def slot_availability_array(events, slots): ","#LINE# #TAB# array = np.ones((len(events), len(slots))) #LINE# #TAB# for row, event in enumerate(events): #LINE# #TAB# #TAB# for col, slot in enumerate(slots): #LINE# #TAB# #TAB# #TAB# if slot in event.unavailability or event.duration > slot.duration: #LINE# #TAB# #TAB# #TAB# #TAB# array[row, col] = 0 #LINE# #TAB# return array"
"Find and load the module  <code> def find_and_load(name, import_): ","#LINE# #TAB# with _ModuleLockManager(name): #LINE# #TAB# #TAB# module = sys.modules.get(name, _NEEDS_LOADING) #LINE# #TAB# #TAB# if module is _NEEDS_LOADING: #LINE# #TAB# #TAB# #TAB# return find_and_load_unlocked(name, import_) #LINE# #TAB# if module is None: #LINE# #TAB# #TAB# message = 'import of {} halted; None in sys.modules'.format(name) #LINE# #TAB# #TAB# raise ModuleNotFoundError(message, name=name) #LINE# #TAB# _lock_unlock_module(name) #LINE# #TAB# return module"
"Updates the profile s auth entry with values set by the user . This will overwrite existing values  <code> def update_config_pwd(msg, cfg): ","#LINE# #TAB# msg_type = msg.__class__.__name__.lower() #LINE# #TAB# key_fmt = msg.profile + ""_"" + msg_type #LINE# #TAB# if isinstance(msg._auth, (MutableSequence, tuple)): #LINE# #TAB# #TAB# cfg.pwd[key_fmt] = "" :: "".join(msg._auth) #LINE# #TAB# else: #LINE# #TAB# #TAB# cfg.pwd[key_fmt] = msg._auth"
matches an item name to an i d in items.json <code> def get_id(item_name): ,"#LINE# #TAB# path = os.path.join(_ROOT, 'items.json') #LINE# #TAB# items = read_json_file(path) #LINE# #TAB# for item_info in items: #LINE# #TAB# #TAB# if item_name == item_info['name']: #LINE# #TAB# #TAB# #TAB# return item_info['id'] #LINE# #TAB# return None"
"Validates the result value is greater or equal to the given lower bound  <code> def result_value_above(value: float=None, lower: float=0.0) ->bool: ",#LINE# #TAB# values = extract_values(value) #LINE# #TAB# if not value: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'NaN' in values: #LINE# #TAB# #TAB# return False #LINE# #TAB# for v in values: #LINE# #TAB# #TAB# if v == '-Inf': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if v == 'Inf': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if float(v) < lower: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"Calculate percentage usage of ' used ' against ' total '  <code> def usage_percent(used, total, round_=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# ret = used / total * 100 #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# ret = 0.0 if isinstance(used, float) or isinstance(total, float) else 0 #LINE# #TAB# if round_ is not None: #LINE# #TAB# #TAB# return round(ret, round_) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ret"
"When enabled , Container _ _ str _ _ shows keys like _ _ index _ etc , otherwise and by default , it hides those keys . _ _ repr _ _ never shows private entries . : param enabled : bool <code> def set_global_print_private_entries(enabled=False): ",#LINE# #TAB# global globalPrintPrivateEntries #LINE# #TAB# globalPrintPrivateEntries = enabled
A chain - of - responsiblity that imports a module from an Iterable . : param modules : A sequence of module names to try importing . : return : The first module to successfully load  <code> def import_chain(modules: Iterable): ,#LINE# #TAB# module = None #LINE# #TAB# for m in modules: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = import_module(m) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# if module is None: #LINE# #TAB# #TAB# raise ImportError('No modules found') #LINE# #TAB# return module
"Finds key values in a nested dictionary . Returns a tuple of the dictionary in which the key was found along with the value <code> def dict_find(d, which_key): ","#LINE# #TAB# if isinstance(d, (list, tuple)): #LINE# #TAB# #TAB# for i in d: #LINE# #TAB# #TAB# #TAB# for result in dict_find(i, which_key): #LINE# #TAB# #TAB# #TAB# #TAB# yield result #LINE# #TAB# elif isinstance(d, dict): #LINE# #TAB# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# #TAB# if k == which_key: #LINE# #TAB# #TAB# #TAB# #TAB# yield d, v #LINE# #TAB# #TAB# #TAB# for result in dict_find(v, which_key): #LINE# #TAB# #TAB# #TAB# #TAB# yield result"
"Deserialize results from a given file handle  <code> def from_file(cls, f): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# for i, line in enumerate(f, 1): #LINE# #TAB# #TAB# #TAB# yield cls.from_json(line) #LINE# #TAB# except DeserializationError as e: #LINE# #TAB# #TAB# raise DeserializationError(f'invalid entry on line {i}') from e"
Attempt to get the currently logged in user <code> def get_current_user(): ,#LINE# #TAB# username = 'root' #LINE# #TAB# try: #LINE# #TAB# #TAB# username = os.getlogin() #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# import getpass #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# username = getpass.getuser() #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return username
"Morph between more then two sequences @param waves sequence : A sequence of wave cycles @param gaps sequence : The size of the gap between each pair of cycles <code> def morph_many(waves, gaps): ","#LINE# #TAB# morphed = [] #LINE# #TAB# prev_wave = None #LINE# #TAB# i = 0 #LINE# #TAB# for curr_wave in waves: #LINE# #TAB# #TAB# if prev_wave is not None: #LINE# #TAB# #TAB# #TAB# if i: #LINE# #TAB# #TAB# #TAB# #TAB# start = 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# start = 0 #LINE# #TAB# #TAB# #TAB# morphed.extend([x for x in _morph_two(prev_wave, curr_wave, #LINE# #TAB# #TAB# #TAB# #TAB# gaps[i])][start:]) #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# prev_wave = curr_wave #LINE# #TAB# return morphed"
"data is list data to sort depends is a dictionary { data item:[items to come after ] } returns a list of items in data sort by depends requirements , keeping order of orginal list otherwise <code> def depends_tsort(data, depends): ","#LINE# #TAB# ret = [] #LINE# #TAB# visited = set([]) #LINE# #TAB# org_data = set(data) #LINE# #TAB# for d in data: #LINE# #TAB# #TAB# _visit(d, org_data, visited, ret, depends) #LINE# #TAB# return ret"
"Extract text from an INSPIRE search  <code> def get_text_from_inspire(search='', resultformat='brief', ot=None): ","#LINE# #TAB# log.info('Search of INSPIRE started...') #LINE# #TAB# data = query_inspire(search, resultformat=resultformat, ot=ot) #LINE# #TAB# if resultformat == 'marcxml' or resultformat == 'json': #LINE# #TAB# #TAB# text = data.decode('utf-8') #LINE# #TAB# else: #LINE# #TAB# #TAB# text = extract_from_data(data) #LINE# #TAB# return text"
Default sorting of GOEA results  <code> def dflt_sortby_objgoea(goea_res): ,"#LINE# #TAB# #TAB# return [getattr(goea_res, 'enrichment'), #LINE# #TAB# #TAB# #TAB# #TAB# getattr(goea_res, 'namespace'), #LINE# #TAB# #TAB# #TAB# #TAB# getattr(goea_res, 'p_uncorrected'), #LINE# #TAB# #TAB# #TAB# #TAB# getattr(goea_res, 'depth'), #LINE# #TAB# #TAB# #TAB# #TAB# getattr(goea_res, 'GO')]"
"Quick and cheap check to see if bucket can be accessed Args : client : a Boto3 client object bucket_name ( str ) : name of bucket to check Returns : bool : Can we access the bucket ? <code> def verify_bucket_access(client, bucket_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# client.head_bucket(Bucket=bucket_name) #LINE# #TAB# #TAB# return True #LINE# #TAB# except botocore.exceptions.ClientError as err: #LINE# #TAB# #TAB# logger.debug('Unable to access bucket: {} (error: {})'.format( #LINE# #TAB# #TAB# #TAB# bucket_name, err)) #LINE# #TAB# #TAB# return False"
"Convert data points to values between 0 and 1 and return new columns of prefixed data . Args : df : Pandas dataframe . columns : List of columns to transform . Returns : Original dataframe with additional prefixed columns  <code> def cols_to_normalize(df, columns): ",#LINE# #TAB# for col in columns: #LINE# #TAB# #TAB# df['norm_' + col] = (df[col] - df[col].min()) / (df[col].max() - df #LINE# #TAB# #TAB# #TAB# [col].min()) #LINE# #TAB# return df
"Use sdmpy to get all sources and ra , dec per scan as dict <code> def sdm_sources(sdmname): ","#LINE# #TAB# from rfpipe import util #LINE# #TAB# sdm = util.getsdm(sdmname) #LINE# #TAB# sourcedict = {} #LINE# #TAB# for row in sdm['Field']: #LINE# #TAB# #TAB# src = str(row.fieldName) #LINE# #TAB# #TAB# sourcenum = int(row.sourceId) #LINE# #TAB# #TAB# direction = str(row.referenceDir) #LINE# #TAB# #TAB# ra, dec = [float(val) for val in direction.split(' ')[3:]] #LINE# #TAB# #TAB# sourcedict[sourcenum] = {} #LINE# #TAB# #TAB# sourcedict[sourcenum]['source'] = src #LINE# #TAB# #TAB# sourcedict[sourcenum]['ra'] = ra #LINE# #TAB# #TAB# sourcedict[sourcenum]['dec'] = dec #LINE# #TAB# return sourcedict"
Convert a duration object to a duration string ( in seconds ) . Args : duration ( dict ) : Duration dictionary . Returns : str : Duration string  <code> def convert_duration_to_string(duration): ,#LINE# #TAB# duration_seconds = 0.0 #LINE# #TAB# if 'seconds' in duration: #LINE# #TAB# #TAB# duration_seconds += duration['seconds'] #LINE# #TAB# if 'nanos' in duration: #LINE# #TAB# #TAB# duration_seconds += duration['nanos'] * 10 ** -9 #LINE# #TAB# if duration_seconds.is_integer(): #LINE# #TAB# #TAB# duration_str = int(duration_seconds) #LINE# #TAB# else: #LINE# #TAB# #TAB# duration_str = '{:0.3f}'.format(duration_seconds) #LINE# #TAB# return str(duration_str) + 's'
Detect if the database exists <code> def database_exists(): ,#LINE# #TAB# from django.db import DatabaseError #LINE# #TAB# from django.core.exceptions import ImproperlyConfigured #LINE# #TAB# from wger.manager.models import User #LINE# #TAB# try: #LINE# #TAB# #TAB# User.objects.count() #LINE# #TAB# except DatabaseError: #LINE# #TAB# #TAB# return False #LINE# #TAB# except ImproperlyConfigured: #LINE# #TAB# #TAB# print('Your settings file seems broken') #LINE# #TAB# #TAB# sys.exit(0) #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"Returns the leading numeric part of a string . > > > numeric_part(""20-alpha "" ) 20 > > > numeric_part(""foo "" ) > > > numeric_part(""16b "" ) 16 <code> def numeric_part(s): ",#LINE# #TAB# m = re_numeric_part.match(s) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return int(m.group(1)) #LINE# #TAB# return None
"Return the list of ForbiddenCombinations that prohibit some part of the requested additions  <code> def find_forbidden_combinations(requested_additions, tea_slug=None): ",#LINE# #TAB# requested_additions = set(requested_additions) #LINE# #TAB# forbidden = ForbiddenCombination.objects.prefetch_related('additions') #LINE# #TAB# if tea_slug: #LINE# #TAB# #TAB# forbidden = forbidden.filter(Q(tea__slug=tea_slug) | Q(tea__isnull= #LINE# #TAB# #TAB# #TAB# True)) #LINE# #TAB# else: #LINE# #TAB# #TAB# forbidden = forbidden.filter(tea__isnull=True) #LINE# #TAB# return [fc for fc in forbidden if fc.forbids_additions(requested_additions) #LINE# #TAB# #TAB# ]
"* str * , one of "" runtime "" , "" original "" , "" frozen "" , coming from "" --file - reference - choice "" Notes : Defaults to runtime for modules and packages , as well as standalone binaries , otherwise original is kept  <code> def get_file_reference_mode(): ",#LINE# #TAB# if options.file_reference_mode is None: #LINE# #TAB# #TAB# value = 'runtime' if shallMakeModule() or isStandaloneMode( #LINE# #TAB# #TAB# #TAB# ) else 'original' #LINE# #TAB# else: #LINE# #TAB# #TAB# value = options.file_reference_mode #LINE# #TAB# return value
"Execute notebook ` nb ` and return notebook with built outputs <code> def fill_notebook(nb, timeout=30): ","#LINE# #TAB# preprocessor = nbc.preprocessors.execute.ExecutePreprocessor(timeout= #LINE# #TAB# #TAB# timeout) #LINE# #TAB# preprocessor.enabled = True #LINE# #TAB# res = nbc.exporter.ResourcesDict() #LINE# #TAB# res['metadata'] = nbc.exporter.ResourcesDict() #LINE# #TAB# output_nb, _ = preprocessor(deepcopy(nb), res) #LINE# #TAB# return output_nb"
Checks if a path is an actual directory <code> def is_dir(dir_name): ,#LINE# #TAB# if not os.path.isdir(dir_name): #LINE# #TAB# #TAB# msg = '{0} is not a directory'.format(dir_name) #LINE# #TAB# #TAB# raise ArgumentTypeError(msg) #LINE# #TAB# else: #LINE# #TAB# #TAB# return dir_name
"Inspect a FilterSet and produce the arguments to pass to a Graphene Field . These arguments will be available to filter against in the GraphQL <code> def get_filtering_args_from_filterset(filterset_class, type): ","#LINE# #TAB# from graphene_django.forms.converter import convert_form_field #LINE# #TAB# args = {} #LINE# #TAB# for name, filter_field in six.iteritems(filterset_class.base_filters): #LINE# #TAB# #TAB# if isinstance(filter_field, OrderingFilter): #LINE# #TAB# #TAB# #TAB# field_type = get_ordering_field(filter_field, type._meta.model) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# field_type = convert_form_field(filter_field.field).Argument() #LINE# #TAB# #TAB# field_type.description = filter_field.label #LINE# #TAB# #TAB# args[name] = field_type #LINE# #TAB# return args"
return a list of default value for index_refine board from a dict initialparameters <code> def fill_list_valueparamir(initialparameters): ,"#LINE# #TAB# list_valueparamIR = [initialparameters['PeakList Folder'], #LINE# #TAB# #TAB# initialparameters['PeakListCor Folder'], initialparameters[ #LINE# #TAB# #TAB# 'IndexRefine PeakList Folder'], initialparameters[ #LINE# #TAB# #TAB# 'PeakList Filename Prefix'], initialparameters[ #LINE# #TAB# #TAB# 'PeakList Filename Suffix'], initialparameters['nbdigits'], #LINE# #TAB# #TAB# initialparameters['startingindex'], initialparameters['finalindex'], #LINE# #TAB# #TAB# initialparameters['stepindex'], initialparameters[ #LINE# #TAB# #TAB# 'Detector Calibration File .det'], initialparameters[ #LINE# #TAB# #TAB# 'GuessedUBMatrix'], initialparameters['MinimumMatchingRate'], #LINE# #TAB# #TAB# initialparameters['IndexRefine Parameters File'], initialparameters #LINE# #TAB# #TAB# ['Selected Peaks from File']] #LINE# #TAB# return list_valueparamIR"
Converts the xml from Alpino into penntreebank format <code> def xml_to_penn(tree): ,"#LINE# #TAB# map_token_begin_node = {} #LINE# #TAB# str = node_to_penn(tree.find('node'), map_token_begin_node) #LINE# #TAB# return str, map_token_begin_node"
Sets all comment attributes on a line to indicate no comment is present . Parameters : oLine : ( line object ) <code> def remove_comment_attributes_from_line(oLine): ,#LINE# #TAB# oLine.isComment = False #LINE# #TAB# oLine.hasComment = False #LINE# #TAB# oLine.hasInlineComment = False #LINE# #TAB# oLine.commentColumn = None
Extracts all used symbols from an iterable of paradigms  <code> def paradigms_to_alphabet(paradigms: List[paradigm.Paradigm]) ->Set[str]: ,"#LINE# #TAB# alphabet = set() #LINE# #TAB# for para in paradigms: #LINE# #TAB# #TAB# for idx, (is_var, slot) in enumerate(para.slots): #LINE# #TAB# #TAB# #TAB# for word in slot: #LINE# #TAB# #TAB# #TAB# #TAB# alphabet |= set(word) #LINE# #TAB# return alphabet - {'_'}"
closes the socket and kills the thread <code> def dont_listen(): ,#LINE# #TAB# global oscThread #LINE# #TAB# if oscThread: #LINE# #TAB# #TAB# oscThread.socket.close() #LINE# #TAB# #TAB# oscThread.isRunning = 0 #LINE# #TAB# #TAB# oscThread = 0
"Calculates steps to run on device  <code> def steps_to_run(current_step, steps_per_epoch, steps_per_loop): ","#LINE# #TAB# if steps_per_loop <= 0: #LINE# #TAB# #TAB# raise ValueError('steps_per_loop should be positive integer.') #LINE# #TAB# if steps_per_loop == 1: #LINE# #TAB# #TAB# return steps_per_loop #LINE# #TAB# remainder_in_epoch = current_step % steps_per_epoch #LINE# #TAB# if remainder_in_epoch != 0: #LINE# #TAB# #TAB# return min(steps_per_epoch - remainder_in_epoch, steps_per_loop) #LINE# #TAB# else: #LINE# #TAB# #TAB# return steps_per_loop"
Prepare reserved tokens and a regex for splitting them out of strings  <code> def prepare_reserved_tokens(reserved_tokens): ,"#LINE# reserved_tokens = [tf.compat.as_text(tok) for tok in reserved_tokens or []] #LINE# dups = _find_duplicates(reserved_tokens) #LINE# if dups: #LINE# #TAB# raise ValueError(""Duplicates found in tokens: %s"" % dups) #LINE# reserved_tokens_re = _make_reserved_tokens_re(reserved_tokens) #LINE# return reserved_tokens, reserved_tokens_re"
delete all subtitles in path recursively <code> def rm_subtitles(path): ,"#LINE# #TAB# sub_exts = ['ass', 'srt', 'sub'] #LINE# #TAB# count = 0 #LINE# #TAB# for root, dirs, files in os.walk(path): #LINE# #TAB# #TAB# for f in files: #LINE# #TAB# #TAB# #TAB# _, ext = os.path.splitext(f) #LINE# #TAB# #TAB# #TAB# ext = ext[1:] #LINE# #TAB# #TAB# #TAB# if ext in sub_exts: #LINE# #TAB# #TAB# #TAB# #TAB# p = os.path.join(root, f) #LINE# #TAB# #TAB# #TAB# #TAB# count += 1 #LINE# #TAB# #TAB# #TAB# #TAB# print('Delete {}'.format(p)) #LINE# #TAB# #TAB# #TAB# #TAB# os.remove(p) #LINE# #TAB# return count"
Get the latest entry in the response queue or None if it is empty . Returns ------- Optional[Action ] instance of the Action class or None <code> def get_nowait(cls) ->Optional['Action']: ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return cls._queue.get_nowait() #LINE# #TAB# except Empty: #LINE# #TAB# #TAB# return None
"from quat = [ vec , scalar ] = [ sin angle/2 ( unitvec(x , y , z ) ) , cos angle/2 ] gives unitvec and angle of rotation around unitvec <code> def fromquat_to_vecangle(quat): ","#LINE# #TAB# normvectpart = math.sqrt(quat[0] ** 2 + quat[1] ** 2 + quat[2] ** 2 + #LINE# #TAB# #TAB# quat[3] ** 2) #LINE# #TAB# angle = math.acos(quat[3] / normvectpart) * 2.0 #LINE# #TAB# unitvec = array(quat[:3]) / math.sin(angle / 2.0) / normvectpart #LINE# #TAB# return unitvec, angle"
Reads in and formats mothur cons.taxonomy file  <code> def read_constaxonomy_file(filepath): ,"#LINE# #TAB# data = pd.read_table(filepath) #LINE# #TAB# classifications = data['Taxonomy'] #LINE# #TAB# classifications = classifications.str.split(';', expand=True).drop(6, #LINE# #TAB# #TAB# axis=1) #LINE# #TAB# classifications.columns = list(range(1, 7)) #LINE# #TAB# features_names = data['OTU'] #LINE# #TAB# data = pd.concat([features_names, classifications], axis=1) #LINE# #TAB# data = data.set_index('OTU') #LINE# #TAB# data.index = data.index.rename(None) #LINE# #TAB# data = data.sort_index() #LINE# #TAB# return data"
Return all non - private tags from a DICOM dataset  <code> def non_private_tags_in_dicom_dataset(ds): ,#LINE# #TAB# non_private_tags = [] #LINE# #TAB# for elem in ds: #LINE# #TAB# #TAB# if not elem.tag.is_private and not (elem.tag.element == 0 and elem. #LINE# #TAB# #TAB# #TAB# tag.group > 6): #LINE# #TAB# #TAB# #TAB# non_private_tags.append(elem.tag) #LINE# #TAB# return non_private_tags
"Searches in the original PDB file for the oligomeric status determined by the author . Called by : clean_and_sort ( ) <code> def author_agrees(oligo_dict, contents, nchains): ","#LINE# #TAB# pattern = 'AUTHOR DETERMINED BIOLOGICAL UNIT: ' + oligo_dict[nchains] #LINE# #TAB# if re.search(pattern, contents): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Given a nested dictionary from k1 - > k2 > value transpose its outer and inner keys so it maps k2 - > k1 - > value  <code> def transpose_nested_dictionary(nested_dict): ,"#LINE# #TAB# result = defaultdict(dict) #LINE# #TAB# for k1, d in nested_dict.items(): #LINE# #TAB# #TAB# for k2, v in d.items(): #LINE# #TAB# #TAB# #TAB# result[k2][k1] = v #LINE# #TAB# return result"
Return dictionary of events for a game with matching i d  <code> def game_events(game_id): ,"#LINE# #TAB# data = mlbgame.data.get_game_events(game_id) #LINE# #TAB# parsed = etree.parse(data) #LINE# #TAB# root = parsed.getroot() #LINE# #TAB# output = {} #LINE# #TAB# innings = root.findall('inning') #LINE# #TAB# for x in innings: #LINE# #TAB# #TAB# output[x.attrib['num']] = {'top': __inning_info(x, 'top'), 'bottom': #LINE# #TAB# #TAB# #TAB# __inning_info(x, 'bottom')} #LINE# #TAB# return output"
"Deals with the post response from a leave room request . : param data : Data received from the server : type data : Dict[str , Any ] <code> def post_leave(data: Dict[str, Any]) ->None: ",#LINE# #TAB# status = data['headers']['status'] #LINE# #TAB# if status == Status.SUCCESS.value: #LINE# #TAB# #TAB# Memory.rooms.pop(data['room_id']) #LINE# #TAB# EventStatus.leave_room = data['headers']['status']
Hook to add global options  <code> def build_option_parser(parser): ,"#LINE# #TAB# parser.add_argument('--os-registration-version', metavar= #LINE# #TAB# #TAB# '<registration-version>', default=utils.env( #LINE# #TAB# #TAB# 'OS_REGISTRATION_VERSION', default=DEFAULT_OS_REGISTRATION_VERSION), #LINE# #TAB# #TAB# help='Client version, default=' + DEFAULT_OS_REGISTRATION_VERSION + #LINE# #TAB# #TAB# ' (Env: DEFAULT_OS_REGISTRATION_VERSION)') #LINE# #TAB# return parser"
Read a box file in EMAN1 box format . : param path : Path to box file : return : List of bounding boxes <code> def read_eman1_boxfile(path): ,"#LINE# #TAB# boxreader = np.atleast_2d(np.genfromtxt(path)) #LINE# #TAB# boxes = [BoundBox(x=box[0], y=box[1], w=box[2], h=box[3]) for box in #LINE# #TAB# #TAB# boxreader] #LINE# #TAB# return boxes"
Strips the value if it is not None . : param value : text to be cleaned up : type value : str or None : return : clean value if one was specified ; None otherwise : rtype : str or None <code> def safe_strip(value): ,#LINE# #TAB# if value is not None: #LINE# #TAB# #TAB# return value.strip() #LINE# #TAB# return None
"Read keras retinanet model from keras.model.save ( ) <code> def read_model(model_path, config): ","#LINE# #TAB# with warnings.catch_warnings(): #LINE# #TAB# #TAB# model = models.load_model(model_path, backbone_name='resnet50') #LINE# #TAB# return model"
Returns an ascending order clause on the versioned unique constraint as well as the version column  <code> def get_order_clause(archive_table): ,"#LINE# #TAB# order_clause = [sa.asc(getattr(archive_table, col_name)) for col_name in #LINE# #TAB# #TAB# archive_table._version_col_names] #LINE# #TAB# order_clause.append(sa.asc(archive_table.version_id)) #LINE# #TAB# return order_clause"
returns a queryset of all relevant categories for a the model_name <code> def get_relevant_categories(cls): ,#LINE# #TAB# result = AllinkCategory.objects.none() #LINE# #TAB# for root in AllinkCategory.get_root_nodes().filter(model_names__contains #LINE# #TAB# #TAB# =[cls._meta.model_name]): #LINE# #TAB# #TAB# result |= root.get_descendants() #LINE# #TAB# return result
Get a dictionary of Backpage city names mapped to their legend value  <code> def city_nums(): ,"#LINE# city_nums = {} #LINE# first_row = 1 #LINE# num = 0 #LINE# fname = pkg_resources.resource_filename(__name__, 'resources/Distance_Matrix.csv') #LINE# with open(fname, 'rU') as csvfile: #LINE# #TAB# reader = csv.reader(csvfile, delimiter = ',') #LINE# #TAB# for row in reader: #LINE# #TAB# if first_row == 1: #LINE# #TAB# #TAB# first_row = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# city_nums[row[0]] = num #LINE# #TAB# #TAB# num = num + 1 #LINE# return city_nums"
dictorsetmaker : test dictorsetmaker_star2 <code> def p_dictorsetmaker_9(p): ,"#LINE# #TAB# keys, values = p[2] #LINE# #TAB# p[0] = keys, [p[1]] + values"
return url to zipped logging <code> def get_logging_link(scenario_uuid): ,"#LINE# #TAB# r = requests.get(url='{}scenarios/{}'.format(LIZARD_URL, scenario_uuid), #LINE# #TAB# #TAB# headers=get_headers()) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# for result in r.json()['result_set']: #LINE# #TAB# #TAB# if result['result_type']['code'] == 'logfiles': #LINE# #TAB# #TAB# #TAB# url = result['attachment_url'] #LINE# #TAB# #TAB# #TAB# return url"
Add params to doc string <code> def add_params_docstring(params): ,"#LINE# #TAB# p_string = ""\nAccepts the following paramters: \n"" #LINE# #TAB# for param in params: #LINE# #TAB# #TAB# p_string += ""name: %s, required: %s, description: %s \n"" % (param['name'], param['required'], param['description']) #LINE# #TAB# return p_string"
Build listing for flagged files pseudo - folder  <code> def flagged_listdir(): ,"#LINE# #TAB# files = [u'{flagged_char}{txt_fname}'.format(flagged_char=FLAGGED_CHAR, #LINE# #TAB# #TAB# txt_fname=fname[fname.rfind(os.path.sep) + 1:].decode('utf8')) for #LINE# #TAB# #TAB# fname in browser.flagged_files] #LINE# #TAB# zipped_files = zip(browser.flagged_files, files) #LINE# #TAB# sorted_files = sorted(zipped_files, key=lambda x: x[1].lower()) #LINE# #TAB# sorted_files.insert(0, (u'..{0}'.format(os.path.sep), u' ..{0}'.format( #LINE# #TAB# #TAB# os.path.sep))) #LINE# #TAB# return sorted_files"
Returns the details of the last git commit <code> def get_last_commit(git_path='git'): ,"#LINE# #TAB# hash_, udate, aname, amail, cname, cmail = ( #LINE# #TAB# #TAB# call((git_path, 'log', '-1', #LINE# #TAB# #TAB# #TAB# '--pretty=format:%H,%ct,%an,%ae,%cn,%ce')).split("","")) #LINE# #TAB# date = time.strftime('%Y-%m-%d %H:%M:%S +0000', time.gmtime(float(udate))) #LINE# #TAB# author = '%s <%s>' % (aname, amail) #LINE# #TAB# committer = '%s <%s>' % (cname, cmail) #LINE# #TAB# return hash_, date, author, committer"
"Return file name with extension by bone name and number of ` topping ` flag  <code> def get_bone_filename(name, topping): ","#LINE# #TAB# bones = {'django0': 'django.py', 'django1': 'django-toppings.py', #LINE# #TAB# #TAB# 'unittest0': 'unit.py', 'unittest1': 'unit-toppings.py'} #LINE# #TAB# return bones[name + str(topping)]"
class decorator to register msg parser <code> def register_parser(cls): ,#LINE# #TAB# assert cls.cls_msg_type is not None #LINE# #TAB# assert cls.cls_msg_type not in _MSG_PARSERS #LINE# #TAB# _MSG_PARSERS[cls.cls_msg_type] = cls.parser #LINE# #TAB# return cls
Convert an attribute set ( to be used in a filter ) to string  <code> def attribute_set_to_expr(attribute_set): ,"#LINE# #TAB# if isinstance(attribute_set, basestring): #LINE# #TAB# #TAB# return quote_and_escape_value(attribute_set) #LINE# #TAB# elif isinstance(attribute_set, ContextAttribute): #LINE# #TAB# #TAB# return '%s.%s' % (attribute_set.context, attribute_set.name) #LINE# #TAB# else: #LINE# #TAB# #TAB# values = ' '.join(attribute_set_to_expr(v) for v in attribute_set) #LINE# #TAB# #TAB# return '(' + values + ')'"
Return the name used when the kinto.core resource was registered along its viewset . : returns : the resource identifier . : rtype : str <code> def current_resource_name(request): ,#LINE# #TAB# service = current_service(request) #LINE# #TAB# resource_name = service.viewset.get_name(service.resource) #LINE# #TAB# return resource_name
This function is receive detail parameters : return : Parameters <code> def get_detailparams() ->dict: ,#LINE# #TAB# params = request.json['action']['detailParams'] #LINE# #TAB# return params
"c - a - a ' : magnesium - hydroxide chloride bicarbonate [ HMW84 ]  <code> def psi_mgoh_cl_hco3_hmw84(T, P): ","#LINE# #TAB# psi = 0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
"Instantiate a Key instance with the given key <code> def jwk_wrap(key, use="""", kid=""""): ","#LINE# #TAB# if isinstance(key, rsa.RSAPublicKey) or isinstance(key, rsa.RSAPrivateKey): #LINE# #TAB# #TAB# kspec = RSAKey(use=use, kid=kid).load_key(key) #LINE# #TAB# elif isinstance(key, str): #LINE# #TAB# #TAB# kspec = SYMKey(key=key, use=use, kid=kid) #LINE# #TAB# elif isinstance(key, ec.EllipticCurvePublicKey): #LINE# #TAB# #TAB# kspec = ECKey(use=use, kid=kid).load_key(key) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception(""Unknown key type:key="" + str(type(key))) #LINE# #TAB# kspec.serialize() #LINE# #TAB# return kspec"
"Compute the Pearson ' score between 2 lists of vectors  <code> def pearson_score(list1, list2): ","#LINE# #TAB# size = len(list1) #LINE# #TAB# sum1 = sum(list1) #LINE# #TAB# sum2 = sum(list2) #LINE# #TAB# sum_sq1 = sum([pow(l, 2) for l in list1]) #LINE# #TAB# sum_sq2 = sum([pow(l, 2) for l in list2]) #LINE# #TAB# prod_sum = sum([(list1[i] * list2[i]) for i in range(size)]) #LINE# #TAB# num = prod_sum - sum1 * sum2 / float(size) #LINE# #TAB# den = sqrt((sum_sq1 - pow(sum1, 2.0) / size) * (sum_sq2 - pow(sum2, 2.0 #LINE# #TAB# #TAB# ) / size)) #LINE# #TAB# return num / den"
"remove unwanted headers , e.g. the ones used for dictionaries but not desirable in the pch <code> def remove_unwanted_headers(allHeadersContent): ","#LINE# #TAB# unwantedHeaders = [] #LINE# #TAB# deprecatedHeaders = [''] #LINE# #TAB# unwantedHeaders.extend(deprecatedHeaders) #LINE# #TAB# for unwantedHeader in unwantedHeaders: #LINE# #TAB# #TAB# allHeadersContent = allHeadersContent.replace('#include ""%s""' % #LINE# #TAB# #TAB# #TAB# unwantedHeader, '') #LINE# #TAB# return allHeadersContent"
"Filter queryset by a comma delimeted cause list <code> def by_causes(queryset, cause_string=None): ","#LINE# if cause_string: #LINE# #TAB# operator, items = get_operator_and_items(cause_string) #LINE# #TAB# q_obj = SQ() #LINE# #TAB# for c in items: #LINE# #TAB# if len(c) > 0: #LINE# #TAB# #TAB# q_obj.add(SQ(causes=c), operator) #LINE# #TAB# queryset = queryset.filter(q_obj) #LINE# return queryset"
Counting the number columns based on the content of this row <code> def get_cols(row_content): ,"#LINE# #TAB# cols = [] #LINE# #TAB# subcell_col = [] #LINE# #TAB# prev_bar = None #LINE# #TAB# for _coord, item in row_content: #LINE# #TAB# #TAB# if isinstance(item, LTTextLine): #LINE# #TAB# #TAB# #TAB# subcell_col.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if prev_bar: #LINE# #TAB# #TAB# #TAB# #TAB# bar_ranges = (prev_bar, item) #LINE# #TAB# #TAB# #TAB# #TAB# col_items = subcell_col if subcell_col else [None] #LINE# #TAB# #TAB# #TAB# #TAB# cols.extend([bar_ranges, col_items]) #LINE# #TAB# #TAB# #TAB# prev_bar = item #LINE# #TAB# #TAB# #TAB# subcell_col = [] #LINE# #TAB# return cols"
Sometime symlinks can get broken if the original files are deleted . Delete such broken links <code> def check_broken_configure_log_links(): ,"#LINE# #TAB# import os #LINE# #TAB# for logfile in ['configure.log', 'configure.log.bkp']: #LINE# #TAB# #TAB# if os.path.islink(logfile) and not os.path.isfile(logfile): #LINE# #TAB# #TAB# #TAB# os.remove(logfile) #LINE# #TAB# return"
Return a per - thread global batcher instance  <code> def global_instance(cls): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return GLOBAL_BATCHER.instance #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# instance = PrioritizedBatcher(**getattr(settings, #LINE# #TAB# #TAB# #TAB# 'PRIORITIZED_BATCHER', {})) #LINE# #TAB# #TAB# GLOBAL_BATCHER.instance = instance #LINE# #TAB# #TAB# return instance"
""" c - a : sodium thiosulfate [ PM73 ]  <code> def bc_na_s2o3_pm73(T, P): ","#LINE# #TAB# b0 = 0.0882 * 3 / 4 #LINE# #TAB# b1 = 1.701 * 3 / 4 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.00705 * 3 / 2 ** (5 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Na'] * i2c['S2O3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
Given an interface specification return a cmdline command name <code> def get_cmdline_command_name(intfspec): ,"#LINE# #TAB# if len(intfspec) > 2: #LINE# #TAB# #TAB# name = intfspec[2] #LINE# #TAB# else: #LINE# #TAB# #TAB# name = intfspec[0].split('.')[-1].replace('_', '-') #LINE# #TAB# return name"
Get the name for a model  <code> def get_name(model_id): ,#LINE# #TAB# name = _names.get(model_id) #LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = 'id = %s (no name)' % str(model_id) #LINE# #TAB# return name
"Finds the value of the input buffer given the type of the field . Since data : param buffer : : param field_type : : param _ byte_order : : param is_string : : return : <code> def get_value_from_type(buffer, field_type, _byte_order, is_string=False): ","#LINE# #TAB# val = _get_value_from_type_format(struct.unpack, buffer, field_type, #LINE# #TAB# #TAB# _byte_order, is_string)[0] #LINE# #TAB# return val"
Parse command - line arguments . @return : The command line arguments . @rtype : C{argparse . Namespace } <code> def parse_args(): ,"#LINE# #TAB# parser = argparse.ArgumentParser(description= #LINE# #TAB# #TAB# 'A command-line program that indexes multiple plain text files and prints the most enountered words and the number of times each word was encountered.' #LINE# #TAB# #TAB# ) #LINE# #TAB# parser.add_argument('filepaths', nargs='*', help= #LINE# #TAB# #TAB# 'Paths to files to be indexed.') #LINE# #TAB# parser.add_argument('-c', '--count', type=int, default=10, help= #LINE# #TAB# #TAB# 'The number of most encountered words to print.') #LINE# #TAB# args = parser.parse_args() #LINE# #TAB# return args"
"Deserializes model from checkpointDir using capnproto <code> def readFromCheckpoint(cls, checkpointDir): ","#LINE# #TAB# checkpointPath = cls._getModelCheckpointFilePath(checkpointDir) #LINE# #TAB# with open(checkpointPath, 'r') as f: #LINE# #TAB# proto = cls.getSchema().read(f, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# traversal_limit_in_words=_TRAVERSAL_LIMIT_IN_WORDS) #LINE# #TAB# model = cls.read(proto) #LINE# #TAB# return model"
Gets process name from process i d <code> def get_process_name(pid): ,#LINE# #TAB# truncProcessName = open('/proc/%s/status' % pid).readline()[6:-1] #LINE# #TAB# processName = truncProcessName #LINE# #TAB# line = open('/proc/%s/cmdline' % pid).readline() #LINE# #TAB# parts = line.split('\x00') #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# cmdName = part.split('/')[-1] #LINE# #TAB# #TAB# if cmdName.startswith(truncProcessName): #LINE# #TAB# #TAB# #TAB# processName = cmdName #LINE# #TAB# return processName
"Decodes a packet and returns its payload : param command_data : the command data payload : type command_data : str : returns : the encoded command , ready to be sent to a remote GDB : rtype : str <code> def remote_decode(data): ",#LINE# #TAB# if data[0] != REMOTE_PREFIX: #LINE# #TAB# #TAB# raise InvalidPacketError #LINE# #TAB# if data[-3] != REMOTE_DELIMITER: #LINE# #TAB# #TAB# raise InvalidPacketError #LINE# #TAB# payload = data[1:-3] #LINE# #TAB# checksum = data[-2:] #LINE# #TAB# if payload == '': #LINE# #TAB# #TAB# expected_checksum = '00' #LINE# #TAB# else: #LINE# #TAB# #TAB# expected_checksum = remote_checksum(payload) #LINE# #TAB# if checksum != expected_checksum: #LINE# #TAB# #TAB# raise InvalidPacketError #LINE# #TAB# return payload
"Heuristically extracts the base indentation from the provided code  <code> def get_base_indentation(code, include_start=False): ","#LINE# #TAB# new_line_indentation = re_new_line_indentation[include_start].finditer(code) #LINE# #TAB# new_line_indentation = tuple(m.groups(0)[0] for m in new_line_indentation) #LINE# #TAB# if new_line_indentation: #LINE# #TAB# #TAB# return min(new_line_indentation, key=len) #LINE# #TAB# else: #LINE# #TAB# #TAB# return """""
"Return filter for analysis request . Parameters - repo : repo slug name , fe . buildtimetrend / python - lib <code> def get_repo_filter(repo=None): ","#LINE# #TAB# if repo is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return {'property_name': '%s.project_name' % KEEN_PROJECT_INFO_NAME, #LINE# #TAB# #TAB# 'operator': 'eq', 'property_value': str(repo)}"
"Check that the declared number of inputs ( the length of ` input_type ` ) and the number of inputs to ` func ` are equal . Parameters ---------- input_type : List[DataType ] func : callable Returns ------- inspect . Signature <code> def valid_function_signature(input_type, func): ","#LINE# #TAB# funcsig = signature(func) #LINE# #TAB# declared_parameter_count = len(input_type) #LINE# #TAB# function_parameter_count = parameter_count(funcsig) #LINE# #TAB# if declared_parameter_count != function_parameter_count: #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# 'Function signature {!r} has {:d} parameters, input_type has {:d}. These must match' #LINE# #TAB# #TAB# #TAB# .format(func.__name__, function_parameter_count, #LINE# #TAB# #TAB# #TAB# declared_parameter_count)) #LINE# #TAB# return funcsig"
"Try to find RSS feeds from a tags in the webpage <code> def get_feeds_from_atags(url: str, html: BeautifulSoup) ->list: ","#LINE# #TAB# possible_feeds = [] #LINE# #TAB# parsed_url = urllib.parse.urlparse(url) #LINE# #TAB# if not (parsed_url.scheme and parsed_url.hostname): #LINE# #TAB# #TAB# return [] #LINE# #TAB# base = parsed_url.scheme + '://' + parsed_url.hostname #LINE# #TAB# atags = html.findAll('a') #LINE# #TAB# for a in atags: #LINE# #TAB# #TAB# href = a.get('href', None) #LINE# #TAB# #TAB# if href: #LINE# #TAB# #TAB# #TAB# if 'xml' in href or 'rss' in href or 'feed' in href: #LINE# #TAB# #TAB# #TAB# #TAB# possible_feeds.append(base + href) #LINE# #TAB# return possible_feeds"
Configures the frozen pool and keeps all kwargs <code> def configure_frozen_pool(kwargs): ,"#LINE# #TAB# _frozen_pool_single_run.kwargs = kwargs #LINE# #TAB# _configure_niceness(kwargs) #LINE# #TAB# _configure_logging(kwargs, extract=False) #LINE# #TAB# traj = kwargs['traj'] #LINE# #TAB# traj.v_full_copy = kwargs['full_copy']"
"makeNa(expression ) Replace values which pass the expression with NA . Replaces all values in ` ` vid ` ` for which ` ` func ` ` evaluates to ` ` True ` ` with ` ` nan ` `  <code> def make_na(dtable, vid, expr): ","#LINE# #TAB# expr = expression.Expression('v{} {}'.format(vid, expr)) #LINE# #TAB# cols = dtable.columns(vid) #LINE# #TAB# for col in cols: #LINE# #TAB# #TAB# col = col.name #LINE# #TAB# #TAB# mask = expr.evaluate(dtable[:], {vid: col}) #LINE# #TAB# #TAB# dtable[mask, col] = np.nan"
"For functions of the form : a / ( exp(k1 * ( x - x1 ) ) + exp(k2 * ( x - x2 ) ) ) <code> def double_exp(x, a, k1, x1, k2, x2, y0=0): ",#LINE# #TAB# ret = np.zeros(len(x)) #LINE# #TAB# try: #LINE# #TAB# #TAB# ret = a / (np.exp(k1 * (x - x1)) + np.exp(k2 * (x - x2))) + y0 #LINE# #TAB# except RuntimeWaring as e: #LINE# #TAB# #TAB# traceback.print_exc() #LINE# #TAB# return ret
Get a client socket <code> def get_client_sock(addr): ,"#LINE# #TAB# ""Get a client socket"" #LINE# #TAB# s = _socket.create_connection(addr) #LINE# #TAB# s.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, True) #LINE# #TAB# s.setblocking(False) #LINE# #TAB# return s"
"This just adds the collection ID to a dict from sqlite . Row  <code> def add_collection_info(row, collection): ",#LINE# #TAB# row = dict(row) #LINE# #TAB# row['collection'] = collection #LINE# #TAB# return row
Can a module ( like scipy or numpy ) be imported without a severe and avoidable performance penalty ? The rational behind this is that some parts of temci do n't need scipy or numpy . : param module : name of the module <code> def can_import(module: str) ->bool: ,"#LINE# #TAB# if sphinx_doc(): #LINE# #TAB# #TAB# return False #LINE# #TAB# if allow_all_imports: #LINE# #TAB# #TAB# return True #LINE# #TAB# if module not in ['scipy', 'numpy', 'init']: #LINE# #TAB# #TAB# return True #LINE# #TAB# if in_standalone_mode: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(sys.argv) == 1 or sys.argv[1] in ['completion', 'version', #LINE# #TAB# #TAB# 'assembler']: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Given a template name , it returns the relative path from the template dir  <code> def get_template_path(name): ","#LINE# #TAB# path = os.path.join(defs.ADWARE_TEMPLATE_BASE_DIR, name) #LINE# #TAB# return path"
"r Calculate the allowed total angular momenta  <code> def perm_j(j1, j2): ",#LINE# #TAB# jmin = abs(j1-j2) #LINE# #TAB# jmax = j1+j2 #LINE# #TAB# return [jmin + i for i in range(jmax-jmin+1)]
Find all non - callable members of a protocol  <code> def non_method_protocol_members(tp: TypeInfo) ->List[str]: ,"#LINE# #TAB# assert tp.is_protocol #LINE# #TAB# result = [] #LINE# #TAB# anytype = AnyType(TypeOfAny.special_form) #LINE# #TAB# instance = Instance(tp, [anytype] * len(tp.defn.type_vars)) #LINE# #TAB# for member in tp.protocol_members: #LINE# #TAB# #TAB# typ = find_member(member, instance, instance) #LINE# #TAB# #TAB# if not isinstance(typ, CallableType): #LINE# #TAB# #TAB# #TAB# result.append(member) #LINE# #TAB# return result"
"Given a filesystem path of a file , return its contents . : param str path : path to a file : return : str ( contents of the file ) <code> def read_path(path: str) ->str: ","#LINE# #TAB# path = os.path.expanduser(path) #LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# contents = f.read() #LINE# #TAB# return contents"
Returns whether the value acts like a standard float  <code> def is_float_like(value): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if isinstance(value, float): return True #LINE# #TAB# #TAB# return float(value) == value and not str(value).isdigit() #LINE# #TAB# except: #LINE# #TAB# #TAB# return False"
This function initializes a socket on the passed ' port ' for listening . If something fails ( such as the port being in use ) the method returns None  <code> def init_socket(port): ,"#LINE# #TAB# s = socket.socket() #LINE# #TAB# s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #LINE# #TAB# s.bind(('', port)) #LINE# #TAB# s.listen(5) #LINE# #TAB# print('server: listening for players on port %d' % port) #LINE# #TAB# return s"
Return the spherical coordinates for coordinates in Cartesian space  <code> def cartesian_to_spherical(vectors): ,"#LINE# #TAB# rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1)) #LINE# #TAB# xx, yy, zz = vectors.T #LINE# #TAB# lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.))) #LINE# #TAB# lons = numpy.degrees(numpy.arctan2(yy, xx)) #LINE# #TAB# depths = EARTH_RADIUS - rr #LINE# #TAB# return lons.T, lats.T, depths"
Returns the HyperKitty argument parser  <code> def setup_cmd_parser(cls): ,"#LINE# #TAB# parser = BackendCommandArgumentParser(cls.BACKEND, from_date=True) #LINE# #TAB# group = parser.parser.add_argument_group('HyperKitty arguments') #LINE# #TAB# group.add_argument('--mboxes-path', dest='mboxes_path', help= #LINE# #TAB# #TAB# 'Path where mbox files will be stored') #LINE# #TAB# parser.parser.add_argument('url', help='URL of the mailing list archiver') #LINE# #TAB# return parser"
Get a NumPy array view of a VNL vector  <code> def get_array_view_from_vnl_vector(vnl_vector): ,"#LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# itksize = vnl_vector.size() #LINE# #TAB# shape = [itksize] #LINE# #TAB# pixelType = 'SC' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlSC._get_array_view_from_vnl_vector(vnl_vector) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_vector) #LINE# #TAB# return itk_view"
"Formats for console logging if coloring is enabled or not . Show the level name if coloring is disabled ( e.g. INFO ) . Also , Root logger should show the logger name  <code> def get_console_log_format(): ","#LINE# #TAB# return {CLI_LOGGER_NAME: {(True): '%(message)s', (False): #LINE# #TAB# #TAB# '%(levelname)s: %(message)s'}, 'root': {(True): #LINE# #TAB# #TAB# '%(name)s : %(message)s', (False): #LINE# #TAB# #TAB# '%(levelname)s: %(name)s : %(message)s'}}"
given message / rfc822 returns the content <code> def decode_message(messageString): ,#LINE# #TAB# package = email.message_from_string(messageString) #LINE# #TAB# message = '' #LINE# #TAB# for part in package.walk(): #LINE# #TAB# #TAB# if not part.is_multipart(): #LINE# #TAB# #TAB# #TAB# message += part.get_payload(decode=True) #LINE# #TAB# return message
"Returns the current version of the app , or None if there is no current version found  <code> def get_current_version_name_safe(): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return modules.get_current_version_name() #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"Set item to last position if position not defined  <code> def before_insert(mapper, connection, target): ","#LINE# #TAB# if target.position is None: #LINE# #TAB# #TAB# func = sa.sql.func #LINE# #TAB# #TAB# stmt = sa.select([func.coalesce(func.max(mapper.mapped_table.c.position), -1)]) #LINE# #TAB# #TAB# target.position = connection.execute(stmt).scalar() + 1"
"Finds the keycode associated with a string representation of a keysym  <code> def lookup_string(conn, kstr): ","#LINE# #TAB# if kstr in keysyms: #LINE# #TAB# #TAB# return get_keycode(conn, keysyms[kstr]) #LINE# #TAB# elif len(kstr) > 1 and kstr.capitalize() in keysyms: #LINE# #TAB# #TAB# return get_keycode(conn, keysyms[kstr.capitalize()]) #LINE# #TAB# return None"
Splits a TAF report into each distinct time period <code> def split_taf(txt: str) -> [str]: ,"#LINE# #TAB# lines = [] #LINE# #TAB# split = txt.split() #LINE# #TAB# last_index = 0 #LINE# #TAB# for i, item in enumerate(split): #LINE# #TAB# #TAB# if starts_new_line(item) and i != 0 and not split[i - 1].startswith('PROB'): #LINE# #TAB# #TAB# #TAB# lines.append(' '.join(split[last_index:i])) #LINE# #TAB# #TAB# #TAB# last_index = i #LINE# #TAB# lines.append(' '.join(split[last_index:])) #LINE# #TAB# return lines"
"Depending on how we extract data from pysam we may end up with either a string or a byte array of nucleotides . For consistency and simplicity we want to only use strings in the rest of our code  <code> def convert_from_bytes_if_necessary(prefix, suffix): ","#LINE# #TAB# if isinstance(prefix, bytes): #LINE# #TAB# #TAB# prefix = prefix.decode('ascii') #LINE# #TAB# if isinstance(suffix, bytes): #LINE# #TAB# #TAB# suffix = suffix.decode('ascii') #LINE# #TAB# return prefix, suffix"
"Creates a tree - like list of choices <code> def mk_dropdown_tree(cls, model, for_node=None): ","#LINE# #TAB# options = [(0, _('-- root --'))] #LINE# #TAB# for node in model.get_root_nodes(): #LINE# #TAB# #TAB# cls.add_subtree(for_node, node, options) #LINE# #TAB# return options"
"Returns the location of an astronomical object in the sky in terms of azimuth and altitude  <code> def get_sky_location(body, location, date): ","#LINE# #TAB# if isinstance(location, str): #LINE# #TAB# #TAB# location = getLocation(location) #LINE# #TAB# if not isinstance(body, RPNAstronomicalObject) or not isinstance(location, #LINE# #TAB# #TAB# RPNLocation) or not isinstance(date, RPNDateTime): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'expected an astronomical object, a location and a date-time') #LINE# #TAB# az, alt = body.getAzimuthAndAltitude(location, date) #LINE# #TAB# return [az.convert('degree'), alt.convert('degree')]"
Returns full path of incoming relative path . Relative path is relative to the script location  <code> def get_full_path(name): ,#LINE# #TAB# pth = Path(__file__).parent.joinpath(name) #LINE# #TAB# return pth
"Toggles ignoring cache for each request . If ` true ` , cache will not be used . : param cacheDisabled : Cache disabled state . : type cacheDisabled : bool <code> def set_cache_disabled(cls, cacheDisabled: Union['bool']): ","#LINE# #TAB# return cls.build_send_payload('set_cache_disabled', {'cacheDisabled': #LINE# #TAB# #TAB# cacheDisabled}), None"
walk all files for a dir <code> def dir_walk(dir): ,"#LINE# #TAB# for f in os.listdir(dir): #LINE# #TAB# #TAB# fullpath = os.path.abspath(os.path.join(dir, f)) #LINE# #TAB# #TAB# if os.path.isdir(fullpath) and not os.path.islink(fullpath): #LINE# #TAB# #TAB# #TAB# for x in dir_walk(fullpath): #LINE# #TAB# #TAB# #TAB# #TAB# yield x #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield fullpath"
Convert a model s verbose name to the model class . This allows us to use the models verbose name in steps  <code> def get_model(name): ,"#LINE# #TAB# model = MODELS.get(name.lower(), None) #LINE# #TAB# assert model, ""Could not locate model by name '%s'"" % name #LINE# #TAB# return model"
Domain has a bad hostname if its canonical https endpoint fails hostname validation <code> def is_bad_hostname(domain): ,"#LINE# #TAB# canonical, https, httpswww = (domain.canonical, domain.https, domain. #LINE# #TAB# #TAB# httpswww) #LINE# #TAB# if canonical.host == 'www': #LINE# #TAB# #TAB# canonical_https = httpswww #LINE# #TAB# else: #LINE# #TAB# #TAB# canonical_https = https #LINE# #TAB# return canonical_https.https_bad_hostname"
"Return a list of the first ` n ` items in ` source_iter `  <code> def iter_take(source_iter, n): ","#LINE# #TAB# source_iter = iter(source_iter) #LINE# #TAB# return [item for _, item in zip(range(n), source_iter)]"
"Decide whether we ignore a local variable from the traceback . We need to avoid reporting on certain things so that we do not end up with a long list of locals which hides the useful information about the function  <code> def ignored_local_variable(name, value): ","#LINE# #TAB# if name.startswith('__'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if inspect.isclass(value): #LINE# #TAB# #TAB# return True #LINE# #TAB# module = getattr(value, '__module__', None) #LINE# #TAB# if module == '__future__': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
returns a list of keys matching pat <code> def select_options(pat): ,"#LINE# #TAB# if pat in _registered_options: #LINE# #TAB# #TAB# return [pat] #LINE# #TAB# keys = sorted(_registered_options.keys()) #LINE# #TAB# if pat == 'all': #LINE# #TAB# #TAB# return keys #LINE# #TAB# return [k for k in keys if re.search(pat, k, re.I)]"
Returns a list of app labels of the given apps_list now properly handles new Django 1 . 7 + application registry  <code> def app_labels(apps_list): ,#LINE# #TAB# if AppConfig is None: #LINE# #TAB# #TAB# return [app.split('.')[-1] for app in apps_list] #LINE# #TAB# return [AppConfig.create(app).label for app in apps_list]
USED FOR TEST ONLY : clean up all records and start again : rtype : str <code> def reset_ks(): ,#LINE# #TAB# result = key_service_api.reset() #LINE# #TAB# return result
Creates a new user class with extracted user attributes for later use . A new user object is needed to avoid overwritting of e.g. ` ` user.records ` `  <code> def extract_user_info(user): ,"#LINE# #TAB# temp_user = User() #LINE# #TAB# copy_attributes = ['antennas', 'name', 'night_start', 'night_end', #LINE# #TAB# #TAB# 'weekend', 'home'] #LINE# #TAB# for attr in copy_attributes: #LINE# #TAB# #TAB# setattr(temp_user, attr, getattr(user, attr)) #LINE# #TAB# return temp_user"
"Create a new thread for given worker  <code> def create_thread(parent, worker, deleteWorkerLater=False): ",#LINE# #TAB# thread = QtCore.QThread(parent) #LINE# #TAB# thread.started.connect(worker.doWork) #LINE# #TAB# worker.finished.connect(thread.quit) #LINE# #TAB# if deleteWorkerLater: #LINE# #TAB# #TAB# thread.finished.connect(worker.deleteLater) #LINE# #TAB# worker.moveToThread(thread) #LINE# #TAB# worker.setParent(parent) #LINE# #TAB# return thread
"Convert raw CLI output to structured data using TextFSM template  <code> def get_structured_data(raw_output, platform, command): ","#LINE# #TAB# template_dir = get_template_dir() #LINE# #TAB# index_file = os.path.join(template_dir, 'index') #LINE# #TAB# textfsm_obj = CliTable(index_file, template_dir) #LINE# #TAB# attrs = {'Command': command, 'Platform': platform} #LINE# #TAB# try: #LINE# #TAB# #TAB# textfsm_obj.ParseCmd(raw_output, attrs) #LINE# #TAB# #TAB# structured_data = clitable_to_dict(textfsm_obj) #LINE# #TAB# #TAB# output = raw_output if structured_data == [] else structured_data #LINE# #TAB# #TAB# return output #LINE# #TAB# except CliTableError: #LINE# #TAB# #TAB# return raw_output"
"Merge src into tgt , overwriting as needed . : param tgt : The target dict . : param src : the source dict  <code> def dict_update_recursive(tgt, src): ","#LINE# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# if key in tgt and isinstance(tgt[key], dict) and isinstance(value, #LINE# #TAB# #TAB# #TAB# collections.Mapping): #LINE# #TAB# #TAB# #TAB# dict_update_recursive(tgt[key], value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tgt[key] = value"
"Returns the in - plane spin from mass1 mass2 and xi2 for the secondary mass  <code> def chi_perp_from_mass1_mass2_xi2(mass1, mass2, xi2): ","#LINE# #TAB# q = q_from_mass1_mass2(mass1, mass2) #LINE# #TAB# a1 = 2 + 3 * q / 2 #LINE# #TAB# a2 = 2 + 3 / (2 * q) #LINE# #TAB# return q**2 * a2 / a1 * xi2"
"Determines the number of terms needed to be within eps of the sum , using the integral test  <code> def int_sum_sinh(alpha, eps): ",#LINE# #TAB# if alpha < 5: #LINE# #TAB# #TAB# n_min = np.ceil(2 / alpha * arctanh(exp(-eps * alpha / sinh(alpha)))) #LINE# #TAB# else: #LINE# #TAB# #TAB# n_min = 1 #LINE# #TAB# return n_min
"Utility method to return the default parsers able to parse an object from a file . Note that MultifileObjectParser is not provided in this list , as it is already added in a hardcoded way in RootParser : return : <code> def get_default_yaml_parsers(parser_finder: ParserFinder, conversion_finder: ","#LINE# #TAB# ConversionFinder) ->List[AnyParser]: #LINE# #TAB# return [SingleFileParserFunction(parser_function=read_object_from_yaml, #LINE# #TAB# #TAB# streaming_mode=True, supported_exts={'.yaml', '.yml'}, #LINE# #TAB# #TAB# supported_types={AnyObject}), SingleFileParserFunction( #LINE# #TAB# #TAB# parser_function=read_collection_from_yaml, custom_name= #LINE# #TAB# #TAB# 'read_collection_from_yaml', streaming_mode=True, supported_exts={ #LINE# #TAB# #TAB# '.yaml', '.yml'}, supported_types={Tuple, Dict, List, Set}, #LINE# #TAB# #TAB# function_args={'conversion_finder': conversion_finder})]"
generated source for method getErrorResponse <code> def get_error_response(e): ,"#LINE# #TAB# response = {'status': e.getStatus(), 'error_msg': e.message} #LINE# #TAB# return response"
"Simply wraps the tkinter function of the "" same "" name  <code> def show_warning(title='Title', message='your message here.'): ","#LINE# #TAB# tkMessageBox.showwarning(title, message) #LINE# #TAB# return"
"Return the given number as a string with a sign in front of it ie . + if the number is positive - otherwise  <code> def signed_number(number, precision=2): ","#LINE# #TAB# prefix = '' if number <= 0 else '+' #LINE# #TAB# number_str = '{}{:.{precision}f}'.format(prefix, number, precision=precision) #LINE# #TAB# return number_str"
"Exact set cover by the dancing links algorithm <code> def dancing_links(size_universe, sets): ","#LINE# #TAB# header = Cell(None, None, 0, None) #LINE# #TAB# col = [] #LINE# #TAB# for j in range(size_universe): #LINE# #TAB# #TAB# col.append(Cell(header, None, 0, None)) #LINE# #TAB# for i in range(len(sets)): #LINE# #TAB# #TAB# row = None #LINE# #TAB# #TAB# for j in sets[i]: #LINE# #TAB# #TAB# #TAB# col[j].S += 1#TAB# #TAB# #TAB# #LINE# #TAB# #TAB# #TAB# row = Cell(row, col[j], i, col[j]) #LINE# #TAB# sol = [] #LINE# #TAB# if solve(header, sol): #LINE# #TAB# #TAB# return sol #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
Appends parent scope name to ` relative_scope_name ` <code> def absolute_scope_name(relative_scope_name): ,#LINE# #TAB# base = get_scope_name() #LINE# #TAB# base = base + '/' if len(base) > 0 else base #LINE# #TAB# return base + relative_scope_name
"Returns the browse record of the article category given by uri <code> def get_article_category(cls, uri, silent=True): ","#LINE# #TAB# category = cls.search([('unique_name', '=', uri)], limit=1) #LINE# #TAB# if not category and not silent: #LINE# #TAB# #TAB# raise RuntimeError('Article category %s not found' % uri) #LINE# #TAB# return category[0] if category else None"
Group windows from one station into channels and count the number of windows in that channel : param sta_win : : return : <code> def sort_windows_on_channel(sta_win): ,"#LINE# #TAB# sort_dict = {} #LINE# #TAB# for trace_id, trace_win in sta_win.items(): #LINE# #TAB# #TAB# chan = trace_id.split('.')[-1][0:2] #LINE# #TAB# #TAB# if chan not in sort_dict: #LINE# #TAB# #TAB# #TAB# sort_dict[chan] = {'traces': [], 'nwins': 0} #LINE# #TAB# #TAB# sort_dict[chan]['traces'].append(trace_id) #LINE# #TAB# #TAB# sort_dict[chan]['nwins'] += len(trace_win) #LINE# #TAB# return sort_dict"
"Extended euclidean algorithm to find modular inverses for integers <code> def prime_field_inv(a: int, n: int) -> int: ","#LINE# #TAB# if a == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# lm, hm = 1, 0 #LINE# #TAB# low, high = a % n, n #LINE# #TAB# while low > 1: #LINE# #TAB# #TAB# r = high // low #LINE# #TAB# #TAB# nm, new = hm - lm * r, high - low * r #LINE# #TAB# #TAB# lm, low, hm, high = nm, new, lm, low #LINE# #TAB# return lm % n"
Standard version number  <code> def get_program_version(): ,#LINE# #TAB# v = about_data['version'] #LINE# #TAB# if about_data['version-tag'] != '': #LINE# #TAB# #TAB# v += '-' + about_data['version-tag'] #LINE# #TAB# return v
"Return a boolean of whether the raw password matches the three part encoded digest . If setter is specified , it 'll be called when you need to regenerate the password  <code> def check_password(password, encoded, setter=None, preferred='default'): ","#LINE# #TAB# if password is None or not is_password_usable(encoded): #LINE# #TAB# #TAB# return False #LINE# #TAB# preferred = get_hasher(preferred) #LINE# #TAB# try: #LINE# #TAB# #TAB# hasher = identify_hasher(encoded) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# hasher_changed = hasher.algorithm != preferred.algorithm #LINE# #TAB# must_update = hasher_changed or preferred.must_update(encoded) #LINE# #TAB# is_correct = hasher.verify(password, encoded) #LINE# #TAB# if not is_correct and not hasher_changed and must_update: #LINE# #TAB# #TAB# hasher.harden_runtime(password, encoded) #LINE# #TAB# if setter and is_correct and must_update: #LINE# #TAB# #TAB# setter(password) #LINE# #TAB# return is_correct"
Active with valid credentials <code> def fail_active(request): ,"#LINE# #TAB# if not check_has_credentials(request, request.user): #LINE# #TAB# #TAB# return str(settings.LOGIN_URL) #LINE# #TAB# return False"
Reset the global session variable . Mostly useful for unit tests  <code> def reset_swift_session(): ,#LINE# #TAB# global SWIFT_SESSION #LINE# #TAB# SWIFT_SESSION = None
Returns the product part of a file . We assume files to be something like domain-language.po . Example : atcontenttypes-pt-br.po <code> def get_product(file): ,#LINE# #TAB# assert file.endswith('.po') or file.endswith('.pot') #LINE# #TAB# file = file.split('.')[0] #LINE# #TAB# file = file.split('-')[0] #LINE# #TAB# return file
Prepares a dict so it could store network information  <code> def make_dict(): ,#LINE# #TAB# global available #LINE# #TAB# global _dict #LINE# #TAB# _dict = {} #LINE# #TAB# for network in available: #LINE# #TAB# #TAB# if PY2: #LINE# #TAB# #TAB# #TAB# _dict[unicode(network.dot11Ssid.SSID)] = network #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _dict[network.dot11Ssid.SSID.decode('utf-8')] = network
"Get all individual comments from the ingredient and return them as a list  <code> def parse_comments(ing: str) ->Tuple[list, str]: ","#LINE# #TAB# comment_match_string = tregex.to_tuple('(\\(.*?\\)|, .*?$)', ing) #LINE# #TAB# if comment_match_string: #LINE# #TAB# #TAB# comment_match_string = [re.escape(s[0]) for s in comment_match_string] #LINE# #TAB# #TAB# comments = tregex.to_tuple('(?:(?<=\\()|(?<=, ))(.+?)(?:(?=\\))|(?=$))' #LINE# #TAB# #TAB# #TAB# , ing) #LINE# #TAB# #TAB# comments = [s[0] for s in comments] #LINE# #TAB# else: #LINE# #TAB# #TAB# comments = [] #LINE# #TAB# return comments, comment_match_string"
"Given a filepath pointing to a JSON file and a property , return the property stored in that JSON file  <code> def get_property_from_json_file(filepath, property): ","#LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# raise Exception(""The file at '{}' does not exist!"".format(filepath)) #LINE# #TAB# with open(filepath, 'r') as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# if property not in data: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# ""Could not find the '{}' key inside of the file located at '{}'!"" #LINE# #TAB# #TAB# #TAB# #TAB# .format(property, filepath)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return data[property]"
"provide tmpdir which is scoped for the whole backend : return : str , path to the temporary directory <code> def get_backend_tmpdir(): ",#LINE# #TAB# global _backend_tmpdir #LINE# #TAB# if _backend_tmpdir is None: #LINE# #TAB# #TAB# _backend_tmpdir = mkdtemp() #LINE# #TAB# return _backend_tmpdir
"Initialize DB object : param db_url : URL to the MongoDB : type db_url : str : param db_name : Database name to use : type db_name : str : rtype : None <code> def init_db(db_url, db_name): ",#LINE# #TAB# global DB #LINE# #TAB# client = pymongo.MongoClient(db_url) #LINE# #TAB# DB = client[db_name]
Get a unique demux sample  <code> def get_sample(sample_id): ,"#LINE# #TAB# pattern = SAMPLE_PATTERN.format(sample_id) #LINE# #TAB# query = Sample.query.filter(or_(Sample.samplename.like(pattern), Sample #LINE# #TAB# #TAB# .samplename == sample_id)) #LINE# #TAB# return query"
"Takes a block_representation and returns the previous block hash <code> def get_prev_block_hash(block_representation, coin_symbol='btc', api_key=None): ","#LINE# #TAB# return get_block_overview(block_representation=block_representation, #LINE# #TAB# #TAB# coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['prev_block']"
"Takes inputs Ft and Fo and returns dF / F <code> def calc_df_f(Ft, Fo): ",#LINE# #TAB# dF_F = (Ft - Fo) / Fo #LINE# #TAB# return dF_F
"Simple password - based authentication - the same password of the host like on the Instagram : param request : request to autheticate : param password : Instagram and Web password of the host user : return : True or False if authentication failed <code> def check_password(request, password): ",#LINE# #TAB# if 'password' in request.headers: #LINE# #TAB# #TAB# if password == request.headers['password']: #LINE# #TAB# #TAB# #TAB# print('PASSWORD OK') #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# print('wrong password') #LINE# #TAB# return False
"Returns the key type , key name and if key is a compound list then returns the index pointed by the field Arguments : field : csv header field <code> def extract_key_and_index(field): ","#LINE# #TAB# for key_type, value in KEY_TYPES.items(): #LINE# #TAB# #TAB# regex = re.compile(value['regex']) #LINE# #TAB# #TAB# match = regex.match(field) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# return tuple([key_type] + list(match.groups())) #LINE# #TAB# return None"
"Takes one metric or a wildcard seriesList followed by a constant n . Draws only the metrics with a minimum value below n  <code> def minimum_below(requestContext, seriesList, n): ",#LINE# #TAB# results = [] #LINE# #TAB# for series in seriesList: #LINE# #TAB# #TAB# val = safeMin(series) #LINE# #TAB# #TAB# if val is None or val <= n: #LINE# #TAB# #TAB# #TAB# results.append(series) #LINE# #TAB# return results
"Creates a list of sets containing jobs at different depths of the dependency tree  <code> def populate_cmdsets(job, cmdsets, depth): ","#LINE# #TAB# if len(cmdsets) < depth: #LINE# #TAB# #TAB# cmdsets.append(set()) #LINE# #TAB# cmdsets[depth-1].add(job.command) #LINE# #TAB# if len(job.dependencies) == 0: #LINE# #TAB# #TAB# return cmdsets #LINE# #TAB# for j in job.dependencies: #LINE# #TAB# #TAB# cmdsets = populate_cmdsets(j, cmdsets, depth+1) #LINE# #TAB# return cmdsets"
"Crop the images to ensure both fit within the bounding box <code> def crop_to_extents(img1, img2, padding): ","#LINE# #TAB# beg_coords1, end_coords1 = crop_coords(img1, padding) #LINE# #TAB# beg_coords2, end_coords2 = crop_coords(img2, padding) #LINE# #TAB# beg_coords = np.fmin(beg_coords1, beg_coords2) #LINE# #TAB# end_coords = np.fmax(end_coords1, end_coords2) #LINE# #TAB# img1 = crop_3dimage(img1, beg_coords, end_coords) #LINE# #TAB# img2 = crop_3dimage(img2, beg_coords, end_coords) #LINE# #TAB# return img1, img2"
"As : func:`node_matcher ` , except that empty resname and false PTM_atom attributes from ` node2 ` are removed  <code> def ptm_resname_match(node1, node2): ","#LINE# #TAB# if 'resname' in node2 and not node2['resname']: #LINE# #TAB# #TAB# node2 = node2.copy() #LINE# #TAB# #TAB# del node2['resname'] #LINE# #TAB# if 'PTM_atom' in node2 and not node2['PTM_atom']: #LINE# #TAB# #TAB# del node2['PTM_atom'] #LINE# #TAB# is_equal = node_matcher(node1, node2) #LINE# #TAB# return is_equal"
"private function , which assigns correct types to h5py extracted values from _ browse_dataset ( ) <code> def assign_types(values): ",#LINE# #TAB# if type(values) == numpy.ndarray: #LINE# #TAB# #TAB# assigned_values = _handle_ndarray(values) #LINE# #TAB# elif type(values) == numpy.float64: #LINE# #TAB# #TAB# assigned_values = float(values) #LINE# #TAB# else: #LINE# #TAB# #TAB# assigned_values = values #LINE# #TAB# return assigned_values
"Get BBNDK runtime metadata . : param session : Requests session object , default is created on the fly . : type session : requests . Session ( ) <code> def runtime_metadata(session=None): ","#LINE# #TAB# rturl = base_metadata_url('runtime') #LINE# #TAB# metadata = base_metadata(rturl, session) #LINE# #TAB# return metadata"
Copy the array if its base points to a parent array  <code> def copy_array_if_base_present(a): ,#LINE# #TAB# if a.base is not None: #LINE# #TAB# #TAB# return a.copy() #LINE# #TAB# return a
"Recursive check specific to arrays  <code> def rec_check_array(schema, path): ","#LINE# #TAB# out = list() #LINE# #TAB# if 'default' in schema: #LINE# #TAB# #TAB# out.append('ERROR : array ' + str_address(path) + #LINE# #TAB# #TAB# #TAB# ': -default- not supported yet') #LINE# #TAB# if 'items' not in schema: #LINE# #TAB# #TAB# out.append('ERROR : array ' + str_address(path) + #LINE# #TAB# #TAB# #TAB# ': -items- is missing') #LINE# #TAB# if 'type' not in schema['items']: #LINE# #TAB# #TAB# out.append('ERROR : array ' + str_address(path) + #LINE# #TAB# #TAB# #TAB# ': -type- of items is missing') #LINE# #TAB# key = 'items' #LINE# #TAB# out.extend(rec_check_schema(schema[key], [*path, key])) #LINE# #TAB# return out"
"Convert an integer to a 1D format with error correction for transformers  <code> def int_to_1d(in_int, max_bits, new_dim_length): ","#LINE# #TAB# bool_array = _int_to_bool_array(in_int, max_bits) #LINE# #TAB# extended_bool_array = _i_extend(bool_array, new_dim_length) #LINE# #TAB# return extended_bool_array"
Remove the last Template in the name  <code> def template_name_from_class_name(class_name): ,#LINE# #TAB# suffix = 'Template' #LINE# #TAB# output = class_name #LINE# #TAB# if (class_name.endswith(suffix)): #LINE# #TAB# #TAB# output = class_name[:-len(suffix)] #LINE# #TAB# return output
"Makes sure that whenever scale is zero we handle it correctly . This happens in most scalers when we have constant features . Adapted from sklearn . preprocessing . data <code> def handle_zeros_in_scale(scale, copy=True): ","#LINE# #TAB# if np.isscalar(scale): #LINE# #TAB# #TAB# if scale == .0: #LINE# #TAB# #TAB# #TAB# scale = 1. #LINE# #TAB# #TAB# return scale #LINE# #TAB# elif isinstance(scale, np.ndarray): #LINE# #TAB# #TAB# if copy: #LINE# #TAB# #TAB# #TAB# scale = scale.copy() #LINE# #TAB# #TAB# scale[scale == 0.0] = 1.0 #LINE# #TAB# return scale"
Returns a list which has been stored as a csv file <code> def read_list(csvFile): ,"#LINE# #TAB# with open(csvFile) as csvFile: #LINE# #TAB# #TAB# reader = csv.reader(csvFile, quotechar='|') #LINE# #TAB# #TAB# out = [] #LINE# #TAB# #TAB# for row in reader: #LINE# #TAB# #TAB# #TAB# out += row #LINE# #TAB# #TAB# return out"
"Common to swagger definition . : param base : The base dict . : param description : An optional description . : param resource : An optional resource . : param options : Any additional options <code> def to_swagger(base=None, description=None, resource=None, options=None): ","#LINE# #TAB# definition = dict_filter(base or {}, options or {}) #LINE# #TAB# if description: #LINE# #TAB# #TAB# definition['description'] = description.format(name=getmeta( #LINE# #TAB# #TAB# #TAB# resource).name if resource else 'UNKNOWN') #LINE# #TAB# if resource: #LINE# #TAB# #TAB# definition['schema'] = {'$ref': '#/definitions/{}'.format(getmeta( #LINE# #TAB# #TAB# #TAB# resource).resource_name)} #LINE# #TAB# return definition"
"For a given method name return the service type which supports it . : param method : the method name to lookup : return : the service type or None , an interface i d needs to be added to this : rtype : str <code> def get_service_type(method): ",#LINE# #TAB# if method in Wifi.serviceTypeLookup.keys(): #LINE# #TAB# #TAB# return Wifi.serviceTypeLookup[method] #LINE# #TAB# return None
Return the restrict identifier ( may be empty )  <code> def check_restrict(cmd): ,"#LINE# #TAB# cmd._check_compiler() #LINE# #TAB# body = textwrap.dedent( #LINE# #TAB# #TAB# ) #LINE# #TAB# for kw in ['restrict', '__restrict__', '__restrict']: #LINE# #TAB# #TAB# st = cmd.try_compile(body % {'restrict': kw}, None, None) #LINE# #TAB# #TAB# if st: #LINE# #TAB# #TAB# #TAB# return kw #LINE# #TAB# return ''"
"Convert a msa from chimerics to a matrix . Each cell has the subexon number ( Index ) or nan for gaps and padding  <code> def create_msa_matrix(chimerics, msa): ","#LINE# #TAB# n_seq = len(msa) #LINE# #TAB# n_col = msa.get_alignment_length() #LINE# #TAB# msa_matrix = np.zeros((n_seq, n_col)) #LINE# #TAB# msa_matrix.fill(np.nan) #LINE# #TAB# for seq_index in range(0, n_seq): #LINE# #TAB# #TAB# _fill_msa_matrix(msa_matrix, chimerics, msa, seq_index) #LINE# #TAB# return msa_matrix"
Delete random symbol from a string <code> def rand_delete(string: str): ,#LINE# #TAB# if len(string) == 0: #LINE# #TAB# #TAB# return string #LINE# #TAB# pos = random.choice(range(len(string))) #LINE# #TAB# return string[:pos] + string[pos + 1:]
"Returns a new Fragment from a dictionary representation  <code> def from_dict(cls, pods): ",#LINE# #TAB# #TAB# frag = cls() #LINE# #TAB# #TAB# frag.content = pods['content'] #LINE# #TAB# #TAB# frag._resources = [FragmentResource(**d) for d in pods['resources']] #LINE# #TAB# #TAB# frag.js_init_fn = pods['js_init_fn'] #LINE# #TAB# #TAB# frag.js_init_version = pods['js_init_version'] #LINE# #TAB# #TAB# frag.json_init_args = pods['json_init_args'] #LINE# #TAB# #TAB# return frag
"Collect outdated packages . @returns : list of packages <code> def collect_packages(pip_cmd='pip', verbose=False): ","#LINE# #TAB# outdated_command = ' '.join((pip_cmd, 'list --outdated --format json')) #LINE# #TAB# stdout, stderr = run_command(outdated_command) #LINE# #TAB# if stderr: #LINE# #TAB# #TAB# print('Error:', stderr.decode()) #LINE# #TAB# if verbose and stdout and stdout != b'[]\n': #LINE# #TAB# #TAB# print(stdout.decode()) #LINE# #TAB# pkgs = json.loads(stdout.decode()) #LINE# #TAB# for p in pkgs: #LINE# #TAB# #TAB# print('{}: {} ({})'.format(p['name'], p['latest_version'], p[ #LINE# #TAB# #TAB# #TAB# 'latest_filetype'])) #LINE# #TAB# return [p['name'] for p in pkgs]"
"when given a dictionary where every key contains a list of IDs replace the keys with the list of files matching those IDs . This is how you get a list of files belonging to each child for each parent  <code> def abf_group_files(groups,folder): ",#LINE# #TAB# assert os.path.exists(folder) #LINE# #TAB# files=os.listdir(folder) #LINE# #TAB# group2={} #LINE# #TAB# for parent in groups.keys(): #LINE# #TAB# #TAB# if not parent in group2.keys(): #LINE# #TAB# #TAB# #TAB# group2[parent]=[] #LINE# #TAB# #TAB# for ID in groups[parent]: #LINE# #TAB# #TAB# #TAB# for fname in [x.lower() for x in files if ID in x.lower()]: #LINE# #TAB# #TAB# #TAB# #TAB# group2[parent].extend([fname]) #LINE# #TAB# return group2
"Function to evaluate accuracy of any data iterator passed to it as an argument <code> def evaluate_accuracy(data_iterator, net): ","#LINE# #TAB# acc = mx.metric.Accuracy() #LINE# #TAB# for data, label in data_iterator: #LINE# #TAB# #TAB# output = net(data) #LINE# #TAB# #TAB# predictions = nd.argmax(output, axis=1) #LINE# #TAB# #TAB# predictions = predictions.reshape((-1, 1)) #LINE# #TAB# #TAB# acc.update(preds=predictions, labels=label) #LINE# #TAB# return acc.get()[1]"
Loads and returns regex patterns dictionary  <code> def load_regex_patterns(): ,"#LINE# #TAB# with pkg_resources.resource_stream(__name__, 'regex_patterns.yml') as file: #LINE# #TAB# #TAB# regex_patterns = yaml.safe_load(file) #LINE# #TAB# return regex_patterns"
"Yield hosts that have a specific vulnerability <code> def get_hosts_by_cve(tree, cve): ",#LINE# #TAB# query = 'ReportItem/cve/text()' #LINE# #TAB# for host in get_hosts(tree): #LINE# #TAB# #TAB# addr = get_addr(host) #LINE# #TAB# #TAB# for cve2 in host.xpath(query): #LINE# #TAB# #TAB# #TAB# if cve == cve2: #LINE# #TAB# #TAB# #TAB# #TAB# yield addr #LINE# #TAB# #TAB# #TAB# #TAB# break
Get admin s emails from django settings <code> def get_admins_from_django(homedir): ,"#LINE# #TAB# return [""root@localhost""] #LINE# #TAB# path = homedir + ""/settings/basic.py"" #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# path = homedir + ""/settings.py"" #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# return #LINE# #TAB# mod = compiler.parseFile(path) #LINE# #TAB# for node in mod.node.nodes: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if node.asList()[0].name == ""ADMINS"": #LINE# #TAB# #TAB# #TAB# #TAB# return [it.nodes[1].value for it in node.asList()[1].asList()] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass"
Returns type object of class specified by name Args : name : full name of the class ( with packages ) Returns : class object <code> def get_class_by_name(name: str) ->type: ,"#LINE# #TAB# components = name.split('.') #LINE# #TAB# mod = __import__(components[0]) #LINE# #TAB# for comp in components[1:]: #LINE# #TAB# #TAB# mod = getattr(mod, comp) #LINE# #TAB# return mod"
If s is a file name read the file and return it s content . Otherwise return the original string . Returns None if the file was opened but errored during reading  <code> def try_read_file(s): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(s, 'r') as f: #LINE# #TAB# #TAB# #TAB# data = f.read() #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return s #LINE# #TAB# except EnvironmentError as ex: #LINE# #TAB# #TAB# print_err('\nFailed to read file: {}\n {}'.format(s, ex)) #LINE# #TAB# #TAB# return None #LINE# #TAB# return data"
Heuristic to determine whether the treebank has blinded texts or not <code> def contains_blinded_text(stats_xml): ,#LINE# #TAB# tree = ET.parse(stats_xml) #LINE# #TAB# root = tree.getroot() #LINE# #TAB# total_tokens = int(root.find('size/total/tokens').text) #LINE# #TAB# unique_lemmas = int(root.find('lemmas').get('unique')) #LINE# #TAB# return unique_lemmas / total_tokens < 0.01
Returns the current projection . : return : Numpy array with projection  <code> def get_projection(): ,#LINE# #TAB# global _projection #LINE# #TAB# return _projection
returns the absolute velocity for the given actor <code> def get_velocity(actor): ,#LINE# #TAB# if actor not in CarlaDataProvider._actor_velocity_map.keys(): #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# return CarlaDataProvider._actor_velocity_map[actor]
Returns ` RequestInfo ` instance . If object was already created during ` ` request ` ` it is returned . Otherwise new instance is created with details populated from ` ` request ` ` . New instance is then cached for reuse on subsequential calls  <code> def create_or_get_from_request(request): ,"#LINE# #TAB# saved = getattr(request, REQUEST_CACHE_FIELD, None) #LINE# #TAB# if isinstance(saved, RequestInfo): #LINE# #TAB# #TAB# return saved #LINE# #TAB# req = RequestInfo() #LINE# #TAB# req.user_ip = request.META.get('REMOTE_ADDR') #LINE# #TAB# req.user_host = request.META.get('REMOTE_HOST') #LINE# #TAB# req.user_agent = request.META.get('HTTP_USER_AGENT') #LINE# #TAB# req.full_path = request.build_absolute_uri(request.get_full_path()) #LINE# #TAB# req.method = request.META.get('REQUEST_METHOD') #LINE# #TAB# req.referer = request.META.get('HTTP_REFERER') #LINE# #TAB# req.save() #LINE# #TAB# setattr(request, REQUEST_CACHE_FIELD, req) #LINE# #TAB# return req"
"Finds the selected ids After the scene has been rendered again in selection mode , this method gathers and returns the ids of the selected object and restores the matrices . : return : The selection stack <code> def find_selection(): ",#LINE# #TAB# hits = glRenderMode(GL_RENDER) #LINE# #TAB# glMatrixMode(GL_PROJECTION) #LINE# #TAB# glPopMatrix() #LINE# #TAB# glMatrixMode(GL_MODELVIEW) #LINE# #TAB# return hits
"Split an iterable of packages into packages that need to be passed through and those that need to have their disk location resolved  <code> def _split_packages(cls, include_packages): ","#LINE# #TAB# #TAB# passthrough_includes = set([ #LINE# #TAB# #TAB# #TAB# six.text_type(package.__name__) #LINE# #TAB# #TAB# #TAB# for package in include_packages #LINE# #TAB# #TAB# #TAB# if not hasattr(package, '__file__') #LINE# #TAB# #TAB# ]) #LINE# #TAB# #TAB# package_file_paths = dict([ #LINE# #TAB# #TAB# #TAB# (six.text_type(os.path.abspath(package.__file__)), six.text_type(package.__name__)) #LINE# #TAB# #TAB# #TAB# for package in include_packages #LINE# #TAB# #TAB# #TAB# if hasattr(package, '__file__') #LINE# #TAB# #TAB# ]) #LINE# #TAB# #TAB# return passthrough_includes, package_file_paths"
"Construct subcommand summaries : param command_summaries : Commands and their summaries : type command_summaries : list of ( str , str ) : returns : The subcommand summaries : rtype : str <code> def make_command_summary_string(command_summaries): ","#LINE# #TAB# doc = '' #LINE# #TAB# for command, summary in command_summaries: #LINE# #TAB# #TAB# doc += '\n\t{:15}\t{}'.format(command, summary.strip()) #LINE# #TAB# return doc"
Parse response and return content - type <code> def get_content_type(response): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = response.headers['Content-Type'] #LINE# #TAB# #TAB# content_type = content_type.split(';', 1)[0] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# content_type = None #LINE# #TAB# return content_type"
"Sets breakpoint on particular native event . : param eventName : Instrumentation name to stop on . : type eventName : str <code> def set_instrumentation_breakpoint(cls, eventName: Union['str']): ","#LINE# #TAB# return cls.build_send_payload('set_instrumentation_breakpoint', { #LINE# #TAB# #TAB# 'eventName': eventName}), None"
"Create roles as defined in the data  <code> def migrate_roles(model, perm_model): ","#LINE# #TAB# result = False #LINE# #TAB# for role, permissions in ROLES: #LINE# #TAB# #TAB# instance, created = model.objects.get_or_create(name=role) #LINE# #TAB# #TAB# result |= created #LINE# #TAB# #TAB# instance.permissions.set(perm_model.objects.filter(codename__in= #LINE# #TAB# #TAB# #TAB# permissions), clear=True) #LINE# #TAB# return result"
Return AT Content of Sequence <code> def at_content(seq): ,#LINE# #TAB# result = float(str(seq).count('A') + str(seq).count('T')) / len(seq) * 100 #LINE# #TAB# return result
"Compute znp flags for ( arg1 - arg2 ) <code> def update_flag_arith_sub_znp(arg1, arg2): ","#LINE# #TAB# e = [] #LINE# #TAB# e += update_flag_zf_eq(arg1, arg2) #LINE# #TAB# e += [m2_expr.ExprAssign(nf, m2_expr.ExprOp('FLAG_SIGN_SUB', arg1, arg2))] #LINE# #TAB# e += update_flag_pf(arg1 - arg2) #LINE# #TAB# return e"
Check if container is running : return bool : container is running <code> def is_running(node=''): ,#LINE# #TAB# from docker import from_env #LINE# #TAB# import requests #LINE# #TAB# if not node: #LINE# #TAB# #TAB# node = 'lnd' #LINE# #TAB# docker_host = from_env() #LINE# #TAB# compose_name = cfg.LND_MODE + '_' + node + '_1' #LINE# #TAB# try: #LINE# #TAB# #TAB# for container in docker_host.containers.list(): #LINE# #TAB# #TAB# #TAB# if compose_name in container.name: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# except ConnectionError: #LINE# #TAB# #TAB# return None #LINE# #TAB# except requests.exceptions.ConnectionError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return False
Provides a select box for country selection <code> def country_field(key='country'): ,"#LINE# #TAB# country_list = list(countries) #LINE# #TAB# title_map = [] #LINE# #TAB# for item in country_list: #LINE# #TAB# #TAB# title_map.append({'value': item.alpha_3, 'name': item.name}) #LINE# #TAB# widget = {'key': key, 'type': 'uiselect', 'titleMap': title_map} #LINE# #TAB# return widget"
"stretch image according to stretchDim = { axis : length , ... } <code> def stretch_img(imageClass, stretchDim, scheme): ","#LINE# #TAB# currentDim = imageClass.dim[:] #LINE# #TAB# newArray = np.copy(imageClass.data) #LINE# #TAB# for axis in stretchDim: #LINE# #TAB# #TAB# if axis in currentDim: #LINE# #TAB# #TAB# #TAB# transposeIndex, currentDim = arrangeDim(currentDim, [axis], True) #LINE# #TAB# #TAB# #TAB# newArray = newArray.transpose(transposeIndex) #LINE# #TAB# #TAB# #TAB# newArray = stretchFirstDimension(newArray, stretchDim[axis], scheme #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# transposeIndex, currentDim = arrangeDim(currentDim, imageClass.dim[:], True #LINE# #TAB# #TAB# ) #LINE# #TAB# newArray = newArray.transpose(transposeIndex) #LINE# #TAB# return newArray"
"Process response from batch caller Args : response ( Object ) : Requests library response object success ( bool ) : success condition given by bool or lambda Returns : str : batch status FINISH | FAILED <code> def process_response(response: object, success: bool) ->str: ",#LINE# #TAB# if response.status_code == 200 and response.text: #LINE# #TAB# #TAB# batch_data: dict = response.json() #LINE# #TAB# #TAB# return batch_data['status'] #LINE# #TAB# if success: #LINE# #TAB# #TAB# return 'FINISH' #LINE# #TAB# return 'FAILED'
"Function to create file <code> def write_file(data, file_ext='', file_name=''): ","#LINE# #TAB# if not file_ext.startswith('.'): #LINE# #TAB# #TAB# file_ext = '.' + file_ext #LINE# #TAB# if not file_name: #LINE# #TAB# #TAB# file_name = TIMESTAMP #LINE# #TAB# file_name += file_ext #LINE# #TAB# file_path = os.path.join(paths.POCSUITE_TMP_PATH, file_name) #LINE# #TAB# fd = open(file_path, 'wb+') #LINE# #TAB# fd.write(data) #LINE# #TAB# fd.close() #LINE# #TAB# return file_path"
"Check no hyphen at the end of rand_name ( ) argument T108 <code> def no_hyphen_at_end_of_rand_name(logical_line, filename): ","#LINE# #TAB# msg = 'T108: hyphen should not be specified at the end of rand_name()' #LINE# #TAB# if RAND_NAME_HYPHEN_RE.match(logical_line): #LINE# #TAB# #TAB# return 0, msg"
Construct the training script args from the hparams <code> def expand_hparams(hparams): ,"#LINE# #TAB# cmd = '' #LINE# #TAB# for field, val in hparams.items(): #LINE# #TAB# #TAB# if type(val) is bool: #LINE# #TAB# #TAB# #TAB# if val is True: #LINE# #TAB# #TAB# #TAB# #TAB# cmd += '--{} '.format(field) #LINE# #TAB# #TAB# elif val != 'None': #LINE# #TAB# #TAB# #TAB# cmd += '--{} {} '.format(field, val) #LINE# #TAB# cmd += ""'"" #LINE# #TAB# return cmd"
Parses or generates a udemy course url of a given link inside a page  <code> def parse_udemy_course_url(course_url): ,"#LINE# #TAB# response = fetch_request(course_url) #LINE# #TAB# course_soup = BeautifulSoup(response.text, 'lxml') #LINE# #TAB# link = course_soup.find('div', class_='link-holder').find('a').get('href') #LINE# #TAB# if 'udemy' in link: #LINE# #TAB# #TAB# return link #LINE# #TAB# udemy_link = fetch_request(link) #LINE# #TAB# if udemy_link.status_code == 200: #LINE# #TAB# #TAB# return udemy_link.url #LINE# #TAB# return 'Not available.'"
Returns external resources  <code> def get_resources(cls): ,"#LINE# #TAB# my_plurals = resource_helper.build_plural_mappings({}, #LINE# #TAB# #TAB# RESOURCE_ATTRIBUTE_MAP) #LINE# #TAB# attributes.PLURALS.update(my_plurals) #LINE# #TAB# attr_map = RESOURCE_ATTRIBUTE_MAP #LINE# #TAB# ext_resources = resource_helper.build_resource_info(my_plurals, #LINE# #TAB# #TAB# attr_map, constants.A10_CERTIFICATE) #LINE# #TAB# return ext_resources"
"Compare 2 sets of public numbers . These is a way to compare 2 public RSA keys . If the sets are the same then the keys are the same  <code> def cmp_public_numbers(pn1, pn2): ",#LINE# #TAB# if pn1.n == pn2.n: #LINE# #TAB# #TAB# if pn1.e == pn2.e: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
Generate new device i d <code> def generate_device_id(seed): ,#LINE# #TAB# m = md5() #LINE# #TAB# m.update(seed.encode('utf-8') + '12345'.encode('utf-8')) #LINE# #TAB# return 'android-' + m.hexdigest()[:16]
"determine if the transaction would be applied , even if zero transactions are found : param transaction : the working transaction : returns : a boolean value to determine if the transaction needs to be filtered out or not <code> def is_transaction_to_be_applied(cls, transaction) ->bool: ",#LINE# #TAB# if (transaction.transaction_type.post_zeros == True or transaction. #LINE# #TAB# #TAB# total_amount > 0): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Returns list of num_seqs sequential , unique names . First argument is ignored ; expect this to be set as a class attribute  <code> def assign_sequential_names(ignored, num_seqs, base_name='seq', start_at=0): ","#LINE# #TAB# return [('%s_%s' % (base_name, i)) for i in range(start_at, start_at + #LINE# #TAB# #TAB# num_seqs)]"
"Return True if code is similar  <code> def code_almost_equal(a, b): ","#LINE# #TAB# split_a = split_and_strip_non_empty_lines(a) #LINE# #TAB# split_b = split_and_strip_non_empty_lines(b) #LINE# #TAB# if len(split_a) != len(split_b): #LINE# #TAB# #TAB# return False #LINE# #TAB# for (index, _) in enumerate(split_a): #LINE# #TAB# #TAB# if ''.join(split_a[index].split()) != ''.join(split_b[index].split()): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Extract element data for psi_tables <code> def extract_element_data(element_map, classnames): ","#LINE# #TAB# element_data = {} #LINE# #TAB# for elements in six.itervalues(element_map): #LINE# #TAB# #TAB# for ufl_element, counter in six.iteritems(elements): #LINE# #TAB# #TAB# #TAB# fiat_element = create_element(ufl_element) #LINE# #TAB# #TAB# #TAB# value_size = product(ufl_element.value_shape()) #LINE# #TAB# #TAB# #TAB# element_classname = classnames['finite_element'][ufl_element] #LINE# #TAB# #TAB# #TAB# element_data[counter] = {'physical_value_size': value_size, #LINE# #TAB# #TAB# #TAB# #TAB# 'num_element_dofs': fiat_element.space_dimension(), #LINE# #TAB# #TAB# #TAB# #TAB# 'classname': element_classname} #LINE# #TAB# return element_data"
"Get Game Release Version List <code> def get_release_version_list(cls, update=False): ","#LINE# #TAB# versions = Mojang.get_version_list(update=update) #LINE# #TAB# release = list(filter(lambda version: version.get('type') == 'release', #LINE# #TAB# #TAB# versions)) #LINE# #TAB# return release"
"Construct a list of CodeUnits from polymorphic inputs  <code> def code_unit_factory(morfs, file_locator): ","#LINE# #TAB# if not isinstance(morfs, (list, tuple)): #LINE# #TAB# #TAB# morfs = [morfs] #LINE# #TAB# globbed = [] #LINE# #TAB# for morf in morfs: #LINE# #TAB# #TAB# if isinstance(morf, string_class) and ('?' in morf or '*' in morf): #LINE# #TAB# #TAB# #TAB# globbed.extend(glob.glob(morf)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# globbed.append(morf) #LINE# #TAB# morfs = globbed #LINE# #TAB# code_units = [CodeUnit(morf, file_locator) for morf in morfs] #LINE# #TAB# return code_units"
This just echoes back the payload  <code> def auth_echo(payload): ,"#LINE# #TAB# currproc = mp.current_process() #LINE# #TAB# engine = getattr(currproc, 'engine', None) #LINE# #TAB# if not engine: #LINE# #TAB# #TAB# currproc.engine, currproc.connection, currproc.table_meta = (authdb #LINE# #TAB# #TAB# #TAB# .get_auth_db(currproc.auth_db_path, echo=False)) #LINE# #TAB# permissions = currproc.table_meta.tables['permissions'] #LINE# #TAB# s = select([permissions]) #LINE# #TAB# result = currproc.engine.execute(s) #LINE# #TAB# serializable_result = list(dict(x) for x in result) #LINE# #TAB# payload['dbtest'] = serializable_result #LINE# #TAB# result.close() #LINE# #TAB# LOGGER.info('responding from process: %s' % currproc.name) #LINE# #TAB# return payload"
Parse the DSSP data file <code> def parse_dssp(filename): ,"#LINE# #TAB# secondary_structure = [] #LINE# #TAB# with open(filename, 'r') as dat_file: #LINE# #TAB# #TAB# next(dat_file) #LINE# #TAB# #TAB# for line in dat_file: #LINE# #TAB# #TAB# #TAB# secondary_structure.append(line.strip().split('=')) #LINE# #TAB# output = [] #LINE# #TAB# for chain in zip(*secondary_structure): #LINE# #TAB# #TAB# assert len(set([len(_) for _ in chain]) #LINE# #TAB# #TAB# #TAB# ) <= 1, 'The length of the secondary structure strings are not equal. Please check your input .dat file.' #LINE# #TAB# #TAB# output.append(numpy.array([list(_) for _ in chain], dtype=f'S1')) #LINE# #TAB# return output"
Contextmanager that change MAXBLOCK in ImageFile  <code> def image_save_buffer_fix(maxblock=1048576): ,#LINE# #TAB# before = ImageFile.MAXBLOCK #LINE# #TAB# ImageFile.MAXBLOCK = maxblock #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# ImageFile.MAXBLOCK = before
"Create Module from a LLVM IR string <code> def parse_assembly(llvmir, context=None): ","#LINE# #TAB# if context is None: #LINE# #TAB# #TAB# context = get_global_context() #LINE# #TAB# llvmir = _encode_string(llvmir) #LINE# #TAB# strbuf = c_char_p(llvmir) #LINE# #TAB# with ffi.OutputString() as errmsg: #LINE# #TAB# #TAB# mod = ModuleRef( #LINE# #TAB# #TAB# #TAB# ffi.lib.LLVMPY_ParseAssembly(context, strbuf, errmsg), #LINE# #TAB# #TAB# #TAB# context) #LINE# #TAB# #TAB# if errmsg: #LINE# #TAB# #TAB# #TAB# mod.close() #LINE# #TAB# #TAB# #TAB# raise RuntimeError(""LLVM IR parsing error\n{0}"".format(errmsg)) #LINE# #TAB# return mod"
"Compute inner product between two time domain waveforms weighted by noisecurve  <code> def inner_product(y,yr,psd): ","#LINE# #TAB# fmin = 40. #LINE# #TAB# fmax = 2000. #LINE# #TAB# fs = 16384. #LINE# #TAB# y = (1./fs)*y #LINE# #TAB# yr = (1./fs)*yr #LINE# #TAB# p = np.multiply(y,np.conjugate(yr))/psd #LINE# #TAB# product = max(abs(fs*sp.ifft(np.multiply(y,np.conjugate(yr))/psd))) #LINE# #TAB# return product"
Calculates the hypothetical total edge cost if the graph were to be complete . : rtype : float : param nxg : The graph to calculate the hypothetical edge cost of . : return : The total edge cost if the graph were complete  <code> def hypothetical_max_edge_cost(nxg: nx.Graph) ->float: ,#LINE# #TAB# complete_graph = copy.deepcopy(nxg) #LINE# #TAB# complete_graph = Solver.complete(complete_graph) #LINE# #TAB# total_edge_cost = Analytics.total_edge_cost(complete_graph) #LINE# #TAB# del complete_graph #LINE# #TAB# return total_edge_cost
Automatically try to discover Django settings files return them as relative module paths  <code> def detect_django_settings(): ,"#LINE# #TAB# matches = [] #LINE# #TAB# for root, dirnames, filenames in os.walk(os.getcwd()): #LINE# #TAB# #TAB# for filename in fnmatch.filter(filenames, '*settings.py'): #LINE# #TAB# #TAB# #TAB# full = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# if 'site-packages' in full: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# full = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# package_path = full.replace(os.getcwd(), '') #LINE# #TAB# #TAB# #TAB# package_module = package_path.replace(os.sep, '.').split('.', 1)[1].replace('.py', '') #LINE# #TAB# #TAB# #TAB# matches.append(package_module) #LINE# #TAB# return matches"
"Get the number of names that exist at the current block <code> def namedb_get_num_names( cur, current_block, include_expired=False ): ","#LINE# #TAB# unexpired_query = """" #LINE# #TAB# unexpired_args = () #LINE# #TAB# if not include_expired: #LINE# #TAB# #TAB# unexpired_query, unexpired_args = namedb_select_where_unexpired_names( current_block ) #LINE# #TAB# #TAB# unexpired_query = 'WHERE {}'.format(unexpired_query) #LINE# #TAB# query = ""SELECT COUNT(name_records.name) FROM name_records JOIN namespaces ON name_records.namespace_id = namespaces.namespace_id "" + unexpired_query + "";"" #LINE# #TAB# args = unexpired_args #LINE# #TAB# num_rows = namedb_select_count_rows( cur, query, args, count_column='COUNT(name_records.name)' ) #LINE# #TAB# return num_rows"
Checks to see if the residue is non - canonical  <code> def check_for_non_canonical(residue): ,"#LINE# #TAB# res_label = list(residue[0])[0][2] #LINE# #TAB# atom_labels = {x[2] for x in itertools.chain(*residue[1].values())} #LINE# #TAB# if all(x in atom_labels for x in ['N', 'CA', 'C', 'O']) and len(res_label #LINE# #TAB# #TAB# ) == 3: #LINE# #TAB# #TAB# return Residue, True #LINE# #TAB# return None"
Get the current plugin information ( from a plugin ) <code> def get_current_from(obj): ,#LINE# #TAB# for plugin in PluginManager.get_all(): #LINE# #TAB# #TAB# if 'module' in plugin and plugin['module'].__name__ == sys.modules[ #LINE# #TAB# #TAB# #TAB# obj.__module__].__package__: #LINE# #TAB# #TAB# #TAB# return plugin #LINE# #TAB# return None
Return a list of url_patterns from the registered routes  <code> def make_url_patterns(): ,"#LINE# #TAB# from django.conf.urls import url #LINE# #TAB# routes = dict(ROUTES) #LINE# #TAB# routes.pop('', None) #LINE# #TAB# result = [] #LINE# #TAB# for route, view in routes.items(): #LINE# #TAB# #TAB# result.append(url(regex_from_route(route), view)) #LINE# #TAB# return result"
Get the long description of a package from its README.rst file Args : dirname ( : obj:`str ` ) : path to the package Returns : : obj:`str ` : long description <code> def get_long_description(dirname): ,"#LINE# #TAB# if os.path.isfile(os.path.join(dirname, 'README.rst')): #LINE# #TAB# #TAB# with open(os.path.join(dirname, 'README.rst'), 'r') as file: #LINE# #TAB# #TAB# #TAB# return file.read() #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
"Sanitizes http request query string <code> def sanitize_http_request_querystring(client, event): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# query_string = force_text(event[""context""][""request""][""url""][""search""], errors=""replace"") #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# return event #LINE# #TAB# if ""="" in query_string: #LINE# #TAB# #TAB# sanitized_query_string = _sanitize_string(query_string, ""&"", ""="") #LINE# #TAB# #TAB# full_url = event[""context""][""request""][""url""][""full""] #LINE# #TAB# #TAB# event[""context""][""request""][""url""][""search""] = sanitized_query_string #LINE# #TAB# #TAB# event[""context""][""request""][""url""][""full""] = full_url.replace(query_string, sanitized_query_string) #LINE# #TAB# return event"
"Returns the opcodes to push the data on the stack  <code> def push_data(cls, data): ","#LINE# #TAB# assert isinstance(data, (bytes, bytearray)) #LINE# #TAB# n = len(data) #LINE# #TAB# if n < OpCodes.OP_PUSHDATA1: #LINE# #TAB# #TAB# return bytes([n]) + data #LINE# #TAB# if n < 256: #LINE# #TAB# #TAB# return bytes([OpCodes.OP_PUSHDATA1, n]) + data #LINE# #TAB# if n < 65536: #LINE# #TAB# #TAB# return bytes([OpCodes.OP_PUSHDATA2]) + pack_le_uint16(n) + data #LINE# #TAB# return bytes([OpCodes.OP_PUSHDATA4]) + pack_le_uint32(n) + data"
This method will initialize ` ` SharedInstance.instance ` ` and return it . The purpose of this method is to have offer single default bitshares instance that can be reused by multiple classes  <code> def shared_bitshares_instance(): ,#LINE# #TAB# if not SharedInstance.instance: #LINE# #TAB# #TAB# clear_cache() #LINE# #TAB# #TAB# SharedInstance.instance = bts.BitShares() #LINE# #TAB# return SharedInstance.instance
Parse document with content  <code> def parse_document(xmlcontent): ,"#LINE# #TAB# document = etree.fromstring(xmlcontent) #LINE# #TAB# body = document.xpath('.//w:body', namespaces=NAMESPACES)[0] #LINE# #TAB# document = doc.Document() #LINE# #TAB# for elem in body: #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}p'): #LINE# #TAB# #TAB# #TAB# document.elements.append(parse_paragraph(document, elem)) #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}tbl'): #LINE# #TAB# #TAB# #TAB# document.elements.append(parse_table(document, elem)) #LINE# #TAB# #TAB# if elem.tag == _name('{{{w}}}sdt'): #LINE# #TAB# #TAB# #TAB# document.elements.append(doc.TOC()) #LINE# #TAB# return document"
"Read a hdf5 file into a dictionary <code> def h5_to_dict(h5, readH5pyDataset=True): ","#LINE# #TAB# h = h5py.File(h5, ""r"") #LINE# #TAB# ret = unwrapArray(h, recursive=True, readH5pyDataset=readH5pyDataset) #LINE# #TAB# if readH5pyDataset: h.close() #LINE# #TAB# return ret"
"return Max Depth for a df row passed with X & Y columns , for use in a SWMM5 Junctions table <code> def max_depth_from_raster(row, dem_pth, dem_adjustment=-4.63): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# from rasterstats import point_query #LINE# #TAB# except: #LINE# #TAB# #TAB# raise ImportError('rasterstats not found. Try pip install rasterstats') #LINE# #TAB# if pd.isnull(row.X) or pd.isnull(row.Y): #LINE# #TAB# #TAB# return None #LINE# #TAB# point = 'POINT({} {})'.format(row.X, row.Y) #LINE# #TAB# rim_elev = point_query(point, dem_pth)[0] + dem_adjustment #LINE# #TAB# invert = row.InvertElev #LINE# #TAB# max_depth = rim_elev - invert #LINE# #TAB# return max_depth"
"This function does not deal well with NaN values , so make sure to remove them before calling  <code> def find_peaks(signal): ","#LINE# #TAB# import scipy.signal #LINE# #TAB# _, peaks_dict = scipy.signal.find_peaks(signal, plateau_size=(None, None)) #LINE# #TAB# res = np.zeros_like(signal, dtype=bool) #LINE# #TAB# for left, right in zip(peaks_dict['left_edges'], peaks_dict['right_edges'] #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# res[left:right + 1] = True #LINE# #TAB# return res"
Return the filenames of the available XML schemas . These are essentially all the files with the .xsd extenstion in the ` DIRPATH_SCHEMAS ` folder : return : a list of XML schema filenames <code> def get_available_xml_schemas(): ,#LINE# #TAB# return [file for file in os.listdir(DIRPATH_SCHEMAS) if file.endswith( #LINE# #TAB# #TAB# '.xsd')]
"Returns pairwise energy . : param points : size : scalar ; smooth_factor : scalar : rtype : pairwise energy matrix  <code> def compute_pairwise_cost(size, smooth_factor): ","#LINE# #TAB# pairwise_size = size #LINE# #TAB# pairwise = -smooth_factor * np.eye(pairwise_size, dtype=np.int32) #LINE# #TAB# step_weight = -smooth_factor * np.arange(pairwise_size)[::-1] #LINE# #TAB# for i in range(pairwise_size): #LINE# #TAB# #TAB# pairwise[(i), :] += np.roll(step_weight, i) #LINE# #TAB# temp = np.triu(pairwise).T + np.triu(pairwise) #LINE# #TAB# np.fill_diagonal(temp, np.diag(temp) / 2) #LINE# #TAB# return temp"
Returns sequence string  <code> def get_sequence(lines): ,#LINE# #TAB# if lines: #LINE# #TAB# #TAB# seq_line = ''.join(lines) #LINE# #TAB# #TAB# seq = seq_line.split('(')[1].split(') def')[0].strip('\n\\') #LINE# #TAB# else: #LINE# #TAB# #TAB# seq = '' #LINE# #TAB# return seq
"Returns each row as a : class:` . PseudoNamedTupleRow ` . This is the fallback factory for cases where : meth:`.named_tuple_factory ` fails to create rows  <code> def pseudo_namedtuple_factory(colnames, rows): ","#LINE# #TAB# return [PseudoNamedTupleRow(od) for od in ordered_dict_factory(colnames, #LINE# #TAB# #TAB# rows)]"
"Check event attributes presence in all the traces of the log Parameters ------------ log Log attributes_set Set of attributes Returns ------------ filtered_set Filtered set of attributes <code> def check_event_attributes_presence(log, attributes_set): ","#LINE# #TAB# keys = list(attributes_set) #LINE# #TAB# for attr in keys: #LINE# #TAB# #TAB# if not verify_if_event_attribute_is_in_each_trace(log, attr): #LINE# #TAB# #TAB# #TAB# attributes_set.remove(attr) #LINE# #TAB# return attributes_set"
"Guesses the atomic number <code> def guess_atomic_number(name, residue=None): ",#LINE# #TAB# name = ''.join(c for c in name if c.isalpha()) #LINE# #TAB# if residue is None or len(residue.atoms) == 1: #LINE# #TAB# #TAB# if len(name) > 1: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# return AtomicNum[name[0].upper() + name[1].lower()] #LINE# #TAB# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# #TAB# return AtomicNum[element_by_name(name)] #LINE# #TAB# return AtomicNum[element_by_name(name)]
"View to download a document  <code> def get_account_credentials(request, accountid): ",#LINE# #TAB# account = User.objects.get(pk=accountid) #LINE# #TAB# if not request.user.can_access(account): #LINE# #TAB# #TAB# raise PermDeniedException() #LINE# #TAB# fname = get_creds_filename(account) #LINE# #TAB# if not os.path.exists(fname): #LINE# #TAB# #TAB# raise KalabashException(_('No document available for this user')) #LINE# #TAB# content = decrypt_file(fname) #LINE# #TAB# if param_tools.get_global_parameter('delete_first_dl'): #LINE# #TAB# #TAB# os.remove(fname) #LINE# #TAB# resp = HttpResponse(content) #LINE# #TAB# resp['Content-Type'] = 'application/pdf' #LINE# #TAB# resp['Content-Length'] = len(content) #LINE# #TAB# resp['Content-Disposition'] = build_header(os.path.basename(fname)) #LINE# #TAB# return resp
"Convenience wrapper for NameCollector <code> def collect_names(tree, ctx=None): ",#LINE# #TAB# visitor = NameCollector(ctx) #LINE# #TAB# visitor.visit(tree) #LINE# #TAB# return visitor.names
"Extract the Package - Name and the Package - Version from the Pacman - Package . : param file_path : Path to the Pacman - Package : return : A PackageInfo()-Object with Package - Version and Package - Name  <code> def get_packageinfo_from_packagefile(cls, file_path): ","#LINE# #TAB# command_args_packageinfo = [cls.executable, '--query', '--file', file_path] #LINE# #TAB# package_info_output = CM.run_command_check_output(command_args_packageinfo) #LINE# #TAB# line_split = package_info_output.split(' ') #LINE# #TAB# package_info = PackageInfo() #LINE# #TAB# package_info.package = line_split[0] #LINE# #TAB# package_info.version = line_split[1].rstrip() #LINE# #TAB# return package_info"
"p(k ) = { \mu^k \over k ! } \exp(-\mu ) <code> def rnd_poisson(l, t=0): ",#LINE# #TAB# u = random.random() #LINE# #TAB# p = math.exp(-l) #LINE# #TAB# k = 0 #LINE# #TAB# s = 0 #LINE# #TAB# while u < s: #LINE# #TAB# #TAB# s += p #LINE# #TAB# #TAB# k += 1 #LINE# #TAB# #TAB# p = 1.0 * l * p / (k + 1) #LINE# #TAB# return k
Returns a list of all the roles for an account . Returns a list containing all the roles for the account  <code> def get_roles(client): ,#LINE# #TAB# #TAB# done = False #LINE# #TAB# #TAB# marker = None #LINE# #TAB# #TAB# roles = [] #LINE# #TAB# #TAB# while not done: #LINE# #TAB# #TAB# #TAB# if marker: #LINE# #TAB# #TAB# #TAB# #TAB# response = client.list_roles(Marker=marker) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# response = client.list_roles() #LINE# #TAB# #TAB# #TAB# roles += response['Roles'] #LINE# #TAB# #TAB# #TAB# if response['IsTruncated']: #LINE# #TAB# #TAB# #TAB# #TAB# marker = response['Marker'] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# done = True #LINE# #TAB# #TAB# return roles
When returning a password from a TTY we assume a user is entering it on a keyboard so we ask for confirmation  <code> def get_tty_password(confirm): ,"#LINE# #TAB# LOG.debug(""Reading password from TTY"") #LINE# #TAB# new_password = getpass('Enter Password: ', stream=sys.stderr) #LINE# #TAB# if not new_password: #LINE# #TAB# #TAB# raise aomi.exceptions.AomiCommand(""Must specify a password"") #LINE# #TAB# if not confirm: #LINE# #TAB# #TAB# return new_password #LINE# #TAB# confirm_password = getpass('Again, Please: ', stream=sys.stderr) #LINE# #TAB# if confirm_password != new_password: #LINE# #TAB# #TAB# raise aomi.exceptions.AomiCommand(""Passwords do not match"") #LINE# #TAB# return new_password"
Returns the number of bits that are set in a 32bit int <code> def number_of_set_bits(x): ,#LINE# #TAB# x -= (x >> 1) & 0x55555555 #LINE# #TAB# x = ((x >> 2) & 0x33333333) + (x & 0x33333333) #LINE# #TAB# x = ((x >> 4) + x) & 0x0f0f0f0f #LINE# #TAB# x += x >> 8 #LINE# #TAB# x += x >> 16 #LINE# #TAB# return x & 0x0000003f
"Filter dataframe to exclude matching columns , based on search for "" s "" : param s : string to search for , exclude matching columns <code> def filter_exclude(df, s): ","#LINE# #TAB# keep = ~np.array([(s in c) for c in df.columns.values]) #LINE# #TAB# return df.iloc[:, (keep)]"
return a param as a list <code> def as_list(param): ,"#LINE# #TAB# if type(param) not in (list, tuple): #LINE# #TAB# #TAB# p = param.strip() #LINE# #TAB# #TAB# if p: #LINE# #TAB# #TAB# #TAB# return [x.strip() for x in p.split(',')] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return [] #LINE# #TAB# else: #LINE# #TAB# #TAB# return param"
"Calculates the request payload size <code> def calculate_size(name, new_value): ",#LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += LONG_SIZE_IN_BYTES #LINE# #TAB# return data_size
Tries to infer a protocol from the file extension  <code> def format_from_extension(fname): ,"#LINE# #TAB# _base, ext = os.path.splitext(fname) #LINE# #TAB# if not ext: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# format = known_extensions[ext.replace('.', '')] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# format = None #LINE# #TAB# return format"
"This function sets up the engine.io endpoint as a route for the application . Note that both GET and POST requests must be hooked up on the engine.io endpoint  <code> def create_route(app, engineio_server, engineio_endpoint): ","#LINE# #TAB# app.add_route(engineio_server.handle_request, engineio_endpoint, #LINE# #TAB# #TAB# methods=['GET', 'POST', 'OPTIONS']) #LINE# #TAB# try: #LINE# #TAB# #TAB# app.enable_websocket() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass"
Return list of strings of maximal length in a set of strings  <code> def get_longest_substrings(string_set): ,"#LINE# #TAB# longest = len(max(string_set, key=lambda x: len(x))) #LINE# #TAB# return [x for x in string_set if len(x) == longest]"
"Helper function to add new my_element to my_list based on its type  <code> def add_to_list(my_list, my_element): ","#LINE# #TAB# if isinstance(my_element, list): #LINE# #TAB# #TAB# for element in my_element: #LINE# #TAB# #TAB# #TAB# my_list = add_to_list(my_list, element) #LINE# #TAB# else: #LINE# #TAB# #TAB# if my_element not in my_list: #LINE# #TAB# #TAB# #TAB# my_list.append(my_element) #LINE# #TAB# return my_list"
Create a mapping ( item to ID / ID to item ) from a dictionary . Items are ordered by decreasing frequency  <code> def create_mapping(dico): ,"#LINE# #TAB# sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0])) #LINE# #TAB# id_to_item = {i: v[0] for i, v in enumerate(sorted_items)} #LINE# #TAB# item_to_id = {v: k for k, v in id_to_item.items()} #LINE# #TAB# return item_to_id, id_to_item"
checks string for potentially malicious inputs for eval function <code> def check_eval(str1): ,"#LINE# #TAB# str1 = str(str1) #LINE# #TAB# excludes = ['import', '__', 'os', 'sys', 'eval', 'exec', 'compile', #LINE# #TAB# #TAB# 'open', 'write', '\n', '\t'] #LINE# #TAB# for i in range(len(excludes)): #LINE# #TAB# #TAB# if excludes[i] in str1: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"First send strings from any given file , one string per line , sends any strings provided on the command line . : param options : ArgumentParser or equivalent to provide options.input and options.strings . : return : string <code> def generate_input(options): ",#LINE# #TAB# if options.input: #LINE# #TAB# #TAB# fp = open(options.input) if options.input != '-' else sys.stdin #LINE# #TAB# #TAB# for string in fp.readlines(): #LINE# #TAB# #TAB# #TAB# yield string #LINE# #TAB# if options.strings: #LINE# #TAB# #TAB# for string in options.strings: #LINE# #TAB# #TAB# #TAB# yield string
Given a list of links separate them into pages that can be displayed to the user and navigated using the 1 - 9 and 0 number keys  <code> def get_link_pages(links): ,#LINE# #TAB# #TAB# link_pages = [] #LINE# #TAB# #TAB# i = 0 #LINE# #TAB# #TAB# while i < len(links): #LINE# #TAB# #TAB# #TAB# link_page = [] #LINE# #TAB# #TAB# #TAB# while i < len(links) and len(link_page) < 10: #LINE# #TAB# #TAB# #TAB# #TAB# link_page.append(links[i]) #LINE# #TAB# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# #TAB# link_pages.append(link_page) #LINE# #TAB# #TAB# return link_pages
for json output add a full stop if ends in et al <code> def copyright_holder_json(soup): ,#LINE# #TAB# holder = None #LINE# #TAB# permissions_tag = raw_parser.article_permissions(soup) #LINE# #TAB# if permissions_tag: #LINE# #TAB# #TAB# holder = node_text(raw_parser.copyright_holder(permissions_tag)) #LINE# #TAB# if holder is not None and holder.endswith('et al'): #LINE# #TAB# #TAB# holder = holder + '.' #LINE# #TAB# return holder
` ` int SDL_JoystickNumHats(SDL_Joystick * ) ` ` Get the number of POV hats on a joystick  <code> def sdl_joysticknumhats(joystick): ,"#LINE# #TAB# joystick_c = unbox(joystick, 'SDL_Joystick *') #LINE# #TAB# rc = lib.sdl_joysticknumhats(joystick_c) #LINE# #TAB# return rc"
"Return a response wrapped in the appropriate wrapper type . Lists will be returned as a ` ` ` ResourceList ` ` ` instance , dicts will be returned as a ` ` ` Resource ` ` ` instance  <code> def wrapped_resource(response): ","#LINE# #TAB# response_content = response.content.decode(response.encoding or 'utf-8') #LINE# #TAB# try: #LINE# #TAB# #TAB# content = json.loads(response_content) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# content = response_content #LINE# #TAB# if isinstance(content, list): #LINE# #TAB# #TAB# result = ResourceList(content) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = Resource(content) #LINE# #TAB# #TAB# if hasattr(result, 'collection'): #LINE# #TAB# #TAB# #TAB# result.collection = ResourceList(result.collection) #LINE# #TAB# result.raw_data = response_content #LINE# #TAB# for attr in ('encoding', 'url', 'status_code', 'reason'): #LINE# #TAB# #TAB# setattr(result, attr, getattr(response, attr)) #LINE# #TAB# return result"
Convert string parameter value to a list with a single element  <code> def to_list(value): ,"#LINE# #TAB# if not value: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif isinstance(value, list): #LINE# #TAB# #TAB# return value #LINE# #TAB# elif isinstance(value, tuple): #LINE# #TAB# #TAB# return list(value) #LINE# #TAB# else: #LINE# #TAB# #TAB# return [value]"
Return a : class:`XsdValidator ` object based on available dependencies  <code> def get_validator(require=True): ,#LINE# #TAB# for validator in VALIDATORS: #LINE# #TAB# #TAB# if validator.enabled(): #LINE# #TAB# #TAB# #TAB# return validator #LINE# #TAB# if require: #LINE# #TAB# #TAB# raise Exception(INSTALL_VALIDATOR_MESSAGE) #LINE# #TAB# return None
return list of names for ENUM states of a Channel . Returns None for non - ENUM Channels <code> def get_enum_strings(chid): ,"#LINE# #TAB# if field_type(chid) == dbr.ENUM: #LINE# #TAB# #TAB# return get_ctrlvars(chid).get('enum_strs', None) #LINE# #TAB# return None"
"Recreate a given subformat  <code> def create_subformat(id_type, id_value, quality): ","#LINE# #TAB# _validate(id_type=id_type, quality=quality) #LINE# #TAB# video_deposit, dep_uuid = _resolve_deposit(id_type, id_value) #LINE# #TAB# master, ar, w, h = _get_master_video(video_deposit) #LINE# #TAB# subformat = can_be_transcoded(quality, ar, w, h) #LINE# #TAB# if subformat: #LINE# #TAB# #TAB# MaintenanceTranscodeVideoTask().s(version_id=master['version_id'], #LINE# #TAB# #TAB# #TAB# preset_quality=subformat['quality'], deposit_id=dep_uuid #LINE# #TAB# #TAB# #TAB# ).apply_async() #LINE# #TAB# #TAB# return subformat"
Read pandoc attributes  <code> def parse_pandoc(attrs): ,"#LINE# #TAB# id_ = attrs[0] #LINE# #TAB# classes = attrs[1] #LINE# #TAB# kvs = OrderedDict(attrs[2]) #LINE# #TAB# return id_, classes, kvs"
Helper function to ensure ' params ' is in a nice state for later use  <code> def initialize_params(params): ,"#LINE# #TAB# if 'order' in params: #LINE# #TAB# #TAB# p = OrderedDict((k, params[k]) for k in params['order']) #LINE# #TAB# else: #LINE# #TAB# #TAB# p = OrderedDict(sorted(params.items(), key=lambda t: t[0])) #LINE# #TAB# for k, pd in p.iteritems(): #LINE# #TAB# #TAB# if not isinstance(pd, Mapping): #LINE# #TAB# #TAB# #TAB# pd = {'init': pd} #LINE# #TAB# #TAB# #TAB# p[k] = pd #LINE# #TAB# #TAB# _initialize_range_params(pd) #LINE# #TAB# #TAB# pd.setdefault('label', k.title()) #LINE# #TAB# return p"
"Ensure that dict has field ' type ' with given value  <code> def validate_type(data_dict: dict, type_name: str) ->dict: ","#LINE# #TAB# data_dict_copy = data_dict.copy() #LINE# #TAB# if 'type' in data_dict_copy: #LINE# #TAB# #TAB# if data_dict_copy['type'] != type_name: #LINE# #TAB# #TAB# #TAB# raise Exception('Object type must be {}, but was instead {}.'. #LINE# #TAB# #TAB# #TAB# #TAB# format(type_name, data_dict['type'])) #LINE# #TAB# else: #LINE# #TAB# #TAB# data_dict_copy['type'] = type_name #LINE# #TAB# return data_dict_copy"
"Trim low quality bases from the 3'-end of the sequence  <code> def trim_trailing(scores, threshold): ","#LINE# #TAB# threshold = abs(threshold) #LINE# #TAB# position = 0 #LINE# #TAB# for position, basescore in enumerate(scores[::-1]): #LINE# #TAB# #TAB# if basescore >= threshold: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return 0, position"
"create a knn model object . : param fname : dataset file : type fname : str : return : model object , lookup table : rtype : tuple <code> def create_knn(fname): ","#LINE# #TAB# knn = cv2.ml.KNearest_create() #LINE# #TAB# data, labels, lookup_table, idx_labels = load_dataset(fname) #LINE# #TAB# knn.train(data, cv2.ml.ROW_SAMPLE, idx_labels) #LINE# #TAB# return knn, lookup_table"
Make directory ' plot ' in the ` outdir ` and return its path <code> def make_plotdir(outdir): ,"#LINE# #TAB# plotdir = os.path.join(outdir, 'plots') #LINE# #TAB# os.makedirs(plotdir, exist_ok=True) #LINE# #TAB# return plotdir"
"Encode an S3Key , this is just a link to the S3 URL , with a special _ _ s3key _ _ type <code> def encode_key(value): ",#LINE# #TAB# ret = {'__type__': '__s3key__'} #LINE# #TAB# ret['__href__'] = value.generate_url(3600) #LINE# #TAB# return ret
Returns returncode of example <code> def run_example(path): ,"#LINE# #TAB# cmd = ""{0} {1}"".format(sys.executable, path) #LINE# #TAB# proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# res = proc.communicate() #LINE# #TAB# if proc.returncode: #LINE# #TAB# #TAB# print(res[1].decode()) #LINE# #TAB# return proc.returncode"
Returns a Piccolo / Picalor compatible ID <code> def toidpic_old(txt): ,"#LINE# #TAB# txt = txt.strip() #LINE# #TAB# if txt == '': #LINE# #TAB# #TAB# return '_EMPTY_' #LINE# #TAB# txt = ascii(txt.replace(' ', '_').replace(' ', '_').replace(',', '_'). #LINE# #TAB# #TAB# replace('=', '_').replace('.', '-').replace('/', '-').replace('(', #LINE# #TAB# #TAB# '-').replace(')-').replace('_-_', '-')) #LINE# #TAB# return txt"
"berDecodeMultiple(content , berdecoder ) - > [ objects ] Decodes everything in content and returns a list of decoded objects . All of content will be decoded , and content must contain complete BER objects  <code> def ber_decode_multiple(content, berdecoder): ","#LINE# #TAB# l = [] #LINE# #TAB# while content: #LINE# #TAB# #TAB# n, bytes = berDecodeObject(berdecoder, content) #LINE# #TAB# #TAB# if n is not None: #LINE# #TAB# #TAB# #TAB# l.append(n) #LINE# #TAB# #TAB# assert bytes <= len(content) #LINE# #TAB# #TAB# content = content[bytes:] #LINE# #TAB# return l"
Read from sys.stdin and return an array of integers . An integer at the beginning of sys.stdin defines the array 's length  <code> def read_int1_d(): ,"#LINE# #TAB# count = stdio.readInt() #LINE# #TAB# a = create1D(count, None) #LINE# #TAB# for i in range(count): #LINE# #TAB# #TAB# a[i] = stdio.readInt() #LINE# #TAB# return a"
Download corpora for multinli  <code> def maybe_download_corpora(tmp_dir): ,"#LINE# mnli_filename = ""MNLI.zip"" #LINE# mnli_finalpath = os.path.join(tmp_dir, ""MNLI"") #LINE# if not tf.gfile.Exists(mnli_finalpath): #LINE# #TAB# zip_filepath = generator_utils.maybe_download( #LINE# #TAB# #TAB# tmp_dir, mnli_filename, _MNLI_URL) #LINE# #TAB# zip_ref = zipfile.ZipFile(zip_filepath, ""r"") #LINE# #TAB# zip_ref.extractall(tmp_dir) #LINE# #TAB# zip_ref.close() #LINE# return mnli_finalpath"
Converts a string argument to a list by splitting it by spaces . Returns the object if not a string <code> def to_list(val): ,"#LINE# #TAB# if isinstance(val, _pyutils.stringtype): #LINE# #TAB# #TAB# return val.split() #LINE# #TAB# return val"
"Get the binary Hamming distance between two sequences of bytes  <code> def hamming_distance(left_bytes, right_bytes): ","#LINE# #TAB# if len(left_bytes) != len(right_bytes): #LINE# #TAB# #TAB# raise ValueError('Length of both sequences must match') #LINE# #TAB# result = 0 #LINE# #TAB# for left, right in zip(left_bytes, right_bytes): #LINE# #TAB# #TAB# diff = left ^ right #LINE# #TAB# #TAB# for j in range(8): #LINE# #TAB# #TAB# #TAB# result += diff >> j & 1 #LINE# #TAB# return result"
Remaps Oscar credit card object as Helcim dictionary . Parameters : card ( obj ) : A credit card object provided by Django Oscar . Returns : dict : Credit card details formatted for django - helcim  <code> def remap_oscar_credit_card(card): ,"#LINE# #TAB# if card.expiry_date: #LINE# #TAB# #TAB# cc_expiry = card.expiry_date.strftime('%m%y') #LINE# #TAB# else: #LINE# #TAB# #TAB# cc_expiry = None #LINE# #TAB# return {'cc_name': card.name, 'cc_number': card.number, 'cc_expiry': #LINE# #TAB# #TAB# cc_expiry, 'cc_cvv': card.ccv}"
Method to check to use shared memory  <code> def safe_worker_check(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return torch.utils.data.get_worker_info() is not None #LINE# #TAB# except: #LINE# #TAB# #TAB# return pytorch_collate._use_shared_memory
"Prepare variables for OTA versus full check . : param ota : The starting version if OTA , None if not . Default is None . : type ota : str <code> def tcl_prep_otaver(ota=None): ","#LINE# #TAB# if ota is not None: #LINE# #TAB# #TAB# mode = 2 #LINE# #TAB# #TAB# fvver = ota #LINE# #TAB# else: #LINE# #TAB# #TAB# mode = 4 #LINE# #TAB# #TAB# fvver = 'AAA000' #LINE# #TAB# return mode, fvver"
"Takes a text and generates a list of possible entities : arg text : the text to look at : returns : generator where each part ( except the first ) starts with an "" & "" <code> def next_possible_entity(text): ","#LINE# #TAB# for i, part in enumerate(AMP_SPLIT_RE.split(text)): #LINE# #TAB# #TAB# if i == 0: #LINE# #TAB# #TAB# #TAB# yield part #LINE# #TAB# #TAB# elif i % 2 == 0: #LINE# #TAB# #TAB# #TAB# yield '&' + part"
This is only for testing pourposes resets the DepotManager status <code> def _clear(cls): ,#LINE# #TAB# #TAB# cls._default_depot = None #LINE# #TAB# #TAB# cls._depots = {} #LINE# #TAB# #TAB# cls._middleware = None #LINE# #TAB# #TAB# cls._aliases = {}
Ensure that the lock is released  <code> def lock_release(lock): ,#LINE# #TAB# if lock is None: #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# lock.release() #LINE# #TAB# except ThreadError: #LINE# #TAB# #TAB# pass
"Create a target node that marks a configuration field  <code> def create_configfield_ref_target_node(target_id, env, lineno): ","#LINE# #TAB# target_node = nodes.target('', '', ids=[target_id]) #LINE# #TAB# if not hasattr(env, 'lsst_configfields'): #LINE# #TAB# #TAB# env.lsst_configfields = {} #LINE# #TAB# env.lsst_configfields[target_id] = { #LINE# #TAB# #TAB# 'docname': env.docname, #LINE# #TAB# #TAB# 'lineno': lineno, #LINE# #TAB# #TAB# 'target': target_node, #LINE# #TAB# } #LINE# #TAB# return target_node"
Turn an SQLAlchemy model into a dict of field names and values  <code> def object_as_dict(obj): ,"#LINE# #TAB# return {c.key: getattr(obj, c.key) #LINE# #TAB# #TAB# #TAB# for c in inspect(obj).mapper.column_attrs}"
Return true of the function has been marked with <code> def is_transform_generator(fn): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if six.PY2: #LINE# #TAB# #TAB# #TAB# fn.func_dict['is_transform_generator'] = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return fn.__dict__.get('is_transform_generator', False) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False"
Returns True if val is a list ( or enumerable ) of strings . False otherwise <code> def is_list_of_strings(vals): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# for val in vals: #LINE# #TAB# #TAB# #TAB# if not isinstance(val, six.string_types): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Compute y_pred and the derivative of the deviance w.r.t coef  <code> def y_pred_deviance_derivative(coef, X, y, weights, family, link): ","#LINE# #TAB# lin_pred = _safe_lin_pred(X, coef) #LINE# #TAB# y_pred = link.inverse(lin_pred) #LINE# #TAB# d1 = link.inverse_derivative(lin_pred) #LINE# #TAB# temp = d1 * family.deviance_derivative(y, y_pred, weights) #LINE# #TAB# if coef.size == X.shape[1] + 1: #LINE# #TAB# #TAB# devp = np.concatenate(([temp.sum()], temp @ X)) #LINE# #TAB# else: #LINE# #TAB# #TAB# devp = temp @ X #LINE# #TAB# return y_pred, devp"
"Check for assertEqual(type(A ) , B ) sentences SL317 <code> def assert_equal_type(logical_line): ","#LINE# #TAB# if asse_equal_type_re.match(logical_line): #LINE# #TAB# #TAB# yield 0, 'SL317: assertEqual(type(A), B) sentences not allowed'"
"Determine if the address is SPECIAL Note : This is super bad , improve it <code> def is_special(ip_address): ","#LINE# #TAB# special = {'224.0.0.251': 'multicast_dns', 'ff02::fb': 'multicast_dns'} #LINE# #TAB# return special[ip_address] if ip_address in special else False"
Retrieve all regions as strings <code> def get_all_regions(): ,"#LINE# #TAB# conn = boto3.client('ec2', region_name='us-east-1') #LINE# #TAB# return [region['RegionName'] for region in conn.describe_regions()[ #LINE# #TAB# #TAB# 'Regions']]"
"Get a notebook name from a path to a notebook <code> def nb_name_from_path(config, path): ","#LINE# #TAB# if path is None: #LINE# #TAB# #TAB# path = os.getcwd() #LINE# #TAB# root = config.get('root', 'path') #LINE# #TAB# root = os.path.join(path, root) #LINE# #TAB# root = os.path.realpath(root) #LINE# #TAB# default_nb_name = os.path.split(os.path.realpath(root))[1] #LINE# #TAB# nb_name = config.get('misc', 'nb-name', fallback=default_nb_name) #LINE# #TAB# return root, nb_name"
"Truncate string values  <code> def truncate_string(value, max_width=None): ","#LINE# #TAB# if isinstance(value, text_type) and max_width is not None and len(value) > max_width: #LINE# #TAB# #TAB# return value[:max_width] #LINE# #TAB# return value"
"Return the integer value for a given option in a section <code> def config_get_int(section, option, raise_exception=True, default=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return __CONFIG.getint(section, option) #LINE# #TAB# except (ConfigParser.NoOptionError, ConfigParser.NoSectionError) as err: #LINE# #TAB# #TAB# if raise_exception and default is None: #LINE# #TAB# #TAB# #TAB# raise err #LINE# #TAB# #TAB# return default"
"Unpack bytes to bits : param arr : list Byte Stream , as a list of uint8 values Returns ------- bit_arr : list Decomposed bit stream as a list of 0/1s of length ( len(arr ) * 8) <code> def decompose_bytes_to_bit_arr(arr): ",#LINE# #TAB# bit_arr = [] #LINE# #TAB# for idx in range(len(arr)): #LINE# #TAB# #TAB# for i in reversed(range(8)): #LINE# #TAB# #TAB# #TAB# bit_arr.append(arr[idx] >> i & 1 << 0) #LINE# #TAB# return bit_arr
"Return an open mysql db connection using the given credentials . Use retries and sleep to be robust to the occassional transient connection failure  <code> def open_conn(host, db, user, password, retries=0, sleep=0.5): ","#LINE# #TAB# assert retries >= 0 #LINE# #TAB# try: #LINE# #TAB# #TAB# return MySQLdb.connect(host=host, user=user, passwd=password, db=db) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# if retries > 0: #LINE# #TAB# #TAB# #TAB# time.sleep(sleep) #LINE# #TAB# #TAB# #TAB# return open_conn(host, db, user, password, retries - 1, sleep) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise"
Parse example_command to get inputs . Each input stored as element in a dictionary list  <code> def parse_example_command(example_command): ,"#LINE# #TAB# cmd = example_command.replace('\n', ' ') #LINE# #TAB# opts = [i.strip() for i in cmd.split('--')] #LINE# #TAB# opt_dict = {} #LINE# #TAB# for opt in opts: #LINE# #TAB# #TAB# opt = opt.split(' ') #LINE# #TAB# #TAB# if not opt[0] in opt_dict.keys(): #LINE# #TAB# #TAB# #TAB# opt_dict[opt[0]] = [opt[1]] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opt_dict[opt[0]].append(opt[1]) #LINE# #TAB# return opt_dict"
"retrieve a content type from an app_name / model_name combo . Throw Http404 if not a valid setting type <code> def get_content_type_from_url_params(app_name, model_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# content_type = ContentType.objects.get_by_natural_key(app_name, #LINE# #TAB# #TAB# #TAB# model_name) #LINE# #TAB# except ContentType.DoesNotExist: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# if content_type not in registry.content_types: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# return content_type"
"retrieve a content type from an app_name / model_name combo . Throw Http404 if not a valid setting type <code> def get_model_from_url_params(app_name, model_name): ","#LINE# #TAB# model = registry.get_by_natural_key(app_name, model_name) #LINE# #TAB# if model is None: #LINE# #TAB# #TAB# raise Http404 #LINE# #TAB# return model"
Return the Ravello application we 're working in  <code> def get_ravello_application(env): ,"#LINE# #TAB# name = env.config.require('ravello', 'application') #LINE# #TAB# apps = env.client.call('POST', '/applications/filter', ravello. #LINE# #TAB# #TAB# simple_filter(name=name)) #LINE# #TAB# if len(apps) == 0: #LINE# #TAB# #TAB# raise RuntimeError('Application `{}` not found'.format(name)) #LINE# #TAB# app = env.client.call('GET', '/applications/{id}'.format(**apps[0])) #LINE# #TAB# for vm in ravello.get_vms(app): #LINE# #TAB# #TAB# if not vm.get('networkConnections'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# vm['networkConnections'].sort(key=lambda c: c['device']['index']) #LINE# #TAB# return app"
Throws despatchbaysdk exceptions in response to suds generic ' Exception ' exceptions <code> def handle_suds_generic_fault(error): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# exception_info = error.args[0].decode() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# exception_info = error.args[0] #LINE# #TAB# if 401 in exception_info: #LINE# #TAB# #TAB# raise exceptions.AuthorizationException('Invalid API credentials' #LINE# #TAB# #TAB# #TAB# ) from error #LINE# #TAB# if 429 in exception_info: #LINE# #TAB# #TAB# raise exceptions.RateLimitException(exception_info[1]) #LINE# #TAB# raise error
"Generate an arista specific name for this ACL . Use a unique name so that OpenStack created ACLs can be distinguishged from the user created ACLs on Arista HW  <code> def acl_name(name, direction): ",#LINE# #TAB# direction = direction.upper() #LINE# #TAB# return 'SG' + '-' + direction + '-' + name
"Create a new GDAL Dataset in memory Useful for various applications that require a Dataset <code> def mem_ds(res, extent, srs=None, dtype=gdal.GDT_Float32): ","#LINE# #TAB# dst_ns = int((extent[2] - extent[0]) / res + 0.99) #LINE# #TAB# dst_nl = int((extent[3] - extent[1]) / res + 0.99) #LINE# #TAB# m_ds = gdal.GetDriverByName('MEM').Create('', dst_ns, dst_nl, 1, dtype) #LINE# #TAB# m_gt = [extent[0], res, 0, extent[3], 0, -res] #LINE# #TAB# m_ds.SetGeoTransform(m_gt) #LINE# #TAB# if srs is not None: #LINE# #TAB# #TAB# m_ds.SetProjection(srs.ExportToWkt()) #LINE# #TAB# return m_ds"
"Read the first line of the file and extract column names from it . Returns a dictionary with two elements : colnames A list of column names indices A dictionary of column indices , indexed by name <code> def load_column_names(filename): ","#LINE# #TAB# f = open(filename, 'r') #LINE# #TAB# line = f.readline() #LINE# #TAB# f.close() #LINE# #TAB# names = line.replace('#', '').replace(' ', '').replace('\n', '').split(',') #LINE# #TAB# indices = {} #LINE# #TAB# for i in range(0, len(names)): #LINE# #TAB# #TAB# indices[names[i]] = i #LINE# #TAB# return {'colnames': names, 'indices': indices}"
"Takes data generated by a map of eigensystem calls and returns the eigenvalue and eigenstate tables Parameters ---------- esys_mapdata : list of tuple of ndarray Returns ------- ( ndarray , ndarray ) eigenvalues and eigenvectors <code> def recast_esys_mapdata(esys_mapdata): ","#LINE# #TAB# paramvals_count = len(esys_mapdata) #LINE# #TAB# eigenenergy_table = np.asarray([esys_mapdata[index][0] for index in #LINE# #TAB# #TAB# range(paramvals_count)]) #LINE# #TAB# eigenstate_table = [esys_mapdata[index][1] for index in range( #LINE# #TAB# #TAB# paramvals_count)] #LINE# #TAB# return eigenenergy_table, eigenstate_table"
Retrieves the grains from the network device if not cached already  <code> def retrieve_grains_cache(proxy=None): ,"#LINE# #TAB# global GRAINS_CACHE #LINE# #TAB# if not GRAINS_CACHE: #LINE# #TAB# #TAB# if proxy and salt.utils.napalm.is_proxy(__opts__): #LINE# #TAB# #TAB# #TAB# GRAINS_CACHE = proxy['napalm.get_grains']() #LINE# #TAB# #TAB# elif not proxy and salt.utils.napalm.is_minion(__opts__): #LINE# #TAB# #TAB# #TAB# GRAINS_CACHE = salt.utils.napalm.call( #LINE# #TAB# #TAB# #TAB# #TAB# DEVICE_CACHE, #LINE# #TAB# #TAB# #TAB# #TAB# 'get_facts', #LINE# #TAB# #TAB# #TAB# #TAB# **{} #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return GRAINS_CACHE"
Check if given regex is of type ECMA 262 or not . : rtype : bool <code> def is_ecma_regex(regex): ,"#LINE# #TAB# parts = regex.split('/') #LINE# #TAB# if len(parts) == 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(parts) < 3: #LINE# #TAB# #TAB# raise ValueError(""Given regex isn't ECMA regex nor Python regex."") #LINE# #TAB# parts.pop() #LINE# #TAB# parts.append('') #LINE# #TAB# raw_regex = '/'.join(parts) #LINE# #TAB# if raw_regex.startswith('/') and raw_regex.endswith('/'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"type parser for user defined and builtin classes . Parameters ---------- arg : class to parse Returns ------- parsed : OrderedDict('value ' , ' signature ' , ' fullargspec ' , ' isbuiltin ' , ' inheritance_tree ) <code> def class_parser(arg): ",#LINE# #TAB# parsed = function_parser(arg) #LINE# #TAB# parsed['inheritance_tree'] = inspect.getmro(arg) #LINE# #TAB# return parsed
Compose the absolute path of a template  <code> def resolve_template(template): ,#LINE# #TAB# if template.startswith('~') or template.startswith(os.sep): #LINE# #TAB# #TAB# pass #LINE# #TAB# elif os.sep in template: #LINE# #TAB# #TAB# template = Path.cwd() / template #LINE# #TAB# else: #LINE# #TAB# #TAB# template = Path(__file__).parent / 'templates' / template #LINE# #TAB# if not template.exists(): #LINE# #TAB# #TAB# raise AssertionError(f'Inexisting template {template}') #LINE# #TAB# return template
"Build a new ReviewConfig object using the ini config file and the defaults if they exist in the app_config <code> def build_review_config(ini_config, app_config=None): ",#LINE# #TAB# config = ReviewConfig(app_config) #LINE# #TAB# if app_config: #LINE# #TAB# #TAB# defaults = get_lintrc_defaults(app_config) #LINE# #TAB# #TAB# if defaults: #LINE# #TAB# #TAB# #TAB# config.load_ini(defaults) #LINE# #TAB# config.load_ini(ini_config) #LINE# #TAB# return config
Returns a list of assset types for a multiple choice dropdown <code> def asset_type_list(): ,"#LINE# #TAB# type_list = DAOrderedDict() #LINE# #TAB# type_list.auto_gather = False #LINE# #TAB# type_list.gathered = True #LINE# #TAB# type_list.elements.update([('savings', 'Savings Account'), ('cd', #LINE# #TAB# #TAB# 'Certificate of Deposit'), ('ira', 'Individual Retirement Account'), #LINE# #TAB# #TAB# ('mutual fund', 'Money or Mutual Fund'), ('stocks', #LINE# #TAB# #TAB# 'Stocks or Bonds'), ('trust', 'Trust Fund'), ('checking', #LINE# #TAB# #TAB# 'Checking Account'), ('vehicle', 'Vehicle'), ('real estate', #LINE# #TAB# #TAB# 'Real Estate'), ('other', 'Other Asset')]) #LINE# #TAB# return type_list"
"Deregisters a decompressor for a specific compression method . Args : decompressor ( type ) : decompressor class . Raises : KeyError : if the corresponding decompressor is not set  <code> def deregister_decompressor(cls, decompressor): ",#LINE# #TAB# compression_method = decompressor.COMPRESSION_METHOD.lower() #LINE# #TAB# if compression_method not in cls._decompressors: #LINE# #TAB# #TAB# raise KeyError('Decompressor for compression method: {0:s} not set.' #LINE# #TAB# #TAB# #TAB# .format(decompressor.COMPRESSION_METHOD)) #LINE# #TAB# del cls._decompressors[compression_method]
Get the checksum for the Google data fetched  <code> def get_checksum(data): ,#LINE# #TAB# checksum = None #LINE# #TAB# try: #LINE# #TAB# #TAB# data_str = json.dumps(data) #LINE# #TAB# #TAB# checksum = hashlib.md5(data_str.encode('utf-8')).hexdigest() #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return checksum
"Take x dictionary and insert / overwrite y dictionary values : type x : dict : type y : dict : return : <code> def merge_dicts(x: dict, y: dict) ->dict: ",#LINE# #TAB# z = x.copy() #LINE# #TAB# z.update(y) #LINE# #TAB# return z
"Return None if cache entry does not exist  <code> def cache_get_last_in_slice(url_dict, start_int, total_int, authn_subj_list): ","#LINE# #TAB# key_str = _gen_cache_key_for_slice(url_dict, start_int, total_int, authn_subj_list) #LINE# #TAB# try: #LINE# #TAB# #TAB# last_ts_tup = django.core.cache.cache.get(key_str) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# last_ts_tup = None #LINE# #TAB# logging.debug('Cache get. key=""{}"" -> last_ts_tup={}'.format(key_str, last_ts_tup)) #LINE# #TAB# return last_ts_tup"
"c - a - a ' : sodium chloride nitrate [ PK74 ]  <code> def psi_na_cl_no3_pk74(T, P): ","#LINE# #TAB# psi = -0.006 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
Transforms the code object into a new code object <code> def transform_bytecode(code_object): ,#LINE# #TAB# new_code_object = create_new_co(code_object) #LINE# #TAB# return new_code_object
"Helper function to see if given name has any of the patterns in given matches <code> def name_matches(name, matches): ",#LINE# #TAB# for m in matches: #LINE# #TAB# #TAB# if name.endswith(m): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if name.lower().endswith('_' + m.lower()): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if name.lower() == m.lower(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"Update steep down areas ( SDAs ) using the new maximum in between ( mib ) value , and the given complement of xi , i.e. ` ` 1 - xi ` `  <code> def update_filter_sdas(sdas, mib, xi_complement, reachability_plot): ","#LINE# #TAB# if np.isinf(mib): #LINE# #TAB# #TAB# return [] #LINE# #TAB# res = [sda for sda in sdas if mib <= reachability_plot[sda['start']] * #LINE# #TAB# #TAB# xi_complement] #LINE# #TAB# for sda in res: #LINE# #TAB# #TAB# sda['mib'] = max(sda['mib'], mib) #LINE# #TAB# return res"
"Run commands in separate processes  <code> def run_commands(env, work_dir, commands, process_start, process_finish): ","#LINE# #TAB# code = 0 #LINE# #TAB# errors = None #LINE# #TAB# for cmd in commands: #LINE# #TAB# #TAB# print('Running command:', cmd) #LINE# #TAB# #TAB# code, errors = run_process(cmd, process_start=process_start, #LINE# #TAB# #TAB# #TAB# process_finish=process_finish, shell=True, env=env, cwd=work_dir) #LINE# #TAB# #TAB# if code: #LINE# #TAB# #TAB# #TAB# return code, errors #LINE# #TAB# return code, errors"
"Return a valid differential class from ` ` value ` ` or raise exception . As originally created , this is only used in the SkyCoord initializer , so if that is refactored , this function my no longer be necessary  <code> def get_diff_cls(value): ","#LINE# #TAB# if value in r.DIFFERENTIAL_CLASSES: #LINE# #TAB# #TAB# value = r.DIFFERENTIAL_CLASSES[value] #LINE# #TAB# elif not isinstance(value, type) or not issubclass(value, r. #LINE# #TAB# #TAB# BaseDifferential): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Differential is {!r} but must be a BaseDifferential class or one of the string aliases {}' #LINE# #TAB# #TAB# #TAB# .format(value, list(r.DIFFERENTIAL_CLASSES))) #LINE# #TAB# return value"
"load input file and returns it , in read - only mode <code> def load_input(name): ","#LINE# #TAB# if debug > 0: #LINE# #TAB# #TAB# print('reading', name) #LINE# #TAB# hf = HDF5File(name, 'r') #LINE# #TAB# d0 = hf.load() #LINE# #TAB# return d0"
"Given a generator which yields an iterable , get an element  <code> def pick_one(gen: GeneratorType, strategy='random'): ",#LINE# #TAB# if 'rand' in strategy.lower(): #LINE# #TAB# #TAB# return random.choice(next(gen)) #LINE# #TAB# return next(gen)[0]
"Extract paragraphs from Wikipedia . Parameters ---------- section : str min_paragraph_length : int , optional ( default : 140 ) Returns ------- paragraphs : list <code> def extract_paragraphs(section, min_paragraph_length=140): ",#LINE# #TAB# paragraphs = [] #LINE# #TAB# for paragraph in section.split('\n'): #LINE# #TAB# #TAB# paragraph = normalize_data(paragraph) #LINE# #TAB# #TAB# if len(paragraph) < min_paragraph_length: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# is_math = '\\displaystyle' in paragraph #LINE# #TAB# #TAB# if is_literature(paragraph) or is_math: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# paragraphs.append(paragraph) #LINE# #TAB# return paragraphs
"Convert an unsigned integer to a numpy binary array with the first element the MSB and the last element the LSB  <code> def to_bin(data, width): ",#LINE# #TAB# data_str = bin(data & (2**width-1))[2:].zfill(width) #LINE# #TAB# return [int(x) for x in tuple(data_str)]
"If contents of ` field ` is not a list of n doubles , make ` field ` RED with tooltip caption ` message ` & return 1 . Otherwise return 0  <code> def is_list_of_n_doubles(field, num, message): ","#LINE# #TAB# field.setText(str(field.text()).replace(',', ' ')) #LINE# #TAB# try: #LINE# #TAB# #TAB# if len(str(field.text()).split()) != num: #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# #TAB# for txt in str(field.text()).split(): #LINE# #TAB# #TAB# #TAB# float(txt) #LINE# #TAB# #TAB# _show_consistency(field, message, True) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# _show_consistency(field, message, False) #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"Return transversals for the group with BSGS base , gens <code> def get_transversals(base, gens): ","#LINE# #TAB# if not base: #LINE# #TAB# #TAB# return [] #LINE# #TAB# stabs = _distribute_gens_by_base(base, gens) #LINE# #TAB# orbits, transversals = _orbits_transversals_from_bsgs(base, stabs) #LINE# #TAB# transversals = [{x: h._array_form for x, h in y.items()} for y in #LINE# #TAB# #TAB# transversals] #LINE# #TAB# return transversals"
"Import a file with full path specification . Allows one to import from anywhere , something _ _ import _ _ does not do  <code> def import_path(fullpath): ","#LINE# #TAB# if PY33: #LINE# #TAB# #TAB# name = os.path.splitext(os.path.basename(fullpath))[0] #LINE# #TAB# #TAB# return machinery.SourceFileLoader(name, fullpath).load_module(name) #LINE# #TAB# else: #LINE# #TAB# #TAB# path, filename = os.path.split(fullpath) #LINE# #TAB# #TAB# filename, ext = os.path.splitext(filename) #LINE# #TAB# #TAB# sys.path.append(path) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module = __import__(filename) #LINE# #TAB# #TAB# #TAB# reload(module) #LINE# #TAB# #TAB# #TAB# return module #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# del sys.path[-1]"
"Fix to avoid packages include in slackbuilds folder <code> def alien_filter(name, location, size, unsize): ","#LINE# #TAB# (fname, flocation, fsize, funsize) = ([] for i in range(4)) #LINE# #TAB# for n, l, s, u in zip(name, location, size, unsize): #LINE# #TAB# #TAB# if ""slackbuilds"" != l: #LINE# #TAB# #TAB# #TAB# fname.append(n) #LINE# #TAB# #TAB# #TAB# flocation.append(l) #LINE# #TAB# #TAB# #TAB# fsize.append(s) #LINE# #TAB# #TAB# #TAB# funsize.append(u) #LINE# #TAB# return [fname, flocation, fsize, funsize]"
"Rebuild the task and the sequence from a config file  <code> def build_from_config(cls, config, dependencies): ","#LINE# #TAB# builder = cls.mro()[1].build_from_config.__func__ #LINE# #TAB# task = builder(cls, config, dependencies) #LINE# #TAB# if 'sequence' in config: #LINE# #TAB# #TAB# pulse_dep = dependencies['exopy.pulses.item'] #LINE# #TAB# #TAB# builder = pulse_dep['exopy_pulses.RootSequence'] #LINE# #TAB# #TAB# conf = config['sequence'] #LINE# #TAB# #TAB# seq = builder.build_from_config(conf, dependencies) #LINE# #TAB# #TAB# task.sequence = seq #LINE# #TAB# return task"
"Check if two module models may be considered the same <code> def models_compatible(model_a: ModuleModel, model_b: ModuleModel) ->bool: ","#LINE# #TAB# if model_a == model_b: #LINE# #TAB# #TAB# return True #LINE# #TAB# return model_b.value in _load_module_definition(V2_MODULE_DEF_VERSION, #LINE# #TAB# #TAB# model_a)['compatibleWith']"
"Archive given files : param files : list of file names : param zip_filename : target zip filename : param base_path : base path for files : return : <code> def archive_files(files, zip_filename, base_path=''): ","#LINE# #TAB# zip_file = zipfile.ZipFile(zip_filename, 'w') #LINE# #TAB# for filename in files: #LINE# #TAB# #TAB# zip_file.write(os.path.join(base_path, filename), filename) #LINE# #TAB# zip_file.close() #LINE# #TAB# return zip_filename"
Unmount VirtualBox Guest Additions CD from the temp directory  <code> def additions_umount(mount_point): ,#LINE# #TAB# ret = __salt__['mount.umount'](mount_point) #LINE# #TAB# if ret: #LINE# #TAB# #TAB# os.rmdir(mount_point) #LINE# #TAB# return ret
"Generate RENEWING time  <code> def gen_renewing_time(lease_time, elapsed=0): ","#LINE# #TAB# renewing_time = int(lease_time) * RENEW_PERC - elapsed #LINE# #TAB# range_fuzz = int(lease_time) * REBIND_PERC - renewing_time #LINE# #TAB# logger.debug('rebinding fuzz range %s', range_fuzz) #LINE# #TAB# fuzz = random.uniform(-(range_fuzz), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# +(range_fuzz)) #LINE# #TAB# renewing_time += fuzz #LINE# #TAB# logger.debug('Renewing time %s.', renewing_time) #LINE# #TAB# return renewing_time"
"Convert : mod:`datetime ` to the Gregorian date as UTC float days , preserving hours , minutes , seconds and microseconds . Return value is a : func:`float `  <code> def dt_to_float_ordinal(dt): ","#LINE# #TAB# if isinstance(dt, (np.ndarray, Index, ABCSeries) #LINE# #TAB# #TAB# ) and is_datetime64_ns_dtype(dt): #LINE# #TAB# #TAB# base = dates.epoch2num(dt.asi8 / 1000000000.0) #LINE# #TAB# else: #LINE# #TAB# #TAB# base = dates.date2num(dt) #LINE# #TAB# return base"
"Set node value ( create a child if necessary ) <code> def set_node(node, value): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# node.firstChild.nodeValue = value #LINE# #TAB# except: #LINE# #TAB# #TAB# createNode(node, value) #LINE# #TAB# return value"
Get a valid semantic version for tag <code> def semantic_version(tag): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# version = list(map(int, tag.split('.'))) #LINE# #TAB# #TAB# assert len(version) == 3 #LINE# #TAB# #TAB# return tuple(version) #LINE# #TAB# except Exception as exc: #LINE# #TAB# #TAB# raise AgileError( #LINE# #TAB# #TAB# #TAB# 'Could not parse ""%s"", please use MAJOR.MINOR.PATCH' % tag #LINE# #TAB# #TAB# #TAB# ) from exc"
Here we clean the queryset <code> def query_clean(query): ,#LINE# #TAB# query += ';' #LINE# #TAB# return query
"A generator which iterates over the vertices in Reverse - Cuthill - McKee order  <code> def rcm_vertex_order(vertices_resources, nets): ","#LINE# #TAB# vertices_neighbours = _get_vertices_neighbours(nets) #LINE# #TAB# for subgraph_vertices in _get_connected_subgraphs(vertices_resources, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# vertices_neighbours): #LINE# #TAB# #TAB# cm_order = _cuthill_mckee(subgraph_vertices, vertices_neighbours) #LINE# #TAB# #TAB# for vertex in reversed(cm_order): #LINE# #TAB# #TAB# #TAB# yield vertex"
Return a list of dictionaries from a result - set  <code> def as_dicts(cursor): ,"#LINE# #TAB# fields = [k[0] for k in cursor.description] #LINE# #TAB# result = [] #LINE# #TAB# rows = cursor.fetchall() #LINE# #TAB# for row in rows: #LINE# #TAB# #TAB# result.append(dict(zip(fields, row))) #LINE# #TAB# return result"
Create the yaml parser object with this factory method  <code> def get_yaml_parser_roundtrip(): ,"#LINE# #TAB# yaml_writer = yamler.YAML(typ='rt', pure=True) #LINE# #TAB# yaml_writer.indent(mapping=2, sequence=4, offset=2) #LINE# #TAB# return yaml_writer"
"Return all of the requirements of ` dist ` that are n't present in ` installed_dists `  <code> def get_missing_reqs(dist, installed_dists): ",#LINE# #TAB# installed_names = set(d.project_name.lower() for d in installed_dists) #LINE# #TAB# missing_requirements = set() #LINE# #TAB# for requirement in dist.requires(): #LINE# #TAB# #TAB# if requirement.project_name.lower() not in installed_names: #LINE# #TAB# #TAB# #TAB# missing_requirements.add(requirement) #LINE# #TAB# #TAB# #TAB# yield requirement
"Removes the suffix from the string if it exists , and returns the result  <code> def remove_suffix(text: str, suffix: str) ->str: ",#LINE# #TAB# if suffix != '' and text.endswith(suffix): #LINE# #TAB# #TAB# return text[:-len(suffix)] #LINE# #TAB# return text
"Return an RGB tuple converted from an XYZ tuple , along with a bool indicating whether clipping occured  <code> def rgb_from_xyz(xyz): ","#LINE# #TAB# x, xl = _lim(0, 3.240479 * xyz[0] - 1.53715 * xyz[1] - 0.498535 * xyz[2], 1 #LINE# #TAB# #TAB# ) #LINE# #TAB# y, yl = _lim(0, -0.969256 * xyz[0] + 1.875992 * xyz[1] + 0.041556 * xyz #LINE# #TAB# #TAB# [2], 1) #LINE# #TAB# z, zl = _lim(0, 0.055648 * xyz[0] - 0.204043 * xyz[1] + 1.057311 * xyz[ #LINE# #TAB# #TAB# 2], 1) #LINE# #TAB# return tuple([x, y, z]), xl or yl or zl"
"given a 28 bit int , returns a 32 bit value with sync - safe bits added may raise ValueError is the value is negative or larger than 28 bits <code> def encode_syncsafe32(i): ",#LINE# #TAB# if i >= 2 ** 28: #LINE# #TAB# #TAB# raise ValueError('value too large') #LINE# #TAB# elif i < 0: #LINE# #TAB# #TAB# raise ValueError('value cannot be negative') #LINE# #TAB# value = 0 #LINE# #TAB# for x in range(4): #LINE# #TAB# #TAB# value |= (i & 127) << x * 8 #LINE# #TAB# #TAB# i >>= 7 #LINE# #TAB# return value
Read IP address allocations from the allocation file  <code> def read_allocations(module): ,"#LINE# #TAB# filename = module.params['allocation_file'] #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# #TAB# content = yaml.safe_load(f) #LINE# #TAB# except IOError as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# return {} #LINE# #TAB# #TAB# module.fail_json(msg= #LINE# #TAB# #TAB# #TAB# 'Failed to open allocation file %s for reading' % filename) #LINE# #TAB# except yaml.YAMLError as e: #LINE# #TAB# #TAB# module.fail_json(msg='Failed to parse allocation file %s as YAML' % #LINE# #TAB# #TAB# #TAB# filename) #LINE# #TAB# if content is None: #LINE# #TAB# #TAB# content = {} #LINE# #TAB# return content"
Return DDP socket object  <code> def get_socket(timeout=3): ,"#LINE# #TAB# sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# sock.settimeout(timeout) #LINE# #TAB# sock.bind((UDP_IP, UDP_PORT)) #LINE# #TAB# return sock"
"Return a list containing tuples of e.g. ( ' _ _ init _ _ ' , ' example / import_test_project/__init__.py ' ) <code> def get_directory_modules(directory, flush_local_modules=False): ","#LINE# #TAB# if local_modules and os.path.dirname(local_modules[0][1]) == directory: #LINE# #TAB# #TAB# return local_modules #LINE# #TAB# if flush_local_modules: #LINE# #TAB# #TAB# del local_modules[:] #LINE# #TAB# if not os.path.isdir(directory): #LINE# #TAB# #TAB# directory = os.path.dirname(directory) #LINE# #TAB# if directory == '': #LINE# #TAB# #TAB# return local_modules #LINE# #TAB# for path in os.listdir(directory): #LINE# #TAB# #TAB# if is_python_file(path): #LINE# #TAB# #TAB# #TAB# module_name = os.path.splitext(path)[0] #LINE# #TAB# #TAB# #TAB# local_modules.append((module_name, os.path.join(directory, path))) #LINE# #TAB# return local_modules"
"Parse a string to command name and arguments . : param comm : A string consisting 1 or more words separated by spaces and an optional json as the last word : return : 2-tuple : command name , args list <code> def parse_command(comm): ","#LINE# #TAB# if ' ' not in comm: #LINE# #TAB# #TAB# return comm, [] #LINE# #TAB# else: #LINE# #TAB# #TAB# words = comm.split(' ') #LINE# #TAB# #TAB# words = [w for w in words if w is not ''] #LINE# #TAB# #TAB# method_name = words.pop(0) #LINE# #TAB# #TAB# return method_name, words"
"Returns the out shaping policy of a distributed virtual portgroup <code> def get_dvportgroup_out_shaping(pg_name, pg_default_port_config): ","#LINE# #TAB# log.trace('Retrieving portgroup\'s \'%s\' out shaping config', pg_name) #LINE# #TAB# out_shaping_policy = pg_default_port_config.outShapingPolicy #LINE# #TAB# if not out_shaping_policy: #LINE# #TAB# #TAB# return {} #LINE# #TAB# return {'average_bandwidth': out_shaping_policy.averageBandwidth.value, #LINE# #TAB# #TAB# #TAB# 'burst_size': out_shaping_policy.burstSize.value, #LINE# #TAB# #TAB# #TAB# 'enabled': out_shaping_policy.enabled.value, #LINE# #TAB# #TAB# #TAB# 'peak_bandwidth': out_shaping_policy.peakBandwidth.value}"
"Given a url with authentication components , extract them into a tuple of username , password . : rtype : ( str , str ) <code> def get_auth_from_url(url): ","#LINE# #TAB# parsed = urlparse(url) #LINE# #TAB# try: #LINE# #TAB# #TAB# auth = unquote(parsed.username), unquote(parsed.password) #LINE# #TAB# except (AttributeError, TypeError): #LINE# #TAB# #TAB# auth = '', '' #LINE# #TAB# return auth"
"Returns tuple of ( num_of_matches , array_of_matches ) arranged highest confidence match first . : param filename : path to file : return : list of possible matches , highest confidence first <code> def magic_file(filename): ","#LINE# #TAB# head, foot = _file_details(filename) #LINE# #TAB# if not head: #LINE# #TAB# #TAB# raise ValueError('Input was empty') #LINE# #TAB# try: #LINE# #TAB# #TAB# info = _identify_all(head, foot, ext_from_filename(filename)) #LINE# #TAB# except PureError: #LINE# #TAB# #TAB# info = [] #LINE# #TAB# info.sort(key=lambda x: x.confidence, reverse=True) #LINE# #TAB# return info"
"A cycled latin square permutation of elements . If elements is a integer the elements=[0 , ... , elements ] is used . Parameters ---------- elements : int or list list of elements or a number <code> def cycled_latin_square(elements): ","#LINE# #TAB# if isinstance(elements, (tuple, list)): #LINE# #TAB# #TAB# idx = cycled_latin_square(len(elements)) #LINE# #TAB# #TAB# square = _square_of_elements(elements, idx) #LINE# #TAB# else: #LINE# #TAB# #TAB# square = [list(range(0, elements))] #LINE# #TAB# #TAB# for r in range(0, elements - 1): #LINE# #TAB# #TAB# #TAB# square.append(_cycle_list(square[r])) #LINE# #TAB# return square"
Returns atom section from mol2 provided as list of strings <code> def _get_atomsection(mol2_lst): ,"#LINE# #TAB# #TAB# started = False #LINE# #TAB# #TAB# for idx, s in enumerate(mol2_lst): #LINE# #TAB# #TAB# #TAB# if s.startswith('@<TRIPOS>ATOM'): #LINE# #TAB# #TAB# #TAB# #TAB# first_idx = idx + 1 #LINE# #TAB# #TAB# #TAB# #TAB# started = True #LINE# #TAB# #TAB# #TAB# elif started and s.startswith('@<TRIPOS>'): #LINE# #TAB# #TAB# #TAB# #TAB# last_idx_plus1 = idx #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# return mol2_lst[first_idx:last_idx_plus1]"
"We do a pre - order traversal of the tree to calculate the text and tail of each node <code> def set_tree_text(dictnode_tree, src): ","#LINE# #TAB# tree = dictnode_tree #LINE# #TAB# stack = [tree] #LINE# #TAB# while stack: #LINE# #TAB# #TAB# node = stack.pop() #LINE# #TAB# #TAB# set_node_text(node, src) #LINE# #TAB# #TAB# stack.extend(reversed(node['children'])) #LINE# #TAB# assert not tree.get('tail') #LINE# #TAB# tree['tail'] = None #LINE# #TAB# return tree"
"Return new returning based on input and context  <code> def merge_returning(options: dict, base_context: Context) ->ReturnType: ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return options['returning'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return base_context.returning if base_context else ReturnType.Records
"Transform data frame to list of dict  <code> def to_index_row_dict(df, index_col=None, use_ordered_dict=True): ","#LINE# #TAB# if index_col: #LINE# #TAB# #TAB# index_list = df[index_col] #LINE# #TAB# else: #LINE# #TAB# #TAB# index_list = df.index #LINE# #TAB# columns = df.columns #LINE# #TAB# if use_ordered_dict: #LINE# #TAB# #TAB# table = OrderedDict() #LINE# #TAB# else: #LINE# #TAB# #TAB# table = dict() #LINE# #TAB# for ind, tp in zip(index_list, itertuple(df)): #LINE# #TAB# #TAB# table[ind] = dict(zip(columns, tp)) #LINE# #TAB# return table"
"Offset time ( in milliseconds ) from a datetime.datetime object to now  <code> def milliseconds_offset(cls, timestamp, now=None): ","#LINE# #TAB# if isinstance(timestamp, (int, float)): #LINE# #TAB# #TAB# base = timestamp #LINE# #TAB# else: #LINE# #TAB# #TAB# base = cls.to_unix(timestamp) #LINE# #TAB# if now is None: #LINE# #TAB# #TAB# now = time.time() #LINE# #TAB# return (now - base) * 1000"
"Create dot graph representations  <code> def generate_dot(nicknames, relations, name, format, program, directed=False): ","#LINE# #TAB# dot_attrs = {'name': name, 'format': format, 'engine': program} #LINE# #TAB# if directed: #LINE# #TAB# #TAB# dot = Digraph(**dot_attrs) #LINE# #TAB# else: #LINE# #TAB# #TAB# dot = Graph(**dot_attrs) #LINE# #TAB# for nickname in nicknames: #LINE# #TAB# #TAB# dot.node(nickname, label=nickname) #LINE# #TAB# max_count = float(max(rel[2] for rel in relations)) #LINE# #TAB# max_width = 4 #LINE# #TAB# for nickname1, nickname2, count in sorted(relations, key=lambda x: x[0]): #LINE# #TAB# #TAB# width = count / max_count * max_width + 1 #LINE# #TAB# #TAB# dot.edge(nickname1, nickname2, style='setlinewidth(%d)' % width) #LINE# #TAB# return dot"
Remove columns ending with I from a pandas . DataFrame Parameters ---------- df : dataFrame Returns ------- None <code> def remove_i_columns(df): ,"#LINE# #TAB# all_columns = list(filter(lambda el: el[-2:] == '_I', df.columns)) #LINE# #TAB# for column in all_columns: #LINE# #TAB# #TAB# del df[column]"
Recursively remove keys with a value of ` ` None ` ` from the ` ` in_dict ` ` collection <code> def remove_nones(in_dict): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return dict((key, remove_nones(val)) for key, val in six.iteritems( #LINE# #TAB# #TAB# #TAB# in_dict) if val is not None) #LINE# #TAB# except (ValueError, AttributeError): #LINE# #TAB# #TAB# return in_dict"
Create a handler for logging purposes . : param debug : Does debug information need to be logged ? : return : The logging handler  <code> def create_logging_handler(debug: bool) ->logging.Handler: ,#LINE# #TAB# ch = logging.StreamHandler(stream=sys.stderr) #LINE# #TAB# ch.setLevel(logging.DEBUG if debug else logging.INFO) #LINE# #TAB# ch.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# '\n%(asctime)s - %(name)s - %(levelname)s - %(message)s')) #LINE# #TAB# return ch
"Internal method to help drop elements from a dict . : param d : : param skip : : return : <code> def drop_from_dict(d: dict, skip: List[object]) ->dict: ","#LINE# #TAB# if d is None: #LINE# #TAB# #TAB# d = None #LINE# #TAB# if isinstance(d, list): #LINE# #TAB# #TAB# return [drop_from_dict(e, skip) for e in d] #LINE# #TAB# if isinstance(d, dict): #LINE# #TAB# #TAB# return {k: drop_from_dict(v, skip) for k, v in d.items() if k not in #LINE# #TAB# #TAB# #TAB# skip} #LINE# #TAB# return d"
"Load all frontend modules specified in the config <code> def load_frontends(config, callback, internal_attributes): ","#LINE# #TAB# frontend_modules = _load_plugins(config.get(""CUSTOM_PLUGIN_MODULE_PATHS""), config[""FRONTEND_MODULES""], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# frontend_filter, config[""BASE""], internal_attributes, callback) #LINE# #TAB# logger.info(""Setup frontends: %s"" % [frontend.name for frontend in frontend_modules]) #LINE# #TAB# return frontend_modules"
Return data about a function source including file name line number and source code  <code> def func_source_data(func): ,"#LINE# #TAB# filename = inspect.getsourcefile(func) #LINE# #TAB# lineno = inspect.getsourcelines(func)[1] #LINE# #TAB# source = inspect.getsource(func) #LINE# #TAB# return filename, lineno, source"
Check if a Turi create model is pickle safe  <code> def is_not_pickle_safe_gl_model_class(obj_class): ,"#LINE# #TAB# if issubclass(obj_class, _toolkits._model.CustomModel): #LINE# #TAB# #TAB# return not obj_class._is_gl_pickle_safe() #LINE# #TAB# return False"
"get list of models . DbReference from XML node entry <code> def get_db_references(cls, entry): ","#LINE# #TAB# #TAB# db_refs = [] #LINE# #TAB# #TAB# for db_ref in entry.iterfind(""./dbReference""): #LINE# #TAB# #TAB# #TAB# db_ref_dict = {'identifier': db_ref.attrib['id'], 'type_': db_ref.attrib['type']} #LINE# #TAB# #TAB# #TAB# db_refs.append(models.DbReference(**db_ref_dict)) #LINE# #TAB# #TAB# return db_refs"
Averages the per - epoch metrics from multiple executions  <code> def average_histories(histories): ,"#LINE# #TAB# averaged = {} #LINE# #TAB# metrics = histories[0].keys() #LINE# #TAB# for metric in metrics: #LINE# #TAB# #TAB# values = [] #LINE# #TAB# #TAB# for epoch_values in six.moves.zip_longest(*[h[metric] for h in #LINE# #TAB# #TAB# #TAB# histories], fillvalue=np.nan): #LINE# #TAB# #TAB# #TAB# values.append(np.nanmean(epoch_values)) #LINE# #TAB# #TAB# averaged[metric] = values #LINE# #TAB# averaged = [dict(zip(metrics, vals)) for vals in zip(*averaged.values())] #LINE# #TAB# return averaged"
Build a string from the current error code + error report structure  <code> def yices_error_string(): ,"#LINE# #TAB# cstrptr = libyices.yices_error_string() #LINE# #TAB# errstr = bytes2str(cast(cstrptr, c_char_p).value) #LINE# #TAB# libyices.yices_free_string(cstrptr) #LINE# #TAB# return errstr"
"Recursively collect the files contained inside dirpath  <code> def collect_files(dirpath, cond=lambda fullname: True): ","#LINE# #TAB# files = [] #LINE# #TAB# for fname in os.listdir(dirpath): #LINE# #TAB# #TAB# fullname = os.path.join(dirpath, fname) #LINE# #TAB# #TAB# if os.path.isdir(fullname): #LINE# #TAB# #TAB# #TAB# files.extend(collect_files(fullname)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if cond(fullname): #LINE# #TAB# #TAB# #TAB# #TAB# files.append(fullname) #LINE# #TAB# return files"
ZigZag Transform : Encodes signed integers so that they can be effectively used with varint encoding . See wire_format.h for more details  <code> def zig_zag_encode(value): ,#LINE# #TAB# if value >= 0: #LINE# #TAB# #TAB# return value << 1 #LINE# #TAB# return value << 1 ^ ~0 | 1
"Read the control file in JSON format and extract the blocks of simulation , general parameters , clustering and spawning : param jsonParams : Control file in JSON format from where the parameters will be read : type jsonParams : json str <code> def load_params(jsonParams): ","#LINE# #TAB# jsonFile = open(jsonParams, 'r').read() #LINE# #TAB# parsedJSON = json.loads(jsonFile) #LINE# #TAB# return parsedJSON[blockNames.ControlFileParams.generalParams], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.spawningBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.simulationBlockname], parsedJSON[ #LINE# #TAB# #TAB# blockNames.ControlFileParams.clusteringBlockname]"
"Get all callable methods of an object that do n't start with underscore returns a list of tuples of the form ( method_name , method ) <code> def methods_of(obj): ","#LINE# #TAB# result = [] #LINE# #TAB# for fn in dir(obj): #LINE# #TAB# #TAB# if callable(getattr(obj, fn)) and not fn.startswith('_'): #LINE# #TAB# #TAB# #TAB# result.append((fn, getattr(obj, fn), getattr(obj, fn + #LINE# #TAB# #TAB# #TAB# #TAB# '_description', None))) #LINE# #TAB# return result"
Return a list of vocabularies that should be skipped when auto output to disk for the specified format  <code> def vocabulary_skip(format='insdc'): ,"#LINE# #TAB# skip = {'type': ('insdc', 'gff3'), 'ID': 'insdc', 'translation': 'gff3', #LINE# #TAB# #TAB# 'strand': 'insdc'} #LINE# #TAB# return [k for k, v in skip.items() if format in v]"
dictorsetmaker_star2 : dictorsetmaker_star2 COMMA test <code> def p_dictorsetmaker_star2_2(p): ,"#LINE# #TAB# keys, values = p[1] #LINE# #TAB# p[0] = keys, values + [p[3]]"
"c - c ' : calcium potassium [ HMW84 ]  <code> def theta_ca_k_hmw84(T, P): ","#LINE# #TAB# theta = 0.032 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
"Validate the match if right character is in a given sequence  <code> def chars_after(chars, match): ",#LINE# #TAB# if match.end >= len(match.input_string): #LINE# #TAB# #TAB# return True #LINE# #TAB# return match.input_string[match.end] in chars
"Wrapper utility that returns a test vpn service  <code> def create_vpnservice(cls, subnet_id, router_id, name=None): ","#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = data_utils.rand_name('vpnservice-') #LINE# #TAB# body = cls.client.create_vpnservice(subnet_id=subnet_id, router_id= #LINE# #TAB# #TAB# router_id, admin_state_up=True, name=name) #LINE# #TAB# vpnservice = body['vpnservice'] #LINE# #TAB# cls.vpnservices.append(vpnservice) #LINE# #TAB# return vpnservice"
Formats output from get_all_rules and returns a table  <code> def get_rule_table(rules): ,"#LINE# #TAB# table = formatting.Table(['Id', 'KeyName'], ""Rules"") #LINE# #TAB# for rule in rules: #LINE# #TAB# #TAB# table.add_row([rule['id'], rule['keyName']]) #LINE# #TAB# return table"
Return a valid ICachedItemMapper . map for schema <code> def schema_map(schema): ,#LINE# #TAB# mapper = {} #LINE# #TAB# for name in getFieldNames(schema): #LINE# #TAB# #TAB# mapper[name] = name #LINE# #TAB# return mapper
Checks whether the given ` uri ` is a * URI * reference : param str uri : the given ` uri ` : return : True if a * URI * reference : rtype : bool <code> def is_uri(uri): ,"#LINE# #TAB# scheme, netloc, path, params, query, fragment = urlparse(uri) #LINE# #TAB# if scheme and netloc and path: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Using the specified input resource assemble a list of rpm URLS  <code> def assemble_remotes(resource): ,"#LINE# #TAB# resource_type = classify_resource_type(resource) #LINE# #TAB# if resource_type is None: #LINE# #TAB# #TAB# juicer.utils.Log.log_debug(""Could not classify or find the input resource."") #LINE# #TAB# #TAB# return [] #LINE# #TAB# elif resource_type == REMOTE_PKG_TYPE: #LINE# #TAB# #TAB# return [resource] #LINE# #TAB# elif resource_type == REMOTE_INDEX_TYPE: #LINE# #TAB# #TAB# return parse_directory_index(resource) #LINE# #TAB# elif resource_type == REMOTE_INPUT_FILE_TYPE: #LINE# #TAB# #TAB# remote_packages, excluded_data = parse_input_file(resource) #LINE# #TAB# #TAB# return remote_packages"
Check if a command is supported : param command : the command to execute : return : boolean <code> def check_command(command): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# call([command]) #LINE# #TAB# except CalledProcessError: #LINE# #TAB# #TAB# return False #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"Fetch first non - none / empty result of applying ` ` apply_func ` `  <code> def fetch_first_result(fget, fset, fdel, apply_func, value_not_found=None): ","#LINE# #TAB# for f in filter(None, (fget, fset, fdel)): #LINE# #TAB# #TAB# result = apply_func(f) #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# return result #LINE# #TAB# return value_not_found"
"Subroutine of ` ` all_intervals_covered ( ) ` ` . Check if column and diagonal strip of x are all white . If so , x can not be covered . Vertex function is not extreme  <code> def white_strip(q, f, x, add_v): ","#LINE# #TAB# for xx in [x, (f - x - 1) % q]: #LINE# #TAB# #TAB# for y in range(1, q): #LINE# #TAB# #TAB# #TAB# for u in [xx, (xx - y) % q]: #LINE# #TAB# #TAB# #TAB# #TAB# if add_v[u, y] and add_v[u + 1, y]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# for y in (list(range(xx)) + list(range(xx + 1, q))): #LINE# #TAB# #TAB# #TAB# if add_v[xx, y + 1] and add_v[xx + 1, y] and (xx + y + 1) % q != f: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"P values for given f values  <code> def ftest_p(f, df_num, df_den): ","#LINE# #TAB# f = np.asanyarray(f) #LINE# #TAB# p = scipy.stats.f.sf(f, df_num, df_den) #LINE# #TAB# return p"
"Decorate each HTTP verb method to check if the request is authenticated <code> def wrap_class(request_handler, validator): ","#LINE# #TAB# METHODS = ['get', 'post', 'put', 'head', 'options', 'delete', 'patch'] #LINE# #TAB# for name in METHODS: #LINE# #TAB# #TAB# method = getattr(request_handler, name) #LINE# #TAB# #TAB# setattr(request_handler, name, _auth_required(method, validator)) #LINE# #TAB# return request_handler"
"Construct a dispatch - usable path from a string , Path instance , or other iterable  <code> def prepare_path(path): ","#LINE# #TAB# if isinstance(path, str): #LINE# #TAB# #TAB# path = PurePosixPath(path) #LINE# #TAB# if isinstance(path, PurePosixPath): #LINE# #TAB# #TAB# path = deque(path.parts[1 if path.root else 0:]) #LINE# #TAB# else: #LINE# #TAB# #TAB# path = deque(path) #LINE# #TAB# return path"
"The approximated Jacobian is <code> def jac_uniform(X, cells): ","#LINE# #TAB# dim = 2 #LINE# #TAB# mesh = MeshTri(X, cells) #LINE# #TAB# jac = numpy.zeros(X.shape) #LINE# #TAB# for k in range(mesh.cells[""nodes""].shape[1]): #LINE# #TAB# #TAB# i = mesh.cells[""nodes""][:, k] #LINE# #TAB# #TAB# fastfunc.add.at( #LINE# #TAB# #TAB# #TAB# jac, #LINE# #TAB# #TAB# #TAB# i, #LINE# #TAB# #TAB# #TAB# ((mesh.node_coords[i] - mesh.cell_barycenters).T * mesh.cell_volumes).T, #LINE# #TAB# #TAB# ) #LINE# #TAB# return 2 / (dim + 1) * jac"
"Triggers when exiting the given testsuite <code> def tsuite_exit(trun, tsuite): ","#LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:tsuite:exit"") #LINE# #TAB# rcode = 0 #LINE# #TAB# for hook in reversed(tsuite[""hooks""][""exit""]):#TAB# #LINE# #TAB# #TAB# rcode = script_run(trun, hook) #LINE# #TAB# #TAB# if rcode: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if trun[""conf""][""VERBOSE""]: #LINE# #TAB# #TAB# cij.emph(""rnr:tsuite:exit { rcode: %r } "" % rcode, rcode) #LINE# #TAB# return rcode"
yield file names of executable files in path <code> def yield_accessible_unix_file_names(path): ,"#LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# return #LINE# #TAB# for file_ in scandir(path): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if file_.is_file() and os.access(file_.path, os.X_OK): #LINE# #TAB# #TAB# #TAB# #TAB# yield file_.name #LINE# #TAB# #TAB# except (FileNotFoundError, NotADirectoryError): #LINE# #TAB# #TAB# #TAB# pass"
"Generate a filesystem path from a namespace name or pid . Generate a filesystem path from a namespace name or pid , and return a filesystem path to the appropriate file . Returns the nspath argument if both nsname and nspid are None  <code> def get_ns_path(nspath=None, nsname=None, nspid=None): ",#LINE# #TAB# if nsname: #LINE# #TAB# #TAB# nspath = '/var/run/netns/%s' % nsname #LINE# #TAB# elif nspid: #LINE# #TAB# #TAB# nspath = '/proc/%d/ns/net' % nspid #LINE# #TAB# if not os.path.exists(nspath): #LINE# #TAB# #TAB# raise ValueError('namespace path %s does not exist' % nspath) #LINE# #TAB# return nspath
"r Groups a list of items by group id  <code> def group_items(items, groupids): ","#LINE# #TAB# if callable(groupids): #LINE# #TAB# #TAB# keyfunc = groupids #LINE# #TAB# #TAB# pair_list = ((keyfunc(item), item) for item in items) #LINE# #TAB# else: #LINE# #TAB# #TAB# pair_list = zip(groupids, items) #LINE# #TAB# groupid_to_items = defaultdict(list) #LINE# #TAB# for key, item in pair_list: #LINE# #TAB# #TAB# groupid_to_items[key].append(item) #LINE# #TAB# return groupid_to_items"
"Return a canonical value for the "" type "" key . Note that this will return [ ] , the empty list , if the value is a list without any allowed type names ; * even though * this is explicitly an invalid value  <code> def get_type(schema: Schema) ->List[str]: ","#LINE# #TAB# type_ = schema.get('type', list(TYPE_STRINGS)) #LINE# #TAB# if isinstance(type_, str): #LINE# #TAB# #TAB# assert type_ in TYPE_STRINGS #LINE# #TAB# #TAB# return [type_] #LINE# #TAB# assert isinstance(type_, list) and set(type_).issubset(TYPE_STRINGS), type_ #LINE# #TAB# return [t for t in TYPE_STRINGS if t in type_]"
Iterate over all child nodes or a node  <code> def iter_child_nodes(node): ,"#LINE# #TAB# for name, field in iter_fields(node): #LINE# #TAB# #TAB# if isinstance(field, AST): #LINE# #TAB# #TAB# #TAB# yield field #LINE# #TAB# #TAB# elif isinstance(field, list): #LINE# #TAB# #TAB# #TAB# for item in field: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(item, AST): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield item"
Check Tendermint compatability with BigchainDB server <code> def tendermint_version_is_compatible(running_tm_ver): ,#LINE# #TAB# tm_ver = running_tm_ver.split('-') #LINE# #TAB# if not tm_ver: #LINE# #TAB# #TAB# return False #LINE# #TAB# for ver in __tm_supported_versions__: #LINE# #TAB# #TAB# if version.parse(ver) == version.parse(tm_ver[0]): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"Positional Arguements : samples : Numpy Array . Required . metric : String . ' l1 ' or ' l2 ' . Required  <code> def get_sample_weight(samples, metric): ","#LINE# #TAB# assert metric in ['l1', 'l2'] #LINE# #TAB# if metric == 'l1': #LINE# #TAB# #TAB# sample_weight = samples.apply(lambda y: 1 / y) #LINE# #TAB# elif metric == 'l2': #LINE# #TAB# #TAB# sample_weight = samples.apply(lambda y: 1 / y ** 2) #LINE# #TAB# sample_weight = sample_weight / sample_weight.sum() #LINE# #TAB# sample_weight = sample_weight.get_values() #LINE# #TAB# return sample_weight"
"iterate all the keys and attributes associated with a class , without using getattr ( ) . Does not use getattr ( ) so that class - sensitive descriptors ( i.e. property.__get _ _ ( ) ) are not called  <code> def iterate_attributes(cls): ","#LINE# #TAB# keys = dir(cls) #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# for c in cls.__mro__: #LINE# #TAB# #TAB# #TAB# if key in c.__dict__: #LINE# #TAB# #TAB# #TAB# #TAB# yield key, c.__dict__[key] #LINE# #TAB# #TAB# #TAB# #TAB# break"
"High level SCT for testing submission results . Args : col_names : names of columns to test for equality . Defaults to all . sort : whether to sort the rows of the results . match : argument passed to test_column  <code> def check_result2(state, col_names=None, sort=False, match='exact'): ","#LINE# #TAB# col_names = list(state.solution_result.keys() #LINE# #TAB# #TAB# ) if col_names is None else col_names #LINE# #TAB# test_has_columns(state) #LINE# #TAB# test_ncols(state) #LINE# #TAB# test_nrows(state) #LINE# #TAB# child = sort_rows(state) if sort else state #LINE# #TAB# for k in col_names: #LINE# #TAB# #TAB# if not match == 'any': #LINE# #TAB# #TAB# #TAB# test_column_name(child, k) #LINE# #TAB# #TAB# test_column(child, k, match=match) #LINE# #TAB# return state"
decode the key and append fields <code> def encode_keys(variants): ,"#LINE# #TAB# keys = variants['key'] #LINE# #TAB# mask = (1 << 48) - 1 #LINE# #TAB# position = np.bitwise_and(keys, mask) >> 4 #LINE# #TAB# mask = (1 << 4) - 1 #LINE# #TAB# typ = np.bitwise_and(keys, mask) >> 1 #LINE# #TAB# mask = 1 #LINE# #TAB# het = np.bitwise_and(keys, mask) #LINE# #TAB# return position, typ, het, variants"
Perform HEAD request and return status code <code> def get_status_code(url): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# request = Request(sanitize_url(url)) #LINE# #TAB# #TAB# request.add_header('User-Agent', #LINE# #TAB# #TAB# #TAB# 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)') #LINE# #TAB# #TAB# request.get_method = lambda : 'HEAD' #LINE# #TAB# #TAB# response = urlopen(request, context=ssl_unverified_context) #LINE# #TAB# #TAB# return response.getcode() #LINE# #TAB# except HTTPError as e: #LINE# #TAB# #TAB# return e.code #LINE# #TAB# except URLError as e: #LINE# #TAB# #TAB# return e.reason #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e, url) #LINE# #TAB# #TAB# return None"
"Core function for a classical damage computation  <code> def classical_damage(riskinputs, riskmodel, param, monitor): ","#LINE# #TAB# result = AccumDict(accum=AccumDict()) #LINE# #TAB# for ri in riskinputs: #LINE# #TAB# #TAB# for out in riskmodel.gen_outputs(ri, monitor): #LINE# #TAB# #TAB# #TAB# for l, loss_type in enumerate(riskmodel.loss_types): #LINE# #TAB# #TAB# #TAB# #TAB# ordinals = ri.assets['ordinal'] #LINE# #TAB# #TAB# #TAB# #TAB# result[l, out.rlzi] += dict(zip(ordinals, out[loss_type])) #LINE# #TAB# return result"
"Registers a plugin class  <code> def RegisterPlugin(cls, plugin_class): ",#LINE# #TAB# plugin_name = plugin_class.NAME.lower() #LINE# #TAB# if plugin_name in cls._plugin_classes: #LINE# #TAB# raise KeyError(( #LINE# #TAB# #TAB# 'Plugin class already set for name: {0:s}.').format( #LINE# #TAB# #TAB# #TAB# plugin_class.NAME)) #LINE# #TAB# cls._plugin_classes[plugin_name] = plugin_class
"Extract details for Application  <code> def get_details(app='groupproject', env='dev', region='us-east-1'): ","#LINE# #TAB# url = '{host}/applications/{app}'.format(host=API_URL, app=app) #LINE# #TAB# request = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT) #LINE# #TAB# if not request.ok: #LINE# #TAB# #TAB# raise SpinnakerAppNotFound('""{0}"" not found.'.format(app)) #LINE# #TAB# app_details = request.json() #LINE# #TAB# LOG.debug('App details: %s', app_details) #LINE# #TAB# group = app_details['attributes'].get('repoProjectKey') #LINE# #TAB# project = app_details['attributes'].get('repoSlug') #LINE# #TAB# generated = gogoutils.Generator(group, project, env=env, region=region, formats=APP_FORMATS) #LINE# #TAB# LOG.debug('Application details: %s', generated) #LINE# #TAB# return generated"
"Alters a potential so that it is periodic across the structure . V - potential ( array ) eps - dielectric constant ( array ) dx - step size ( nm ) <code> def calc_periodic_potn(V, eps, dx): ",#LINE# #TAB# pseudoz = np.cumsum(dx / eps) #LINE# #TAB# return V[-1] / pseudoz[-1] * pseudoz
"Wrapper utility that returns a test floating IP  <code> def create_floatingip(cls, external_network_id): ","#LINE# #TAB# body = cls.floating_ips_client.create_floatingip(floating_network_id= #LINE# #TAB# #TAB# external_network_id) #LINE# #TAB# fip = body['floatingip'] #LINE# #TAB# cls.addClassResourceCleanup(test_utils.call_and_ignore_notfound_exc, #LINE# #TAB# #TAB# cls.floating_ips_client.delete_floatingip, fip['id']) #LINE# #TAB# return fip"
"2D list column align : param list|tuple rows : rows : param bool left : default True : rtype : list <code> def col_align(rows, left=True): ","#LINE# #TAB# cols = [list(c) for c in zip(*rows)] #LINE# #TAB# max_len = [max([len(item) for item in col]) for col in cols] #LINE# #TAB# for i, col in enumerate(cols): #LINE# #TAB# #TAB# for j, elem in enumerate(col): #LINE# #TAB# #TAB# #TAB# if left: #LINE# #TAB# #TAB# #TAB# #TAB# col[j] += ' ' * (max_len[i] - len(elem)) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# col[j] = ' ' * (max_len[i] - len(elem)) + col[j] #LINE# #TAB# return [list(r) for r in zip(*cols)]"
Clean a dictionary of dwgsim key - value pairs  <code> def format_dwgsim_params(params: Mapping) ->str: ,"#LINE# #TAB# formatted_params = '' #LINE# #TAB# for key, value in params.items(): #LINE# #TAB# #TAB# if key in ('extra', 'output_prefix'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key = '1' if key == 'r1' else key #LINE# #TAB# #TAB# key = '2' if key == 'r2' else key #LINE# #TAB# #TAB# if value is True: #LINE# #TAB# #TAB# #TAB# formatted_params += f' -{key}' #LINE# #TAB# #TAB# elif value is False: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# formatted_params += f' -{key} {value}' #LINE# #TAB# return formatted_params"
"Exports a .tsv file of the dataframe created by generated_pairwise_gene_match_df ( ) <code> def export_pairwise_gene_match_report(df: pd.DataFrame, outdir: Path) ->Path: ","#LINE# #TAB# outname = outdir / 'pairwise_gene_match_report.tsv' #LINE# #TAB# df.to_csv(outname, sep='\t', index=None) #LINE# #TAB# return outname"
"get_file_diff : Download files from nodes Args : tree ( ChannelManager ) : manager to handle communication to Kolibri Studio Returns : list of files that are not on Kolibri Studio <code> def get_file_diff(tree, files_to_diff): ",#LINE# #TAB# config.LOGGER.info(' Checking if files exist on Kolibri Studio...') #LINE# #TAB# file_diff = tree.get_file_diff(files_to_diff) #LINE# #TAB# return file_diff
The result folder Parameters ---------- project_folder : str Returns ------- result_folder : str <code> def result_folder(project_folder: str) ->str: ,"#LINE# #TAB# dirname = 'matrices' #LINE# #TAB# file_path = os.path.join(project_folder, dirname) #LINE# #TAB# return file_path"
"Pad a timeseries to match the specified [ start end ) limits <code> def pad_series(ts, pad, start, end): ","#LINE# #TAB# span = ts.span #LINE# #TAB# pada = max(int((span[0] - start) * ts.sample_rate.value), 0) #LINE# #TAB# padb = max(int((end - span[1]) * ts.sample_rate.value), 0) #LINE# #TAB# if pada or padb: #LINE# #TAB# #TAB# return ts.pad((pada, padb), mode='constant', constant_values=(pad,)) #LINE# #TAB# return ts"
"Get the encrypted GitHub token in Travis . Make sure the contents this variable do not leak . The ` ` run ( ) ` ` function will remove this from the output , so always use it  <code> def get_token(): ","#LINE# #TAB# token = os.environ.get('GH_TOKEN', None) #LINE# #TAB# if not token: #LINE# #TAB# #TAB# token = 'GH_TOKEN environment variable not set' #LINE# #TAB# token = token.encode('utf-8') #LINE# #TAB# return token"
"Compute clustering index Parameters ---------- Nu : int Number of units Nc : int Number of clusters Returns ------- clust : float 0 if units are not clustered ( checkerboard ) 1 if units form a single cluster <code> def single_clustering(Nu, Nc): ",#LINE# #TAB# clust = 1 - (Nc / Nu - 1 / Nu) / (1 - 1 / Nu) #LINE# #TAB# return clust
"If the user has asked for each directory name to be shortened will return the name up to their specified length . Otherwise returns the full name  <code> def maybe_shorten_name(powerline, name): ","#LINE# #TAB# max_size = powerline.segment_conf(""cwd"", ""max_dir_size"") #LINE# #TAB# if max_size: #LINE# #TAB# #TAB# return name[:max_size] #LINE# #TAB# return name"
Render a string for space section <code> def format_space(experiment): ,"#LINE# #TAB# space_string = SPACE_TEMPLATE.format(title=format_title('Space'), #LINE# #TAB# #TAB# params='\n'.join(name + ': ' + experiment.space[name]. #LINE# #TAB# #TAB# get_prior_string() for name in experiment.space.keys())) #LINE# #TAB# return space_string"
Create neo4j safe properties <code> def sanitize_data(data): ,"#LINE# #TAB# from pandas import isnull #LINE# #TAB# sanitized = dict() #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# sanitized[k] = v #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isnull(v): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if v is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# sanitized[k] = v #LINE# #TAB# return sanitized"
json serializer that deals with dates <code> def json_int_dttm_ser(obj): ,"#LINE# #TAB# val = base_json_conv(obj) #LINE# #TAB# if val is not None: #LINE# #TAB# #TAB# return val #LINE# #TAB# if isinstance(obj, (datetime, pd.Timestamp)): #LINE# #TAB# #TAB# obj = datetime_to_epoch(obj) #LINE# #TAB# elif isinstance(obj, date): #LINE# #TAB# #TAB# obj = (obj - EPOCH.date()).total_seconds() * 1000 #LINE# #TAB# else: #LINE# #TAB# #TAB# raise TypeError('Unserializable object {} of type {}'.format(obj, #LINE# #TAB# #TAB# #TAB# type(obj))) #LINE# #TAB# return obj"
Build the Resnet50 based level 1 model : param lock_base_model : : return : <code> def build_model_resnet50_avg(lock_base_model: bool): ,"#LINE# #TAB# base_model = ResNet50(input_shape=INPUT_SHAPE, include_top=False, #LINE# #TAB# #TAB# pooling=None) #LINE# #TAB# if lock_base_model: #LINE# #TAB# #TAB# for layer in base_model.layers: #LINE# #TAB# #TAB# #TAB# layer.trainable = False #LINE# #TAB# x = GlobalAveragePooling2D(name='avg_pool_final')(base_model.layers[-2] #LINE# #TAB# #TAB# .output) #LINE# #TAB# res = Dense(NB_CLASSES, activation='sigmoid', name='classes', #LINE# #TAB# #TAB# kernel_initializer='zero', kernel_regularizer=l1(1e-05))(x) #LINE# #TAB# model = Model(inputs=base_model.inputs, outputs=res) #LINE# #TAB# return model"
Parse PNG file bytes to Pillow Image <code> def parse_buffer_to_png(data): ,#LINE# #TAB# images = [] #LINE# #TAB# c1 = 0 #LINE# #TAB# c2 = 0 #LINE# #TAB# data_len = len(data) #LINE# #TAB# while c1 < data_len: #LINE# #TAB# #TAB# if data[c2:c2 + 4] == b'IEND' and (c2 + 8 == data_len or data[c2+9:c2+12] == b'PNG'): #LINE# #TAB# #TAB# #TAB# images.append(Image.open(BytesIO(data[c1:c2 + 8]))) #LINE# #TAB# #TAB# #TAB# c1 = c2 + 8 #LINE# #TAB# #TAB# #TAB# c2 = c1 #LINE# #TAB# #TAB# c2 += 1 #LINE# #TAB# return images
"Reviews an IPS patch to insure it has only one EOF marker . : param fhpatch : File handler of IPS patch : returns : True if exactly one marker , else False <code> def eof_check(fhpatch): ","#LINE# #TAB# check, counter, i = deque(maxlen=3), 0, 0 #LINE# #TAB# for val in iter(partial(fhpatch.read, 1), b''): #LINE# #TAB# #TAB# check.append(val) #LINE# #TAB# #TAB# if b''.join(check) == b'EOF': #LINE# #TAB# #TAB# #TAB# counter += 1 #LINE# #TAB# return counter == 1"
Get sequence length from user . : return : sequence length as int <code> def get_len(): ,#LINE# #TAB# seq_len = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# seq_len = int(input(LENGTH_MESSAGE)) #LINE# #TAB# #TAB# #TAB# if seq_len >= 3: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# print(LENGTH_ERROR1) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# print(LENGTH_ERROR2) #LINE# #TAB# return seq_len
Get a list of exposed actions that are callable via the ` ` do_action ( ) ` ` method  <code> def list_actions(cls): ,"#LINE# #TAB# actions = ['start', 'stop', 'restart', 'status'] #LINE# #TAB# for func_name in dir(cls): #LINE# #TAB# #TAB# func = getattr(cls, func_name) #LINE# #TAB# #TAB# if not hasattr(func, '__call__') or not getattr(func, #LINE# #TAB# #TAB# #TAB# '__daemonocle_exposed__', False): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# action = func_name.replace('_', '-') #LINE# #TAB# #TAB# if action not in actions: #LINE# #TAB# #TAB# #TAB# actions.append(action) #LINE# #TAB# return actions"
Load the list of known accounting tags <code> def load_accounting_tags(path=ACCOUNTING_GROUPS_FILE): ,"#LINE# #TAB# with open(path, 'r') as fobj: #LINE# #TAB# #TAB# return json.load(fobj)['groups']"
"Load the peer Java object of the ML instance  <code> def load_java_obj(cls, clazz): ","#LINE# #TAB# java_class = cls._java_loader_class(clazz) #LINE# #TAB# java_obj = _jvm() #LINE# #TAB# for name in java_class.split('.'): #LINE# #TAB# #TAB# java_obj = getattr(java_obj, name) #LINE# #TAB# return java_obj"
Parse rate and return number of seconds to pause between calls to C{scale }  <code> def parse_rate(rate): ,#LINE# #TAB# if not rate: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 10
Fix for tasks without a module . Provides backwards compatibility with < 0 . 1 . 5 <code> def fix_module(job): ,"#LINE# #TAB# modules = settings.RQ_JOBS_MODULE #LINE# #TAB# if not type(modules) == tuple: #LINE# #TAB# #TAB# modules = [modules] #LINE# #TAB# for module in modules: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# module_match = importlib.import_module(module) #LINE# #TAB# #TAB# #TAB# if hasattr(module_match, job.task): #LINE# #TAB# #TAB# #TAB# #TAB# job.task = '{}.{}'.format(module, job.task) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return job"
Convert some HTML tags to latex equivalents  <code> def convert_html_subscripts_to_latex(text): ,"#LINE# #TAB# text = re.sub(""<sub>(.*?)</sub>"", r""$_{\1}$"", text) #LINE# #TAB# text = re.sub(""<sup>(.*?)</sup>"", r""$^{\1}$"", text) #LINE# #TAB# return text"
Recursively strip all strings found on an object . : param obj : obj to process : return : object processed <code> def strip_strings(obj): ,"#LINE# #TAB# result = obj #LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# result = [strip_strings(obj[i]) for i in range(len(obj))] #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# result = {k: strip_strings(v) for k, v in obj.items()} #LINE# #TAB# elif isinstance(obj, str): #LINE# #TAB# #TAB# result = obj.strip() #LINE# #TAB# return result"
"Return model prop by name : type model_cls : spannerorm.base_model . BaseModel : param model_cls : : type prop_name : str : param prop_name : : rtype : property | None : return : <code> def get_model_prop_by_name(cls, model_cls, prop_name): ","#LINE# #TAB# for key, prop in inspect.getmembers(model_cls, Helper.is_property): #LINE# #TAB# #TAB# if key == prop_name: #LINE# #TAB# #TAB# #TAB# return prop #LINE# #TAB# return None"
delete all messages from Lizard inbox <code> def clear_inbox(): ,"#LINE# #TAB# url = '{}inbox/'.format(LIZARD_URL) #LINE# #TAB# r = requests.get(url=url, headers=get_headers(), params={'limit': #LINE# #TAB# #TAB# RESULT_LIMIT}, timeout=10) #LINE# #TAB# r.raise_for_status() #LINE# #TAB# messages = r.json()['results'] #LINE# #TAB# for msg in messages: #LINE# #TAB# #TAB# msg_id = msg['id'] #LINE# #TAB# #TAB# read_url = '{}inbox/{}/read/'.format(LIZARD_URL, msg_id) #LINE# #TAB# #TAB# r = requests.post(url=read_url, headers=get_headers(), timeout=10) #LINE# #TAB# return True"
Google Analytics adapter factory <code> def google_analytics_factory(context): ,#LINE# #TAB# annotations = IAnnotations(context) #LINE# #TAB# adapter = annotations.get(ANALYTICS_ANNOTATIONS_KEY) #LINE# #TAB# if adapter is None: #LINE# #TAB# #TAB# adapter = annotations[ANALYTICS_ANNOTATIONS_KEY] = GoogleAnalytics() #LINE# #TAB# return adapter
Reset environment variable GMT_DATADIR to its old value  <code> def reset_gmt_datadir(old_gmt_datadir): ,#LINE# #TAB# if old_gmt_datadir: #LINE# #TAB# #TAB# os.environ['GMT_DATADIR'] = old_gmt_datadir #LINE# #TAB# else: #LINE# #TAB# #TAB# del os.environ['GMT_DATADIR']
Opens all json - config files in a directory and returns a list of each files content  <code> def open_config_files(default_config_path): ,"#LINE# #TAB# configs = [] #LINE# #TAB# for filename in glob(internal_config_path + '*.json'): #LINE# #TAB# #TAB# fp = open(filename, 'r') #LINE# #TAB# #TAB# conf = json.load(fp, object_pairs_hook=OrderedDict) #LINE# #TAB# #TAB# configs.append(conf) #LINE# #TAB# return configs"
Given a file name to a valid file returns the file object  <code> def file_contents(file_name): ,"#LINE# #TAB# curr_dir = os.path.abspath(os.path.dirname(__file__)) #LINE# #TAB# with open(os.path.join(curr_dir, file_name)) as the_file: #LINE# #TAB# #TAB# contents = the_file.read() #LINE# #TAB# return contents"
"Return the cluster or None if it does not exist  <code> def cluster_get(context, cluster, show_progress=False): ","#LINE# #TAB# if show_progress: #LINE# #TAB# #TAB# cluster = IMPL.cluster_provision_progress_update(context, cluster) #LINE# #TAB# else: #LINE# #TAB# #TAB# cluster = IMPL.cluster_get(context, cluster) #LINE# #TAB# if cluster: #LINE# #TAB# #TAB# return cluster.to_dict(show_progress) #LINE# #TAB# return None"
The function to encode kwargs . : param : to_encode : kwargs to encode . : return : encoded kwargs : dict <code> def encode_kw11(to_encode): ,"#LINE# #TAB# encoded_kw = {} #LINE# #TAB# if not isinstance(to_encode, dict): #LINE# #TAB# #TAB# return encoded_kw #LINE# #TAB# for k, v in to_encode.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# int(k) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# encoded_kw.update({k: v}) #LINE# #TAB# return encoded_kw"
"A series of resblocks starting with a downsampling Convolution2D <code> def resblock_body(x, num_filters, num_blocks): ","#LINE# #TAB# x = ZeroPadding2D(((1, 0), (1, 0)))(x) #LINE# #TAB# x = DarknetConv2D_BN_Leaky(num_filters, (3, 3), strides=(2, 2))(x) #LINE# #TAB# for i in range(num_blocks): #LINE# #TAB# #TAB# y = compose(DarknetConv2D_BN_Leaky(num_filters // 2, (1, 1)), #LINE# #TAB# #TAB# #TAB# DarknetConv2D_BN_Leaky(num_filters, (3, 3)))(x) #LINE# #TAB# #TAB# x = Add()([x, y]) #LINE# #TAB# return x"
define the epicyclic_gearing second simulation : annulus - planet <code> def eg_sim_annulus_planet(c): ,#LINE# #TAB# sg_c = annulus_gearring_constraint(c) #LINE# #TAB# sg_c['gear_type'] = 'i' #LINE# #TAB# sg_c['second_gear_type'] = 'e' #LINE# #TAB# i_gear_profile = gear_profile.gear_profile() #LINE# #TAB# i_gear_profile.apply_external_constraint(sg_c) #LINE# #TAB# i_gear_profile.run_simulation('gear_profile_simulation_A') #LINE# #TAB# return 1
"Convert all fractional coordinates to cartesian  <code> def frac2cart_all(frac_coordinates, lattice_array): ","#LINE# #TAB# coordinates = deepcopy(frac_coordinates) #LINE# #TAB# for coord in range(coordinates.shape[0]): #LINE# #TAB# #TAB# coordinates[coord] = cartisian_from_fractional(coordinates[coord], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# lattice_array) #LINE# #TAB# return coordinates"
"retrieve the aperture correction for this exposure <code> def get_apcor(expnum, ccd, version='p', prefix=None): ","#LINE# #TAB# uri = get_uri(expnum, ccd, ext=APCOR_EXT, version=version, prefix=prefix) #LINE# #TAB# apcor_file_name = tempfile.NamedTemporaryFile() #LINE# #TAB# client.copy(uri, apcor_file_name.name) #LINE# #TAB# apcor_file_name.seek(0) #LINE# #TAB# return [float(x) for x in apcor_file_name.readline().split()]"
binyenstack(stack ) Applies Yen 's algorithm on a set of images  <code> def binary_yen_stack(stack): ,"#LINE# #TAB# row, col, dep = np.shape(stack) #LINE# #TAB# bin_stack = np.empty([row, col, dep]) #LINE# #TAB# for cross in range(dep): #LINE# #TAB# #TAB# aux_thres = threshold_yen(stack[:, :, (cross)]) #LINE# #TAB# #TAB# bin_stack[:, :, (cross)] = stack[:, :, (cross)] <= aux_thres #LINE# #TAB# return bin_stack"
"Remove prefix ` start ` from sting ` s ` . Raises a : py : exc:`ValueError ` if ` s ` did n't start with ` start `  <code> def drop_prefix(s, start): ",#LINE# #TAB# l = len(start) #LINE# #TAB# if s[:l] != start: #LINE# #TAB# #TAB# raise ValueError('string does not start with expected value') #LINE# #TAB# return s[l:]
Normalizes glyph left margin  <code> def normalize_glyph_left_margin(value): ,"#LINE# #TAB# if not isinstance(value, (int, float)) and value is not None: #LINE# #TAB# #TAB# raise TypeError(""Glyph left margin must be an :ref:`type-int-float`, "" #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ""not %s."" % type(value).__name__) #LINE# #TAB# return value"
"Subsamples new vector from vector of orig items . Returns all items if requested sample is larger than number of items . This version uses the cumsum / frequency distribution method  <code> def subsample_freq_dist_nonzero(counts, n, dtype=uint): ","#LINE# #TAB# if counts.sum() <= n: #LINE# #TAB# #TAB# return counts #LINE# #TAB# result = zeros(len(counts), dtype=dtype) #LINE# #TAB# nz = counts.nonzero()[0] #LINE# #TAB# compressed = counts.take(nz) #LINE# #TAB# sums = compressed.cumsum() #LINE# #TAB# total = sums[-1] #LINE# #TAB# del compressed #LINE# #TAB# curr = n #LINE# #TAB# while curr: #LINE# #TAB# #TAB# pick = randint(0, total) #LINE# #TAB# #TAB# index = sums.searchsorted(pick, side='right') #LINE# #TAB# #TAB# result[nz[index]] += 1 #LINE# #TAB# #TAB# sums[index:] -= 1 #LINE# #TAB# #TAB# curr -= 1 #LINE# #TAB# #TAB# total -= 1 #LINE# #TAB# return result"
"Decode dictionary back into unicode : param dict[str , object ] d : Keys : rtype : dict[unicode , object ] <code> def decode_dict(d): ","#LINE# #TAB# return {k.decode(): (v.decode() if isinstance(v, bytes) else v) for k, #LINE# #TAB# #TAB# v in six.iteritems(d)}"
"Take an object , and if it is a PyMacaron model , serialize it into json so it can be passed as argument of a Celery task . Otherwise return the object unchanged <code> def model_to_task_arg(o): ","#LINE# #TAB# if not isinstance(o, PyMacaronModel): #LINE# #TAB# #TAB# return o #LINE# #TAB# j = o.to_json() #LINE# #TAB# j['__pymacaron_model_name'] = o.get_model_name() #LINE# #TAB# log.debug('Serialized PyMacaronModel %s to celery task arg' % j[ #LINE# #TAB# #TAB# '__pymacaron_model_name']) #LINE# #TAB# return j"
Executes the method provided in settings to get the CAS server URL dynamically <code> def get_server_url(request): ,"#LINE# #TAB# if django_settings.CAS_SERVER_METHOD: #LINE# #TAB# #TAB# import sys #LINE# #TAB# #TAB# mod, member = django_settings.CAS_SERVER_METHOD.rsplit('.', 1) #LINE# #TAB# #TAB# __import__(mod) #LINE# #TAB# #TAB# module = sys.modules[mod] #LINE# #TAB# #TAB# method = getattr(module, member) #LINE# #TAB# #TAB# return method(request) #LINE# #TAB# return django_settings.CAS_SERVER_URL"
Validate the structures so that all of them should have the same lattice Args : structures : list of pymatgen structures Returns : <code> def validate_structures(structures: List[Structure]): ,#LINE# #TAB# lattices = [i.lattice for i in structures] #LINE# #TAB# if len(set(lattices)) > 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Hashes and returns the message ` ` message ` ` . : param message : bytes : rtype : bytes <code> def crypto_hash_sha512(message): ,"#LINE# #TAB# digest = ffi.new('unsigned char[]', crypto_hash_sha512_BYTES) #LINE# #TAB# rc = lib.crypto_hash_sha512(digest, message, len(message)) #LINE# #TAB# ensure(rc == 0, 'Unexpected library error', raising=exc.RuntimeError) #LINE# #TAB# return ffi.buffer(digest, crypto_hash_sha512_BYTES)[:]"
"Determines the number of days of a specific month in a specific year  <code> def max_days_in_month(year, month): ","#LINE# #TAB# if month in (1, 3, 5, 7, 8, 10, 12): #LINE# #TAB# #TAB# return 31 #LINE# #TAB# if month in (4, 6, 9, 11): #LINE# #TAB# #TAB# return 30 #LINE# #TAB# if year % 400 == 0 or year % 100 != 0 and year % 4 == 0: #LINE# #TAB# #TAB# return 29 #LINE# #TAB# return 28"
"original ' , ' are replaced as ' _ _ ( CM ) _ _ ' and new lines are as ' _ _ ( LF ) _ _ ' <code> def get_output_from_url_chunks_iter(url): ","#LINE# #TAB# response = _requests.get(url) #LINE# #TAB# if response.status_code != 200: #LINE# #TAB# #TAB# return None #LINE# #TAB# chunk_urls = _ast.literal_eval(_base64.b64decode(response.text).decode()) #LINE# #TAB# for chunk_url in _tqdm.tqdm(iterable=chunk_urls, desc= #LINE# #TAB# #TAB# 'downloading and parsing outputs'): #LINE# #TAB# #TAB# chunk_response = _requests.get(chunk_url) #LINE# #TAB# #TAB# text = _base64.b64decode(chunk_response.text).decode() #LINE# #TAB# #TAB# yield text"
returns True if the values in the list L are all co - prime otherwise it returns False  <code> def co_prime(l): ,"#LINE# #TAB# for i, j in combinations(l, 2): #LINE# #TAB# #TAB# if euclid(i, j) != 1: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Update the control board with the specified fitted parameters  <code> def update_control_board_calibration(control_board, fitted_params): ",#LINE# #TAB# control_board.a0_series_resistance = fitted_params['fitted R'].values #LINE# #TAB# control_board.a0_series_capacitance = fitted_params['fitted C'].values
This method returns the total deaths across all countries and also returns total deaths per country . : param instance : instance of the class : return dict <code> def get_total_stats(instance): ,"#LINE# #TAB# stats = dict() #LINE# #TAB# current_records = get_all_records_by_country(instance) #LINE# #TAB# for _country_id in current_records: #LINE# #TAB# #TAB# for _action in instance._actions_files: #LINE# #TAB# #TAB# #TAB# stats[_action] = stats.get(_action, 0) + current_records[ #LINE# #TAB# #TAB# #TAB# #TAB# _country_id][_action] #LINE# #TAB# #TAB# stats['last_updated'] = current_records[_country_id]['last_updated'] #LINE# #TAB# return stats"
Parse game.xml data to find the game information  <code> def get_game(tree): ,"#LINE# #TAB# valid_keys = ('game_pk', 'type', 'local_game_time', 'game_time_et', #LINE# #TAB# #TAB# 'gameday_sw', 'home_team', 'away_team', 'stadium', 'plate_umpire') #LINE# #TAB# tree.attrib['local_game_time'] = create_time(tree.attrib['local_game_time'] #LINE# #TAB# #TAB# ) #LINE# #TAB# return {k: v for k, v in tree.attrib.items() if k in valid_keys}"
"Finds lines containing the search string and replaces the whole line with the specified replacement string  <code> def replace_lines_in_files(search_string, replacement_line): ","#LINE# #TAB# paths = _s.dialogs.MultipleFiles('DIS AND DAT|*.*') #LINE# #TAB# if paths == []: return #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# _shutil.copy(path, path+"".backup"") #LINE# #TAB# #TAB# lines = read_lines(path) #LINE# #TAB# #TAB# for n in range(0,len(lines)): #LINE# #TAB# #TAB# #TAB# if lines[n].find(search_string) >= 0: #LINE# #TAB# #TAB# #TAB# #TAB# print(lines[n]) #LINE# #TAB# #TAB# #TAB# #TAB# lines[n] = replacement_line.strip() + ""\n"" #LINE# #TAB# #TAB# write_to_file(path, join(lines, '')) #LINE# #TAB# return"
finds the FN that has terminals as children <code> def get_lowest_fn(p): ,#LINE# #TAB# s = set() #LINE# #TAB# for term in extract_terminals(p): #LINE# #TAB# #TAB# s.update([edge.parent for edge in term.incoming if is_foundational( #LINE# #TAB# #TAB# #TAB# edge.parent)]) #LINE# #TAB# return s
"Returns byte padding required to move the base pointer into proper alignment <code> def calc_padding_for_alignment(align, base): ",#LINE# #TAB# rmdr = int(base) % align #LINE# #TAB# if rmdr == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return align - rmdr
Returns whether cancel transition can be performed or not . Returns True only if all analyses are in unassigned status <code> def guard_cancel(analysis_request): ,"#LINE# #TAB# for partition in analysis_request.getDescendants(all_descendants=False): #LINE# #TAB# #TAB# if not isTransitionAllowed(partition, ""cancel""): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# cancellable_states = [""unassigned"", ""registered""] #LINE# #TAB# for analysis in analysis_request.objectValues(""Analysis""): #LINE# #TAB# #TAB# if api.get_workflow_status_of(analysis) not in cancellable_states: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Determine if given host is localhost equivalent  <code> def is_localhost(host): ,"#LINE# #TAB# if host in ('localhost', '127.0.0.1'): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# if IPv6Address(host) == IPv6Address('::1'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except AddressValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False"
"for the alerts and backends keyword arguments ... - provides resonable defaults - accept a single alert / backend or a list of them - accept alert / backend class or the a string containing the alert / backend i d <code> def super_accepter(arg, lookup_dict): ","#LINE# #TAB# if arg is None: #LINE# #TAB# #TAB# return lookup_dict.values() #LINE# #TAB# if not isinstance(arg, (tuple, list)): #LINE# #TAB# #TAB# arg = [arg] #LINE# #TAB# ids = (a if isinstance(a, basestring) else a.id for a in arg) #LINE# #TAB# _set = {} #LINE# #TAB# ids = (_set.setdefault(id, id) for id in ids if id not in _set) #LINE# #TAB# return [lookup_dict[id] for id in ids]"
"Get fields of the given index : param connection : MySQLDb connection : param db : database : param tbl : table : param index : index name : return : list of field names <code> def get_index_fields(connection, db, tbl, index): ","#LINE# #TAB# cur = connection.cursor() #LINE# #TAB# query = ( #LINE# #TAB# #TAB# ""SELECT COLUMN_NAME FROM information_schema.STATISTICS WHERE TABLE_SCHEMA='%s' AND TABLE_NAME='%s' AND INDEX_NAME='%s' ORDER BY SEQ_IN_INDEX"" #LINE# #TAB# #TAB# ) #LINE# #TAB# LOG.info('Executing %s', query % (db, tbl, index)) #LINE# #TAB# cur.execute(query % (db, tbl, index)) #LINE# #TAB# cols = [] #LINE# #TAB# for row in cur.fetchall(): #LINE# #TAB# #TAB# cols.append(row[0]) #LINE# #TAB# return cols"
Works like ` mkdir -p ` : creates all ( not yet existing ) directories in a given path  <code> def mkdir_p(path): ,#LINE# #TAB# if not path: #LINE# #TAB# #TAB# return #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as exc: #LINE# #TAB# #TAB# if exc.errno == errno.EEXIST and os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise
Get and store repository handler for named project <code> def get_project_repository(project_element): ,#LINE# #TAB# repo_handler = FileRepository(target_dir=ProjectUtils.get_target_dir( #LINE# #TAB# #TAB# project_element=project_element)) #LINE# #TAB# StateHolder.repositories[StateHolder.name] = repo_handler #LINE# #TAB# return repo_handler
"translates that a key of say 1 is the ' OUG - Displacement vector ' table <code> def get_table_from_table_code(table_code: int, table_name: str, is_msc: ",#LINE# #TAB# bool=True) ->str: #LINE# #TAB# try: #LINE# #TAB# #TAB# if is_msc: #LINE# #TAB# #TAB# #TAB# table = MSC_TABLE_CONTENT[table_code] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# table = NX_TABLE_CONTENT[table_code] #LINE# #TAB# except: #LINE# #TAB# #TAB# print(f'count not determine the table description for {table_name}') #LINE# #TAB# #TAB# raise #LINE# #TAB# return table
Gets the name of all recipes present in the run_list of a node <code> def get_recipes_in_node(node): ,"#LINE# #TAB# recipes = [] #LINE# #TAB# for elem in node.get('run_list', []): #LINE# #TAB# #TAB# if elem.startswith(""recipe""): #LINE# #TAB# #TAB# #TAB# recipe = elem.split('[')[1].split(']')[0] #LINE# #TAB# #TAB# #TAB# recipes.append(recipe) #LINE# #TAB# return recipes"
"Splits an original filename into dict with keys base , ext  <code> def split_original(name): ","#LINE# #TAB# if not (isinstance(name, str) or isinstance(name, unicode)): #LINE# #TAB# #TAB# raise OriginalError('Invalid name.') #LINE# #TAB# if not name: #LINE# #TAB# #TAB# raise OriginalError('Invalid name.') #LINE# #TAB# split = name.rsplit('.', 1) #LINE# #TAB# return {'base': split[0], 'ext': split[1] if len(split) == 2 else ''}"
Realise preprocess for an input pillow image and convert it to numpy array . Parameters ---------- image An input image to be processed . Returns ------- Preprocessed numpy array  <code> def image_to_tensor(image: Image) ->np.ndarray: ,"#LINE# #TAB# mean = [0.485, 0.456, 0.406] #LINE# #TAB# std = [0.229, 0.224, 0.225] #LINE# #TAB# image = image.convert('RGB') #LINE# #TAB# tensor = np.asarray(image) #LINE# #TAB# tensor = crop_and_resize(tensor, 456) #LINE# #TAB# tensor = tensor / 255.0 #LINE# #TAB# tensor[..., 0] -= mean[0] #LINE# #TAB# tensor[..., 1] -= mean[1] #LINE# #TAB# tensor[..., 2] -= mean[2] #LINE# #TAB# tensor[..., 0] /= std[0] #LINE# #TAB# tensor[..., 1] /= std[1] #LINE# #TAB# tensor[..., 2] /= std[2] #LINE# #TAB# assert tensor.shape == (456, 456, 3) #LINE# #TAB# return tensor"
Opens an xlsx file ( the template ) and returns the openpyxl object  <code> def open_openpyxl_template(template_file): ,"#LINE# #TAB# wb = load_workbook(template_file, keep_vba=True) #LINE# #TAB# logger.info('Opening {} as an openpyxl object'.format(template_file)) #LINE# #TAB# return wb"
platform specific default root path  <code> def arduino_default_path(): ,#LINE# #TAB# if sys.platform == 'darwin': #LINE# #TAB# #TAB# s = path('/Applications/Arduino.app/Contents/Resources/Java') #LINE# #TAB# elif sys.platform == 'win32': #LINE# #TAB# #TAB# s = None #LINE# #TAB# else: #LINE# #TAB# #TAB# s = path('/usr/share/arduino/') #LINE# #TAB# return s
"Converts a line from the properties file and adds it to the properties dictionary  <code> def convert_line(line: str, prop_dict: Dict[str, str]) ->None: ","#LINE# #TAB# key, value = line.split('=', 1) #LINE# #TAB# value = replace_el_with_var(value.strip(), prop_dict, quote=False) #LINE# #TAB# prop_dict[key.strip()] = value"
> > > parse_duration('00:01 ' ) 1 > > > parse_duration('03:33 ' ) 213 > > > parse_duration('01:14:00 ' ) 4440 <code> def parse_duration(s: str) ->int: ,#LINE# #TAB# parts = s.split(':') #LINE# #TAB# seconds = int(parts[-1]) #LINE# #TAB# if len(parts) > 1: #LINE# #TAB# #TAB# seconds += int(parts[-2]) * 60 #LINE# #TAB# if len(parts) == 3: #LINE# #TAB# #TAB# seconds += int(parts[0]) * 3600 #LINE# #TAB# return seconds
Adjust environmental variables which can cause conflicts with installed anaconda python  <code> def clean_env_variables(): ,"#LINE# #TAB# for k in ['PYTHONPATH', 'PYTHONHOME']: #LINE# #TAB# #TAB# os.environ.pop(k, None) #LINE# #TAB# os.environ['PYTHONNOUSERSITE'] = '1'"
Return a list of the names of the persistent attributes of ` ` cls ` ` . ... except collections  <code> def persistent_attribute_names_of(cls): ,"#LINE# #TAB# return [prop.key for prop in class_mapper(cls).iterate_properties if #LINE# #TAB# #TAB# isinstance(prop, ColumnProperty)]"
"Get a list of devices that have search_string in the host name <code> def get_list_devices_by_hostname(search_string, database_name='devices.db'): ","#LINE# #TAB# conn = sqlite3.connect(database_name) #LINE# #TAB# curs = conn.cursor() #LINE# #TAB# search_string = search_string + '%' #LINE# #TAB# curs.execute( #LINE# #TAB# #TAB# 'SELECT ip,hostname,last_seen FROM devices WHERE hostname LIKE ?', #LINE# #TAB# #TAB# (search_string,)) #LINE# #TAB# rows = sorted(curs.fetchall()) #LINE# #TAB# return rows"
Return the end datetime as determined by dtend or duration  <code> def get_end_date(obj): ,"#LINE# #TAB# if hasattr(obj, 'dtend'): #LINE# #TAB# #TAB# enddate = obj.dtend.value #LINE# #TAB# elif hasattr(obj, 'duration'): #LINE# #TAB# #TAB# enddate = obj.dtstart.value + obj.duration.value #LINE# #TAB# else: #LINE# #TAB# #TAB# enddate = obj.dtstart.value + timedelta(days=1) #LINE# #TAB# return enddate"
Parse and return dictionary of typaliases  <code> def typealiases_parser(typealiases): ,"#LINE# #TAB# return {alias: types.from_string_alias('sytypealias {0} = {1}'.format( #LINE# #TAB# #TAB# alias, value['type'])) for alias, value in typealiases.items()}"
"Return a setting from a section , optionally provide default  <code> def get_setting(section, name, default=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# setting = deepcopy(settings.DJANGO_ICONS[section][name]) #LINE# #TAB# except (AttributeError, KeyError, TypeError): #LINE# #TAB# #TAB# setting = default #LINE# #TAB# return setting"
Default menu choice foratted for inclusion in a prompt string  <code> def get_formatted_default_menu_choice(state): ,#LINE# #TAB# default_choice = get_default_menu_choice(state) #LINE# #TAB# if default_choice: #LINE# #TAB# #TAB# return '[$(red)' + default_choice + '$(nrm)] ' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
Return a toplevel logger  <code> def default_logger(name): ,#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# logger_handler = logging.StreamHandler() #LINE# #TAB# formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s') #LINE# #TAB# logger_handler.setFormatter(formatter) #LINE# #TAB# logger.addHandler(logger_handler) #LINE# #TAB# return logger
"Set the joined alias on the child , for Django > = 1.8.0 : param child : : param alias_map : <code> def set_child_joined_alias(child, alias_map): ","#LINE# #TAB# for table in alias_map: #LINE# #TAB# #TAB# join = alias_map[table] #LINE# #TAB# #TAB# if not isinstance(join, Join): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# lhs = join.parent_alias #LINE# #TAB# #TAB# if (lhs == child.alias and table == child.related_alias or lhs == #LINE# #TAB# #TAB# #TAB# child.related_alias and table == child.alias): #LINE# #TAB# #TAB# #TAB# child.set_joined_alias(table) #LINE# #TAB# #TAB# #TAB# break"
Return a dict for form datas population : param dict request_parameters : A dictionnary containing the parameters we want to rerturn : returns : The param dict : rtype : dict <code> def get_oidc_datas(request_parameters): ,#LINE# #TAB# logger.debug(request_parameters) #LINE# #TAB# logger.debug('client_id' in request_parameters) #LINE# #TAB# result = {} #LINE# #TAB# for key in REQUIRED_FIELDS: #LINE# #TAB# #TAB# if key == 'scope' and 'openid' not in request_parameters[key]: #LINE# #TAB# #TAB# #TAB# raise KeyError('Invalid scope declaration') #LINE# #TAB# #TAB# result[key] = request_parameters[key] #LINE# #TAB# for key in OPTIONNAL_FIELDS: #LINE# #TAB# #TAB# if key in request_parameters: #LINE# #TAB# #TAB# #TAB# result[key] = request_parameters[key] #LINE# #TAB# return result
Return the length of the blimpy header in bytes <code> def len_header(filename): ,"#LINE# #TAB# with open(filename, 'rb') as f: #LINE# #TAB# #TAB# header_sub_count = 0 #LINE# #TAB# #TAB# eoh_found = False #LINE# #TAB# #TAB# while not eoh_found: #LINE# #TAB# #TAB# #TAB# header_sub = f.read(512) #LINE# #TAB# #TAB# #TAB# header_sub_count += 1 #LINE# #TAB# #TAB# #TAB# if b'HEADER_END' in header_sub: #LINE# #TAB# #TAB# #TAB# #TAB# idx_end = header_sub.index(b'HEADER_END') + len(b'HEADER_END') #LINE# #TAB# #TAB# #TAB# #TAB# eoh_found = True #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# idx_end = (header_sub_count -1) * 512 + idx_end #LINE# #TAB# return idx_end"
"Plot the swim speed during experimental indices <code> def plot_swim_speed(exp_ind, swim_speed): ","#LINE# #TAB# import numpy #LINE# #TAB# fig, ax = plt.subplots() #LINE# #TAB# ax.title.set_text('Swim speed from depth change and pitch angle (m/s^2') #LINE# #TAB# ax.plot(exp_ind, swim_speed, linewidth=_linewidth, label='speed') #LINE# #TAB# ymax = numpy.ceil(swim_speed[~numpy.isnan(swim_speed)].max()) #LINE# #TAB# ax.set_ylim(0, ymax) #LINE# #TAB# ax.legend(loc='upper right') #LINE# #TAB# plt.show() #LINE# #TAB# return ax"
"Class method that will return a Certificate object by its ID  <code> def get_object(cls, api_token, cert_id): ","#LINE# #TAB# #TAB# certificate = cls(token=api_token, id=cert_id) #LINE# #TAB# #TAB# certificate.load() #LINE# #TAB# #TAB# return certificate"
"Check parameter and expand it to a file name  <code> def expand_file_name(path, doc_root): ","#LINE# #TAB# if not path.startswith('/'): #LINE# #TAB# #TAB# return os.path.join(doc_root, path) #LINE# #TAB# else: #LINE# #TAB# #TAB# return path"
converts txt to a integer number and returns this number return None if txt is not a number <code> def get_int_from_str(txt): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# number = int(txt) #LINE# #TAB# #TAB# if number is not 0: #LINE# #TAB# #TAB# #TAB# return number #LINE# #TAB# except: #LINE# #TAB# #TAB# return None
"Check if the value is a * NoneValue * in function of the type : param type : type of value : param value : value to check : rtype : * bool * , True if it is a * NoneValue * <code> def is_none(type, value): ","#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(value, str) and value.upper() == 'NONE': #LINE# #TAB# #TAB# return True #LINE# #TAB# if value == '' and type is not str: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Reset configuration context : return : None <code> def reset_context(): ,#LINE# #TAB# global _INI_FILE_TRIED_TO_BE_LOADED #LINE# #TAB# global _INI_FILE_CONTENT #LINE# #TAB# _INI_FILE_TRIED_TO_BE_LOADED = False #LINE# #TAB# _INI_FILE_CONTENT = None
"return the full path of the first index in indir , or None if not found <code> def match_index(indices, indir): ","#LINE# #TAB# for filename in indices: #LINE# #TAB# #TAB# index = os.path.join(indir, filename) #LINE# #TAB# #TAB# if os.path.isfile(index): #LINE# #TAB# #TAB# #TAB# return index #LINE# #TAB# return None"
Adds the specified path to the output logging paths if it is not already in the listed paths  <code> def add_output_path(path: str = None) -> str: ,#LINE# #TAB# cleaned = paths.clean(path or os.getcwd()) #LINE# #TAB# if cleaned not in _logging_paths: #LINE# #TAB# #TAB# _logging_paths.append(cleaned) #LINE# #TAB# return cleaned
"Finds the first free port starting at start_port for the service and returns an open port not reserved by riptide or another application Used by additional ports logic ( get_additional_port ) , may be used by other system parts  <code> def find_open_port_starting_at(start_port: int): ","#LINE# #TAB# port_cfg = PortsConfig.get() #LINE# #TAB# port_found = False #LINE# #TAB# current_port = start_port #LINE# #TAB# while not port_found: #LINE# #TAB# #TAB# if _is_open(current_port, port_cfg['ports']): #LINE# #TAB# #TAB# #TAB# return current_port #LINE# #TAB# #TAB# current_port += 1"
Update the modification time ( mtime ) of the specified file . : param file_path : Path of the file to update . : return : the new and the old file modification time  <code> def update_mtime(file_path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# new_mtime = os.path.getmtime(file_path) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# with db.DbHelper() as dbh: #LINE# #TAB# #TAB# #TAB# dbh.delete_file(file_path) #LINE# #TAB# #TAB# return None, None #LINE# #TAB# else: #LINE# #TAB# #TAB# with db.DbHelper() as dbh: #LINE# #TAB# #TAB# #TAB# old_mtime = dbh.get_file_mtime(file_path) #LINE# #TAB# #TAB# #TAB# dbh.update_file(file_path, new_mtime) #LINE# #TAB# #TAB# return new_mtime, old_mtime"
this needs to be called in between definitions of nodes in order to set on which page their documentation will go . Names must correspond to the url of the page the documentation should be on  <code> def set_current_documentation_page(name): ,#LINE# #TAB# global _documentation_html_builder #LINE# #TAB# global _current_documentation_page #LINE# #TAB# _documentation_html_builder[name] = [] #LINE# #TAB# _current_documentation_page = name
Load the default config file from the default ipython_dir  <code> def load_default_config(ipython_dir=None): ,"#LINE# #TAB# if ipython_dir is None: #LINE# #TAB# #TAB# ipython_dir = get_ipython_dir() #LINE# #TAB# profile_dir = os.path.join(ipython_dir, 'profile_default') #LINE# #TAB# cl = PyFileConfigLoader(default_config_file_name, profile_dir) #LINE# #TAB# try: #LINE# #TAB# #TAB# config = cl.load_config() #LINE# #TAB# except ConfigFileNotFound: #LINE# #TAB# #TAB# config = Config() #LINE# #TAB# return config"
Helper function for formatting the content of the options line <code> def format_options(options): ,"#LINE# #TAB# lines = [''] #LINE# #TAB# for token in re.split(r'( -|\[\[|\]\])', options): #LINE# #TAB# #TAB# if token in ['[[',']]']: #LINE# #TAB# #TAB# #TAB# lines.append(token) #LINE# #TAB# #TAB# #TAB# lines.append('') #LINE# #TAB# #TAB# elif token == ' -': #LINE# #TAB# #TAB# #TAB# lines.append(token) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# lines[-1] += token #LINE# #TAB# return '<span style=""display:block"">' + '</span><span style=""display:block"">'.join(line for line in lines if line.strip()) + '</span>'"
Returns os data  <code> def os_info(): ,"#LINE# #TAB# return { #LINE# #TAB# #TAB# 'uname': dict(platform.uname()._asdict()), #LINE# #TAB# #TAB# 'path': os.environ.get('PATH', '').split(':'), #LINE# #TAB# #TAB# 'shell': os.environ.get('SHELL', '/bin/sh'), #LINE# #TAB# }"
"Changes the starting point of a circular SeqRecord by n_bases bases  <code> def rotate_circular_record(record, n_bases): ","#LINE# #TAB# new_record = deepcopy(record) #LINE# #TAB# new_record.seq = record.seq[n_bases:] + record.seq[:n_bases] #LINE# #TAB# for f in new_record.features: #LINE# #TAB# #TAB# f.location += -n_bases #LINE# #TAB# #TAB# if max(f.location.start, f.location.end) <= 0: #LINE# #TAB# #TAB# #TAB# f.location += len(record) #LINE# #TAB# return new_record"
"r Return a mask to reduce the density of points in irregularly - spaced data  <code> def reduce_point_density(points, radius, priority=None): ","#LINE# #TAB# if points.ndim < 2: #LINE# #TAB# #TAB# points = points.reshape(-1, 1) #LINE# #TAB# tree = cKDTree(points) #LINE# #TAB# if priority is not None: #LINE# #TAB# #TAB# sorted_indices = np.argsort(priority)[::-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# sorted_indices = range(len(points)) #LINE# #TAB# keep = np.ones(len(points), dtype=np.bool) #LINE# #TAB# for ind in sorted_indices: #LINE# #TAB# #TAB# if keep[ind]: #LINE# #TAB# #TAB# #TAB# neighbors = tree.query_ball_point(points[ind], radius) #LINE# #TAB# #TAB# #TAB# keep[neighbors] = False #LINE# #TAB# #TAB# #TAB# keep[ind] = True #LINE# #TAB# return keep"
Get Image_Name for the image_id specified . Args : inst_img_id ( str ) : image_id to get name value from . Returns : aminame ( str ) : name of the image  <code> def get_one_aminame(inst_img_id): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# aminame = EC2R.Image(inst_img_id).name #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# aminame = 'Unknown' #LINE# #TAB# return aminame
"Parses entered response string to string error message without the error code . E.g. : response = ' -110,""Command error "" ' returns : ' Command error '  <code> def parse_err_query_response(response: str) ->str: ","#LINE# #TAB# m = re.match('(-?[\\d]+).*""(.*)""', response) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return m.group(2) #LINE# #TAB# else: #LINE# #TAB# #TAB# return response"
"Get AST node and return a contract function <code> def exec_contract(cls, node: ast.AST) ->Optional[Callable]: ","#LINE# #TAB# if type(node) is ast.Call and not node.args: #LINE# #TAB# #TAB# return cls.exec_contract(node.func) #LINE# #TAB# if not isinstance(node, ast.Attribute): #LINE# #TAB# #TAB# return None #LINE# #TAB# if node.value.id != 'deal': #LINE# #TAB# #TAB# return None #LINE# #TAB# contract = getattr(_aliases, node.attr, None) #LINE# #TAB# if contract is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# return contract"
"Returns detailed information about registered blueprint routes matching the BlueprintBundle path <code> def get_blueprint_routes(app, base_path): ","#LINE# #TAB# #TAB# routes = [] #LINE# #TAB# #TAB# for child in app.url_map.iter_rules(): #LINE# #TAB# #TAB# #TAB# if child.rule.startswith(base_path): #LINE# #TAB# #TAB# #TAB# #TAB# relative_path = child.rule[len(base_path):] #LINE# #TAB# #TAB# #TAB# #TAB# routes.append({ #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'path': relative_path, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'endpoint': child.endpoint, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'methods': list(child.methods) #LINE# #TAB# #TAB# #TAB# #TAB# }) #LINE# #TAB# #TAB# return routes"
Import a dotted module path and return the attribute / class designated by the last name in the path . Raise ImportError if the import failed  <code> def import_string(dotted_path: str) ->ty.Any: ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
List all resource managers names . Returns all service names and all combination of service.resource names . : param admin_required : None - > returns all ResourceManagers True - > returns only admin ResourceManagers False - > returns only non admin ResourceManagers <code> def list_resource_names(admin_required=None): ,"#LINE# #TAB# res_mgrs = discover.itersubclasses(base.ResourceManager) #LINE# #TAB# if admin_required is not None: #LINE# #TAB# #TAB# res_mgrs = filter(lambda cls: cls._admin_required == admin_required, #LINE# #TAB# #TAB# #TAB# res_mgrs) #LINE# #TAB# names = set() #LINE# #TAB# for cls in res_mgrs: #LINE# #TAB# #TAB# names.add(cls._service) #LINE# #TAB# #TAB# names.add('%s.%s' % (cls._service, cls._resource)) #LINE# #TAB# return names"
Return DateTimeBaseInput class from django . forms . widgets module <code> def get_base_input(test=False): ,#LINE# #TAB# from django.forms.widgets import DateTimeBaseInput #LINE# #TAB# if 'get_context' in dir(DateTimeBaseInput) and not test: #LINE# #TAB# #TAB# base_input = DateTimeBaseInput #LINE# #TAB# else: #LINE# #TAB# #TAB# from bootstrap_datepicker_plus._compatibility import ( #LINE# #TAB# #TAB# #TAB# CompatibleDateTimeBaseInput #LINE# #TAB# #TAB# ) #LINE# #TAB# #TAB# base_input = CompatibleDateTimeBaseInput #LINE# #TAB# return base_input
"Group files that match by globs for merging rather than by explicit pairs  <code> def find_glob_matches(in_files, metadata): ","#LINE# #TAB# reg_files = copy.deepcopy(in_files) #LINE# #TAB# glob_files = [] #LINE# #TAB# for glob_search in [x for x in metadata.keys() if ""*"" in x]: #LINE# #TAB# #TAB# cur = [] #LINE# #TAB# #TAB# for fname in in_files: #LINE# #TAB# #TAB# #TAB# if fnmatch.fnmatch(fname, ""*/%s"" % glob_search): #LINE# #TAB# #TAB# #TAB# #TAB# cur.append(fname) #LINE# #TAB# #TAB# #TAB# #TAB# reg_files.remove(fname) #LINE# #TAB# #TAB# assert cur, ""Did not find file matches for %s"" % glob_search #LINE# #TAB# #TAB# glob_files.append(cur) #LINE# #TAB# return reg_files, glob_files"
simple routine to connect to an existing database and list a few stops bin / connect - tester --database_url sqlite:///gtfs.db _ no_gtfs_zip_needed _ <code> def db_connect_tester(): ,"#LINE# #TAB# from gtfsdb import Database, Stop, Route, StopTime #LINE# #TAB# args, kwargs = get_args('connect-tester') #LINE# #TAB# db = Database(**kwargs) #LINE# #TAB# for s in db.session.query(Stop).limit(2): #LINE# #TAB# #TAB# print(s.stop_name) #LINE# #TAB# for r in db.session.query(Route).limit(2): #LINE# #TAB# #TAB# print(r.route_name) #LINE# #TAB# stop_times = StopTime.get_departure_schedule(db.session, stop_id='11411') #LINE# #TAB# for st in stop_times: #LINE# #TAB# #TAB# print(st.get_direction_name()) #LINE# #TAB# #TAB# break"
"Partially taken from typing . get_type_hints  <code> def resolve_annotations(raw_annotations: Dict[str, AnyType], module_name: Optional[str]) -> Dict[str, AnyType]: ","#LINE# #TAB# if module_name: #LINE# #TAB# #TAB# base_globals: Optional[Dict[str, Any]] = sys.modules[module_name].__dict__ #LINE# #TAB# else: #LINE# #TAB# #TAB# base_globals = None #LINE# #TAB# annotations = {} #LINE# #TAB# for name, value in raw_annotations.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# value = ForwardRef(value, is_argument=False) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# value = _eval_type(value, base_globals, None) #LINE# #TAB# #TAB# except NameError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# annotations[name] = value #LINE# #TAB# return annotations"
"Use one of rules and apply it to identification <code> def use_rule(name, rule): ",#LINE# #TAB# result = name #LINE# #TAB# parts = rule.split() #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# if part[0] == '-': #LINE# #TAB# #TAB# #TAB# shift = len(part[1:]) #LINE# #TAB# #TAB# #TAB# result = result[:-shift] #LINE# #TAB# #TAB# elif part[0] == '+': #LINE# #TAB# #TAB# #TAB# result = result + part[1:] #LINE# #TAB# return result
Get as close to the repo root as possible  <code> def get_root_dir(): ,"#LINE# #TAB# root_dir = op.abspath(op.join(op.dirname(__file__), '..')) #LINE# #TAB# up_dir = op.join(root_dir, '..') #LINE# #TAB# if op.isfile(op.join(up_dir, 'setup.py')) and all(op.isdir(op.join( #LINE# #TAB# #TAB# up_dir, x)) for x in ('mne', 'examples', 'doc')): #LINE# #TAB# #TAB# root_dir = op.abspath(up_dir) #LINE# #TAB# return root_dir"
"Mark testcases missing in source code  <code> def mark_missing_in_source(config, testcases_root, svn_dir): ","#LINE# #TAB# if not svn_dir: #LINE# #TAB# #TAB# logger.warning( #LINE# #TAB# #TAB# #TAB# 'Not marking testcases missing in source code, SVN repo not specified.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return testcases_root #LINE# #TAB# utils.check_lookup_methods(config) #LINE# #TAB# xml_root = copy.deepcopy(testcases_root) #LINE# #TAB# missing_ids = _get_missing_in_source(config, xml_root, svn_dir) #LINE# #TAB# _append_missing_testcases(xml_root, missing_ids) #LINE# #TAB# return xml_root"
Pull Retrosheet game logs for a given season <code> def season_game_logs(season): ,"#LINE# #TAB# max_year = int(datetime.now().year) - 1 #LINE# #TAB# if season > max_year or season < 1871: #LINE# #TAB# #TAB# raise ValueError('Season must be between 1871 and {}'.format(max_year)) #LINE# #TAB# file_name = 'GL{}.TXT'.format(season) #LINE# #TAB# z = get_zip_file(gamelog_url.format(season)) #LINE# #TAB# data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='""') #LINE# #TAB# data.columns = gamelog_columns #LINE# #TAB# return data"
"Validate referenced values in functions  <code> def validate_function_ref_variables(value, input_names): ","#LINE# #TAB# add_info = '' #LINE# #TAB# value = value.replace('{{', '').replace('}}', '').strip() #LINE# #TAB# if value.startswith('inputs.'): #LINE# #TAB# #TAB# add_info = _validate_inputs_outputs_var_format(value) #LINE# #TAB# #TAB# if not add_info: #LINE# #TAB# #TAB# #TAB# name = value.split('.')[-1] #LINE# #TAB# #TAB# #TAB# if name not in input_names: #LINE# #TAB# #TAB# #TAB# #TAB# add_info = ( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# f'Invalid reference: {value}. Cannot find {name} in inputs.' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# add_info = 'Only ""inputs"" variables are allowed in functions.'"
Gets all of the logged event info changes for a mod : return : Returns a dict of new and modded event info entries : rtype : dict <code> def get_events_for_mod(mod: util.BcmlMod) ->{}: ,"#LINE# #TAB# events = {} #LINE# #TAB# if (mod.path / 'logs' / 'eventinfo.yml').exists(): #LINE# #TAB# #TAB# loader = yaml.CSafeLoader #LINE# #TAB# #TAB# yaml_util.add_constructors(loader) #LINE# #TAB# #TAB# events = yaml.load((mod.path / 'logs' / 'eventinfo.yml').open('r'), #LINE# #TAB# #TAB# #TAB# Loader=loader) #LINE# #TAB# return events"
"Deserializes a dict and its elements  <code> def deserialize_dict(data, boxed_type): ","#LINE# #TAB# return {k: _deserialize(v, boxed_type) #LINE# #TAB# #TAB# #TAB# for k, v in six.iteritems(data)}"
Method to create unique list of repositories from the list of repositories given . : param repo_list : List of repositories which might contain duplicates . : return : List of repositories with no duplicate in them  <code> def get_unique_repositories(repo_list): ,#LINE# #TAB# unique_list = list() #LINE# #TAB# included = defaultdict(lambda : False) #LINE# #TAB# for repo in repo_list: #LINE# #TAB# #TAB# if not included[repo.full_name]: #LINE# #TAB# #TAB# #TAB# unique_list.append(repo) #LINE# #TAB# #TAB# #TAB# included[repo.full_name] = True #LINE# #TAB# return unique_list
Find the hashtags in a given text <code> def find_hashtags(text): ,"#LINE# #TAB# hashtags = re.findall('(#\\w*[0-9a-zA-Z]+\\w*[0-9a-zA-Z])', text) #LINE# #TAB# return hashtags"
"Wrapper to get subject parameter from Nexus  <code> def get_nexus_subject_param(vicon, name, param): ","#LINE# #TAB# value = vicon.GetSubjectParam(name, param) #LINE# #TAB# if type(value) == tuple: #LINE# #TAB# #TAB# value = value[0] if value[1] else None #LINE# #TAB# return value"
"Check if the array type of ` a ` is of type specified by ` dtype ` parameter . Raises ------ TypeError If the array type does not match ` dtype ` <code> def check_dtype(a, dtype=np.floating): ","#LINE# #TAB# if not np.issubdtype(a.dtype, dtype): #LINE# #TAB# #TAB# msg = f'{dtype} type expected but found {a.dtype}' #LINE# #TAB# #TAB# raise TypeError(msg) #LINE# #TAB# return True"
"reserve_memory : size and keep time : param mem_size_str : 1 GB | 1 MB | 1 KB ... : param reserve_time : 60(s ) : return : <code> def reserve_memory(mem_size_str='1GB', reserve_time=3600): ",#LINE# #TAB# mem_size_byte = strsize_to_byte(mem_size_str) #LINE# #TAB# s = ' ' * mem_size_byte #LINE# #TAB# time.sleep(reserve_time) #LINE# #TAB# return True
"Return the field names ( channel names ) from dbf file fn . With usecols , return only names corresponding to the integers in usecols  <code> def channel_names(fn, usecols=None): ","#LINE# #TAB# with open(fn, 'rb') as fo: #LINE# #TAB# #TAB# names = dbfreader(fo).next() #LINE# #TAB# return usecols and [names[i] for i in usecols] or names"
Fix rare cloud layer issues <code> def sanitize_cloud(cloud: str) -> str: ,#LINE# #TAB# if len(cloud) < 4: #LINE# #TAB# #TAB# return cloud #LINE# #TAB# if not cloud[3].isdigit() and cloud[3] != '/': #LINE# #TAB# #TAB# if cloud[3] == 'O': #LINE# #TAB# #TAB# #TAB# cloud = cloud[:3] + '0' + cloud[4:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# cloud = cloud[:3] + cloud[4:] + cloud[3] #LINE# #TAB# return cloud
Input : { cluster_item : cluster_name } dictionary Output : { cluster_name : set([cluster_items ] ) } dictionary <code> def mapping_to_sets(mapping): ,"#LINE# #TAB# s = defaultdict(set) #LINE# #TAB# for m, k in mapping.items(): #LINE# #TAB# #TAB# s[k].add(m) #LINE# #TAB# s.default_factory = None #LINE# #TAB# return s"
"Finds a substring in a string . : param s : String you are searching in . : param char : String you are searching for . : return : If the substring is found it returns the index of the first occurrence , otherwise returns -1  <code> def find_str(s, char): ",#LINE# #TAB# index = 0 #LINE# #TAB# if char in s: #LINE# #TAB# #TAB# c = char[0] #LINE# #TAB# #TAB# for ch in s: #LINE# #TAB# #TAB# #TAB# if ch == c: #LINE# #TAB# #TAB# #TAB# #TAB# if s[index:index + len(char)] == char: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return index #LINE# #TAB# #TAB# #TAB# index += 1 #LINE# #TAB# return -1
"implement full DCM using PX4 native SD log data <code> def px4_update(IMU, ATT): ","#LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# px4_state = PX4_State(degrees(ATT.Roll), degrees(ATT.Pitch), #LINE# #TAB# #TAB# #TAB# degrees(ATT.Yaw), IMU._timestamp) #LINE# #TAB# gyro = Vector3(IMU.GyroX, IMU.GyroY, IMU.GyroZ) #LINE# #TAB# accel = Vector3(IMU.AccX, IMU.AccY, IMU.AccZ) #LINE# #TAB# px4_state.update(gyro, accel, IMU._timestamp) #LINE# #TAB# return px4_state"
"For commands executed directly via an SSH command - line , SSH looks up the user 's shell via getpwuid ( ) and only defaults to /bin / sh if that field is missing or empty  <code> def get_user_shell(): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# pw_shell = pwd.getpwuid(os.geteuid()).pw_shell #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pw_shell = None #LINE# #TAB# return pw_shell or '/bin/sh'
Returns dictionary of named gateways for given VPC { name : gateway } <code> def get_gateway_dict(vpc): ,#LINE# #TAB# return {u.get_name(gateway): gateway for gateway in vpc. #LINE# #TAB# #TAB# internet_gateways.all()}
"Gets the type.group.label + key spec from an Element in an Overlay  <code> def get_overlay_spec(o, k, v): ","#LINE# #TAB# k = wrap_tuple(k) #LINE# #TAB# return (type(v).__name__, v.group, v.label) + k if len(o.kdims) else (type #LINE# #TAB# #TAB# (v).__name__,) + k"
"Return image inside a QLabel object <code> def get_image_label(name, default=""not_found.png""): ","#LINE# #TAB# label = QLabel() #LINE# #TAB# label.setPixmap(QPixmap(get_image_path(name, default))) #LINE# #TAB# return label"
"Changes @thin , @auto , @shadow to bold <code> def colorize_headlines_visitor(c, p, item): ","#LINE# #TAB# t = p.h.split(None, 1) #LINE# #TAB# if t and t[0] in ['@file', '@thin', '@auto', '@shadow']: #LINE# #TAB# #TAB# f = item.font(0) #LINE# #TAB# #TAB# f.setBold(True) #LINE# #TAB# #TAB# item.setFont(0, f) #LINE# #TAB# raise leoPlugins.TryNext"
"Get an iterator if documents from a tree or document - like object  <code> def iter_documents(obj, path, ext): ","#LINE# #TAB# if is_tree(obj): #LINE# #TAB# #TAB# log.debug('iterating over tree...') #LINE# #TAB# #TAB# for document in obj.documents: #LINE# #TAB# #TAB# #TAB# path2 = os.path.join(path, document.prefix + ext) #LINE# #TAB# #TAB# #TAB# yield document, path2 #LINE# #TAB# else: #LINE# #TAB# #TAB# log.debug('iterating over document-like object...') #LINE# #TAB# #TAB# yield obj, path"
Returns lists of lists when given tuples of tuples <code> def to_lists(x): ,"#LINE# #TAB# if isinstance(x, tuple): #LINE# #TAB# #TAB# return [to_lists(el) for el in x] #LINE# #TAB# return x"
"Read gtfs trips.txt as a pandas dataframe Parameters ---------- textfile_path : str director of text file textfile : str name of text file Returns ------- df : pandas . DataFrame <code> def read_gtfs_trips(textfile_path, textfile): ","#LINE# #TAB# if textfile != 'trips.txt': #LINE# #TAB# #TAB# raise ValueError('{} is not a proper GTFS file name'.format(textfile)) #LINE# #TAB# df = pd.read_csv(os.path.join(textfile_path, textfile), dtype={ #LINE# #TAB# #TAB# 'trip_id': object, 'service_id': object, 'route_id': object, (7): #LINE# #TAB# #TAB# object}, low_memory=False) #LINE# #TAB# if len(df) == 0: #LINE# #TAB# #TAB# raise ValueError('{} has no records'.format(os.path.join( #LINE# #TAB# #TAB# #TAB# textfile_path, textfile))) #LINE# #TAB# df.rename(columns=lambda x: x.strip(), inplace=True) #LINE# #TAB# return df"
Search and return the location of the shared Git templates directory . : rtype : string or None <code> def git_templates(): ,"#LINE# #TAB# search_locations = ['/usr/share/git-core/templates', #LINE# #TAB# #TAB# '/usr/local/share/git-core/templates', #LINE# #TAB# #TAB# '/usr/local/git/share/git-core/templates'] #LINE# #TAB# for possible_location in search_locations: #LINE# #TAB# #TAB# if isdir(possible_location): #LINE# #TAB# #TAB# #TAB# return possible_location #LINE# #TAB# return None"
Will only give a find on classes that is a subclass of MicroService with the exception that the class is not allowed to be a direct ResponseMicroService or RequestMicroService  <code> def micro_service_filter(cls): ,"#LINE# #TAB# is_microservice_module = issubclass(cls, MicroService) #LINE# #TAB# is_correct_subclass = cls != MicroService and cls != ResponseMicroService and cls != RequestMicroService #LINE# #TAB# return is_microservice_module and is_correct_subclass"
Recursively count nodes to compute the cardinality of a subtree for each node <code> def postorder_count(node): ,#LINE# #TAB# card = 0 #LINE# #TAB# if node in left.keys(): #LINE# #TAB# #TAB# postorder_count(left[node]) #LINE# #TAB# #TAB# card += card_subtree[left[node]] #LINE# #TAB# if node in right.keys(): #LINE# #TAB# #TAB# postorder_count(right[node]) #LINE# #TAB# #TAB# card += card_subtree[right[node]] #LINE# #TAB# card_subtree[node] = 1 + card
"Clone plugin data of a dashboard entry  <code> def clone_plugin_data(dashboard_entry, request=None): ",#LINE# #TAB# if dashboard_entry: #LINE# #TAB# #TAB# plugin = dashboard_entry.get_plugin(request=request) #LINE# #TAB# #TAB# if plugin: #LINE# #TAB# #TAB# #TAB# plugin_data = plugin._clone_plugin_data(dashboard_entry) #LINE# #TAB# #TAB# #TAB# if plugin_data is None: #LINE# #TAB# #TAB# #TAB# #TAB# plugin_data = dashboard_entry.plugin_data #LINE# #TAB# #TAB# #TAB# return plugin_data
"Read only star fit parameters and coordinates from a FITS file . : param fits : An open fitsio . FITS object : param extname : The name of the extension to read from : returns : the arrays coords and params <code> def read_coords_params(cls, fits, extname): ","#LINE# #TAB# if extname not in fits: #LINE# #TAB# #TAB# raise RuntimeError('{0} not found in FITS object'.format(extname)) #LINE# #TAB# colnames = fits[extname].get_colnames() #LINE# #TAB# columns = ['u', 'v', 'params'] #LINE# #TAB# for key in columns: #LINE# #TAB# #TAB# if key not in colnames: #LINE# #TAB# #TAB# #TAB# raise RuntimeError('{0} not found in table'.format(key)) #LINE# #TAB# data = fits[extname].read(columns=columns) #LINE# #TAB# coords = np.array([data['u'], data['v']]).T #LINE# #TAB# params = data['params'] #LINE# #TAB# return coords, params"
"a - a ' : triborate bicarbonate [ FW86 ]  <code> def theta_b3o3oh4_hco3_fw86(T, P): ","#LINE# #TAB# theta = -0.1 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
get class device info <code> def get_section_list(school_id): ,"#LINE# #TAB# code, data = get_section_by_school(school_id=school_id) #LINE# #TAB# if code: #LINE# #TAB# #TAB# logger.error('Error, query section info, Detail: {}'.format(data)) #LINE# #TAB# return code, data"
Create a blueprint for replacing Loan Item  <code> def create_loan_replace_item_blueprint(app): ,"#LINE# #TAB# blueprint = Blueprint('invenio_circulation_loan_replace_item', __name__, #LINE# #TAB# #TAB# url_prefix='') #LINE# #TAB# _, view_options = _get_loan_endpoint_options(app) #LINE# #TAB# view_options['ctx']['loader'] = loan_replace_item_loader #LINE# #TAB# replace_item_view = LoanReplaceItemResource.as_view(LoanReplaceItemResource #LINE# #TAB# #TAB# .view_name.format(CIRCULATION_LOAN_PID_TYPE), **view_options) #LINE# #TAB# url = 'circulation/loans/<{0}:pid_value>/replace-item'.format( #LINE# #TAB# #TAB# _LOANID_CONVERTER) #LINE# #TAB# blueprint.add_url_rule(url, view_func=replace_item_view, methods=['POST']) #LINE# #TAB# return blueprint"
"Fix the values of all numeric variables in namespace recursively  <code> def fix_numeric(ns: Mapping[str, Value]) ->Dict[str, Value]: ","#LINE# #TAB# numeric = {} #LINE# #TAB# subs = {} #LINE# #TAB# ns = dict(ns) #LINE# #TAB# size = None #LINE# #TAB# while len(ns) != size: #LINE# #TAB# #TAB# size = len(ns) #LINE# #TAB# #TAB# for k, v in list(ns.items()): #LINE# #TAB# #TAB# #TAB# if v.is_numeric: #LINE# #TAB# #TAB# #TAB# #TAB# numeric[k] = ns.pop(k) #LINE# #TAB# #TAB# #TAB# #TAB# subs[k] = v.value #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# ns[k] = v.replace(**subs) #LINE# #TAB# return {**ns, **numeric}"
"Return a concatenation of prefix , object and suffix . Returns empty string if obj is not "" truthy "" ( i.e. , not None or empty container ) <code> def conditional_str(prefix, obj, suffix): ","#LINE# #TAB# return ' '.join([prefix, str(obj), suffix]) if obj is not None and len(obj #LINE# #TAB# #TAB# ) > 0 else ''"
"Compute an exit code for mutmut mutation testing <code> def compute_exit_code(config, exception=None): ",#LINE# #TAB# code = 0 #LINE# #TAB# if exception is not None: #LINE# #TAB# #TAB# code = code | 1 #LINE# #TAB# if config.surviving_mutants > 0: #LINE# #TAB# #TAB# code = code | 2 #LINE# #TAB# if config.surviving_mutants_timeout > 0: #LINE# #TAB# #TAB# code = code | 4 #LINE# #TAB# if config.suspicious_mutants > 0: #LINE# #TAB# #TAB# code = code | 8 #LINE# #TAB# return code
"ISRI Arabic stemmer based on algorithm : Arabic Stemming without a root dictionary . Information Science Research Institute . University of Nevada , Las Vegas , USA  <code> def nltk_isri_stemmer(input_dict): ","#LINE# #TAB# return {'tagger': {'object': nltk.stem.isri.ISRIStemmer(), 'function': #LINE# #TAB# #TAB# 'stem'}}"
"Decode a Base X encoded string into the number : param str base_num : Number encoded in the given base : param int base : Length of the base to use : param str alphabet : The alphabet to use for encoding <code> def from_base_n(base_num, base=None, alphabet=None): ",#LINE# #TAB# if alphabet is None: #LINE# #TAB# #TAB# alphabet = _DEFAULT_BASE_ALPHABET #LINE# #TAB# if base is None: #LINE# #TAB# #TAB# base = len(alphabet) #LINE# #TAB# if not 0 <= base <= len(alphabet): #LINE# #TAB# #TAB# raise ValueError('Invalid base length: %s' % base) #LINE# #TAB# strlen = len(base_num) #LINE# #TAB# num = 0 #LINE# #TAB# idx = 0 #LINE# #TAB# for char in base_num: #LINE# #TAB# #TAB# power = strlen - (idx + 1) #LINE# #TAB# #TAB# num += alphabet.index(char) * base ** power #LINE# #TAB# #TAB# idx += 1 #LINE# #TAB# return num
Removes duckling entity predictions <code> def remove_duckling_entities(entity_predictions): ,#LINE# #TAB# patched_entity_predictions = [] #LINE# #TAB# for entities in entity_predictions: #LINE# #TAB# #TAB# patched_entities = [] #LINE# #TAB# #TAB# for e in entities: #LINE# #TAB# #TAB# #TAB# if e['extractor'] not in duckling_extractors: #LINE# #TAB# #TAB# #TAB# #TAB# patched_entities.append(e) #LINE# #TAB# #TAB# patched_entity_predictions.append(patched_entities) #LINE# #TAB# return patched_entity_predictions
Return the attributes of : class:`settings . G ` to their original values <code> def reset_attributes(cls): ,#LINE# #TAB# cls.Env = None #LINE# #TAB# cls.log = print #LINE# #TAB# cls.debug = print #LINE# #TAB# cls.warn = warnings.warn
Convert nd4j array to numpy array <code> def to_numpy(nd4j_array): ,"#LINE# #TAB# buff = nd4j_array.data() #LINE# #TAB# address = buff.pointer().address() #LINE# #TAB# dtype = get_context_dtype() #LINE# #TAB# mapping = {'double': ctypes.c_double, 'float': ctypes.c_float} #LINE# #TAB# Pointer = ctypes.POINTER(mapping[dtype]) #LINE# #TAB# pointer = ctypes.cast(address, Pointer) #LINE# #TAB# np_array = np.ctypeslib.as_array(pointer, tuple(nd4j_array.shape())) #LINE# #TAB# return np_array"
Get creator function by name  <code> def get_creator_by_name(name): ,"#LINE# #TAB# return {'docker(container)': Container.creator, #LINE# #TAB# #TAB# #TAB# 'shell': Bash.creator, 'docker(image)': Image.creator, #LINE# #TAB# #TAB# #TAB# 'python': Script.creator, 'packer': Packer.creator, #LINE# #TAB# #TAB# #TAB# 'ansible(simple)': Ansible.creator}[name]"
"Returns attribute value from downloaded data . Parameters : - data : dict - The result of ` get_data ` function - attribute_name : str - Attribute name from ` API_FIELDS ` <code> def get_attribute(data: dict, attribute_name: str): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# value = data[API_FIELDS[attribute_name]]['1'][0] #LINE# #TAB# #TAB# if len(value.keys()) > 1: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# #TAB# return value['val'] #LINE# #TAB# except (KeyError, IndexError): #LINE# #TAB# #TAB# return value"
"Function takes input of auth class object auth object and URL and returns a BOOL of TRUE if the authentication was successful  <code> def check_imc_creds(auth, url): ","#LINE# #TAB# test_url = '/imcrs' #LINE# #TAB# f_url = url + test_url #LINE# #TAB# try: #LINE# #TAB# #TAB# response = requests.get(f_url, auth=auth, headers=HEADERS, verify=False) #LINE# #TAB# #TAB# return bool(response.status_code == 200) #LINE# #TAB# except requests.exceptions.RequestException as error: #LINE# #TAB# #TAB# return ""Error:\n"" + str(error) + "" test_imc_creds: An Error has occured"""
"Standard simple csv loader <code> def load_csv(base_file, csv_name, sep=',', convert_float=False): ","#LINE# #TAB# filepath = dirname(abspath(base_file)) #LINE# #TAB# filename = join(filepath, csv_name) #LINE# #TAB# engine = 'python' if sep != ',' else 'c' #LINE# #TAB# float_precision = {} #LINE# #TAB# if engine == 'c': #LINE# #TAB# #TAB# float_precision = {'float_precision': 'high'} #LINE# #TAB# data = read_csv(filename, sep=sep, engine=engine, **float_precision) #LINE# #TAB# if convert_float: #LINE# #TAB# #TAB# data = data.astype(float) #LINE# #TAB# return data"
"create sql - code for search <code> def search_mailingaddr_valid(cls, name, clause): ","#LINE# #TAB# tab_validadr = cls.get_mailingaddr_valid_sql() #LINE# #TAB# tab_party = cls.__table__() #LINE# #TAB# Operator = fields.SQL_OPERATORS[clause[1]] #LINE# #TAB# if clause[2] == True: #LINE# #TAB# #TAB# adrvalid = tab_party.select(tab_party.id, where=tab_party.id.in_( #LINE# #TAB# #TAB# #TAB# tab_validadr)) #LINE# #TAB# else: #LINE# #TAB# #TAB# adrvalid = tab_party.select(tab_party.id, where=~tab_party.id.in_( #LINE# #TAB# #TAB# #TAB# tab_validadr)) #LINE# #TAB# return [('id', 'in', adrvalid)]"
"A helper function for cyclic shifting a one dimensional array  <code> def cycle_vector(vector, vector_length, shift): ",#LINE# #TAB# result = numpy.empty(vector_length) #LINE# #TAB# result[0:shift] = vector[-shift:] #LINE# #TAB# result[shift:] = vector[0:-shift] #LINE# #TAB# return result
Convert camel case name to underscore name  <code> def camel_to_underscore(name): ,"#LINE# #TAB# name = re.sub(r'(?<!\b)(?<!_)([A-Z][a-z])', r'_\1', name) #LINE# #TAB# name = re.sub(r'(?<!\b)(?<!_)([a-z])([A-Z])', r'\1_\2', name) #LINE# #TAB# name = name.lower() #LINE# #TAB# return name"
Gets the APIC class to an acitoolkit class mapping dictionary : returns : dict of APIC class names to acitoolkit classes <code> def get_toolkit_to_apic_classmap(cls): ,"#LINE# #TAB# return {'fvAp': AppProfile, 'fvBD': BridgeDomain, 'vzCPIf': #LINE# #TAB# #TAB# ContractInterface, 'fvCtx': Context, 'vzBrCP': Contract, 'vzFilter': #LINE# #TAB# #TAB# Filter, 'vzTaboo': Taboo, 'l3extOut': OutsideL3}"
Hexadecimal byte validator for argparse  <code> def parse_hex(value): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return bytes.fromhex(''.join(value.split())) #LINE# #TAB# except ValueError as e: #LINE# #TAB# #TAB# raise ArgumentTypeError('Not hexadecimal.') from e
Take the type string of a Node and create the queryable type string : param type_string : the plugin_type_string attribute of a Node : return : the type string that can be used to query for <code> def get_query_type_from_type_string(type_string): ,"#LINE# #TAB# is_valid_node_type_string(type_string, raise_on_false=True) #LINE# #TAB# if type_string == '': #LINE# #TAB# #TAB# return '' #LINE# #TAB# type_path = type_string.rsplit('.', 2)[0] #LINE# #TAB# type_string = type_path + '.' #LINE# #TAB# return type_string"
"Calculates eigenvectors and values of an extended contact map : param contactMap : Contact map : type contactMap : np.array : returns : ( numpy.ndarray , numpy.ndarray ) -- eigenvalues , eigenvectors <code> def calculate_contact_map_eigen(contactMap): ","#LINE# #TAB# nLig, nCA = contactMap.shape #LINE# #TAB# extendedCM = np.zeros((nLig + nCA, nLig + nCA)) #LINE# #TAB# extendedCM[nCA:, :nCA] = contactMap #LINE# #TAB# extendedCM[:nCA, nCA:] = contactMap.T #LINE# #TAB# assert (extendedCM == extendedCM.T).all( #LINE# #TAB# #TAB# ), 'Extended ContactMap not symmetric' #LINE# #TAB# eiv, eic = np.linalg.eigh(extendedCM) #LINE# #TAB# return eiv, eic"
"common method to account for vtk endian quirks and efficiently adding points <code> def numpy_to_vtk_points(nodes, points=None, dtype='<f', deep=1): ","#LINE# #TAB# assert isinstance(nodes, np.ndarray), type(nodes) #LINE# #TAB# if points is None: #LINE# #TAB# #TAB# points = vtk.vtkPoints() #LINE# #TAB# #TAB# nnodes, ndim = nodes.shape #LINE# #TAB# #TAB# assert ndim == 3, nodes.shape #LINE# #TAB# #TAB# points.SetNumberOfPoints(nnodes) #LINE# #TAB# #TAB# nodes = np.asarray(nodes, dtype=np.dtype(dtype)) #LINE# #TAB# points_array = numpy_to_vtk(num_array=nodes, deep=deep, array_type=vtk. #LINE# #TAB# #TAB# VTK_FLOAT) #LINE# #TAB# points.SetData(points_array) #LINE# #TAB# return points"
checks if mac address found from nmap that matches raspberry pi Accepts : ip_address var as string Returns : nothing Prints : found ip of pi if found <code> def check_macs(ip_address): ,"#LINE# #TAB# data = check_output(f'nmap -sP {ip_address}', shell=True) #LINE# #TAB# if 'B8:27:EB' in str(data) or 'DC:A6:32' in str(data): #LINE# #TAB# #TAB# print(f'Found pi: {ip_address}') #LINE# #TAB# else: #LINE# #TAB# #TAB# return #LINE# #TAB# return"
"suggestParser(ext , text ) Given a filename extension and text , returns the name of the suggested parser corresponding to the language of the file . See also registerFilenameExtension ( ) and registerShebangKeyword ( ) <code> def suggest_parser(cls, ext, text): ",#LINE# #TAB# parser = cls.suggest_parserfromText(text) #LINE# #TAB# if parser == '': #LINE# #TAB# #TAB# parser = cls.suggest_parserfromFilenameExtension(ext) #LINE# #TAB# parser = cls.getParserByName(parser).disambiguate(text) #LINE# #TAB# return parser
"Read a SourceList object from the path of a directory containing source files . Parameters ---------- path : str Directory containing ` mpdaf.sdetect . Source ` files <code> def from_path(cls, path): ",#LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# raise ValueError('Invalid path: {}'.format(path)) #LINE# #TAB# slist = cls() #LINE# #TAB# for f in glob.glob(path + '/*.fits'): #LINE# #TAB# #TAB# slist.append(cls.source_class.from_file(f)) #LINE# #TAB# return slist
Returns the image URLs and filenames for the chapter Finds the image URLs from the ' chapter_preload_images ' variable from the chapter page <code> def get_chapter_urls(soup): ,"#LINE# #TAB# div_tag = soup.find('div', {'class': 'reading-content'}) #LINE# #TAB# js_tag = div_tag.script #LINE# #TAB# img_vars = re.findall('http[^,]*\\.jpg', js_tag.text) #LINE# #TAB# img_urls = (url.replace('\\', '') for url in img_vars) #LINE# #TAB# return [[url.split('/')[-1], url] for url in img_urls]"
Verify is a list is a list of points <code> def is_point(pointlist): ,"#LINE# #TAB# if len(pointlist) in [2, 3]: #LINE# #TAB# #TAB# if isinstance(pointlist[0], (int, float)) and isinstance(pointlist[ #LINE# #TAB# #TAB# #TAB# 1], (int, float)): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Convert UTC to GPS time : param timestamp : UTC timestamp in seconds . : return : GPS timestamp in seconds  <code> def utc_to_gps(timestamp): ,"#LINE# #TAB# offset = next((seconds for date, seconds in LEAP_SECONDS if timestamp >= #LINE# #TAB# #TAB# utc_from_string(date)), 0) #LINE# #TAB# return timestamp + offset"
Check if directory exists .. todo : : Should check that it is actually a directory Parameters ---------- path : str Path to check Returns ------- out : bool True if directory exists <code> def check_dir_existence(path: str) ->bool: ,#LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Take a name for a language or a feature which has come from somewhere like a CLDF dataset and make sure it does not contain any characters which will cause trouble for BEAST or postanalysis tools  <code> def sanitise_name(name): ,"#LINE# #TAB# name = name.replace(' ', '_') #LINE# #TAB# return name"
Calculate how many items must be in the collection to satisfy this slice returns ` None ` for slices may vary based on the length of the underlying collection such as ` lst[-1 ] ` or ` lst [ : : ] ` <code> def slice_required_len(slice_obj): ,#LINE# #TAB# if slice_obj.step and slice_obj.step != 1: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.start is None and slice_obj.stop is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.start and slice_obj.start < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.stop and slice_obj.stop < 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# if slice_obj.stop: #LINE# #TAB# #TAB# if slice_obj.start and slice_obj.start > slice_obj.stop: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# #TAB# return slice_obj.stop #LINE# #TAB# return slice_obj.start + 1
get last update time prints out the date and returns the date(str ) <code> def get_last_update(): ,"#LINE# #TAB# last_update = open(datapath(False, 'Spyder', 'datefile.txt')).readlines()[0 #LINE# #TAB# #TAB# ] #LINE# #TAB# print('Last update time: ' + last_update) #LINE# #TAB# return last_update"
"Get the version which is currently configured by the package <code> def get_version(package_name, ignore_cache=False): ","#LINE# #TAB# if ignore_cache: #LINE# #TAB# #TAB# with microcache.temporarily_disabled(): #LINE# #TAB# #TAB# #TAB# found = helpers.regex_in_package_file( #LINE# #TAB# #TAB# #TAB# #TAB# VERSION_SET_REGEX, '_version.py', package_name, return_match=True #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# found = helpers.regex_in_package_file( #LINE# #TAB# #TAB# #TAB# VERSION_SET_REGEX, '_version.py', package_name, return_match=True #LINE# #TAB# #TAB# ) #LINE# #TAB# if found is None: #LINE# #TAB# #TAB# raise ProjectError('found {}, but __version__ is not defined') #LINE# #TAB# current_version = found['version'] #LINE# #TAB# return current_version"
"AICc[K ] = AIC[K ] + 2*(K+1)*(K+2)/(NF - K-2 ) Assumiamo di tenere tutti i k < = K <code> def dct_aicc(yk, theory_var=None): ","#LINE# #TAB# aic = dct_AIC(yk, theory_var) #LINE# #TAB# KK = np.arange(yk.size - 2) #LINE# #TAB# aic[:-2] = aic[:-2] + 2.0 * (KK + 1) * (KK + 2) / (yk.size - KK - 2.0) #LINE# #TAB# aic[-2] = aic[-3] #LINE# #TAB# aic[-1] = aic[-3] #LINE# #TAB# return aic"
"new_y = dp_unnormalise(y , normalisation_parameters ) Unnormalises the data , y , using the parameters passed in normalisation_parameters  <code> def dp_unnormalise(y, normalisation_parameters): ",#LINE# #TAB# y = y * normalisation_parameters['std'] #LINE# #TAB# y = y + normalisation_parameters['mean'] #LINE# #TAB# return y
"Converts a var_instance to a var array one <code> def to_vararray(var_instance, bounds): ","#LINE# #TAB# #TAB# assert isinstance(var_instance, SymbolVAR) #LINE# #TAB# #TAB# from symbols import BOUNDLIST #LINE# #TAB# #TAB# from symbols import VARARRAY #LINE# #TAB# #TAB# assert isinstance(bounds, BOUNDLIST) #LINE# #TAB# #TAB# var_instance.__class__ = VARARRAY #LINE# #TAB# #TAB# var_instance.class_ = CLASS.array #LINE# #TAB# #TAB# var_instance.bounds = bounds #LINE# #TAB# #TAB# return var_instance"
"Return True if at least one command requiring ` ` cmake ` ` to run is found in ` ` commands ` `  <code> def should_run_cmake(commands, cmake_with_sdist): ","#LINE# #TAB# for expected_command in ['build', 'build_ext', 'develop', 'install', #LINE# #TAB# #TAB# 'install_lib', 'bdist', 'bdist_dumb', 'bdist_egg', 'bdist_rpm', #LINE# #TAB# #TAB# 'bdist_wininst', 'bdist_wheel', 'test']: #LINE# #TAB# #TAB# if expected_command in commands: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# if 'sdist' in commands and cmake_with_sdist: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"Calculate Dest values using Anne Chao 's harmonic mean  <code> def multilocus_d_est(Ht_est, Hs_est, n): ","#LINE# #TAB# pairs = zip(Ht_est, Hs_est) #LINE# #TAB# Dest_values = [D_est(pair[0], pair[1], 2) for pair in pairs] #LINE# #TAB# D_est_ = harmonic_mean_chao(Dest_values) #LINE# #TAB# return D_est_"
""" c - a : samarium chloride [ PM73 ]  <code> def bc_sm_cl_pm73(T, P): ","#LINE# #TAB# b0 = 0.933 * 2 / 3 #LINE# #TAB# b1 = 8.273 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.0728 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Sm'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
Create unique labels  <code> def get_labels(labels): ,"#LINE# #TAB# label_u = unique_labels(labels) #LINE# #TAB# label_u_line = [i + ""_line"" for i in label_u] #LINE# #TAB# return label_u, label_u_line"
Glob for files to be registered on an IBL session : param ses_path : pathlib . Path of the session : return : a list of files to potentially be registered <code> def glob_session(ses_path): ,#LINE# #TAB# fl = [] #LINE# #TAB# for gp in REGISTRATION_GLOB_PATTERNS: #LINE# #TAB# #TAB# fl.extend(list(ses_path.glob(gp))) #LINE# #TAB# return fl
"c - c'-a : potassium sodium chloride [ PP87ii ]  <code> def psi_k_na_cl_pp87ii(T, P): ","#LINE# #TAB# psi = -0.00681 + 1.68e-05 * T #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"Create a config from a string and optional dict . See get_default  <code> def from_string(config_string, based_on=None, filename='<string>'): ","#LINE# #TAB# defaults = based_on if based_on is not None else get_default().__dict__ #LINE# #TAB# code = compile(config_string, filename, 'exec') #LINE# #TAB# config = imp.new_module('config') #LINE# #TAB# config.__dict__.update(defaults) #LINE# #TAB# exec(code, config.__dict__) #LINE# #TAB# return config"
Detect if the fetched page is last page of search results  <code> def is_last_page(grab): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# next_link = grab.xpath_one('//a[@id=""pnnext""]') #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# logging.debug('No results found') #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Returns distance of each item in qs from the mean . WARNING : Slow method used only for compatibility testing . Do not use  <code> def dists_from_mean_slow(qs): ,"#LINE# #TAB# n = len(qs) #LINE# #TAB# average = mean(qs, axis=0) #LINE# #TAB# result = zeros(n, 'float64') #LINE# #TAB# for i in range(n): #LINE# #TAB# #TAB# result[i] = euclidean_distance(average, qs[i]) #LINE# #TAB# return result"
Calculate the pixel coordinates from the standard pixel - map file by default it gets rotated by 90 degrees clockwise to show the same orientation as MARS and fact - tools <code> def get_pixel_coords(): ,"#LINE# #TAB# df = get_pixel_dataframe() #LINE# #TAB# return df.x.values, df.y.values"
"Helper method that wraps the package_search action . * unless overridden , sorts results by metadata_modified date * unless overridden , sets a default item limit <code> def package_search(data_dict): ","#LINE# #TAB# context = {u'model': model, u'session': model.Session, u'user': g.user, #LINE# #TAB# #TAB# u'auth_user_obj': g.userobj} #LINE# #TAB# if u'sort' not in data_dict or not data_dict['sort']: #LINE# #TAB# #TAB# data_dict['sort'] = u'metadata_modified desc' #LINE# #TAB# if u'rows' not in data_dict or not data_dict['rows']: #LINE# #TAB# #TAB# data_dict['rows'] = ITEMS_LIMIT #LINE# #TAB# query = logic.get_action(u'package_search')(context, data_dict.copy()) #LINE# #TAB# return query['count'], query['results']"
Create the n - dimensional NDNT kernel . Parameters ---------- shape : tuple of int The shape of the neighborhood around each pixel . Returns ------- kernel : array_like <code> def create_kernel(shape): ,"#LINE# #TAB# indices = np.array(list(itertools.product([-1, 0], repeat=len(shape)))) #LINE# #TAB# ref = np.sum(indices[0]) & 1 #LINE# #TAB# parity = np.array([(1 if -sum(i) & 1 == ref else -1) for i in indices]) #LINE# #TAB# indices = tuple([indices[:, (i)] for i in range(indices.shape[1])]) #LINE# #TAB# kernel = np.zeros(shape, dtype=np.float32) #LINE# #TAB# kernel[indices] = parity #LINE# #TAB# return kernel"
Function to create a dictionary containing edges of y reversed . INPUTS = = = = = = y : ADnum OUTPUTS = = = = = = = A dictionary <code> def reverse_graph(y): ,"#LINE# #TAB# d = y.graph #LINE# #TAB# parents = {} #LINE# #TAB# for key in d: #LINE# #TAB# #TAB# neighbors = d[key] #LINE# #TAB# #TAB# for neighbor in neighbors: #LINE# #TAB# #TAB# #TAB# if neighbor[0] not in parents: #LINE# #TAB# #TAB# #TAB# #TAB# parents[neighbor[0]] = [] #LINE# #TAB# #TAB# #TAB# parents[neighbor[0]].append((key, neighbor[1])) #LINE# #TAB# return parents"
"Returns a tuplet containing : - the result of the depth_first_search ( ) function starting at ' root ' ( is a tuplet ) - a dot format output of the given graph ( display it using graphviz dotty command ) <code> def output_graph(graph, root=None): ","#LINE# #TAB# dfs = depth_first_search(graph, root) #LINE# #TAB# dot = write(graph) #LINE# #TAB# return [dfs, dot]"
"Test whether @uri should be filtered by @filter_list  <code> def is_uri_to_be_filtered(uri, filter_list): ","#LINE# #TAB# match = False #LINE# #TAB# if filter_list: #LINE# #TAB# #TAB# for uri_filter in filter_list: #LINE# #TAB# #TAB# #TAB# if re.search(uri_filter, uri, flags=re.IGNORECASE): #LINE# #TAB# #TAB# #TAB# #TAB# match = True #LINE# #TAB# #TAB# return match"
"Fetch framework variable according given attr_name . Return a new reusing variable through create_parameter way Args : attr_name : string , attr name of parameter Returns : framework_var : framework . Varialbe <code> def fetch_framework_var(attr_name): ","#LINE# #TAB# scope = fluid.executor.global_scope() #LINE# #TAB# core_var = scope.find_var(attr_name) #LINE# #TAB# if core_var == None: #LINE# #TAB# #TAB# raise KeyError( #LINE# #TAB# #TAB# #TAB# 'Unable to find the variable:{}. Synchronize paramsters before initialization or attr_name does not exist.' #LINE# #TAB# #TAB# #TAB# .format(attr_name)) #LINE# #TAB# shape = core_var.get_tensor().shape() #LINE# #TAB# framework_var = fluid.layers.create_parameter(shape=shape, dtype= #LINE# #TAB# #TAB# 'float32', attr=fluid.ParamAttr(name=attr_name)) #LINE# #TAB# return framework_var"
"Safely run command in certain path <code> def run_command(path, command): ","#LINE# #TAB# path = str(path) #LINE# #TAB# if not Path(path).is_dir(): #LINE# #TAB# #TAB# raise ValueError('{} is not a directory'.format(path)) #LINE# #TAB# command_ = 'cd {path} && {cmd}'.format(path=quote(path), cmd=command) #LINE# #TAB# out = subprocess.check_output(command_, shell=True) #LINE# #TAB# s = out.decode('utf-8') #LINE# #TAB# if s[-1:] == '\n': #LINE# #TAB# #TAB# s = s[:-1] #LINE# #TAB# return s"
Serializes the request payload to match the format requested by NBG APIs . Serializations : - datetime : ( Z - suffixed ISO format ) <code> def serialize_request_payload(data: dict) ->dict: ,"#LINE# #TAB# payload = data.copy() #LINE# #TAB# for key, value in payload.items(): #LINE# #TAB# #TAB# if isinstance(value, datetime): #LINE# #TAB# #TAB# #TAB# payload[key] = _serialize_datetime(value) #LINE# #TAB# return payload"
Load any supported file format . : param Path path : File to load . : return : A dictionary with the parsed content . : rtype : dict <code> def load_file(path): ,"#LINE# #TAB# extension = path.suffix #LINE# #TAB# if extension not in load_file.supported_formats: #LINE# #TAB# #TAB# raise RuntimeError( #LINE# #TAB# #TAB# #TAB# 'Unknown file format ""{}"" for file {}. Supported formats are :{}.' #LINE# #TAB# #TAB# #TAB# .format(extension, path, ', '.join(sorted(load_file. #LINE# #TAB# #TAB# #TAB# supported_formats.keys())))) #LINE# #TAB# content = load_file.supported_formats[extension](path) #LINE# #TAB# return content"
Remove supporting and input files after testing is complete <code> def remove_supporting(written_files: List[str]): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# for supporting_file in written_files: #LINE# #TAB# #TAB# #TAB# os.remove(supporting_file) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# pass
Assemble memory configuration for a Spark session : param memory : string with memory configuration for spark : return : memory configuration <code> def get_spark_memory_config(memory=SparkDefault.MEMORY): ,"#LINE# #TAB# if not memory: #LINE# #TAB# #TAB# return () #LINE# #TAB# memory = memory.split(',') #LINE# #TAB# if len(memory) != 3: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Expected 3 memory parameters but got %s. Please check --help to see how -m/--memory should be used.' #LINE# #TAB# #TAB# #TAB# % len(memory)) #LINE# #TAB# return 'spark.executor.memory=' + memory[0 #LINE# #TAB# #TAB# ], 'spark.driver.memory=' + memory[1 #LINE# #TAB# #TAB# ], 'spark.driver.maxResultSize=' + memory[2]"
"Gradient of slice operation <code> def minpy_getitem_grad(arr, index, g): ","#LINE# #TAB# ret = np.zeros_like(arr) #LINE# #TAB# np.add.at(ret, index, g) #LINE# #TAB# return ret"
r Remove all negative entries from sparse matrix  <code> def remove_negative_entries(A): ,"#LINE# #TAB# A = A.tocoo() #LINE# #TAB# data = A.data #LINE# #TAB# row = A.row #LINE# #TAB# col = A.col #LINE# #TAB# pos = data > 0.0 #LINE# #TAB# datap = data[pos] #LINE# #TAB# rowp = row[pos] #LINE# #TAB# colp = col[pos] #LINE# #TAB# Aplus = coo_matrix((datap, (rowp, colp)), shape=A.shape) #LINE# #TAB# return Aplus"
"Apply the index of each item  <code> def apply_items_index(mcs, items): ","#LINE# #TAB# flag_count = 0 #LINE# #TAB# item_size = 0 #LINE# #TAB# for item in items.values(): #LINE# #TAB# #TAB# if isinstance(item, Flag): #LINE# #TAB# #TAB# #TAB# item.set_index(flag_count) #LINE# #TAB# #TAB# #TAB# flag_count += 1 #LINE# #TAB# #TAB# #TAB# item_size = flag_count // 8 + (flag_count % 8 != 0) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# item.set_index(item_size) #LINE# #TAB# #TAB# #TAB# item_size += item.size #LINE# #TAB# return item_size"
"Adapter for rendering an object of : class : ` crabpy.gateway.capakey . Sectie ` to json  <code> def item_sectie_adapter(obj, request): ","#LINE# #TAB# return {'id': obj.id, 'afdeling': {'id': obj.afdeling.id, 'naam': obj. #LINE# #TAB# #TAB# afdeling.naam, 'gemeente': {'id': obj.afdeling.gemeente.id, 'naam': #LINE# #TAB# #TAB# obj.afdeling.gemeente.naam}}, 'centroid': obj.centroid, #LINE# #TAB# #TAB# 'bounding_box': obj.bounding_box}"
Get the current security policy : param user : user to use for security policy <code> def get_security_policy(user: Optional[IPrincipal]=None) ->ISecurityPolicy: ,"#LINE# #TAB# if user is None: #LINE# #TAB# #TAB# user = get_authenticated_user() #LINE# #TAB# #TAB# if user is None: #LINE# #TAB# #TAB# #TAB# from guillotina.auth.users import AnonymousUser #LINE# #TAB# #TAB# #TAB# user = AnonymousUser() #LINE# #TAB# security_policies = task_vars.security_policies.get() #LINE# #TAB# if security_policies is None: #LINE# #TAB# #TAB# security_policies = {} #LINE# #TAB# #TAB# task_vars.security_policies.set(security_policies) #LINE# #TAB# if user.id not in security_policies: #LINE# #TAB# #TAB# security_policies[user.id] = get_adapter(user, ISecurityPolicy) #LINE# #TAB# return security_policies[user.id]"
! @brief Convert a byte array into a halfword array <code> def byte_list_to_u16le_list(byteData): ,"#LINE# #TAB# data = [] #LINE# #TAB# for i in range(0, len(byteData), 2): #LINE# #TAB# #TAB# data.append(byteData[i] | byteData[i + 1] << 8) #LINE# #TAB# return data"
"Standard Negative Log Likelihood . Fit 1 parameter ` ` n_signal ` ` , in 2 dimensions ` ` alpha , energy ` ` ( ` ` alpha ` ` is separation to source )  <code> def std_nlnlike2d(params, endog, n_bkg_exp, signal_2d, background_2d): ","#LINE# #TAB# n_sig = params[0] #LINE# #TAB# alpha, energy = endog #LINE# #TAB# alpha = np.atleast_1d(alpha) #LINE# #TAB# energy = np.atleast_1d(energy) #LINE# #TAB# n_tot = n_bkg_exp #LINE# #TAB# s = signal_2d.pdf(alpha, energy) #LINE# #TAB# b = background_2d.pdf(alpha, energy) #LINE# #TAB# sumlogl = np.ma.sum(np.ma.log(n_sig / n_tot * s + (n_tot - n_sig) / #LINE# #TAB# #TAB# n_tot * b)) #LINE# #TAB# return -sumlogl"
Get Flow File Safetly <code> def get_flow_file(flow_name): ,"#LINE# #TAB# flow_name = flow_name + '.flow' #LINE# #TAB# flow_dir = settings.FLOWS_DIR #LINE# #TAB# requested_path = os.path.join(flow_dir, flow_name) #LINE# #TAB# pfx = os.path.commonprefix((os.path.realpath(requested_path), flow_dir)) #LINE# #TAB# if pfx == flow_dir and os.path.exists(requested_path): #LINE# #TAB# #TAB# return requested_path #LINE# #TAB# return False"
adds SBO terms for demands and exchanges <code> def add_sbo(model): ,"#LINE# #TAB# for r in model.reactions: #LINE# #TAB# #TAB# if r.annotation.get(""sbo""): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if len(r.metabolites) != 1: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# met_id = list(r._metabolites)[0].id #LINE# #TAB# #TAB# if r.id.startswith(""EX_"") and r.id == ""EX_"" + met_id: #LINE# #TAB# #TAB# #TAB# r.annotation[""sbo""] = ""SBO:0000627"" #LINE# #TAB# #TAB# elif r.id.startswith(""DM_"") and r.id == ""DM_"" + met_id: #LINE# #TAB# #TAB# #TAB# r.annotation[""sbo""] = ""SBO:0000628"""
Compute the rectangle from a WKT string  <code> def wkt_to_rectangle(extent): ,"#LINE# #TAB# geometry = QgsGeometry.fromWkt(extent) #LINE# #TAB# if not geometry.isGeosValid(): #LINE# #TAB# #TAB# return None #LINE# #TAB# polygon = geometry.asPolygon()[0] #LINE# #TAB# if len(polygon) != 5: #LINE# #TAB# #TAB# return None #LINE# #TAB# if polygon[0] != polygon[4]: #LINE# #TAB# #TAB# return None #LINE# #TAB# rectangle = QgsRectangle( #LINE# #TAB# #TAB# QgsPointXY(polygon[0].x(), polygon[0].y()), #LINE# #TAB# #TAB# QgsPointXY(polygon[2].x(), polygon[2].y())) #LINE# #TAB# return rectangle"
"Assembly Module Scope Name and model scope name <code> def get_model_var_scope(module_scope, model_name): ",#LINE# #TAB# model_var_scope = module_scope + '_' + model_name #LINE# #TAB# return model_var_scope
Flip RGB to BGR image data ( numpy ndarray ) . Also accepts rgbA / bgrA and single channel images without crashing  <code> def flip_layers(nparray): ,"#LINE# #TAB# if nparray is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if len(nparray.shape) == 3: #LINE# #TAB# #TAB# if nparray.shape[2] == 4: #LINE# #TAB# #TAB# #TAB# return nparray[..., [2, 1, 0, 3]] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return nparray[:, :, ::-1] #LINE# #TAB# return nparray"
Converts an epoch timestamp to human readable time  <code> def epoch_to_human_time(epoch_time): ,"#LINE# #TAB# if isinstance(epoch_time, int): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# d = datetime.datetime.fromtimestamp(epoch_time / 1000) #LINE# #TAB# #TAB# #TAB# return d.strftime(""%m-%d-%Y %H:%M:%S "") #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return None"
dataset.cache_root is the root directory in which the given dataset has been cached  <code> def cache_root(custom_directory): ,"#LINE# #TAB# if custom_directory is not None: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif config['data_cache_root'] is None: #LINE# #TAB# #TAB# path = tempfile.mkdtemp(prefix='npythy_data_cache_') #LINE# #TAB# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# #TAB# raise ValueError('Could not find or create cache directory') #LINE# #TAB# #TAB# config['data_cache_root'] = path #LINE# #TAB# #TAB# atexit.register(shutil.rmtree, path) #LINE# #TAB# return config['data_cache_root']"
Return architecture name from given LLDB target . : param lldb . SBTarget target : LLDB target . : return : Architecture name or None if can not find architecture . : rtype : str | None <code> def architecture_name_from_target(target): ,#LINE# #TAB# triple = target.GetTriple() #LINE# #TAB# if triple.startswith('i386'): #LINE# #TAB# #TAB# return 'i386' #LINE# #TAB# elif triple.startswith('x86_64'): #LINE# #TAB# #TAB# return 'x86_64' #LINE# #TAB# elif triple.startswith('armv7'): #LINE# #TAB# #TAB# return 'armv7' #LINE# #TAB# elif triple.startswith('armv7s'): #LINE# #TAB# #TAB# return 'armv7s' #LINE# #TAB# elif triple.startswith('arm64'): #LINE# #TAB# #TAB# return 'arm64' #LINE# #TAB# return None
"Return a printable character , or ' . ' for non - printable ones  <code> def print_character(ordchr): ",#LINE# #TAB# if 31 < ordchr < 126 and ordchr != 92: #LINE# #TAB# #TAB# return chr(ordchr) #LINE# #TAB# else: #LINE# #TAB# #TAB# return '.'
"Get snapshot by name from the snapshot tree root . : param name : Snapshot name : param root : Current root node in the snapshot tree : return : None in the snapshot tree with given snapshot name <code> def get_snapshot_from_tree(name, root): ","#LINE# #TAB# if not root: #LINE# #TAB# #TAB# return None #LINE# #TAB# if root.name == name: #LINE# #TAB# #TAB# return root.snapshot #LINE# #TAB# if not hasattr(root, 'childSnapshotList') or not root.childSnapshotList: #LINE# #TAB# #TAB# return None #LINE# #TAB# for node in root.childSnapshotList: #LINE# #TAB# #TAB# snapshot = VMwareVolumeOps.get_snapshot_from_tree(name, node) #LINE# #TAB# #TAB# if snapshot: #LINE# #TAB# #TAB# #TAB# return snapshot"
return the scalar cartesian norm of a single 3 elements vector <code> def norme_vec(vec1): ,"#LINE# #TAB# norm = np.sqrt(np.inner(vec1, vec1)) #LINE# #TAB# return norm"
Find the MIDI output name for a connected FaderPort  <code> def find_faderport_output_name(number=0): ,#LINE# #TAB# outs = [i for i in mido.get_output_names() if i.lower().startswith('faderport')] #LINE# #TAB# if 0 <= number < len(outs): #LINE# #TAB# #TAB# return outs[number] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
Generates a valid Python name from a system model name . Args : name ( str ) : The system model name . Returns : str : The generated name  <code> def make_name(name: str) ->str: ,"#LINE# #TAB# for c in '-#*%.:': #LINE# #TAB# #TAB# name = name.replace(c, '_') #LINE# #TAB# if keyword.iskeyword(name): #LINE# #TAB# #TAB# return name + '_' #LINE# #TAB# if name == 'id': #LINE# #TAB# #TAB# return 'id_' #LINE# #TAB# if name == 'type': #LINE# #TAB# #TAB# return 'type_' #LINE# #TAB# return name"
"Parse the output of GNU time filename : a path to the GNU time output Returns a DataFrame <code> def parse_gnu_time_file(filename, isdatetime=False): ","#LINE# #TAB# index = os.path.splitext(os.path.basename(filename))[0] #LINE# #TAB# if isdatetime: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# index = pd.to_datetime(index, format='%Y%m%d_%H%M%S') #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return pd.DataFrame(OrderedDict(map(parse_gnu_time_line, f)), #LINE# #TAB# #TAB# #TAB# #TAB# index=(index,)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# print(filename, file=sys.stderr) #LINE# #TAB# #TAB# #TAB# raise ValueError"
Is there a suitable handler for this control : param ` control ` : the control instance to check if a handler for it exists  <code> def has_ctrl_handler(control): ,"#LINE# #TAB# klass = control.__class__ #LINE# #TAB# if hasattr(control, '_persistentHandler'): #LINE# #TAB# #TAB# return True #LINE# #TAB# for handler, subclasses in STANDALONE_HANDLERS: #LINE# #TAB# #TAB# for subclass in subclasses: #LINE# #TAB# #TAB# #TAB# if issubclass(klass, subclass): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# for handler, subclasses in HANDLERS: #LINE# #TAB# #TAB# for subclass in subclasses: #LINE# #TAB# #TAB# #TAB# if issubclass(klass, subclass): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Return function_name(arg[0 ] , arg[1 ] , ... ) as a string <code> def simple_call_string(function_name, argument_list): ","#LINE# #TAB# return function_name + '(' + ', '.join([(var + '=' + repr(value)) for #LINE# #TAB# #TAB# var, value in argument_list]) + ')'"
Run a command in the console Returns : error terminal_output <code> def run_console_command(command): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# terminal_output = check_output(command, shell=True) #LINE# #TAB# #TAB# error = 0 #LINE# #TAB# except CalledProcessError as err: #LINE# #TAB# #TAB# terminal_output = err.output #LINE# #TAB# #TAB# error = err.returncode #LINE# #TAB# try: #LINE# #TAB# #TAB# terminal_output = terminal_output.decode('utf-8') #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# terminal_output = 'Error decoding terminal output.' #LINE# #TAB# return error, terminal_output"
"Gets all foreign keys on * to_table * referencing * from_table * <code> def get_fk(from_table, to_table): ",#LINE# #TAB# foreign_keys = [] #LINE# #TAB# for foreign_key in to_table.foreign_keys: #LINE# #TAB# #TAB# if foreign_key.column.table == from_table: #LINE# #TAB# #TAB# #TAB# foreign_keys.append(foreign_key) #LINE# #TAB# return foreign_keys
"Compare two ` mtime ` values ( e.g. for sorting ) <code> def cmp_mtime(a, b): ","#LINE# #TAB# digits = sys.float_info.dig #LINE# #TAB# a = round(a, digits) #LINE# #TAB# b = round(b, digits) #LINE# #TAB# return b - a"
"Return job class from RQ settings , otherwise return Job . If ` job_class ` is not None , it is used as an override ( can be python import path as string )  <code> def get_job_class(job_class=None): ","#LINE# #TAB# RQ = getattr(settings, 'RQ', {}) #LINE# #TAB# if job_class is None: #LINE# #TAB# #TAB# job_class = RQ.get('JOB_CLASS', Job) #LINE# #TAB# if isinstance(job_class, str): #LINE# #TAB# #TAB# job_class = import_attribute(job_class) #LINE# #TAB# return job_class"
Try to guess request country <code> def guess_request_country(request): ,"#LINE# #TAB# geoipCountry = request and request.META.get('HTTP_X_GEOIP_COUNTRY') #LINE# #TAB# if geoipCountry is not None: #LINE# #TAB# #TAB# return geoipCountry #LINE# #TAB# if hasattr(settings, 'LANG_COUNTRY'): #LINE# #TAB# #TAB# lang = get_language() #LINE# #TAB# #TAB# return settings.LANG_COUNTRY.get(lang, DEFAULT_COUNTRY) #LINE# #TAB# return DEFAULT_COUNTRY"
"Get the home folder of gridCAl , and if it does not exist , create it : return : folder path string <code> def get_create_gridcal_folder(): ","#LINE# #TAB# home = str(Path.home()) #LINE# #TAB# gc_folder = os.path.join(home, '.GridCal') #LINE# #TAB# if not os.path.exists(gc_folder): #LINE# #TAB# #TAB# os.makedirs(gc_folder) #LINE# #TAB# return gc_folder"
"Given a file path , parse it based on its extension ( YAML or JSON ) and return the values as a Python dictionary . JSON is the default if an extension ca n't be determined  <code> def load_config(path: str) ->dict: ","#LINE# #TAB# __, ext = os.path.splitext(path) #LINE# #TAB# if ext in ['.yaml', '.yml']: #LINE# #TAB# #TAB# loader = ruamel.yaml.safe_load #LINE# #TAB# else: #LINE# #TAB# #TAB# loader = json.load #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# return loader(f) or {}"
Returns if a request has an okay error code otherwise raises InvalidRequest  <code> def checkResponse(request): ,"#LINE# #TAB# #TAB# if str(request.status_code)[0] not in ['2', '3']: #LINE# #TAB# #TAB# #TAB# w = str(request.text).split('\\r')[0][2:] #LINE# #TAB# #TAB# #TAB# raise InvalidRequest(w) #LINE# #TAB# #TAB# return"
Taking a Matlab file of Arbin data from the CALCE database and imports it into a host of different NumPy arrays Filename : Matlab file <code> def import_matlab_data(filename): ,#LINE# #TAB# df = hdf5storage.loadmat(filename) #LINE# #TAB# return df
"Expand abbreviations and otherwise canonicalize street name and number <code> def row_canonicalize_street_and_number(sd, row): ",#LINE# #TAB# row['NUMBER'] = (row['NUMBER'] or '').strip() #LINE# #TAB# if row['NUMBER'].endswith('.0'): #LINE# #TAB# #TAB# row['NUMBER'] = row['NUMBER'][:-2] #LINE# #TAB# row['STREET'] = expand_street_name(row['STREET']) #LINE# #TAB# return row
Get the current directory into which logs will be written  <code> def log_dir(): ,#LINE# #TAB# if LogOptions._LOG_DIR is None: #LINE# #TAB# #TAB# LogOptions._LOG_DIR = app.get_options().twitter_common_log_log_dir #LINE# #TAB# return LogOptions._LOG_DIR
"Get info from .cih file in path , return it as dict . : param path : Path to .cih file . : return : info_dict : .cih file contents as dict  <code> def get_info(path): ","#LINE# #TAB# wanted_info = ['Date', 'Camera Type', 'Record Rate(fps)', #LINE# #TAB# #TAB# 'Shutter Speed(s)', 'Total Frame', 'Image Width', 'Image Height', #LINE# #TAB# #TAB# 'File Format', 'EffectiveBit Depth', 'Comment Text', 'Color Bit'] #LINE# #TAB# info_dict = collections.OrderedDict([]) #LINE# #TAB# with open(path, 'r') as file: #LINE# #TAB# #TAB# for line in file: #LINE# #TAB# #TAB# #TAB# line = line.rstrip().split(' : ') #LINE# #TAB# #TAB# #TAB# if line[0] in wanted_info: #LINE# #TAB# #TAB# #TAB# #TAB# key, value = line[0], line[1] #LINE# #TAB# #TAB# #TAB# #TAB# info_dict[key] = bytes(value, 'utf-8').decode('unicode_escape') #LINE# #TAB# return info_dict"
Search for all records of a dolphin by label <code> def dolphin_label_hits(label): ,"#LINE# #TAB# logger.debug('Searching index for {0}'.format(label)) #LINE# #TAB# q = Q('match', dolphins=label) #LINE# #TAB# s = Search().query(q).index('dolphins').doc_type(DolphinSighting) #LINE# #TAB# logger.debug('Executing elastic search {0}'.format(s.to_dict())) #LINE# #TAB# response = execute_search(s) #LINE# #TAB# logger.debug('Search found {0} results'.format(response.hits.total)) #LINE# #TAB# return response.hits.hits"
Compatibility wrapper . Taken from storm setup.py  <code> def find_packages(library_name): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# from setuptools import find_packages #LINE# #TAB# #TAB# return find_packages() #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass #LINE# #TAB# packages = [] #LINE# #TAB# for directory, subdirectories, files in os.walk(library_name): #LINE# #TAB# #TAB# if '__init__.py' in files: #LINE# #TAB# #TAB# #TAB# packages.append(directory.replace(os.sep, '.')) #LINE# #TAB# return packages"
"Turn the type argument into something useful  <code> def get_valid_type(value, type_): ","#LINE# #TAB# if type_ in (None, ''): #LINE# #TAB# #TAB# if type(value) in (bool, int, float, str, type('')): #LINE# #TAB# #TAB# #TAB# type_ = type(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# typestr = type_.__name__ if isinstance(type_, type) else str(type_) #LINE# #TAB# typestr += '' #LINE# #TAB# if not typestr in Transformers: #LINE# #TAB# #TAB# return None #LINE# #TAB# return typestr"
"Break a string at sentences and dump as wrapped literal YAML  <code> def save_text(text, end='\n'): ",#LINE# #TAB# if text: #LINE# #TAB# #TAB# return _Literal(text + end) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
Check if augmentations are loaded in already or not  <code> def check_augs(augs): ,"#LINE# #TAB# if isinstance(augs, dict): #LINE# #TAB# #TAB# return process_aug_dict(augs) #LINE# #TAB# elif isinstance(augs, Compose): #LINE# #TAB# #TAB# return augs"
Finds the first occurrence of term in English column of database and returns as a word  <code> def find_english_word(term): ,#LINE# #TAB# word = db['words'].find_one(english=term) #LINE# #TAB# if word is None: #LINE# #TAB# #TAB# raise LookupError #LINE# #TAB# return word
"attempt to detect whether APM or PX4 attributes names are in use <code> def sniff_field_spelling(mlog, source): ","#LINE# #TAB# position_field_type_default = position_field_types[0] #LINE# #TAB# msg = mlog.recv_match(source) #LINE# #TAB# mlog._rewind() #LINE# #TAB# position_field_selection = [spelling for spelling in position_field_types if hasattr(msg, spelling[0])] #LINE# #TAB# return position_field_selection[0] if position_field_selection else position_field_type_default"
Create a redis connection  <code> def get_redis(): ,"#LINE# #TAB# if settings.REDIS_URL is None: #LINE# #TAB# #TAB# return get_fakeredis() #LINE# #TAB# conn = Redis(connection_pool=get_redis_pool(), decode_responses=True) #LINE# #TAB# wait_for_redis(conn) #LINE# #TAB# return conn"
Return the name of the table s primary key <code> def primary_key(cls): ,#LINE# #TAB# #TAB# if cls.__from_class__: #LINE# #TAB# #TAB# #TAB# cls = cls.__from_class__ #LINE# #TAB# #TAB# return cls.__table__.primary_key.columns.values()[0].name
"Compute element - wise mean and variance from min and max values . Args : minval : A tensor of minimum values . maxval : A tensor of maximum values . Returns : Tuple of ( mean , variance )  <code> def range_moments(minval, maxval): ","#LINE# #TAB# mean = (maxval + minval) / 2 #LINE# #TAB# variance = tf.square((maxval - minval) / 2) #LINE# #TAB# return mean, variance"
"Smoothly truncate a time domain impulse response <code> def truncate_impulse(impulse, ntaps, window='hanning'): ","#LINE# #TAB# out = impulse.copy() #LINE# #TAB# trunc_start = int(ntaps / 2) #LINE# #TAB# trunc_stop = out.size - trunc_start #LINE# #TAB# window = signal.get_window(window, ntaps) #LINE# #TAB# out[0:trunc_start] *= window[trunc_start:ntaps] #LINE# #TAB# out[trunc_stop:out.size] *= window[0:trunc_start] #LINE# #TAB# out[trunc_start:trunc_stop] = 0 #LINE# #TAB# return out"
Returns whether the given query options expect a possible count of zero  <code> def expects_none(options): ,"#LINE# #TAB# if any(options.get(key) is not None for key in [""count"", ""maximum"", ""minimum"", ""between""]): #LINE# #TAB# #TAB# return matches_count(0, options) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"Display recent oneliners , return bottom margin and adj . offset  <code> def display_oneliners(term, top_margin, offset): ","#LINE# #TAB# _padding = 3 #LINE# #TAB# n_liners = term.height - _padding - top_margin #LINE# #TAB# txt, count, offset = generate_recent_oneliners(term, n_liners, offset) #LINE# #TAB# echo(u''.join((term.move(top_margin + 1, 0), txt, u'\r\n'))) #LINE# #TAB# bot_margin = top_margin + count + _padding #LINE# #TAB# return bot_margin, offset"
Convert an observation dict into a raw array if the original observation space was not a Dict space  <code> def dict_to_obs(obs_dict): ,#LINE# #TAB# if set(obs_dict.keys()) == {None}: #LINE# #TAB# #TAB# return obs_dict[None] #LINE# #TAB# return obs_dict
"Extract guides out of type - annotations  <code> def extract_guides(func: Callable) ->Dict[str, Tuple[Callable]]: ","#LINE# #TAB# return {arg: get_guides(annotation) for arg, annotation in func. #LINE# #TAB# #TAB# __annotations__.items()}"
"Update configuration target with given ` ` project ` ` and ` ` repository ` ` <code> def update_conf(conf, target, project, repository): ","#LINE# #TAB# if project: #LINE# #TAB# #TAB# conf.projects[target] = project #LINE# #TAB# if repository: #LINE# #TAB# #TAB# if target == 'default': #LINE# #TAB# #TAB# #TAB# conf.images[target] = repository #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if not isinstance(conf.projects[target], dict): #LINE# #TAB# #TAB# #TAB# #TAB# conf.projects[target] = {'id': conf.projects[target]} #LINE# #TAB# #TAB# #TAB# conf.projects[target]['image'] = repository"
"Helper function to select correct indices given the local state values in basis.n_states . Args : correlations : list of correlations to be computed basis : an instance of a basis class . Returns : list of correlations in terms of indices <code> def correlations_to_indices(correlations, basis): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# l_0 = tuple([list(basis.n_states).index(_l[0]) for _l in correlations]) #LINE# #TAB# #TAB# l_1 = tuple([list(basis.n_states).index(_l[1]) for _l in correlations]) #LINE# #TAB# except ValueError as ve: #LINE# #TAB# #TAB# raise ValueError('correlations value ' + str(ve) + #LINE# #TAB# #TAB# #TAB# ' is not in basis.n_states') #LINE# #TAB# return l_0, l_1"
"Splits as close to the end as possible  <code> def split_msg(msgs, max_len): ","#LINE# #TAB# msg = '' #LINE# #TAB# while len(msg.encode()) < max_len: #LINE# #TAB# #TAB# if len(msg.encode()) + len(msgs[0]) > max_len: #LINE# #TAB# #TAB# #TAB# return msg, msgs #LINE# #TAB# #TAB# char = msgs.pop(0).decode() #LINE# #TAB# #TAB# if char == ' ' and len(msg.encode()) > max_len - 15: #LINE# #TAB# #TAB# #TAB# return msg, msgs #LINE# #TAB# #TAB# msg += char #LINE# #TAB# return msg, msgs"
Process RNAz dat file format ( generated by rnazCluster.pl : param filepath : : return : <code> def process_dat(filepath): ,"#LINE# #TAB# sizes = [] #LINE# #TAB# with open(filepath, 'r') as fh: #LINE# #TAB# #TAB# for line in fh: #LINE# #TAB# #TAB# #TAB# if line.startswith('locus'): #LINE# #TAB# #TAB# #TAB# #TAB# x = line.split() #LINE# #TAB# #TAB# #TAB# #TAB# start = int(x[2]) #LINE# #TAB# #TAB# #TAB# #TAB# end = int(x[3]) #LINE# #TAB# #TAB# #TAB# #TAB# length = end - start #LINE# #TAB# #TAB# #TAB# #TAB# sizes.append(length) #LINE# #TAB# return sizes"
"Date to string with forced timezone information  <code> def to_date(string, fmt=None, output='str'): ","#LINE# #TAB# if string.startswith('UTC='): #LINE# #TAB# #TAB# string = string[4:] + 'Z' #LINE# #TAB# if fmt is None: #LINE# #TAB# #TAB# date_object = parse(string) #LINE# #TAB# else: #LINE# #TAB# #TAB# date_object = DT.datetime.strptime(string, fmt) #LINE# #TAB# if date_object.tzinfo is None: #LINE# #TAB# #TAB# date_object = date_object.replace(tzinfo=tzutc()) #LINE# #TAB# if output == 'str': #LINE# #TAB# #TAB# return date_object.strftime(DATE_FMT) #LINE# #TAB# elif output == 'date': #LINE# #TAB# #TAB# return date_object"
"Gather bytes for checksum calculation <code> def checksum_field_bytes(ctx: Dict[str, Any]) ->bytearray: ",#LINE# #TAB# x = bytearray(ctx['header'].data) #LINE# #TAB# x += ctx['_']['token'] #LINE# #TAB# if 'data' in ctx: #LINE# #TAB# #TAB# x += ctx['data'].data #LINE# #TAB# return x
Set up the auth handler using the authfile in ` ` rootdir ` ` rootdir The root directory to sync  <code> def setup_auth_handler(rootdir): ,"#LINE# #TAB# set_api_keys() #LINE# #TAB# authfile = pjoin(rootdir, '.pycsync') #LINE# #TAB# fapi.set_auth_handler(authfile) #LINE# #TAB# return True"
returns True if type represents C ++ volatile type False otherwise <code> def is_volatile(type_): ,"#LINE# #TAB# nake_type = remove_alias(type_) #LINE# #TAB# if isinstance(nake_type, cpptypes.volatile_t): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif isinstance(nake_type, cpptypes.const_t): #LINE# #TAB# #TAB# return is_volatile(nake_type.base) #LINE# #TAB# elif isinstance(nake_type, cpptypes.array_t): #LINE# #TAB# #TAB# return is_volatile(nake_type.base) #LINE# #TAB# return False"
Expand group names  <code> def expand_groups(grp): ,"#LINE# #TAB# p = re.compile(r""(?P<name>.+)\[(?P<start>\d+)-(?P<end>\d+)\]"") #LINE# #TAB# m = p.match(grp) #LINE# #TAB# if m is not None: #LINE# #TAB# #TAB# s = int(m.group('start')) #LINE# #TAB# #TAB# e = int(m.group('end')) #LINE# #TAB# #TAB# n = m.group('name') #LINE# #TAB# #TAB# return list(map(lambda x: n + str(x), range(s, e + 1))) #LINE# #TAB# else: #LINE# #TAB# #TAB# return [grp]"
"get class device info <code> def get_class_device_info(school_id, sn): ","#LINE# #TAB# code, data = get_device_info(school_id=school_id, sn=sn) #LINE# #TAB# if code: #LINE# #TAB# #TAB# logger.error('Error, query class device info, Detail: {}'.format(data)) #LINE# #TAB# return code, data"
Returns the list of fields in the given model instance . Checks whether to use the official _meta API or use the raw data . This method excludes many to many fields  <code> def get_fields_in_model(instance): ,"#LINE# #TAB# assert isinstance(instance, Model) #LINE# #TAB# use_api = hasattr(instance._meta, 'get_fields') and callable(instance._meta.get_fields) #LINE# #TAB# if use_api: #LINE# #TAB# #TAB# return [f for f in instance._meta.get_fields() if track_field(f)] #LINE# #TAB# return instance._meta.fields"
Extract version information as a dictionary from version . py  <code> def get_version_info(): ,"#LINE# #TAB# version_info = {} #LINE# #TAB# with open(os.path.join(""refcycle"", ""version.py""), 'r') as f: #LINE# #TAB# #TAB# version_code = compile(f.read(), ""version.py"", 'exec') #LINE# #TAB# #TAB# exec(version_code, version_info) #LINE# #TAB# return version_info"
Yield headers and data from a Google Analytics report . Args : results ( dict ) : Google Analytics statistics results . Yields : list : CSV - style headers and data from the report  <code> def generate_row_data(results): ,"#LINE# #TAB# report = results.get('reports')[0] #LINE# #TAB# column_header = report.get('columnHeader', {}) #LINE# #TAB# metric_headers = column_header.get('metricHeader', {}).get( #LINE# #TAB# #TAB# 'metricHeaderEntries', []) #LINE# #TAB# headers = column_header.get('dimensions', []).copy() #LINE# #TAB# headers.extend(header.get('name') for header in metric_headers) #LINE# #TAB# yield headers #LINE# #TAB# for row in report.get('data', {}).get('rows', []): #LINE# #TAB# #TAB# yield row['dimensions'] + row['metrics'][0]['values']"
"Split up the full concept path into a category_cd and a data label  <code> def split_concept_cd(concept_cd, join_char='+'): ","#LINE# #TAB# category_code, data_label = concept_cd.rsplit(join_char, 1) #LINE# #TAB# return category_code, data_label"
"Verify a XML document signature usign RSA - SHA1 , return True if valid <code> def rsa_verify(xml, signature, key, c14n_exc=True): ","#LINE# #TAB# if key.startswith('-----BEGIN PUBLIC KEY-----'): #LINE# #TAB# #TAB# bio = BIO.MemoryBuffer(key) #LINE# #TAB# #TAB# rsa = RSA.load_pub_key_bio(bio) #LINE# #TAB# else: #LINE# #TAB# #TAB# rsa = RSA.load_pub_key(certificate) #LINE# #TAB# pubkey = EVP.PKey() #LINE# #TAB# pubkey.assign_rsa(rsa) #LINE# #TAB# pubkey.reset_context(md='sha1') #LINE# #TAB# pubkey.verify_init() #LINE# #TAB# pubkey.verify_update(canonicalize(xml, c14n_exc)) #LINE# #TAB# ret = pubkey.verify_final(base64.b64decode(signature)) #LINE# #TAB# return ret == 1"
"Convert sigma to full - width half - max <code> def sigma_fwhm(sigma, shape='gaus'): ",#LINE# #TAB# if shape.lower() in alias_dict['gaus']: #LINE# #TAB# #TAB# A = 2 * np.sqrt(2 * np.log(2)) #LINE# #TAB# elif shape.lower() in alias_dict['sech2']: #LINE# #TAB# #TAB# A = 2 * np.arccosh(np.sqrt(2)) #LINE# #TAB# else: #LINE# #TAB# #TAB# A = 1 #LINE# #TAB# return A * sigma
"order_name -- Limit a text to 20 chars length , if necessary strips the middle of the text and substitute it for an ellipsis . name -- text to be limited  <code> def order_name(name): ","#LINE# #TAB# name = re.sub('^.*/', '', name) #LINE# #TAB# if len(name) <= 20: #LINE# #TAB# #TAB# return name #LINE# #TAB# return name[:10] + '...' + name[-7:]"
"Using the list of tuples created from ` parse_heart_json ` , generates a CSV file and saves it to the directory of choice . Returns the file path if successful , None otherwise  <code> def write_json(hr_data, filename): ","#LINE# #TAB# hr_dicts = [{'timestamp': datetime.strftime(x[0], '%Y-%m-%d %H:%M:%S'), #LINE# #TAB# #TAB# 'heartRate': x[1]} for x in hr_data] #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filename, 'w') as hr_export: #LINE# #TAB# #TAB# #TAB# json.dump(hr_dicts, hr_export) #LINE# #TAB# #TAB# #TAB# return os.path.realpath(hr_export.name) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print('An error occured while writing the JSON file: {}'.format(e)) #LINE# #TAB# #TAB# return None"
"Hash parts of a file for basic identification By default , the first and final 512kB are hashed  <code> def hash_file_partially(path, size=524288): ","#LINE# #TAB# fsize = path.stat().st_size #LINE# #TAB# size = min(size, fsize) #LINE# #TAB# with path.open('rb') as fd: #LINE# #TAB# #TAB# head = fd.read(size) #LINE# #TAB# #TAB# fd.seek(fsize - size) #LINE# #TAB# #TAB# tail = fd.read(size) #LINE# #TAB# #TAB# hexhash = hashlib.md5(head + tail).hexdigest() #LINE# #TAB# return hexhash"
returns config file from config directory . Any unset values default to DEFAULT_CONFIG configuration <code> def get_config(config_dir=None): ,"#LINE# #TAB# if not config_dir: #LINE# #TAB# #TAB# config_dir = CONFIG #LINE# #TAB# if not config_dir.exists(): #LINE# #TAB# #TAB# init_default_config() #LINE# #TAB# with open(config_dir, 'r') as f: #LINE# #TAB# #TAB# config = toml.loads(f.read()) #LINE# #TAB# config = update_config(config, config_dir=config_dir) #LINE# #TAB# Path(config['requests']['cache_dir']).mkdir(exist_ok=True, parents=True) #LINE# #TAB# Path(config['history_file_css']).touch(exist_ok=True) #LINE# #TAB# Path(config['history_file_xpath']).touch(exist_ok=True) #LINE# #TAB# return config"
"Take a list of dictionaries , and tokenize / normalize  <code> def normalize_params(params): ","#LINE# #TAB# fields = set() #LINE# #TAB# for p in params: #LINE# #TAB# #TAB# fields.update(p) #LINE# #TAB# fields = sorted(fields) #LINE# #TAB# params2 = list(pluck(fields, params, MISSING)) #LINE# #TAB# tokens = [tuple(x if isinstance(x, (int, float, str)) else id(x) for x in #LINE# #TAB# #TAB# p) for p in params2] #LINE# #TAB# return fields, tokens, params2"
Return a list of tags ( with NRML namespace removed ) representing the order of the nodes within a node <code> def get_taglist(node): ,"#LINE# #TAB# return [re.sub('\\{[^}]*\\}', '', copy(subnode.tag)) for subnode in #LINE# #TAB# #TAB# node.nodes]"
Iterate through all venv folders for globally installed packages . : rtype : Iterator[str ] <code> def iter_global_packages(): ,#LINE# #TAB# for dir in Path(GLOBAL_FOLDER).iterdir(): #LINE# #TAB# #TAB# yield dir.name
"Retrieve order data  <code> def get_order(account, order_url): ","#LINE# #TAB# return account.get_request(order_url, parse_json_result=True, #LINE# #TAB# #TAB# fail_on_error=True)[0]"
Registers all notifications in the application  <code> def register_notifications(): ,#LINE# #TAB# for app_path in settings.INSTALLED_APPS: #LINE# #TAB# #TAB# if app_path != 'ahem': #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# module = import_module('%s.notifications' % app_path) #LINE# #TAB# #TAB# #TAB# #TAB# load_notifications(module) #LINE# #TAB# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# #TAB# pass
"Retrieve id from input which can be num or id  <code> def usable_id(cls, id): ",#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# qry_id = int(id) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# qry_id = None #LINE# #TAB# #TAB# if not qry_id: #LINE# #TAB# #TAB# #TAB# msg = 'unknown identifier %s' % id #LINE# #TAB# #TAB# #TAB# cls.error(msg) #LINE# #TAB# #TAB# return qry_id
Check whether the code is run from Ipython  <code> def is_run_from_ipython(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# __IPYTHON__ #LINE# #TAB# #TAB# return True #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False
"Get route name from RAML resource URI . : param resource_uri : String representing RAML resource URI . : returns string : String with route name , which is : resource_uri : stripped of non - word characters  <code> def get_route_name(resource_uri): ","#LINE# #TAB# resource_uri = resource_uri.strip('/') #LINE# #TAB# resource_uri = re.sub('\\W', '', resource_uri) #LINE# #TAB# return resource_uri"
"Not sure why this is needed for migrations , there is probably a better way  <code> def site_default_id(): ",#LINE# #TAB# site = site_default() #LINE# #TAB# if site: #LINE# #TAB# #TAB# return site.id
"Computes covariance matrix for giving boundary points , according to Peura et al . 1997 , "" Efficiency of Simple Shape Descriptors "" <code> def compute_covariance_matrix(points): ","#LINE# #TAB# dist_to_center = compute_dist_to_center(points) #LINE# #TAB# covariance_matrix = list() #LINE# #TAB# for i_point in dist_to_center: #LINE# #TAB# #TAB# covariance_matrix.append(np.outer(i_point, i_point)) #LINE# #TAB# covariance_matrix = np.asarray(covariance_matrix) #LINE# #TAB# covariance_matrix = 1.0 * np.sum(covariance_matrix, 0) / points.shape[0] #LINE# #TAB# return covariance_matrix"
Tests for existence of a directory on the string filepath <code> def dir_exists(dirpath): ,#LINE# #TAB# if os.path.exists(dirpath) and os.path.isdir(dirpath): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Display bandwidth before and after permutation <code> def print_bandwidth_before_after_permutation(A, permutation): ","#LINE# #TAB# orig_band = compute_bandwidth_CSR(A) #LINE# #TAB# print(' Original bandwidth: ', orig_band) #LINE# #TAB# plot_mat_connectivity(A, 'orig_order.png') #LINE# #TAB# Aperm = A[(permutation), :][:, (permutation)] #LINE# #TAB# post_band = compute_bandwidth_CSR(Aperm) #LINE# #TAB# plot_mat_connectivity(Aperm, 'post_order.png') #LINE# #TAB# print(' Permuted bandwidth: ', post_band) #LINE# #TAB# return"
"Convert datetime in a dictionary : param date : Date in datetime format [ Datetime ] : return : Dictionary with all components of the date : year , month , day , hour , minute , second [ Dictionary ] <code> def datetime_to_json(date): ","#LINE# #TAB# year = str(date.year) #LINE# #TAB# month = str(date.month).zfill(2) #LINE# #TAB# day = str(date.day).zfill(2) #LINE# #TAB# hour = str(date.hour).zfill(2) #LINE# #TAB# minute = str(date.minute).zfill(2) #LINE# #TAB# second = str(date.second).zfill(2) #LINE# #TAB# json_date = {'year': year, 'month': month, 'day': day, 'hour': hour, #LINE# #TAB# #TAB# 'minute': minute, 'second': second} #LINE# #TAB# return json_date"
"Given a fastq file , read through the file : param fastq : name of fastq file : param pkl : name of pickle file to write to : return : <code> def base_quality_single_threaded(fastq, pkl): ","#LINE# #TAB# bq_mat = np.zeros((500, 100), dtype=np.uint64) #LINE# #TAB# idx = list(range(500)) #LINE# #TAB# for r in pysam.FastqFile(filename=fastq, persist=False): #LINE# #TAB# #TAB# bqa = r.get_quality_array() #LINE# #TAB# #TAB# bq_mat[idx[:len(bqa)], bqa] += 1 #LINE# #TAB# pickle.dump(bq_mat, open(pkl, 'wb')) #LINE# #TAB# return bq_mat"
"Get component name from filename  <code> def get_component(filename, default='global'): ","#LINE# #TAB# if hasattr(filename, 'decode'): #LINE# #TAB# #TAB# filename = filename.decode() #LINE# #TAB# parts = filename.split(os.path.sep) #LINE# #TAB# if len(parts) >= 3: #LINE# #TAB# #TAB# if parts[1] in 'modules legacy ext'.split(): #LINE# #TAB# #TAB# #TAB# return parts[2] #LINE# #TAB# if len(parts) >= 2: #LINE# #TAB# #TAB# if parts[1] in 'base celery utils'.split(): #LINE# #TAB# #TAB# #TAB# return parts[1] #LINE# #TAB# if len(parts) >= 1: #LINE# #TAB# #TAB# if parts[0] in 'grunt docs'.split(): #LINE# #TAB# #TAB# #TAB# return parts[0] #LINE# #TAB# return default"
"Return named params from a technical string <code> def parse_technical_string_params(cls, val): ","#LINE# #TAB# ttype, params, mode = _extract_technical_string_parts(val) #LINE# #TAB# if params: #LINE# #TAB# #TAB# raise InvalidTechnicalException( #LINE# #TAB# #TAB# #TAB# 'Invalid %s technical string: %s, no args allowed' % (ttype, val)) #LINE# #TAB# return {}"
Returns djinfo template context as dictionary <code> def get_context(): ,"#LINE# #TAB# return {'settings': settings_as_dict(), 'environment': env_as_dict(), #LINE# #TAB# #TAB# 'django': {'version': __version__}}"
Get all python files from the list of files  <code> def filter_python_files(files): ,"#LINE# #TAB# ""Get all python files from the list of files."" #LINE# #TAB# py_files = [] #LINE# #TAB# for f in files: #LINE# #TAB# #TAB# extension = os.path.splitext(f)[-1] #LINE# #TAB# #TAB# if extension: #LINE# #TAB# #TAB# #TAB# if extension == '.py': #LINE# #TAB# #TAB# #TAB# #TAB# py_files.append(f) #LINE# #TAB# #TAB# elif 'python' in open(f, 'r').readline(): #LINE# #TAB# #TAB# #TAB# py_files.append(f) #LINE# #TAB# #TAB# elif 'python script' in bash('file {}'.format(f)).value().lower(): #LINE# #TAB# #TAB# #TAB# py_files.append(f) #LINE# #TAB# return py_files"
Makes sure that a description starts with a capital letter and ends with a period for consistency  <code> def fix_descriptions(description: Optional[str]) ->Optional[str]: ,"#LINE# #TAB# if description is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# description = textwrap.dedent(description) #LINE# #TAB# description = description.strip('\n').rstrip('\n') #LINE# #TAB# if description[-1] not in [c for c in '!\'"":?.']: #LINE# #TAB# #TAB# description += '.' #LINE# #TAB# first_letter = description[0] #LINE# #TAB# capitalized = first_letter.capitalize() #LINE# #TAB# description = capitalized + description[1:] #LINE# #TAB# return description"
Cleans the notebook to be suitable for inclusion in the docs  <code> def clean_for_doc(nb): ,"#LINE# #TAB# new_cells = [] #LINE# #TAB# for cell in nb.worksheets[0].cells: #LINE# #TAB# #TAB# if ""input"" in cell and cell[""input""].strip() == ""%pylab inline"": #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if ""outputs"" in cell: #LINE# #TAB# #TAB# #TAB# outputs = [_i for _i in cell[""outputs""] if ""text"" not in _i or #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# not _i[""text""].startswith(""<obspy.core"")] #LINE# #TAB# #TAB# #TAB# cell[""outputs""] = outputs #LINE# #TAB# #TAB# new_cells.append(cell) #LINE# #TAB# nb.worksheets[0].cells = new_cells #LINE# #TAB# return nb"
"decode a string as ascii or utf8 if possible ( as required by the sftp protocol ) . if neither works , just return a byte string because the server probably does n't know the filename 's encoding  <code> def to_unicode(s): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return s.encode('ascii') #LINE# #TAB# except (UnicodeError, AttributeError): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return s.decode('utf-8') #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return s"
Context Mgr to asserting no environment variable of the given name exists ( sto enable the testing of the case where no env var of this name exists ) <code> def no_env(key): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# orig_value = os.environ[key] #LINE# #TAB# #TAB# del os.environ[key] #LINE# #TAB# #TAB# env_has_key = True #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# env_has_key = False #LINE# #TAB# yield #LINE# #TAB# if env_has_key: #LINE# #TAB# #TAB# os.environ[key] = orig_value #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# del os.environ[key] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass
"Make api method docs inheritted  <code> def _fix_docs(this_abc, child_class): ","#LINE# #TAB# #TAB# if sys.version_info >= (3, 5): #LINE# #TAB# #TAB# #TAB# return child_class #LINE# #TAB# #TAB# if not issubclass(child_class, this_abc): #LINE# #TAB# #TAB# #TAB# raise KappaError('Cannot fix docs of class that is not decendent.') #LINE# #TAB# #TAB# for name, child_func in vars(child_class).items(): #LINE# #TAB# #TAB# #TAB# if callable(child_func) and not child_func.__doc__: #LINE# #TAB# #TAB# #TAB# #TAB# if name in this_abc.__abstractmethods__: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# parent_func = getattr(this_abc, name) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# child_func.__doc__ = parent_func.__doc__ #LINE# #TAB# #TAB# return child_class"
"Handle None if preprocess , else handles anything not in DIR_STRS  <code> def clean_direction(dir_list, preprocess=False): ","#LINE# #TAB# if preprocess: #LINE# #TAB# #TAB# return [(UND if not isinstance(the_dir, str) else the_dir) for #LINE# #TAB# #TAB# #TAB# the_dir in dir_list] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [(UND if the_dir not in DIR_STRS else the_dir) for the_dir in #LINE# #TAB# #TAB# #TAB# dir_list]"
Initialize global log properties to their default values  <code> def reset_global_log_properties(): ,#LINE# #TAB# global _global_log_props #LINE# #TAB# _global_log_props = _default_global_log_props
Given a JSON representation of all the models in a graph return a dict of new model objects  <code> def instantiate_references_json(references_json): ,"#LINE# #TAB# references = {} #LINE# #TAB# for obj in references_json: #LINE# #TAB# #TAB# obj_id = obj['id'] #LINE# #TAB# #TAB# obj_type = obj.get('subtype', obj['type']) #LINE# #TAB# #TAB# cls = get_class(obj_type) #LINE# #TAB# #TAB# instance = cls.__new__(cls, id=obj_id) #LINE# #TAB# #TAB# if instance is None: #LINE# #TAB# #TAB# #TAB# raise RuntimeError('Error loading model from JSON (type: %s, id: %s)' % (obj_type, obj_id)) #LINE# #TAB# #TAB# references[instance.id] = instance #LINE# #TAB# return references"
"Get current context Not initialized context raises : class:` . ContextIsNotInitializedError ` , <code> def get_current(cls) ->'Context': ","#LINE# #TAB# if not hasattr(cls.thread_local, 'nanohttp_context'): #LINE# #TAB# #TAB# raise ContextIsNotInitializedError('Context is not initialized yet.') #LINE# #TAB# return cls.thread_local.nanohttp_context"
"Takes a number ( pages ) and the reordering scheme to use . Returns a list of indexes to use with page reordering  <code> def shuffle_pages(num, scheme): ","#LINE# #TAB# book = [] #LINE# #TAB# counter = 0 #LINE# #TAB# if scheme == SCHEME_A6_PERFECT: #LINE# #TAB# #TAB# sequence = [3, -1, 5, 1, -3, -3, -1, -1] #LINE# #TAB# elif scheme == SCHEME_A5_PERFECT: #LINE# #TAB# #TAB# sequence = [3, -1, -1, -1] #LINE# #TAB# for page in range(num): #LINE# #TAB# #TAB# book.append(0) #LINE# #TAB# for page in range(num): #LINE# #TAB# #TAB# if counter == len(sequence): #LINE# #TAB# #TAB# #TAB# counter = 0 #LINE# #TAB# #TAB# book[page] = page + sequence[counter] #LINE# #TAB# #TAB# counter += 1 #LINE# #TAB# return book"
Watch for action with timeouts . This method will listen for custom objects that times out  <code> def watch_for_pystol_timeouts(stop): ,#LINE# #TAB# while True: #LINE# #TAB# #TAB# return True
"List a LZH archive  <code> def list_lzh (archive, compression, cmd, verbosity, interactive): ",#LINE# #TAB# cmdlist = [cmd] #LINE# #TAB# if verbosity > 1: #LINE# #TAB# #TAB# cmdlist.append('v') #LINE# #TAB# else: #LINE# #TAB# #TAB# cmdlist.append('l') #LINE# #TAB# cmdlist.append(archive) #LINE# #TAB# return cmdlist
"Get two sorted array sets from the connections tuples , one for the first elements and one for the second <code> def extract_sets_from_connections(connections): ","#LINE# #TAB# setA = np.array(sorted({i[0] for i in connections})) #LINE# #TAB# setB = np.array(sorted({i[1] for i in connections})) #LINE# #TAB# return setA, setB"
"Create a matrix of active links leaving each node  <code> def setup_active_outlink_matrix(shape, node_status=None, return_count=True): ","#LINE# #TAB# links = active_outlinks(shape, node_status=node_status) #LINE# #TAB# if return_count: #LINE# #TAB# #TAB# return links, active_outlink_count_per_node(shape) #LINE# #TAB# else: #LINE# #TAB# #TAB# return links"
"Create or extend an argument tree tdict from path  <code> def add_path(tdict, path): ",#LINE# #TAB# t = tdict #LINE# #TAB# for step in path[:-2]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# t = t[step] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# t[step] = {} #LINE# #TAB# #TAB# #TAB# t = t[step] #LINE# #TAB# t[path[-2]] = path[-1] #LINE# #TAB# return tdict
Check mysql has innodb_large_prefix set <code> def mysql_large_prefix_check(engine): ,"#LINE# #TAB# if not str(engine.url).startswith('mysql'): #LINE# #TAB# #TAB# return False #LINE# #TAB# variables = dict(engine.execute( #LINE# #TAB# #TAB# 'show variables where variable_name like ""innodb_large_prefix"" or variable_name like ""innodb_file_format"";' #LINE# #TAB# #TAB# ).fetchall()) #LINE# #TAB# if variables['innodb_file_format'] == 'Barracuda' and variables[ #LINE# #TAB# #TAB# 'innodb_large_prefix'] == 'ON': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Gets a Cachet component by id <code> def get_component(id): ,#LINE# #TAB# components = cachet.Components(endpoint=ENDPOINT) #LINE# #TAB# component = json.loads(components.get(id=id)) #LINE# #TAB# return component['data']
Ensure the state transition matrix rows sum to 1 <code> def normalize_row_probability(x): ,"#LINE# #TAB# x /= x.sum(axis=1, keepdims=True) #LINE# #TAB# x[np.isnan(x)] = 0 #LINE# #TAB# return x"
Converts the given datetime to an ISO String . This assumes the datetime is UTC  <code> def to_isostring(dt): ,#LINE# #TAB# if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) > timedelta(0): #LINE# #TAB# #TAB# logging.warn( #LINE# #TAB# #TAB# #TAB# 'Warning: aware datetimes are interpreted as if they were naive') #LINE# #TAB# return dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'
"As a stupid programmer I like to use the upper left corner of the document as the 0,0 coords therefore we need to do some fancy calculations <code> def get_coords(x, y, w, h, pagesize): ","#LINE# #TAB# ax, ay = pagesize #LINE# #TAB# if x < 0: #LINE# #TAB# #TAB# x = ax + x #LINE# #TAB# if y < 0: #LINE# #TAB# #TAB# y = ay + y #LINE# #TAB# if w is not None and h is not None: #LINE# #TAB# #TAB# if w <= 0: #LINE# #TAB# #TAB# #TAB# w = ax - x + w #LINE# #TAB# #TAB# if h <= 0: #LINE# #TAB# #TAB# #TAB# h = ay - y + h #LINE# #TAB# #TAB# return x, ay - y - h, w, h #LINE# #TAB# return x, ay - y"
"Parses control file contents , yields a list of steps  <code> def parse_control_file(data, args): ","#LINE# #TAB# stepnames = set() #LINE# #TAB# all_outputs = set() #LINE# #TAB# result = [] #LINE# #TAB# for lineno, line, isindented in preprocess_lines(data, args): #LINE# #TAB# #TAB# if isindented: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# result[-1].parse_line(line, lineno, all_outputs) #LINE# #TAB# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# #TAB# raise ParseError(lineno, 'expected step header') from None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# result.append(Step(line, lineno, stepnames)) #LINE# #TAB# return result"
Return a boolean value for whether the given file is ignored by git  <code> def ignored_by_git(filename): ,"#LINE# #TAB# with chdir(get_root()): #LINE# #TAB# #TAB# result = run_command(f'git check-ignore -q {filename}', capture=True) #LINE# #TAB# #TAB# return result.code == 0"
Verifies that the given two - letter country code is valid . : param code : A two - letter country code : type code : str : return : The passed value : rtype : str : raises : voluptuous . Invalid <code> def country_code(code): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# pycountry.countries.get(alpha2=code) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# raise Invalid('Invalid country code') #LINE# #TAB# else: #LINE# #TAB# #TAB# return code
"get read cateroties : param bam : input bam path : param out_txt : output file name : return : output summary name <code> def bam_sum(bam, out_txt): ",#LINE# #TAB# cmd = 'samtools flagstat ' + bam + '> out_txt' #LINE# #TAB# run(cmd) #LINE# #TAB# return out_txt
"Generate a filename to be used by packer  <code> def generate_packer_filename(provider, region, builder): ","#LINE# #TAB# filename = '{0}_{1}_{2}.json'.format(provider, region, builder) #LINE# #TAB# return filename"
seen_union : UNION IDENTIFIER <code> def p_seen_union(p): ,"#LINE# #TAB# val = _make_empty_struct(p[2]) #LINE# #TAB# setattr(thrift_stack[-1], p[2], val) #LINE# #TAB# p[0] = val"
"Shorten long URLs to fit on one line  <code> def shorten_url(url, cols, shorten): ",#LINE# #TAB# cols = (cols - 6) * 0.85 #LINE# #TAB# if shorten is False or len(url) < cols: #LINE# #TAB# #TAB# return url #LINE# #TAB# split = int(cols * 0.5) #LINE# #TAB# return url[:split] + '...' + url[-split:]
"Return a report that matches ` ` name ` ` : param request_handler : : param name : : rtype : : class:`fineract.objects.report . Report ` <code> def get_by_name(cls, request_handler, name) ->Optional['Report']: ","#LINE# #TAB# data = request_handler.make_request('GET', '/reports') #LINE# #TAB# for item in data: #LINE# #TAB# #TAB# if item['reportName'] == name: #LINE# #TAB# #TAB# #TAB# return cls(request_handler, item, False) #LINE# #TAB# return None"
"Only for testing purposes  <code> def from_schema(cls, tag, schema): ","#LINE# #TAB# cls.tag = tag #LINE# #TAB# cls.schema = schema #LINE# #TAB# cls._parser = generate_parser(tag, schema) #LINE# #TAB# return cls"
Plot a graph using a circular layout . Parameters ---------- graph : networkx . Graph Graph representation of interaction network <code> def plot_graph(graph): ,"#LINE# #TAB# edge_widths = nx.get_edge_attributes(graph, 'width') #LINE# #TAB# edge_colors = nx.get_edge_attributes(graph, 'color') #LINE# #TAB# node_colors = nx.get_node_attributes(graph, 'color') #LINE# #TAB# nx.draw_circular(graph, with_labels=True, width=edge_widths.values(), #LINE# #TAB# #TAB# edgelist=edge_colors.keys(), edge_color=edge_colors.values(), #LINE# #TAB# #TAB# nodelist=node_colors.keys(), node_color=node_colors.values()) #LINE# #TAB# return"
Check if desktop is locked . Return bool . Desktop is locked if press Win + L Ctrl + Alt + Del or in remote desktop mode  <code> def is_desktop_locked() -> bool: ,"#LINE# #TAB# isLocked = False #LINE# #TAB# desk = ctypes.windll.user32.OpenDesktopW(ctypes.c_wchar_p('Default'), 0, 0, 0x0100) #LINE# #TAB# if desk: #LINE# #TAB# #TAB# isLocked = not ctypes.windll.user32.SwitchDesktop(desk) #LINE# #TAB# #TAB# ctypes.windll.user32.CloseDesktop(desk) #LINE# #TAB# return isLocked"
Returns a dictionary with catalogs definitions  <code> def get_catalog_definitions(): ,#LINE# #TAB# final = {} #LINE# #TAB# analysis_request = bika_catalog_analysisrequest_listing_definition #LINE# #TAB# analysis = bika_catalog_analysis_listing_definition #LINE# #TAB# autoimportlogs = bika_catalog_autoimportlogs_listing_definition #LINE# #TAB# worksheet = bika_catalog_worksheet_listing_definition #LINE# #TAB# report = bika_catalog_report_definition #LINE# #TAB# final.update(analysis_request) #LINE# #TAB# final.update(analysis) #LINE# #TAB# final.update(autoimportlogs) #LINE# #TAB# final.update(worksheet) #LINE# #TAB# final.update(report) #LINE# #TAB# return final
"The main method for the Netify app , when started via UWSGI  <code> def uwsgi_main(config_file): ",#LINE# #TAB# netify_app = NetifyApp(Config.load_config(config_file)) #LINE# #TAB# netify_app.register_views(Views) #LINE# #TAB# netify_app.flask_app.logger.info('NETIFY Loaded.') #LINE# #TAB# return netify_app.flask_app
Return job job status code from the response of the qstat command <code> def get_job_status(response): ,#LINE# #TAB# if 'Following jobs do not exist' in response: #LINE# #TAB# #TAB# return COMPLETED #LINE# #TAB# else: #LINE# #TAB# #TAB# return RUNNING
Interpolate the edges of the range gates from their centers  <code> def interpolate_range_edges(ranges): ,"#LINE# #TAB# edges = np.empty((ranges.shape[0] + 1,), dtype=ranges.dtype) #LINE# #TAB# edges[1:-1] = (ranges[:-1] + ranges[1:]) / 2.0 #LINE# #TAB# edges[0] = ranges[0] - (ranges[1] - ranges[0]) / 2.0 #LINE# #TAB# edges[-1] = ranges[-1] - (ranges[-2] - ranges[-1]) / 2.0 #LINE# #TAB# edges[edges < 0] = 0 #LINE# #TAB# return edges"
"Convert a label to a LUT ID number  <code> def get_lut_id(lut, label, use_lut): ","#LINE# #TAB# if not use_lut: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# assert isinstance(label, str) #LINE# #TAB# mask = lut['name'] == label #LINE# #TAB# assert mask.sum() == 1 #LINE# #TAB# return lut['id'][mask]"
Make sure string is unicode try to default with utf8 or base64 if failed  <code> def unicode_string(string): ,"#LINE# #TAB# if isinstance(string, six.text_type): #LINE# #TAB# #TAB# return string #LINE# #TAB# try: #LINE# #TAB# #TAB# return string.decode(""utf8"") #LINE# #TAB# except UnicodeDecodeError: #LINE# #TAB# #TAB# return '[BASE64-DATA]' + base64.b64encode(string) + '[/BASE64-DATA]'"
"Determine if test is better than control  <code> def conversion_uplift(control: Tuple[float], test: Tuple[float]) ->float: ",#LINE# #TAB# control_rate = conversion_rate(control) #LINE# #TAB# test_rate = conversion_rate(test) #LINE# #TAB# return (test_rate - control_rate) / control_rate
if you introduce new save - able parameter dictionaries then you have to include them here <code> def pack_prms(): ,"#LINE# #TAB# config_dict = { #LINE# #TAB# #TAB# ""Paths"": prms.Paths.to_dict(), #LINE# #TAB# #TAB# ""FileNames"": prms.FileNames.to_dict(), #LINE# #TAB# #TAB# ""Db"": prms.Db.to_dict(), #LINE# #TAB# #TAB# ""DbCols"": prms.DbCols.to_dict(), #LINE# #TAB# #TAB# ""DataSet"": prms.DataSet.to_dict(), #LINE# #TAB# #TAB# ""Reader"": prms.Reader.to_dict(), #LINE# #TAB# #TAB# ""Instruments"": prms.Instruments.to_dict(), #LINE# #TAB# #TAB# ""Batch"": prms.Batch.to_dict(), #LINE# #TAB# } #LINE# #TAB# return config_dict"
"Get VLAN i d associated with a portgroup on a DVS  <code> def get_portgroup_details(session, dvs_name, pg_name): ","#LINE# #TAB# port_group_mor = get_portgroup_mor_by_name(session, dvs_name, pg_name) #LINE# #TAB# vlan_id = constants.DEAD_VLAN #LINE# #TAB# if port_group_mor: #LINE# #TAB# #TAB# port_group_config = session._call_method(vim_util, #LINE# #TAB# #TAB# #TAB# 'get_dynamic_property', port_group_mor, #LINE# #TAB# #TAB# #TAB# 'DistributedVirtualPortgroup', 'config') #LINE# #TAB# #TAB# vlan_id = port_group_config.defaultPortConfig.vlan.vlanId #LINE# #TAB# return vlan_id"
"Coerces an optional value into a list of one or zero elements . > > > optional_to_list(""a "" ) [ ' a ' ] > > > optional_to_list(None ) [ ] <code> def optional_to_list(option: Optional[_T]) ->List[_T]: ",#LINE# #TAB# if option is not None: #LINE# #TAB# #TAB# return [option] #LINE# #TAB# return []
Checks if a module exists and can be imported <code> def module_exists(module_name): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# __import__(module_name) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
Check that database is in project and that it has activities Returns database_name if all checks pass <code> def check_database(database_name): ,"#LINE# #TAB# if database_name not in databases: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Database {} has not been set up, run setup_project first'. #LINE# #TAB# #TAB# #TAB# format(database_name)) #LINE# #TAB# if len(Database(database_name)) == 0: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Database {} is empty, run setup_project with overwrite_database=True' #LINE# #TAB# #TAB# #TAB# .format(database_name)) #LINE# #TAB# return database_name"
"Provides a score out of 10 that determines the relevance of the search result <code> def matching_details(song_name, song_title, artist): ","#LINE# #TAB# match_name = difflib.SequenceMatcher(None, song_name, song_title).ratio() #LINE# #TAB# match_title = difflib.SequenceMatcher(None, song_name, artist + song_title #LINE# #TAB# #TAB# ).ratio() #LINE# #TAB# if max(match_name, match_title) >= 0.55: #LINE# #TAB# #TAB# return True, max(match_name, match_title) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False, (match_name + match_title) / 2"
Given a list of dialects choose the one to use as the canonical version  <code> def choose_dialect(dialects): ,#LINE# #TAB# if len(dialects) == 0: #LINE# #TAB# #TAB# return constants.dialect #LINE# #TAB# final_order = [] #LINE# #TAB# for dialect in dialects: #LINE# #TAB# #TAB# for o in dialect['order']: #LINE# #TAB# #TAB# #TAB# if o not in final_order: #LINE# #TAB# #TAB# #TAB# #TAB# final_order.append(o) #LINE# #TAB# dialect = dialects[0] #LINE# #TAB# dialect['order'] = final_order #LINE# #TAB# return dialect
set the initialization state of the local node @param initialized : True if node initialized @type initialized : bool <code> def set_initialized(initialized): ,#LINE# #TAB# global _client_ready #LINE# #TAB# _client_ready = initialized
"Set whether or not validators are run . By default , they are run  <code> def set_run_validators(run): ","#LINE# #TAB# if not isinstance(run, bool): #LINE# #TAB# #TAB# raise TypeError(""'run' must be bool."") #LINE# #TAB# global _run_validators #LINE# #TAB# _run_validators = run"
"Create PDF and store it in filedir_pdf <code> def create_pdf(pdf_name: str, pdf_bytes: bytes) ->None: ","#LINE# #TAB# if pdf_name in pdf_loc: #LINE# #TAB# #TAB# return #LINE# #TAB# tempfile.tempdir = filedir_pdfs #LINE# #TAB# temp_file = tempfile.NamedTemporaryFile(mode='wb', suffix='.pdf', #LINE# #TAB# #TAB# prefix=pdf_name[:-4] + '_', delete=False) #LINE# #TAB# try: #LINE# #TAB# #TAB# for line in BytesIO(pdf_bytes): #LINE# #TAB# #TAB# #TAB# temp_file.write(line) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# logging.info('Could not create pdf from bytes') #LINE# #TAB# finally: #LINE# #TAB# #TAB# temp_file.close() #LINE# #TAB# #TAB# pdf_loc[pdf_name] = temp_file.name"
Reduce test case graph to a test - only graph  <code> def reduce_deps(graph): ,"#LINE# #TAB# ret = {} #LINE# #TAB# for case, deps in graph.items(): #LINE# #TAB# #TAB# test_deps = util.OrderedSet(d.check.name for d in deps) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ret[case.check.name] |= test_deps #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# ret[case.check.name] = test_deps #LINE# #TAB# return ret"
"Reads in data from a directory of CSV files ; assumes the directory only contains CSV files  <code> def read_dir(dirPath, numLabels, modify=False): ","#LINE# samplesDict = defaultdict(list) #LINE# for _, _, files in os.walk(dirPath): #LINE# #TAB# for f in files: #LINE# #TAB# basename, extension = os.path.splitext(os.path.basename(f)) #LINE# #TAB# if ""."" in basename and extension == "".csv"": #LINE# #TAB# #TAB# category = basename.split(""."")[-1] #LINE# #TAB# #TAB# if modify: #LINE# #TAB# #TAB# category = category.replace(""0"", ""/"") #LINE# #TAB# #TAB# category = category.replace(""_"", "" "") #LINE# #TAB# #TAB# samplesDict[category] = readCSV( #LINE# #TAB# #TAB# os.path.join(dirPath, f), numLabels=numLabels) #LINE# return samplesDict"
"Gets query parameters for creating a signed URL  <code> def get_signed_query_params_v2(credentials, expiration, string_to_sign): ","#LINE# #TAB# ensure_signed_credentials(credentials) #LINE# #TAB# signature_bytes = credentials.sign_bytes(string_to_sign) #LINE# #TAB# signature = base64.b64encode(signature_bytes) #LINE# #TAB# service_account_name = credentials.signer_email #LINE# #TAB# return { #LINE# #TAB# #TAB# ""GoogleAccessId"": service_account_name, #LINE# #TAB# #TAB# ""Expires"": str(expiration), #LINE# #TAB# #TAB# ""Signature"": signature, #LINE# #TAB# }"
"check vaules in selinux mode <code> def check_selinux_label(api, domain, logger): ","#LINE# #TAB# pid, context = get_pid_context(domain, logger) #LINE# #TAB# logger.debug('The context of %d is %s' % (int(pid), context)) #LINE# #TAB# get_enforce = get_security_policy(logger) #LINE# #TAB# if api[0] in context: #LINE# #TAB# #TAB# if api[1] == get_enforce: #LINE# #TAB# #TAB# #TAB# logger.debug(""PASS: '%s'"" % api) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# logger.debug(""Fail: '%s'"" % api[1]) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.debug(""Fail: '%s'"" % api[0]) #LINE# #TAB# #TAB# return False"
Execute fsync on a file ensuring it is synced to disk Returns the file stats : param str file_path : The file to sync : return : file stat : raise OSError : If something fails <code> def fsync_file(file_path): ,"#LINE# #TAB# file_fd = os.open(file_path, os.O_RDONLY) #LINE# #TAB# file_stat = os.fstat(file_fd) #LINE# #TAB# os.fsync(file_fd) #LINE# #TAB# os.close(file_fd) #LINE# #TAB# return file_stat"
If obj is argparse . Namespace or optparse . Values we ll return a dict representation of it else return the original object  <code> def namespace_to_dict(obj): ,"#LINE# #TAB# if isinstance(obj, (argparse.Namespace, optparse.Values)): #LINE# #TAB# #TAB# return vars(obj) #LINE# #TAB# return obj"
"To see if we are a console app , check if we can treat stdin like a tty , file or socket  <code> def is_console_app(): ","#LINE# #TAB# if not sys.stdin.isatty(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# fcntl.ioctl(sys.stdin, termios.FIONREAD, _sock_size) #LINE# #TAB# #TAB# except EnvironmentError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"@return : the existing loggers , in a list of C{(name , logger ) } tuples <code> def get_all_existing_loggers(): ","#LINE# #TAB# return [x for x in logging.Logger.manager.loggerDict.items()] + [( #LINE# #TAB# #TAB# logging.root.name, logging.root)]"
Blindly find and return the first egg we find <code> def get_egg(directory): ,#LINE# #TAB# eggs = list(directory.glob('*.egg')) #LINE# #TAB# if eggs: #LINE# #TAB# #TAB# return str(eggs[0]) #LINE# #TAB# return ''
"Given a dataframe N x N , returns mean M x M matrix of grouped mean rois INPUTS ------- df : pd . DataFrame , shape = N x N rois : np.array like , shape N <code> def group_dataframe_from_rois(df, rois): ","#LINE# #TAB# rois = pd.Series(rois) #LINE# #TAB# rois_df = df.groupby(by=rois, axis=0).mean().groupby(by=rois, axis=1).mean( #LINE# #TAB# #TAB# ) #LINE# #TAB# return rois_df"
"Calculate cov_x from fjac ipvt and p as is done in leastsq <code> def calc_cov_x(infodic, p): ","#LINE# #TAB# fjac = infodic[""fjac""] #LINE# #TAB# ipvt = infodic[""ipvt""] #LINE# #TAB# n = len(p) #LINE# #TAB# perm = np.take(np.eye(n), ipvt - 1, 0) #LINE# #TAB# r = np.triu(np.transpose(fjac)[:n, :]) #LINE# #TAB# R = np.dot(r, perm) #LINE# #TAB# try: #LINE# #TAB# #TAB# cov_x = np.linalg.inv(np.dot(np.transpose(R), R)) #LINE# #TAB# except LinAlgError: #LINE# #TAB# #TAB# cov_x = None #LINE# #TAB# return cov_x"
"Calculates a moving average of iterable over n elements <code> def moving_average(iterable, n): ","#LINE# #TAB# it = iter(iterable) #LINE# #TAB# d = deque(islice(it, n - 1)) #LINE# #TAB# d.appendleft(0.0) #LINE# #TAB# s = sum(d) #LINE# #TAB# for elem in it: #LINE# #TAB# #TAB# s += elem - d.popleft() #LINE# #TAB# #TAB# d.append(elem) #LINE# #TAB# #TAB# yield s / n"
"Check whether the application < name > is installed  <code> def has_app(name, check_platform=True): ",#LINE# #TAB# if check_platform: #LINE# #TAB# #TAB# Environment._platform_is_windows() #LINE# #TAB# return which(str(name)) is not None
"Calc qi_power on t_x tuple returning tuple <code> def fq6_qi_pow(t_x, i): ","#LINE# #TAB# global frob_coeffs #LINE# #TAB# i %= 6 #LINE# #TAB# if i == 0: #LINE# #TAB# #TAB# return t_x #LINE# #TAB# a, b = fq2_qi_pow(t_x[:2], i) #LINE# #TAB# c, d = fq2_mul(fq2_qi_pow(t_x[2:4], i), frob_coeffs[6, i, 1]) #LINE# #TAB# e, f = fq2_mul(fq2_qi_pow(t_x[4:6], i), frob_coeffs[6, i, 2]) #LINE# #TAB# return a, b, c, d, e, f"
"even / odd hit test on interior of a path  <code> def evenodd_hit(x, y, fills): ","#LINE# #TAB# in_count = 0 #LINE# #TAB# for hit, _ in _triangle_strip_hits(fills, x, y): #LINE# #TAB# #TAB# if hit: #LINE# #TAB# #TAB# #TAB# in_count += 1 #LINE# #TAB# return in_count % 2 == 1"
"Compute division of polynomials over GF(2 ) . Given a and b , it finds two polynomials q and r such that : a = b*q + r with deg(r)<deg(b ) <code> def div_gf2(a, b): ","#LINE# #TAB# if a < b: #LINE# #TAB# #TAB# return 0, a #LINE# #TAB# deg = number.size #LINE# #TAB# q = 0 #LINE# #TAB# r = a #LINE# #TAB# d = deg(b) #LINE# #TAB# while deg(r) >= d: #LINE# #TAB# #TAB# s = 1 << deg(r) - d #LINE# #TAB# #TAB# q ^= s #LINE# #TAB# #TAB# r ^= _mult_gf2(b, s) #LINE# #TAB# return q, r"
create a database connection to the SQLite database specified by the db_file : param db_file : database file : return : Connection object or None <code> def create_connection(db_file): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = sqlite3.connect(db_file) #LINE# #TAB# #TAB# return conn #LINE# #TAB# except: #LINE# #TAB# #TAB# print(bcolors.FAIL, 'Error! cannot create the database connection.', #LINE# #TAB# #TAB# #TAB# bcolors.ENDC) #LINE# #TAB# return None"
"Check whether the code cache for a particular piece of code is valid . Returns a tuple containing : a boolean representing whether the cached code should be used , and the cached code ( or ` ` None ` ` if the cache should not be used )  <code> def code_cache_check(cachefname): ","#LINE# #TAB# ccode = None #LINE# #TAB# run_cached = False #LINE# #TAB# if os.path.isfile(cachefname): #LINE# #TAB# #TAB# with open(cachefname, 'rb') as cfile: #LINE# #TAB# #TAB# #TAB# if not _check_cache_versions(cfile): #LINE# #TAB# #TAB# #TAB# #TAB# return False, None #LINE# #TAB# #TAB# #TAB# ccode = marshal.load(cfile) #LINE# #TAB# #TAB# #TAB# run_cached = True #LINE# #TAB# return run_cached, ccode"
"Adjust the evaluation interval adaptively . If cur_dev_size < = thres_dev_size , return base_interval ; else , linearly increase the interval ( round to integer times of base interval )  <code> def get_adaptive_eval_interval(cur_dev_size, thres_dev_size, base_interval): ",#LINE# #TAB# if cur_dev_size <= thres_dev_size: #LINE# #TAB# #TAB# return base_interval #LINE# #TAB# else: #LINE# #TAB# #TAB# alpha = round(cur_dev_size / thres_dev_size) #LINE# #TAB# #TAB# return base_interval * alpha
Returns a dictionary of important dates <code> def important_dates(year): ,#LINE# #TAB# output = {} #LINE# #TAB# data = mlbgame.data.get_important_dates(year) #LINE# #TAB# important_dates = etree.parse(data).getroot().\ #LINE# #TAB# #TAB# find('queryResults').find('row') #LINE# #TAB# try: #LINE# #TAB# #TAB# for x in important_dates.attrib: #LINE# #TAB# #TAB# #TAB# output[x] = important_dates.attrib[x] #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# raise ValueError('Unable to find important dates for {}.'.format(year)) #LINE# #TAB# return output
"Frobenius norm log likelihood  <code> def log_lik_frob(S, D, variance): ",#LINE# #TAB# logl = -0.5 * (S.size * np.log(2.0 * np.pi * variance) + squared_norm(S - #LINE# #TAB# #TAB# D) / variance) #LINE# #TAB# return logl
Parse the history node of a pubmed xml response Parameters ---------- node : XML node or None Returns ------- pubdate : dict or none <code> def parse_history_node(node): ,"#LINE# #TAB# if node is None: #LINE# #TAB# #TAB# return #LINE# #TAB# pubdate = {} #LINE# #TAB# for element in node.getchildren(): #LINE# #TAB# #TAB# status, date = parse_pubmedpubdate_node(element) #LINE# #TAB# #TAB# if status is not None and date is not None: #LINE# #TAB# #TAB# #TAB# pubdate[status] = date #LINE# #TAB# return pubdate or None"
Returns set of names of sequenes which have meaningful part ( without X or - ) longer than 30 % of trimmed alignment  <code> def length_check(trimmed_aln): ,"#LINE# #TAB# correct_length = {} #LINE# #TAB# for record in SeqIO.parse(trimmed_aln, 'fasta'): #LINE# #TAB# #TAB# if '@' in record.name: #LINE# #TAB# #TAB# #TAB# true_length = len(str(record.seq).replace('-', '').replace('X', '') #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# if true_length / len(record.seq) > 0.3: #LINE# #TAB# #TAB# #TAB# #TAB# correct_length[record.name] = round(true_length / len( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# record.seq), 2) #LINE# #TAB# return correct_length"
"Download OFX files and combine them into one It expects an ' accounts ' list of ofxclient . Account objects as well as an optional ' days ' specifier which defaults to 60 <code> def combined_download(accounts, days=60): ",#LINE# #TAB# client = Client(institution=None) #LINE# #TAB# out_file = StringIO() #LINE# #TAB# out_file.write(client.header()) #LINE# #TAB# out_file.write('<OFX>') #LINE# #TAB# for a in accounts: #LINE# #TAB# #TAB# ofx = a.download(days=days).read() #LINE# #TAB# #TAB# stripped = ofx.partition('<OFX>')[2].partition('</OFX>')[0] #LINE# #TAB# #TAB# out_file.write(stripped) #LINE# #TAB# out_file.write('</OFX>') #LINE# #TAB# out_file.seek(0) #LINE# #TAB# return out_file
"Given multiple sample matrices , return a 1D array of all differences between each sample matrix 's pixels and their corresponding median background value . All input matrices must have the same shape , nonzero coordinates and format  <code> def reps_bg_diff(mats: Iterable[sp.spmatrix]) ->np.ndarray: ",#LINE# #TAB# bg = median_bg(mats) #LINE# #TAB# diffs = [(m.data - bg.data) for m in mats] #LINE# #TAB# diffs = np.hstack(diffs) #LINE# #TAB# return diffs
"Function to avoid code duplication that takes returns the right profile i d <code> def process_profile_input(profile=None, geni_input=None, type_geni='g'): ","#LINE# #TAB# profile_to_use = None #LINE# #TAB# if profile is not None: #LINE# #TAB# #TAB# profile_to_use = profile.geni_specific_data['url'] #LINE# #TAB# elif geni_input is not None: #LINE# #TAB# #TAB# profile_to_use = process_geni_input(geni_input, type_geni) #LINE# #TAB# return profile_to_use"
"Check whether range of values can be stored into defined data type . : param mn : range minimum : param mx : range maximum : param dtype : : return : <code> def suits_with_dtype(mn, mx, dtype): ",#LINE# #TAB# type_info = np.iinfo(dtype) #LINE# #TAB# if mx <= type_info.max and mn >= type_info.min: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
Generate a nonce with the current timestamp <code> def mk_nonce(when=None): ,"#LINE# #TAB# salt = cryptutil.randomString(6, NONCE_CHARS) #LINE# #TAB# if when is None: #LINE# #TAB# #TAB# t = gmtime() #LINE# #TAB# else: #LINE# #TAB# #TAB# t = gmtime(when) #LINE# #TAB# time_str = strftime(time_fmt, t) #LINE# #TAB# return time_str + salt"
Emits cardinality decorator at end of type <code> def range_cardinality(slot: SlotDefinition) ->str: ,#LINE# #TAB# if slot.multivalued: #LINE# #TAB# #TAB# card_str = '1..*' if slot.required else '0..*' #LINE# #TAB# else: #LINE# #TAB# #TAB# card_str = '1..1' if slot.required else '0..1' #LINE# #TAB# return f' <sub><b>{card_str}</b></sub>'
Determine if this given value is an IPv6 address value as defined by the ipaddress module . Parameters ---------- value : str The value to check Returns ------- bool True if the value is any valid IP thing False otherwise <code> def is_host_ip6(value): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return bool(ipaddress.IPv6Address(value)) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass
Return a tileset info for this time interval <code> def tileset_info(filename): ,"#LINE# #TAB# with open(filename, 'r') as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# #TAB# return {'min_pos': [0], 'max_pos': [data['len']], 'max_width': data #LINE# #TAB# #TAB# #TAB# ['len'], 'start_value': data['start'], 'end_value': data['end']}"
"Validator for the combination of statuses . Check if the combination of statuses of lease , reservations and events is valid : param lease_id : Lease ID : param status : Lease status : return : True if the combination is valid <code> def is_valid_combination(cls, lease_id, status): ","#LINE# #TAB# reservations = db_api.reservation_get_all_by_lease_id(lease_id) #LINE# #TAB# if any([(r['status'] not in COMBINATIONS[status]['reservation']) for r in #LINE# #TAB# #TAB# reservations]): #LINE# #TAB# #TAB# return False #LINE# #TAB# for event_type in ('start_lease', 'end_lease'): #LINE# #TAB# #TAB# event = db_api.event_get_first_sorted_by_filters('lease_id', 'asc', #LINE# #TAB# #TAB# #TAB# {'lease_id': lease_id, 'event_type': event_type}) #LINE# #TAB# #TAB# if event['status'] not in COMBINATIONS[status][event_type]: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Initiate log file  <code> def init_logs(): ,"#LINE# #TAB# start_time = dt.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M') #LINE# #TAB# logname = os.path.join(os.path.expanduser(""~"") + ""/nanoGUI_"" + start_time + "".log"") #LINE# #TAB# handlers = [logging.FileHandler(logname)] #LINE# #TAB# logging.basicConfig( #LINE# #TAB# #TAB# format='%(asctime)s %(message)s', #LINE# #TAB# #TAB# handlers=handlers, #LINE# #TAB# #TAB# level=logging.INFO) #LINE# #TAB# logging.info('NanoGUI {} started with NanoPlot {}'.format(__version__, nanoplot.__version__)) #LINE# #TAB# logging.info('Python version is: {}'.format(sys.version.replace('\n', ' '))) #LINE# #TAB# return logname"
Make sure input filename exists  <code> def file_exists(filename): ,#LINE# #TAB# if not Path(filename).exists(): #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError('File not found: ' + filename) #LINE# #TAB# return filename
Initialize a basic logger for HTMap for the CLI  <code> def start_htmap_logger(): ,#LINE# #TAB# htmap.settings['CLI.SPINNERS_ON'] = False #LINE# #TAB# htmap_logger = logging.getLogger('htmap') #LINE# #TAB# htmap_logger.setLevel(logging.DEBUG) #LINE# #TAB# handler = logging.StreamHandler(stream=sys.stderr) #LINE# #TAB# handler.setLevel(logging.DEBUG) #LINE# #TAB# handler.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s ~ %(levelname)s ~ %(name)s:%(lineno)d ~ %(message)s')) #LINE# #TAB# htmap_logger.addHandler(handler) #LINE# #TAB# return handler
"Calculate percentage of sites removed given alignment size and number of sites trimmed  <code> def get_removed_fraction(untrimmed_alignment_size, no_sites_trimmed): ",#LINE# #TAB# removed_fraction = no_sites_trimmed / untrimmed_alignment_size #LINE# #TAB# return removed_fraction
"A 6 digit hex string representing an ( r , g , b ) color <code> def extract_color(color): ","#LINE# #TAB# HEX_BASE = 16 #LINE# #TAB# NUM_CHAR = 2 #LINE# #TAB# LARGEST_COLOR = 255 #LINE# #TAB# color = color.lower() #LINE# #TAB# color = [color[i:i + NUM_CHAR] for i in range(0, len(color), NUM_CHAR)] #LINE# #TAB# color = [int(hex_string, HEX_BASE) for hex_string in color] #LINE# #TAB# color = [(c / LARGEST_COLOR) for c in color] #LINE# #TAB# return color[R_INDEX], color[G_INDEX], color[B_INDEX]"
"Calculate contsign of v and pack as kv  <code> def pack_contsign_as_key_image_as_val(k, v): ","#LINE# #TAB# from contsign_tool import calc_comma_contsign #LINE# #TAB# contsign = calc_comma_contsign(v) #LINE# #TAB# return contsign, v"
In order to reuse function output_emacs_table / output_other_table We generate table contains cells with type MyTableCell <code> def gen_table_from_list(csv_content): ,"#LINE# #TAB# ret_table = [] #LINE# #TAB# for row in csv_content: #LINE# #TAB# #TAB# current_row = [] #LINE# #TAB# #TAB# for col in row: #LINE# #TAB# #TAB# #TAB# col = to_unicode(col) #LINE# #TAB# #TAB# #TAB# current_row.append(MyTableCell(col, 'csv')) #LINE# #TAB# #TAB# ret_table.append(current_row) #LINE# #TAB# return ret_table"
"Fetch school data from external API and parse it , at once . Parameters ---------- code School unit code  <code> def fetch_and_parse_school_unit_data(code: str) ->dict: ",#LINE# #TAB# next(api_rate_limiter) #LINE# #TAB# data = fetch_school_unit_data(code) #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed = parse_school_unit_data(data) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# logging.error(f'ValueError: {code}') #LINE# #TAB# #TAB# parsed = {} #LINE# #TAB# #TAB# pass #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# logging.error(f'KeyError: {code}') #LINE# #TAB# #TAB# parsed = {} #LINE# #TAB# #TAB# pass #LINE# #TAB# return parsed
Strip every element of a list and keep a list of ordered unique values : param tab : list to strip : type tab : list : return : stripped list with unique values : rtype : list <code> def strip_and_uniq(tab): ,#LINE# #TAB# _list = [] #LINE# #TAB# for elt in tab: #LINE# #TAB# #TAB# val = elt.strip() #LINE# #TAB# #TAB# if val and val not in _list: #LINE# #TAB# #TAB# #TAB# _list.append(val) #LINE# #TAB# return _list
"Proximal operator for maximum entropy regularization  <code> def prox_max_entropy(X, step, gamma=1): ","#LINE# #TAB# from scipy.special import lambertw #LINE# #TAB# gamma_ = _step_gamma(step, gamma) #LINE# #TAB# above = X > 0 #LINE# #TAB# X[above] = gamma_ * np.real(lambertw(np.exp(X[above]/gamma_ - 1) / gamma_)) #LINE# #TAB# return X"
"Adds operating system , requires "" i d "" and "" operatingsystem - id ""  <code> def add_operatingsystem(cls, options=None): ","#LINE# #TAB# cls.command_sub = 'add-operatingsystem' #LINE# #TAB# result = cls.execute(cls._construct_command(options), output_format='csv') #LINE# #TAB# return result"
"Subtract from a marking the postset of t and adds the preset Parameters ------------ m Marking t Transition Returns ------------ diff_mark Difference marking <code> def diff_mark(m, t): ",#LINE# #TAB# for a in t.out_arcs: #LINE# #TAB# #TAB# p = a.target #LINE# #TAB# #TAB# w = a.weight #LINE# #TAB# #TAB# if p in m and w <= m[p]: #LINE# #TAB# #TAB# #TAB# m[p] = m[p] - w #LINE# #TAB# #TAB# #TAB# if m[p] == 0: #LINE# #TAB# #TAB# #TAB# #TAB# del m[p] #LINE# #TAB# for a in t.in_arcs: #LINE# #TAB# #TAB# p = a.source #LINE# #TAB# #TAB# w = a.weight #LINE# #TAB# #TAB# if not p in m: #LINE# #TAB# #TAB# #TAB# m[p] = 0 #LINE# #TAB# #TAB# m[p] = m[p] + w #LINE# #TAB# return m
"Normalize 2d array between two values To check <code> def normalize_2d(im, min_value, max_value): ",#LINE# #TAB# im = (im - np.min(im)) / (np.max(im) - np.min(im)) #LINE# #TAB# im = im * (max_value - min_value) + min_value #LINE# #TAB# return im
"Calculates the Conversion Factor and physical amplitude of motion in nms by comparison of the ratio of the heights of the z signal and second harmonic of z  <code> def calc_z0_and_conv_factor_from_ratio_of_harmonics(z, z2, NA=0.999): ","#LINE# #TAB# V1 = calc_mean_amp(z) #LINE# #TAB# V2 = calc_mean_amp(z2) #LINE# #TAB# ratio = V2/V1 #LINE# #TAB# beta = 4*ratio #LINE# #TAB# laserWavelength = 1550e-9 #LINE# #TAB# k0 = (2*pi)/(laserWavelength) #LINE# #TAB# WaistSize = laserWavelength/(pi*NA) #LINE# #TAB# Zr = pi*WaistSize**2/laserWavelength #LINE# #TAB# z0 = beta/(k0 - 1/Zr) #LINE# #TAB# ConvFactor = V1/z0 #LINE# #TAB# T0 = 300 #LINE# #TAB# return z0, ConvFactor"
Returns a list of label strings loaded from the provided path  <code> def get_label_vocab(vocab_path): ,"#LINE# if vocab_path: #LINE# #TAB# try: #LINE# #TAB# with tf.io.gfile.GFile(vocab_path, 'r') as f: #LINE# #TAB# #TAB# return [line.rstrip('\n') for line in f] #LINE# #TAB# except tf.errors.NotFoundError as err: #LINE# #TAB# tf.logging.error('error reading vocab file: %s', err) #LINE# return []"
"pre : reacsbounds={reac:(lo , hi ) } , all external reactions blocked post : model with reactions in internal cycles ( for doing elementary modes ) <code> def internal_cycles(model, allowedreacs=None, reacsbounds={}, tol=1e-10): ",#LINE# #TAB# state = model.GetState() #LINE# #TAB# model.SetConstraints(reacsbounds) #LINE# #TAB# if allowedreacs == None: #LINE# #TAB# #TAB# rd = model.AllFluxRange() #LINE# #TAB# #TAB# allowedreacs = rd.Allowed(tol=tol).keys() #LINE# #TAB# model.SetState(state) #LINE# #TAB# if len(allowedreacs) > 0: #LINE# #TAB# #TAB# armodel = model.SubModel(allowedreacs) #LINE# #TAB# #TAB# return armodel
Set list of safe writeable paths <code> def set_safe_paths(list_of_paths): ,#LINE# #TAB# global _SAFE_PATHS #LINE# #TAB# _SAFE_PATHS = list_of_paths
"Convert a PyEphem Date into a timezone aware python datetime <code> def ephem_to_timezone(date, tzinfo): ","#LINE# #TAB# seconds, microseconds = _ephem_convert_to_seconds_and_microseconds(date) #LINE# #TAB# date = dt.datetime.fromtimestamp(seconds, tzinfo) #LINE# #TAB# date = date.replace(microsecond=microseconds) #LINE# #TAB# return date"
"Restrict the csv files to the adequate number of subjects <code> def filter_csv_by_n_subjects(participants, n_adult, n_child): ","#LINE# #TAB# child_ids = participants[participants['Child_Adult'] == 'child'][ #LINE# #TAB# #TAB# 'participant_id'][:n_child] #LINE# #TAB# adult_ids = participants[participants['Child_Adult'] == 'adult'][ #LINE# #TAB# #TAB# 'participant_id'][:n_adult] #LINE# #TAB# ids = np.hstack([adult_ids, child_ids]) #LINE# #TAB# participants = participants[np.in1d(participants['participant_id'], ids)] #LINE# #TAB# participants = participants[np.argsort(participants, order='Child_Adult')] #LINE# #TAB# return participants"
"Search for python ini files in given paths , append files with full paths for P.getParameters ( ) to read . Current paths given are : where this code is executing , one up , current directory <code> def get_params_files(paths=ini_paths): ","#LINE# #TAB# p_params_files = [] #LINE# #TAB# for path in ini_paths: #LINE# #TAB# #TAB# for f in os.listdir(os.path.abspath(path)): #LINE# #TAB# #TAB# #TAB# ini_file = re.search('pipelin(.*).yml', f) #LINE# #TAB# #TAB# #TAB# if ini_file: #LINE# #TAB# #TAB# #TAB# #TAB# ini_file = os.path.join(os.path.abspath(path), ini_file.group() #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# #TAB# p_params_files.append(ini_file) #LINE# #TAB# return p_params_files"
Apply log_softmax to the last dimension of input x : param x : tensor : return : <code> def log_softmax(x): ,"#LINE# #TAB# ndim = x.ndim #LINE# #TAB# if ndim <= 2: #LINE# #TAB# #TAB# return tensor.nnet.logsoftmax(x) #LINE# #TAB# else: #LINE# #TAB# #TAB# original_shape = x.shape #LINE# #TAB# #TAB# M, N = 1, original_shape[-1] #LINE# #TAB# #TAB# for i in range(x.ndim - 1): #LINE# #TAB# #TAB# #TAB# M = M * original_shape[i] #LINE# #TAB# #TAB# x = tensor.reshape(x, (M, N)) #LINE# #TAB# #TAB# x = tensor.nnet.logsoftmax(x) #LINE# #TAB# #TAB# x = tensor.reshape(x, original_shape) #LINE# #TAB# #TAB# return x"
Quote string containing unsafe characters for both group - name and items : param s : string to make safe : return : <code> def quote_str(s: str) ->str: ,"#LINE# #TAB# m = SAFE_STR_RE.match(s) #LINE# #TAB# if m and m[0] == s: #LINE# #TAB# #TAB# return escape_str(s) #LINE# #TAB# has_single_quotes = ""'"" in s #LINE# #TAB# has_double_quotes = '""' in s #LINE# #TAB# if has_single_quotes and has_double_quotes: #LINE# #TAB# #TAB# return '""' + escape_str(s, {'""'}) + '""' #LINE# #TAB# elif has_single_quotes: #LINE# #TAB# #TAB# return '""' + escape_str(s) + '""' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ""'"" + escape_str(s) + ""'"""
format date for display <code> def format_date(date): ,"#LINE# #TAB# units = (3600 * 24 * 365, 'year', 'years'), (3600 * 24 * 30, 'month', #LINE# #TAB# #TAB# 'months'), (3600 * 24 * 7, 'week', 'weeks'), (3600 * 24, 'day', 'days' #LINE# #TAB# #TAB# ), (3600, 'hour', 'hours'), (60, 'minute', 'minutes') #LINE# #TAB# then = time.mktime(date.timetuple()) #LINE# #TAB# now = time.time() #LINE# #TAB# t_delta = int(now - then) #LINE# #TAB# for u, unit, unit_plural in units: #LINE# #TAB# #TAB# r = int(t_delta / u) #LINE# #TAB# #TAB# if r: #LINE# #TAB# #TAB# #TAB# return '%i %s' % (r, r == 1 and unit or unit_plural) #LINE# #TAB# return ''"
"function for unpacking the tableswitch op arguments <code> def unpack_tableswitch(bc, offset): ","#LINE# #TAB# jump = (offset % 4) #LINE# #TAB# if jump: #LINE# #TAB# #TAB# offset += (4 - jump) #LINE# #TAB# (default, low, high), offset = _unpack(_struct_iii, bc, offset) #LINE# #TAB# joffs = list() #LINE# #TAB# for _index in range((high - low) + 1): #LINE# #TAB# #TAB# j, offset = _unpack(_struct_i, bc, offset) #LINE# #TAB# #TAB# joffs.append(j) #LINE# #TAB# return (default, low, high, joffs), offset"
"Check for assertEqual(A , None ) or assertEqual(None , A ) sentences N318 <code> def assert_equal_none(logical_line): ","#LINE# #TAB# res = asse_equal_start_with_none_re.search(logical_line #LINE# #TAB# #TAB# ) or asse_equal_end_with_none_re.search(logical_line) #LINE# #TAB# if res: #LINE# #TAB# #TAB# yield 0, 'N318: assertEqual(A, None) or assertEqual(None, A) sentences not allowed'"
Return a decoded unicode string . False values are returned untouched  <code> def to_unicode(text): ,"#LINE# #TAB# if not text or isinstance(text, unicode if PY2 else str): #LINE# #TAB# #TAB# return text #LINE# #TAB# try: #LINE# #TAB# #TAB# return text.decode(""UTF-8"") #LINE# #TAB# except UnicodeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return text.decode(""CP1252"") #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return text"
"Return all binary states for a system  <code> def all_states(n, big_endian=False): ","#LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return #LINE# #TAB# for state in product((0, 1), repeat=n): #LINE# #TAB# #TAB# if big_endian: #LINE# #TAB# #TAB# #TAB# yield state #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield state[::-1]"
Get the NN model that s being analyzed from the request context . Put the model in the request context if it is not yet there  <code> def get_model(): ,"#LINE# #TAB# if not hasattr(g, 'model'): #LINE# #TAB# #TAB# g.model = load_model(current_app.config['MODEL_CLS_PATH'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# current_app.config['MODEL_CLS_NAME'], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# current_app.config['MODEL_LOAD_ARGS']) #LINE# #TAB# return g.model"
"Get a single value from a request GET parameter . Optionally error if it is missing  <code> def get_request_param(request: web.Request, name: str, error_if_missing: ",#LINE# #TAB# bool=True) ->Optional[str]: #LINE# #TAB# if name not in request.rel_url.query: #LINE# #TAB# #TAB# if error_if_missing: #LINE# #TAB# #TAB# #TAB# raise errors.NotFound() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# ret = request.rel_url.query[name] #LINE# #TAB# return ret
Check if there is a correct function name used in the function definition . : param tokens : tokens to check : return : None if no error found otherwise a string representing what type of error was found <code> def check_invalid_function_name(tokens: List[TokenInfo]) ->Optional[str]: ,#LINE# #TAB# if len(tokens) > 1 and tokens[1].string == '=': #LINE# #TAB# #TAB# return '=' #LINE# #TAB# if len(tokens) >= 6: #LINE# #TAB# #TAB# should_be_variable_name = tokens[1] #LINE# #TAB# #TAB# if (should_be_variable_name.type == tokenize.NAME and utils. #LINE# #TAB# #TAB# #TAB# is_correct_variable_name(should_be_variable_name.string)): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return should_be_variable_name.string
"Converts argument strings in key = value or key . namespace = value form to dictionary entries <code> def parse_extension_arg(arg, arg_dict): ","#LINE# #TAB# match = re.match(r'^(([^\d\W]\w*)(\.[^\d\W]\w*)*)=(.*)$', arg) #LINE# #TAB# if match is None: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""invalid extension argument '%s', must be in key=value form"" % arg #LINE# #TAB# #TAB# ) #LINE# #TAB# name = match.group(1) #LINE# #TAB# value = match.group(4) #LINE# #TAB# arg_dict[name] = value"
Checks for deferred email that otherwise fill up the queue  <code> def deferred_emails(): ,"#LINE# #TAB# status = SERVER_STATUS['OK'] #LINE# #TAB# count = Message.objects.deferred().count() #LINE# #TAB# if DEFERRED_WARNING_THRESHOLD <= count < DEFERRED_DANGER_THRESHOLD: #LINE# #TAB# #TAB# status = SERVER_STATUS['WARNING'] #LINE# #TAB# if count >= DEFERRED_DANGER_THRESHOLD: #LINE# #TAB# #TAB# status = SERVER_STATUS['DANGER'] #LINE# #TAB# return { #LINE# #TAB# #TAB# 'label': 'Deferred Email', #LINE# #TAB# #TAB# 'status': status, #LINE# #TAB# #TAB# 'info': 'There are currently {0} deferred messages.'.format(count) #LINE# #TAB# }"
Return potential locations of IACA installation  <code> def serach_path(): ,"#LINE# #TAB# operating_system = get_os() #LINE# #TAB# return [os.path.expanduser(""~/.kerncraft/iaca/{}/"".format(operating_system)), #LINE# #TAB# #TAB# #TAB# os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/iaca/{}/'.format( #LINE# #TAB# #TAB# #TAB# #TAB# operating_system)]"
"read Graph and construct flowMatrix <code> def get_flow_matrix(G, nodelist=None): ","#LINE# #TAB# if nodelist is None: #LINE# #TAB# #TAB# FM = nx.to_numpy_matrix(G) #LINE# #TAB# FM = nx.to_numpy_matrix(G, nodelist) #LINE# #TAB# return FM"
"Return the highest intensity peak coordinates  <code> def get_high_intensity_peaks(image, mask, num_peaks): ",#LINE# #TAB# coord = np.nonzero(mask) #LINE# #TAB# intensities = image[coord] #LINE# #TAB# idx_maxsort = np.argsort(intensities)[::-1] #LINE# #TAB# coord = np.transpose(coord)[idx_maxsort] #LINE# #TAB# if len(coord) > num_peaks: #LINE# #TAB# #TAB# coord = coord[:num_peaks] #LINE# #TAB# return coord
"Get the plugin templates from the templates directory . If more than one template is found or passed , a dialog is raised to choose one  <code> def get_templates(cls): ","#LINE# #TAB# tempList = [] #LINE# #TAB# pluginName = cls.getName() #LINE# #TAB# tDir = cls.getPluginTemplateDir() #LINE# #TAB# if os.path.exists(tDir): #LINE# #TAB# #TAB# for file in glob.glob1(tDir, '*' + SCIPION_JSON_TEMPLATES): #LINE# #TAB# #TAB# #TAB# t = Template(pluginName, os.path.join(tDir, file)) #LINE# #TAB# #TAB# #TAB# tempList.append(t) #LINE# #TAB# return tempList"
"Calculate downsampling value  <code> def calc_downsample(w, h, target=400): ",#LINE# #TAB# if w > h: #LINE# #TAB# #TAB# return h / target #LINE# #TAB# elif h >= w: #LINE# #TAB# #TAB# return w / target
Converts a kOS STRING or STRING_VALUE into a bytes object <code> def string_to_bytes(value: str) ->bytes: ,#LINE# #TAB# len_bytes = bytes([len(value)]) #LINE# #TAB# str_bytes = value.encode('UTF-8') #LINE# #TAB# return len_bytes + str_bytes
Remove text nodes containing only whitespace <code> def cleanup_nodes(doc): ,#LINE# #TAB# for node in doc.documentElement.childNodes: #LINE# #TAB# #TAB# if node.nodeType == Node.TEXT_NODE and node.nodeValue.isspace(): #LINE# #TAB# #TAB# #TAB# doc.documentElement.removeChild(node) #LINE# #TAB# return doc
"Disable tracing for the provided blacklist URLs by default not tracing the exporter url  <code> def disable_tracing_hostname(url, blacklist_hostnames=None): ","#LINE# #TAB# if blacklist_hostnames is None: #LINE# #TAB# #TAB# _tracer = execution_context.get_opencensus_tracer() #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# blacklist_hostnames = [ #LINE# #TAB# #TAB# #TAB# #TAB# '{}:{}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# _tracer.exporter.host_name, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# _tracer.exporter.port #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# ] #LINE# #TAB# #TAB# except(AttributeError): #LINE# #TAB# #TAB# #TAB# blacklist_hostnames = [] #LINE# #TAB# return url in blacklist_hostnames"
"` ` int SDL_SemPost(SDL_sem * ) ` ` Atomically increases the semaphore 's count ( not blocking ) . : return : 0 , or -1 on error  <code> def sdl_sempost(sem): ","#LINE# #TAB# sem_c = unbox(sem, 'SDL_sem *') #LINE# #TAB# rc = lib.sdl_sempost(sem_c) #LINE# #TAB# if rc == -1: #LINE# #TAB# #TAB# raise SDLError() #LINE# #TAB# return rc"
"Returns a JobBinary that does not contain a data field The data column uses deferred loading  <code> def job_binary_create(context, values): ",#LINE# #TAB# job_binary = m.JobBinary() #LINE# #TAB# job_binary.update(values) #LINE# #TAB# session = get_session() #LINE# #TAB# try: #LINE# #TAB# #TAB# with session.begin(): #LINE# #TAB# #TAB# #TAB# session.add(job_binary) #LINE# #TAB# except db_exc.DBDuplicateEntry as e: #LINE# #TAB# #TAB# raise ex.DBDuplicateEntry(_('Duplicate entry for JobBinary: %s') % #LINE# #TAB# #TAB# #TAB# e.columns) #LINE# #TAB# return job_binary
"Return generator of provided queue : param q : queue : type q : Queue . Queue : return : generator : rtype generator <code> def queue_get_all(cls, q): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# yield q.get_nowait() #LINE# #TAB# except Queue.Empty: #LINE# #TAB# #TAB# pass
Take a reporter editions dict and flatten it returning a dict for use in the DictWriter  <code> def make_editions_dict(editions): ,"#LINE# #TAB# d = {} #LINE# #TAB# nums = ['1', '2', '3', '4', '5', '6'] #LINE# #TAB# num_counter = 0 #LINE# #TAB# for k, date_dict in editions.items(): #LINE# #TAB# #TAB# d['edition%s' % nums[num_counter]] = k #LINE# #TAB# #TAB# if date_dict['start'] is not None: #LINE# #TAB# #TAB# #TAB# d['start_e%s' % nums[num_counter]] = date_dict['start'].isoformat() #LINE# #TAB# #TAB# if date_dict['end'] is not None: #LINE# #TAB# #TAB# #TAB# d['end_e%s' % nums[num_counter]] = date_dict['end'].isoformat() #LINE# #TAB# #TAB# num_counter += 1 #LINE# #TAB# return d"
"Method by which a message of type ' GOSSIP NOTIFY ' is packed / encoded : param data_type : the data type of the message : return : dict , code and data <code> def pack_gossip_notify(data_type): ","#LINE# #TAB# b_reserved = bytes([0]) #LINE# #TAB# b_type = short_to_bytes(data_type) #LINE# #TAB# return {'code': MESSAGE_CODE_NOTIFY, 'data': b_reserved + b_reserved + #LINE# #TAB# #TAB# b_type}"
"Parse bracketed list  <code> def bracketed_list(l, r, sep, expr, allow_missing_close=False): ","#LINE# strict = l != '[' #LINE# closer = sym(r) if not allow_missing_close else p.Optional(sym(r)) #LINE# if strict: #LINE# #TAB# return sym(l) - listMembers(sep, expr) - closer #LINE# else: #LINE# #TAB# return sym(l) + listMembers(sep, expr) + closer"
"Download the data from Yann 's website , unless it 's already here  <code> def maybe_download(filename): ","#LINE# #TAB# if not tf.gfile.Exists(WORK_DIRECTORY): #LINE# #TAB# #TAB# tf.gfile.MakeDirs(WORK_DIRECTORY) #LINE# #TAB# filepath = os.path.join(WORK_DIRECTORY, filename) #LINE# #TAB# if not tf.gfile.Exists(filepath): #LINE# #TAB# #TAB# filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, #LINE# #TAB# #TAB# #TAB# filepath) #LINE# #TAB# #TAB# with tf.gfile.GFile(filepath) as f: #LINE# #TAB# #TAB# #TAB# size = f.size() #LINE# #TAB# #TAB# print('Successfully downloaded', filename, size, 'bytes.') #LINE# #TAB# return filepath"
"Linear Weighted Moving Average . Formula : LWMA = SUM(DATA[i ] ) * i / SUM(i ) <code> def linear_weighted_moving_average(data, period): ","#LINE# #TAB# catch_errors.check_for_period_error(data, period) #LINE# #TAB# idx_period = list(range(1, period + 1)) #LINE# #TAB# lwma = [(sum([(i * idx_period[data[idx - (period - 1):idx + 1].index(i) #LINE# #TAB# #TAB# ]) for i in data[idx - (period - 1):idx + 1]]) / sum(range(1, len( #LINE# #TAB# #TAB# data[idx + 1 - period:idx + 1]) + 1))) for idx in range(period - 1, #LINE# #TAB# #TAB# len(data))] #LINE# #TAB# lwma = fill_for_noncomputable_vals(data, lwma) #LINE# #TAB# return lwma"
"Initializes a parameter list . If there is a "" START "" row , then everything below it will be saved as a list  <code> def initialize_from_parammat(paramMat): ",#LINE# #TAB# startFlag = 0 #LINE# #TAB# paramDict = dict() #LINE# #TAB# paramList = list() #LINE# #TAB# for i in paramMat: #LINE# #TAB# #TAB# if startFlag == 0: #LINE# #TAB# #TAB# #TAB# if i[0] == 'START': #LINE# #TAB# #TAB# #TAB# #TAB# startFlag += 1 #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# paramDict[i[0]] = i[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# paramList.append(i) #LINE# #TAB# paramDict['paramList'] = paramList #LINE# #TAB# return paramDict
"Check whether the array has non negative entries  <code> def check_nonnegative(entry: Union[sparse.csr_matrix, np.ndarray]): ",#LINE# #TAB# if not has_nonnegative_entries(entry): #LINE# #TAB# #TAB# raise ValueError('Only nonnegative values are expected.') #LINE# #TAB# else: #LINE# #TAB# #TAB# return
Determine if a string is a valid unit . Args : ustr ( str ) : String representation to test . Returns : bool : True if the string is a valid unit . False otherwise  <code> def is_unit(ustr): ,#LINE# #TAB# ustr = backwards.as_str(ustr) #LINE# #TAB# if is_null_unit(ustr): #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# as_unit(ustr) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Classify atoms in the given formula according to whether they occur as positive or negative literals <code> def classify_atom_occurrences_in_formula(phi: Formula): ,"#LINE# #TAB# pos, neg, fun = set(), set(), set() #LINE# #TAB# for atom, truthval in collect_literals_from_conjunction(phi): #LINE# #TAB# #TAB# if atom.predicate.builtin: #LINE# #TAB# #TAB# #TAB# fun.update(symref(t) for t in collect_unique_nodes(atom, lambda #LINE# #TAB# #TAB# #TAB# #TAB# x: isinstance(x, CompoundTerm))) #LINE# #TAB# #TAB# elif truthval: #LINE# #TAB# #TAB# #TAB# pos.add(symref(atom)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# neg.add(symref(atom)) #LINE# #TAB# return pos, neg, fun"
"Enumerate over SQLAlchemy query object ` ` q ` ` and yield individual results fetched in batches of size ` ` limit ` ` using SQL LIMIT and OFFSET  <code> def enumerate_query_by_limit(q, limit=1000): ","#LINE# #TAB# for offset in count(0, limit): #LINE# #TAB# #TAB# r = q.offset(offset).limit(limit).all() #LINE# #TAB# #TAB# for row in r: #LINE# #TAB# #TAB# #TAB# yield row #LINE# #TAB# #TAB# if len(r) < limit: #LINE# #TAB# #TAB# #TAB# break"
"load a dict from a string <code> def load_dict(dict_str, str_ok=False): ",#LINE# #TAB# if str_ok: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mydict = json.loads(dict_str) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# mydict = dict_str #LINE# #TAB# else: #LINE# #TAB# #TAB# my_dict = json.loads(dict_str) #LINE# #TAB# return dict_str
Wrapper to work around lack of .total_seconds ( ) method in Python 3.1  <code> def total_seconds(td): ,"#LINE# #TAB# if hasattr(td, 'total_seconds'): #LINE# #TAB# #TAB# return td.total_seconds() #LINE# #TAB# return td.days * 3600 * 24 + td.seconds + td.microseconds / 100000.0"
"Returns image choices for the prompt . : param list images : list of digitalocean . Image . Image instances : return : list of 2-len tuples <code> def prepare_image_choices(images: List[Image]) ->List[Tuple[str, Image]]: ","#LINE# #TAB# filtered_images = filter(lambda x: x.slug is not None, images) #LINE# #TAB# sorted_images = sorted(filtered_images, key=lambda x: x.slug, reverse=True) #LINE# #TAB# return [(f'{image.name} ({image.distribution})', image) for image in #LINE# #TAB# #TAB# sorted_images if image.public and image.type == 'snapshot']"
"Choose the most common item from the list , or the first item if all items are unique  <code> def most_common_item(lst): ","#LINE# #TAB# lst = [l for l in lst if l] #LINE# #TAB# if lst: #LINE# #TAB# #TAB# return max(set(lst), key=lst.count) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
"Remove a preorder hash  <code> def namedb_preorder_remove( cur, preorder_hash ): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# query, values = namedb_delete_prepare( cur, 'preorder_hash', preorder_hash, 'preorders' ) #LINE# #TAB# except Exception, e: #LINE# #TAB# #TAB# log.exception(e) #LINE# #TAB# #TAB# log.error(""FATAL: Failed to delete preorder with hash '%s'"" % preorder_hash ) #LINE# #TAB# #TAB# os.abort() #LINE# #TAB# log.debug(namedb_format_query(query, values)) #LINE# #TAB# namedb_query_execute( cur, query, values ) #LINE# #TAB# return True"
"Get the current focused declaration object . Returns ------- result : Widget or None The declaration for the currently focused proxy , or None if the current focus widget does not map to a proxy  <code> def focused_declaration(): ",#LINE# #TAB# fp = focused_proxy() #LINE# #TAB# return fp and fp.declaration
"Decodes the specified integer representing edges in Cortex graph . Returns a tuple ( forward , reverse ) which contain the list of nucleotides that we append to a kmer and its reverse complement to obtain other kmers in the Cortex graph  <code> def decode_edges(edges): ","#LINE# #TAB# bases = ['A', 'C', 'G', 'T'] #LINE# #TAB# fwd = [] #LINE# #TAB# for j in range(4): #LINE# #TAB# #TAB# if 1 << j & edges != 0: #LINE# #TAB# #TAB# #TAB# fwd.append(bases[j]) #LINE# #TAB# rev = [] #LINE# #TAB# bases.reverse() #LINE# #TAB# for j in range(4): #LINE# #TAB# #TAB# if 1 << j + 4 & edges != 0: #LINE# #TAB# #TAB# #TAB# rev.append(bases[j]) #LINE# #TAB# return fwd, rev"
"Return the inertia tensor of a cylinder  <code> def cylinder_inertia(mass, radius, height, transform=None): ","#LINE# #TAB# h2, r2 = height ** 2, radius ** 2 #LINE# #TAB# diagonal = np.array([((mass * h2) / 12) + ((mass * r2) / 4), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ((mass * h2) / 12) + ((mass * r2) / 4), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# (mass * r2) / 2]) #LINE# #TAB# inertia = diagonal * np.eye(3) #LINE# #TAB# if transform is not None: #LINE# #TAB# #TAB# inertia = transform_inertia(transform, inertia) #LINE# #TAB# return inertia"
"Filter list of repositories acc . to the list passed : param repos : list of repos to check pull request for : param ignore_repos : list of repos to ignore : return : filtered list of repos <code> def filter_repos(repos, ignore_repos): ","#LINE# #TAB# logger.info('Filtering list of repositories') #LINE# #TAB# if not ignore_repos: #LINE# #TAB# #TAB# return repos #LINE# #TAB# else: #LINE# #TAB# #TAB# final_list_of_repos = filter(lambda x: x.name not in ignore_repos, #LINE# #TAB# #TAB# #TAB# repos) #LINE# #TAB# #TAB# logger.info('Final filtered repository count is ' + str(len( #LINE# #TAB# #TAB# #TAB# final_list_of_repos))) #LINE# #TAB# #TAB# return final_list_of_repos"
Filter to convert given timestamp to UTC date / time  <code> def timestamp_utc(value): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return dt_util.utc_from_timestamp(value).strftime(DATE_STR_FORMAT) #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# return value"
"return an english string representing a rate . delta is assumed to be a rate as "" 1 per delta ""  <code> def natural_rate(delta: Union[float, datetime.timedelta]): ","#LINE# #TAB# if isinstance(delta, datetime.timedelta): #LINE# #TAB# #TAB# delta = delta.total_seconds() #LINE# #TAB# rate = 1 / delta #LINE# #TAB# for i, multiplier in enumerate((1, 60, 60, 24, 7)): #LINE# #TAB# #TAB# rate *= multiplier #LINE# #TAB# #TAB# if rate > 1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# duration = ('second', 'minute', 'hour', 'day', 'week')[i] #LINE# #TAB# return f'{rate:.2g} per {duration}'"
Open a PDF file with PyPDF2  <code> def read_pdf(filename): ,"#LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# raise CommandError('{} does not exist'.format(filename)) #LINE# #TAB# pdf = PdfFileReader(open(filename, 'rb')) #LINE# #TAB# if pdf.isEncrypted: #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# pw = prompt_for_pw(filename) #LINE# #TAB# #TAB# #TAB# matched = pdf.decrypt(pw) #LINE# #TAB# #TAB# #TAB# if matched: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# print('The password did not match.') #LINE# #TAB# return pdf"
"convert argparsed strings to lists for an argument <code> def convert_strings_to_list(args, argname): ","#LINE# #TAB# if argname in args: #LINE# #TAB# #TAB# args[argname] = args[argname].split(',') if args[argname] else [] #LINE# #TAB# #TAB# return [arg.strip() for arg in args[argname] if arg.strip()] #LINE# #TAB# return []"
Used to get credentials from bearer auth : param auth : < str > : return : < dict > <code> def get_bearer_auth_credentials(auth): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# session_token = auth.strip() #LINE# #TAB# except binascii.Error: #LINE# #TAB# #TAB# raise HTTPBadRequest() #LINE# #TAB# try: #LINE# #TAB# #TAB# session = validate_session(session_token) #LINE# #TAB# except OktaError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return {'client_id': session['user_id'], 'client_secret': None, #LINE# #TAB# #TAB# 'access_token': session_token}"
mimetypes.guess_extension introduces some bizarre randomness . This function removes it for the extensions that interest us most  <code> def extension_for_type(mime_type): ,#LINE# #TAB# ext = None #LINE# #TAB# if 'image/png' == mime_type: #LINE# #TAB# #TAB# ext = '.png' #LINE# #TAB# elif 'text/plain' == mime_type: #LINE# #TAB# #TAB# ext = '.txt' #LINE# #TAB# elif 'text/html' == mime_type: #LINE# #TAB# #TAB# ext = '.html' #LINE# #TAB# elif 'application/json' == mime_type: #LINE# #TAB# #TAB# ext = '.json' #LINE# #TAB# elif mime_type: #LINE# #TAB# #TAB# ext = mimetypes.guess_extension(mime_type) #LINE# #TAB# return ext or '.bin'
"Return a ChEBI name corresponding to the given ChEBI ID  <code> def get_chebi_name_from_id(chebi_id, offline=False): ",#LINE# #TAB# chebi_name = chebi_id_to_name.get(chebi_id) #LINE# #TAB# if chebi_name is None and not offline: #LINE# #TAB# #TAB# chebi_name = get_chebi_name_from_id_web(chebi_id) #LINE# #TAB# return chebi_name
"Get an actor by its universally unique URN . : param actor_urn : actor URN : type actor_urn : string : returns : : class:`pykka . ActorRef ` or : class:`None ` if not found <code> def get_by_urn(cls, actor_urn): ",#LINE# #TAB# with cls._actor_refs_lock: #LINE# #TAB# #TAB# refs = [ref for ref in cls._actor_refs if ref.actor_urn == actor_urn] #LINE# #TAB# #TAB# if refs: #LINE# #TAB# #TAB# #TAB# return refs[0]
"Return resource referenced by bookmark <code> def get_document_by_bookmark(bookmark: str) ->Dict[str, Any]: ","#LINE# #TAB# payload = {'bookMark': bookmark} #LINE# #TAB# resp = sess.get(LIBRARY_URL, params=payload) #LINE# #TAB# resp_json = resp.json() #LINE# #TAB# if resp_json.get('errors'): #LINE# #TAB# #TAB# for error in resp_json.get('errors'): #LINE# #TAB# #TAB# #TAB# if error['code'] == 'invalid.bookmark.format': #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError('Invalid bookmark') #LINE# #TAB# results = _extract_results(resp_json) #LINE# #TAB# return results"
"add new path build on idx to all paths <code> def update_paths(paths, idx): ","#LINE# #TAB# if paths: #LINE# #TAB# #TAB# return paths + ['{}-{}'.format(paths[-1], idx)] #LINE# #TAB# return [str(idx)]"
Copy a RO object into a RW structure made with standard Python classes  <code> def make_rw(obj: Any): ,"#LINE# #TAB# if isinstance(obj, RoDict): #LINE# #TAB# #TAB# return {k: make_rw(v) for k, v in obj.items()} #LINE# #TAB# elif isinstance(obj, RoList): #LINE# #TAB# #TAB# return [make_rw(x) for x in obj] #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
Flattens a given material . Args : material ( dict ) : material config . Returns : list <code> def flatten_material(material): ,"#LINE# #TAB# lattice = material['lattice'] #LINE# #TAB# return [material['_id'], material['name'], ', '.join(material['tags']), #LINE# #TAB# #TAB# len(material['basis']['coordinates']), lattice['a'], lattice['b'], #LINE# #TAB# #TAB# lattice['c'], lattice['alpha'], lattice['beta'], lattice['gamma']]"
"Just read the config for a project <code> def read_config(fs, settings_path='settings.ini'): ","#LINE# #TAB# if '://' in fs: #LINE# #TAB# #TAB# fs = open_fs(fs) #LINE# #TAB# else: #LINE# #TAB# #TAB# fs = OSFS(fs) #LINE# #TAB# cfg = SettingsContainer.read(fs, settings_path) #LINE# #TAB# return cfg"
checks if the structure object is charge balanced <code> def _is_charge_balanced(struct): ,#LINE# #TAB# #TAB# if sum([s.specie.oxi_state for s in struct.sites]) == 0.0: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False
"Computes symmetric normalization of A , dealing with sparse A and batch mode automatically . : param A : Tensor or SparseTensor with rank k = { 2 , 3 } . : return : Tensor or SparseTensor of rank k <code> def normalize_a(A): ","#LINE# #TAB# D = degrees(A) #LINE# #TAB# D = tf.sqrt(D)[:, (None)] + K.epsilon() #LINE# #TAB# perm = (0, 2, 1) if K.ndim(A) == 3 else (1, 0) #LINE# #TAB# output = A / D / ops.transpose(D, perm=perm) #LINE# #TAB# return output"
Set native ( unsorted ) CUDA device ID Parameters ---------- idx : int . Specifies the ( unsorted ) native index of the device  <code> def set_native_id(idx): ,#LINE# #TAB# import ctypes as ct #LINE# #TAB# from .util import safe_call as safe_call #LINE# #TAB# from .library import backend as backend #LINE# #TAB# if backend.name() != 'cuda': #LINE# #TAB# #TAB# raise RuntimeError('Invalid backend loaded') #LINE# #TAB# safe_call(backend.get().afcu_set_native_id(idx)) #LINE# #TAB# return
Return maximum number of MIDI outputs . Returns ------- max_outputs : int Maximum number of MIDI outputs  <code> def get_max_outputs(): ,#LINE# #TAB# max_outputs = RPR.GetMaxMidiOutputs() #LINE# #TAB# return max_outputs
Merges concatanates given input array into lines of strings . Args : input_array ( array ) : Parsed string of array  <code> def merge_input(input_array): ,"#LINE# #TAB# product(*input_array) #LINE# #TAB# with open('merged.txt', 'w') as merged_file: #LINE# #TAB# #TAB# for input_list in product(*input_array): #LINE# #TAB# #TAB# #TAB# merged_str = ''.join(str(e) for e in input_list) #LINE# #TAB# #TAB# #TAB# merged_file.write(merged_str + '\n') #LINE# #TAB# merged_file.close() #LINE# #TAB# print('Merged file created successfully!') #LINE# #TAB# return True"
"Similar to SortVersions , but take the list of versions returned by FindVersions  <code> def sort_version_tuples(versions, reverse=False): ","#LINE# #TAB# vers_map = dict([(t[1], t) for t in versions[::-1]]) #LINE# #TAB# return [vers_map[k] for k in SortVersions(vers_map, reverse)]"
"Is this at least libvips x.y ? <code> def at_least_libvips(x, y): ",#LINE# #TAB# major = version(0) #LINE# #TAB# minor = version(1) #LINE# #TAB# return major > x or major == x and minor >= y
"Checks if the signature is compatible with signature . @param signature : string The signature to check if compatible . @param withSignature : string The signature to check with . @return : boolean True if the signature is compatible , False otherwise  <code> def is_compatible(signature, withSignature): ","#LINE# #TAB# assert isinstance(signature, str), 'Invalid signature %s' % signature #LINE# #TAB# assert isinstance(withSignature, str #LINE# #TAB# #TAB# ), 'Invalid with signature %s' % withSignature #LINE# #TAB# if signature == withSignature: #LINE# #TAB# #TAB# return True #LINE# #TAB# if not len(withSignature) > 8: #LINE# #TAB# #TAB# return False #LINE# #TAB# if len(signature) > 8: #LINE# #TAB# #TAB# signature = signature[:8] #LINE# #TAB# for k in range(0, len(withSignature), 8): #LINE# #TAB# #TAB# if signature == withSignature[k:k + 8]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Return a positive CONTENT_LENGTH in a safe way ( return 0 otherwise )  <code> def get_content_length(environ): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return max(0, int(environ.get('CONTENT_LENGTH', 0))) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 0"
"this mutates passed in ex ( any BaseX ) , replacing nested selects with their ( flattened ) output <code> def replace_subqueries(ex, tables, table_ctor): ","#LINE# #TAB# if isinstance(ex, sqparse2.SelectX): #LINE# #TAB# #TAB# old_tables, ex.tables = ex.tables, [] #LINE# #TAB# for path in treepath.sub_slots(ex, lambda x: isinstance(x, sqparse2. #LINE# #TAB# #TAB# SelectX)): #LINE# #TAB# #TAB# ex[path] = sqparse2.Literal(flatten_scalar(run_select(ex[path], #LINE# #TAB# #TAB# #TAB# tables, table_ctor))) #LINE# #TAB# if isinstance(ex, sqparse2.SelectX): #LINE# #TAB# #TAB# ex.tables = old_tables #LINE# #TAB# return ex"
"Perform a single conversion from an input buffer to an output buffer  <code> def src_simple(input_data, output_data, ratio, converter_type, channels): ","#LINE# #TAB# input_frames, _ = _check_data(input_data) #LINE# #TAB# output_frames, _ = _check_data(output_data) #LINE# #TAB# data = ffi.new('SRC_DATA*') #LINE# #TAB# data.input_frames = input_frames #LINE# #TAB# data.output_frames = output_frames #LINE# #TAB# data.src_ratio = ratio #LINE# #TAB# data.data_in = ffi.cast('float*', ffi.from_buffer(input_data)) #LINE# #TAB# data.data_out = ffi.cast('float*', ffi.from_buffer(output_data)) #LINE# #TAB# error = _lib.src_simple(data, converter_type, channels) #LINE# #TAB# return error, data.input_frames_used, data.output_frames_gen"
"Normalize the host part of the URL <code> def normalize_host(scheme, host): ","#LINE# #TAB# host, _, port = host.partition(':') #LINE# #TAB# if port == '80' and scheme in ('http', None): #LINE# #TAB# #TAB# port = '' #LINE# #TAB# if port == '443' and scheme == 'https': #LINE# #TAB# #TAB# port = '' #LINE# #TAB# if port: #LINE# #TAB# #TAB# return '{0}:{1}'.format(host, port) #LINE# #TAB# return host"
"Check if user can create topic . Args : category ( str ) : Category name . forum ( obj ) : Object forum . user ( obj ) : Object user . Returns : bool : If the user can create topic  <code> def user_can_create_topic(category, forum, user): ","#LINE# #TAB# is_moderator = is_user_moderator_forum(category, forum, user) #LINE# #TAB# is_register = Register.objects.filter(forum=forum, user=user).count() #LINE# #TAB# if user.is_superuser or is_moderator or is_register > 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Validate bids unless opts say otherwise <code> def validate_bids(opts): ,"#LINE# #TAB# from ..utils.bids import validate_input_dir #LINE# #TAB# if not (opts.recon_only or opts.skip_bids_validation): #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# 'Making sure the input data is BIDS compliant (warnings can be ignored in most cases).' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# validate_input_dir(os.name, opts.bids_dir, opts.participant_label) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"Create , populate and return the VersioneerConfig ( ) object  <code> def get_config(): ",#LINE# #TAB# cfg = VersioneerConfig() #LINE# #TAB# cfg.VCS = 'git' #LINE# #TAB# cfg.style = 'pep440' #LINE# #TAB# cfg.tag_prefix = 'None' #LINE# #TAB# cfg.parentdir_prefix = 'None' #LINE# #TAB# cfg.versionfile_source = 'src/ikwilnaarhuis/_version.py' #LINE# #TAB# cfg.verbose = False #LINE# #TAB# return cfg
"Returns helpful similar matches for an invalid flag  <code> def get_flag_suggestions(attempt, longopt_list): ","#LINE# if len(attempt) <= 2 or not longopt_list: #LINE# #TAB# return [] #LINE# option_names = [v.split('=')[0] for v in longopt_list] #LINE# distances = [(_damerau_levenshtein(attempt, option[0:len(attempt)]), option) #LINE# #TAB# #TAB# #TAB# for option in option_names] #LINE# distances.sort() #LINE# least_errors, _ = distances[0] #LINE# if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len(attempt): #LINE# #TAB# return [] #LINE# suggestions = [] #LINE# for errors, name in distances: #LINE# #TAB# if errors == least_errors: #LINE# #TAB# suggestions.append(name) #LINE# #TAB# else: #LINE# #TAB# break #LINE# return suggestions"
Checks if claimset_data contains tuples with an integer entry . : param claimset_data : list of tuples or strings . : type claimset_data : list of tuples or strings : return : true if tuples with numbers ; false if not <code> def check_for_number(claimset_data): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# for entry in claimset_data: #LINE# #TAB# #TAB# #TAB# if len(entry) <= 1: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# elif not isinstance(entry[0], int): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return True #LINE# #TAB# except: #LINE# #TAB# #TAB# return False"
"Whether end of alignment segment is a linear end Parameters ----- cigar_tuple : tuple of cigar Returns ----- int 1 for linear end , 0 for ambiguous end <code> def is_linear(cigar_tuple): ",#LINE# #TAB# if cigar_tuple[0] == 0 and cigar_tuple[1] >= 5: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
Configure whether this module should assume that it is being run from a jupyter notebook . This sets some global variables related to how progress for long measurement sequences is indicated  <code> def notebook_mode(m): ,#LINE# #TAB# global NOTEBOOK_MODE #LINE# #TAB# global TRANGE #LINE# #TAB# NOTEBOOK_MODE = m #LINE# #TAB# if NOTEBOOK_MODE: #LINE# #TAB# #TAB# TRANGE = tqdm.tnrange #LINE# #TAB# else: #LINE# #TAB# #TAB# TRANGE = tqdm.trange
Create a Python identifier for the detector to be used as the class name <code> def class_name_from_prefix(prefix): ,#LINE# #TAB# class_name = category_to_identifier(prefix).capitalize() #LINE# #TAB# if class_name.startswith('_'): #LINE# #TAB# #TAB# return 'Detector' + class_name.lstrip('_').capitalize() #LINE# #TAB# return class_name
Load library by searching possible path  <code> def load_lib(): ,"#LINE# #TAB# lib_path = libinfo.find_lib_path() #LINE# #TAB# lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL) #LINE# #TAB# lib.MXGetLastError.restype = ctypes.c_char_p #LINE# #TAB# return lib"
"Start and bind the FluidDB client : param app : The Flask instance : param sandbox : Whether to use the sandbox , optional , default : False <code> def init_fluiddb(app, sandbox=False): ",#LINE# #TAB# if sandbox: #LINE# #TAB# #TAB# fluid = sandbox_fluid() #LINE# #TAB# else: #LINE# #TAB# #TAB# fluid = Fluid() #LINE# #TAB# #TAB# fluid.bind() #LINE# #TAB# app.fluiddb = fluid #LINE# #TAB# return fluid
"A function that converts the x , y coordinates to polar ones . -Does not do the circular correction <code> def cartesian_to_polar(x, y, center): ","#LINE# #TAB# corrected_x = x - center[0] #LINE# #TAB# corrected_y = y - center[1] #LINE# #TAB# theta = np.arctan2(corrected_y, corrected_x) #LINE# #TAB# r = np.sqrt(corrected_x ** 2 + corrected_y ** 2) #LINE# #TAB# return theta, r"
Minimizes aliasing effects on spectra and cross - spectra df : pandas . DataFrame containing spectrum or cross - spectrum <code> def anti_aliasing(spec): ,#LINE# #TAB# RA = np.array([(1.0 + np.cos(np.pi * k / N)) for k in range(N / 2 + 1)] #LINE# #TAB# #TAB# ) / 2.0 #LINE# #TAB# return spec * RA ** 2.0
"Helper method for ` ` get_tree ` `  <code> def node_to_dict(cls, node, json, json_fields): ","#LINE# #TAB# if json: #LINE# #TAB# #TAB# pk_name = node.get_pk_name() #LINE# #TAB# #TAB# result = {'id': getattr(node, pk_name), 'label': node.__repr__()} #LINE# #TAB# #TAB# if json_fields: #LINE# #TAB# #TAB# #TAB# result.update(json_fields(node)) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = {'node': node} #LINE# #TAB# return result"
"This function is needed for Python 3 , because a subprocess can return bytes instead of a string  <code> def decode_to_string(toDecode): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return toDecode.decode('utf-8') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return toDecode
Get a list of the parameters in the operator . Parameters ---------- operator : LinOp The operator to extract the parameters from . Returns ------- list A list of parameter objects  <code> def get_expr_params(operator): ,"#LINE# #TAB# if operator.type == lo.PARAM: #LINE# #TAB# #TAB# return operator.data.parameters() #LINE# #TAB# else: #LINE# #TAB# #TAB# params = [] #LINE# #TAB# #TAB# for arg in operator.args: #LINE# #TAB# #TAB# #TAB# params += get_expr_params(arg) #LINE# #TAB# #TAB# if isinstance(operator.data, lo.LinOp): #LINE# #TAB# #TAB# #TAB# params += get_expr_params(operator.data) #LINE# #TAB# #TAB# return params"
"Set up X10 dimmers over a mochad controller  <code> def setup_platform(hass, config, add_entities, discovery_info=None): ","#LINE# #TAB# devs = config.get(CONF_DEVICES) #LINE# #TAB# add_entities([MochadLight(hass, CONTROLLER.ctrl, dev) for dev in devs]) #LINE# #TAB# return True"
"Open file for reading respecting Python version and OS differences . Sets newline to Linux line endings on Windows and Python 3 When encode = False does not set encoding on nix and Python 3 to keep as bytes <code> def open_fr(file_name, encoding=ENCODING, encode=True): ","#LINE# #TAB# if sys.version_info >= (3, 0, 0): #LINE# #TAB# #TAB# if os.name == 'nt': #LINE# #TAB# #TAB# #TAB# file_obj = io.open(file_name, 'r', newline='', encoding=encoding) #LINE# #TAB# #TAB# elif encode: #LINE# #TAB# #TAB# #TAB# file_obj = io.open(file_name, 'r', encoding=encoding) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# file_obj = io.open(file_name, 'r') #LINE# #TAB# else: #LINE# #TAB# #TAB# file_obj = io.open(file_name, 'r', encoding=encoding) #LINE# #TAB# return file_obj"
"Translate the values of the list of 3-tuples according to the provided look - up table Is a generator . : param list triangle_list : a list of 3-tuples of integers : param dict lut : a dictionary mapping current to new values <code> def translate_indexes(triangle_list, lut): ","#LINE# #TAB# for t0, t1, t2 in triangle_list: #LINE# #TAB# #TAB# yield lut[t0], lut[t1], lut[t2]"
Return true if cls has on overridable _ _ init _ _  <code> def ctor_overridable(cls): ,"#LINE# #TAB# prev_init = getattr(cls, '__init__', None) #LINE# #TAB# if not callable(prev_init): #LINE# #TAB# #TAB# return True #LINE# #TAB# if prev_init in [object.__init__, _auto_init]: #LINE# #TAB# #TAB# return True #LINE# #TAB# if getattr(prev_init, '_clobber_ok', False): #LINE# #TAB# #TAB# return True #LINE# #TAB# print(cls, prev_init, getattr(prev_init, '_clobber_ok', 'missing')) #LINE# #TAB# return False"
"Transforms an unpacked SmartyParseObject into a .utils . Ghid . If using algo zero , also eliminates the address and replaces with None  <code> def ghid_transform(unpacked_spo): ","#LINE# #TAB# ghid = Ghid(algo=unpacked_spo['algo'], address=unpacked_spo['address']) #LINE# #TAB# return ghid"
"Transform a vtkActor and return a new object  <code> def transform_filter(actor, transformation): ","#LINE# #TAB# tf = vtk.vtkTransformPolyDataFilter() #LINE# #TAB# tf.SetTransform(transformation) #LINE# #TAB# prop = None #LINE# #TAB# if isinstance(actor, vtk.vtkPolyData): #LINE# #TAB# #TAB# tf.SetInputData(actor) #LINE# #TAB# else: #LINE# #TAB# #TAB# tf.SetInputData(actor.polydata()) #LINE# #TAB# #TAB# prop = vtk.vtkProperty() #LINE# #TAB# #TAB# prop.DeepCopy(actor.GetProperty()) #LINE# #TAB# tf.Update() #LINE# #TAB# tfa = Actor(tf.GetOutput()) #LINE# #TAB# if prop: #LINE# #TAB# #TAB# tfa.SetProperty(prop) #LINE# #TAB# return tfa"
"Use pandas to group clusters by cluster identifier df = pandas dataframe cluster_id = column that identifies cluster membership , which can be a basic string like "" hospital cluster A "" date_col = onset or report date column <code> def group_clusters(df, cluster_id, date_col): ",#LINE# #TAB# clusters = df[df[date_col].notnull()] #LINE# #TAB# groups = clusters.groupby(clusters[cluster_id]) #LINE# #TAB# return groups
"Run namedex in the commandline This function runs when the module is ran as a script . It only prints the results , it does not output to stout  <code> def command_line(): ",#LINE# #TAB# name = input('Enter name: ') #LINE# #TAB# print(create_soundex(name)) #LINE# #TAB# return 0
"Trace every executed line  <code> def trace_full (frame, event, arg): ","#LINE# #TAB# if event == ""line"": #LINE# #TAB# #TAB# _trace_line(frame, event, arg) #LINE# #TAB# else: #LINE# #TAB# #TAB# _trace(frame, event, arg) #LINE# #TAB# return trace_full"
Generates a random long . The length of said long varies by platform  <code> def get_long(): ,"#LINE# #TAB# pbRandomData = c_ulong() #LINE# #TAB# size_of_long = wintypes.DWORD(sizeof(pbRandomData)) #LINE# #TAB# ok = c_int() #LINE# #TAB# hProv = c_ulong() #LINE# #TAB# ok = windll.Advapi32.CryptAcquireContextA(byref(hProv), None, None, PROV_RSA_FULL, 0) #LINE# #TAB# ok = windll.Advapi32.CryptGenRandom(hProv, size_of_long, byref(pbRandomData)) #LINE# #TAB# return pbRandomData.value"
"Normalizes given node as str or dict with headers <code> def normalize_node(node, headers=None): ","#LINE# #TAB# headers = {} if headers is None else headers #LINE# #TAB# if isinstance(node, str): #LINE# #TAB# #TAB# url = normalize_url(node) #LINE# #TAB# #TAB# return {'endpoint': url, 'headers': headers} #LINE# #TAB# url = normalize_url(node['endpoint']) #LINE# #TAB# node_headers = node.get('headers', {}) #LINE# #TAB# return {'endpoint': url, 'headers': {**headers, **node_headers}}"
Calculate the remaining bytes of a UTF-8 sequence if given the first character . UTF-8 ONLY ! <code> def remaining_bytes(char): ,"#LINE# #TAB# num = ord(char) #LINE# #TAB# remaining = -1 #LINE# #TAB# while num > 127: #LINE# #TAB# #TAB# remaining += 1 #LINE# #TAB# #TAB# num = (num << 1) % 256 #LINE# #TAB# if remaining == 0 or remaining > 3: #LINE# #TAB# #TAB# raise RuntimeError('Not valid UTF-8 leading byte: {}'.format(bin( #LINE# #TAB# #TAB# #TAB# ord(char)))) #LINE# #TAB# assert remaining in (-1, 1, 2, 3) #LINE# #TAB# return remaining"
"Checks if a given gtype is sane <code> def is_grouping_sane(cls, gtype): ","#LINE# #TAB# if gtype == cls.SHUFFLE or gtype == cls.ALL or gtype == cls.LOWEST or gtype == cls.NONE: #LINE# #TAB# return True #LINE# #TAB# elif isinstance(gtype, cls.FIELDS): #LINE# #TAB# return gtype.gtype == topology_pb2.Grouping.Value(""FIELDS"") and \ #LINE# #TAB# #TAB# #TAB# gtype.fields is not None #LINE# #TAB# elif isinstance(gtype, cls.CUSTOM): #LINE# #TAB# return gtype.gtype == topology_pb2.Grouping.Value(""CUSTOM"") and \ #LINE# #TAB# #TAB# #TAB# gtype.python_serialized is not None #LINE# #TAB# else: #LINE# #TAB# return False"
"Calculates the effective outer indices of the intermediate tensor corresponding to the subgraph ` ` s ` `  <code> def dp_calc_legs(g, all_tensors, s, inputs, i1_cut_i2_wo_output, i1_union_i2): ","#LINE# #TAB# r = g & (all_tensors ^ s) #LINE# #TAB# if r: #LINE# #TAB# #TAB# i_r = set.union(*_bitmap_select(r, inputs)) #LINE# #TAB# else: #LINE# #TAB# #TAB# i_r = set() #LINE# #TAB# i_contract = i1_cut_i2_wo_output - i_r #LINE# #TAB# return i1_union_i2 - i_contract"
"A helper function that wraps socket.getaddrinfo and returns None when it fails to , e.g. resolve one of the hostnames . Used to address PYTHON-895  <code> def addrinfo_or_none(contact_point, port): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, #LINE# #TAB# #TAB# #TAB# socket.SOCK_STREAM) #LINE# #TAB# except socket.gaierror: #LINE# #TAB# #TAB# log.debug('Could not resolve hostname ""{}"" with port {}'.format( #LINE# #TAB# #TAB# #TAB# contact_point, port)) #LINE# #TAB# #TAB# return None"
Make Pylint aware of our custom logger methods  <code> def verboselogs_class_transform(cls): ,"#LINE# #TAB# if cls.name == 'RootLogger': #LINE# #TAB# #TAB# for meth in ['notice', 'spam', 'success', 'verbose']: #LINE# #TAB# #TAB# #TAB# cls.locals[meth] = [scoped_nodes.Function(meth, None)]"
Compute mapping from parameters to devices by looking at the device option of the op that creates the blob has <code> def infer_blob_devices(net): ,#LINE# #TAB# mapping = {} #LINE# #TAB# for op in net.Proto().op: #LINE# #TAB# #TAB# op_device = op.device_option #LINE# #TAB# #TAB# if op_device is None: #LINE# #TAB# #TAB# #TAB# op_device = caffe2_pb2.DeviceOption(caffe2_pb2.CPU) #LINE# #TAB# #TAB# for b in op.output: #LINE# #TAB# #TAB# #TAB# mapping[b] = op_device #LINE# #TAB# return mapping
Get applicant from email address  <code> def get_applicant_from_email(email): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# applicant = Person.active.get(email=email) #LINE# #TAB# #TAB# existing_person = True #LINE# #TAB# except Person.DoesNotExist: #LINE# #TAB# #TAB# applicant = Applicant.objects.create(email=email) #LINE# #TAB# #TAB# existing_person = False #LINE# #TAB# except Person.MultipleObjectsReturned: #LINE# #TAB# #TAB# applicant = None #LINE# #TAB# #TAB# existing_person = False #LINE# #TAB# return applicant, existing_person"
"Summarizes a single nc file given its basename  <code> def nc_file_summary(cls, nc_filename): ",#LINE# #TAB# if cls.nc is None: #LINE# #TAB# #TAB# cls.load_file(nc_name) #LINE# #TAB# result = {} #LINE# #TAB# result['flag_counts'] = cls.flag_counts() #LINE# #TAB# return result
"Create windows each containing the same number of accessible bases  <code> def equally_accessible_windows(is_accessible, size, start=0, stop=None, step=None): ","#LINE# #TAB# pos_accessible, = np.nonzero(is_accessible) #LINE# #TAB# pos_accessible += 1 #LINE# #TAB# if start: #LINE# #TAB# #TAB# pos_accessible = pos_accessible[pos_accessible >= start] #LINE# #TAB# if stop: #LINE# #TAB# #TAB# pos_accessible = pos_accessible[pos_accessible <= stop] #LINE# #TAB# windows = moving_statistic(pos_accessible, lambda v: [v[0], v[-1]], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# size=size, step=step) #LINE# #TAB# return windows"
azure notebooks does not allow access to URL parameters . this is a cheap function to parse them when provided manually  <code> def parse_params(url): ,#LINE# #TAB# rc = {} #LINE# #TAB# if url: #LINE# #TAB# #TAB# components = parse.urlsplit(url) #LINE# #TAB# #TAB# rc = dict(parse.parse_qsl(components.query)) #LINE# #TAB# #TAB# rc['url' #LINE# #TAB# #TAB# #TAB# ] = components.scheme + '://' + components.netloc + components.path #LINE# #TAB# #TAB# rc['baseUrl'] = '/'.join(rc['url'].split('/')[:-1]) #LINE# #TAB# return rc
add i d to json rpc if not present <code> def add_get_id(d): ,#LINE# #TAB# if 'id' not in d: #LINE# #TAB# #TAB# d['id'] = str(uuid.uuid4()) #LINE# #TAB# return d['id']
Get a configuration variable group named |name| <code> def get_group(name: str) -> _Group: ,#LINE# #TAB# global _groups #LINE# #TAB# if name in _groups: #LINE# #TAB# #TAB# return _groups[name] #LINE# #TAB# group = _Group(name) #LINE# #TAB# _groups[name] = group #LINE# #TAB# return group
Yield all the uninstallation paths for dist based on RECORD - without - . pyc <code> def uninstallation_paths(dist): ,"#LINE# #TAB# from pip.utils import FakeFile #LINE# #TAB# r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD'))) #LINE# #TAB# for row in r: #LINE# #TAB# #TAB# path = os.path.join(dist.location, row[0]) #LINE# #TAB# #TAB# yield path #LINE# #TAB# #TAB# if path.endswith('.py'): #LINE# #TAB# #TAB# #TAB# dn, fn = os.path.split(path) #LINE# #TAB# #TAB# #TAB# base = fn[:-3] #LINE# #TAB# #TAB# #TAB# path = os.path.join(dn, base + '.pyc') #LINE# #TAB# #TAB# #TAB# yield path"
"forms queue 's context entry <code> def queue_context_entry(exchange, queue_name, routing=None): ","#LINE# #TAB# if routing is None: #LINE# #TAB# #TAB# routing = queue_name #LINE# #TAB# queue_entry = QueueContextEntry(mq_queue=queue_name, mq_exchange= #LINE# #TAB# #TAB# exchange, mq_routing_key=routing) #LINE# #TAB# return queue_entry"
"Parse command - line credential string , e.g. ` cloud_files = my_files ` <code> def parse_credential(value): ","#LINE# #TAB# match = re.match('([A-Za-z]\\w*)=(.+)$', value) #LINE# #TAB# if not match: #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError('Must be in the form of type=name') #LINE# #TAB# return {match.group(1): match.group(2)}"
Calculates the largest divisor of a natural number num_n <code> def largest_div(num_n): ,"#LINE# #TAB# num_d = np.ceil(np.sqrt(num_n)) #LINE# #TAB# while True: #LINE# #TAB# #TAB# if num_n % num_d == 0: #LINE# #TAB# #TAB# #TAB# return int(num_d), int(num_n / num_d) #LINE# #TAB# #TAB# num_d -= 1"
"A function to cover the common case of creating a config for a bot  <code> def bot_config(player_config_path: Path, team: Team) -> 'PlayerConfig': ","#LINE# #TAB# #TAB# bot_config = PlayerConfig() #LINE# #TAB# #TAB# bot_config.bot = True #LINE# #TAB# #TAB# bot_config.rlbot_controlled = True #LINE# #TAB# #TAB# bot_config.team = team.value #LINE# #TAB# #TAB# bot_config.config_path = str(player_config_path.absolute()) #LINE# #TAB# #TAB# config_bundle = get_bot_config_bundle(bot_config.config_path) #LINE# #TAB# #TAB# bot_config.name = config_bundle.name #LINE# #TAB# #TAB# bot_config.loadout_config = load_bot_appearance(config_bundle.get_looks_config(), bot_config.team) #LINE# #TAB# #TAB# return bot_config"
Recursively scans a directory for all valid bot configs . : param root_dir : Directory to scan . : return : The set of bot configs that were found  <code> def scan_directory_for_bot_configs(root_dir) ->Set[BotConfigBundle]: ,"#LINE# #TAB# configs = set() #LINE# #TAB# for filename in glob.iglob(os.path.join(root_dir, '**/*.cfg'), #LINE# #TAB# #TAB# recursive=True): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# bundle = get_bot_config_bundle(filename) #LINE# #TAB# #TAB# #TAB# configs.add(bundle) #LINE# #TAB# #TAB# except (NoSectionError, MissingSectionHeaderError, NoOptionError, #LINE# #TAB# #TAB# #TAB# AttributeError, ParsingError, FileNotFoundError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return configs"
"Registers a path specification credentials . Args : credentials ( Credentials ) : credentials . Raises : KeyError : if credentials object is already set for the corresponding type indicator  <code> def register_credentials(cls, credentials): ",#LINE# #TAB# if credentials.type_indicator in cls._credentials: #LINE# #TAB# #TAB# raise KeyError( #LINE# #TAB# #TAB# #TAB# 'Credentials object already set for type indicator: {0:s}.'. #LINE# #TAB# #TAB# #TAB# format(credentials.type_indicator)) #LINE# #TAB# cls._credentials[credentials.type_indicator] = credentials
"Return a Point instance from a given list <code> def from_list(cls, l): ","#LINE# #TAB# #TAB# if len(l) == 3: #LINE# #TAB# #TAB# #TAB# #TAB# x, y, z = map(float, l) #LINE# #TAB# #TAB# #TAB# #TAB# return cls(x, y, z) #LINE# #TAB# #TAB# elif len(l) == 2: #LINE# #TAB# #TAB# #TAB# x, y = map(float, l) #LINE# #TAB# #TAB# #TAB# return cls(x, y) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise AttributeError"
"Provided with a TLSCiphertext instance c and a stream or block cipher alg the function decrypts c . data and returns a newly created TLSPlaintext  <code> def tls_decrypt(alg, c): ",#LINE# #TAB# p = TLSPlaintext() #LINE# #TAB# p.type = c.type #LINE# #TAB# p.version = c.version #LINE# #TAB# p.data = alg.decrypt(c.data) #LINE# #TAB# p.len = len(p.data) #LINE# #TAB# return p
"Set property values for Polygon  <code> def set_contourf_properties(stroke_width, fcolor, fill_opacity, contour_levels, contourf_idx, unit): ","#LINE# #TAB# return { #LINE# #TAB# #TAB# ""stroke"": fcolor, #LINE# #TAB# #TAB# ""stroke-width"": stroke_width, #LINE# #TAB# #TAB# ""stroke-opacity"": 1, #LINE# #TAB# #TAB# ""fill"": fcolor, #LINE# #TAB# #TAB# ""fill-opacity"": fill_opacity, #LINE# #TAB# #TAB# ""title"": ""%.2f"" % contour_levels[contourf_idx] + ' ' + unit #LINE# #TAB# }"
Returns an approximate vector representing the major axis of points <code> def major_axis(points): ,"#LINE# #TAB# U, S, V = np.linalg.svd(points) #LINE# #TAB# axis = util.unitize(np.dot(S, V)) #LINE# #TAB# return axis"
"Takes bytes and returns a GITypelib or raises GIError <code> def new_from_memory(cls, data): ","#LINE# #TAB# #TAB# size = len(data) #LINE# #TAB# #TAB# copy = g_memdup(data, size) #LINE# #TAB# #TAB# ptr = cast(copy, POINTER(guint8)) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with gerror(GIError) as error: #LINE# #TAB# #TAB# #TAB# #TAB# return GITypelib._new_from_memory(ptr, size, error) #LINE# #TAB# #TAB# except GIError: #LINE# #TAB# #TAB# #TAB# free(copy) #LINE# #TAB# #TAB# #TAB# raise"
"WSGI application creation helper : param controller : Overrides default application controller : param transactional : Adds transaction hook for all requests <code> def build_wsgi_app(controller=None, transactional=False): ","#LINE# #TAB# request_hooks = [hooks.JSONErrorHook()] #LINE# #TAB# if transactional: #LINE# #TAB# #TAB# request_hooks.append(hooks.BarbicanTransactionHook()) #LINE# #TAB# if newrelic_loaded: #LINE# #TAB# #TAB# request_hooks.insert(0, hooks.NewRelicHook()) #LINE# #TAB# wsgi_app = pecan.Pecan(controller or versions.AVAILABLE_VERSIONS[ #LINE# #TAB# #TAB# versions.DEFAULT_VERSION](), hooks=request_hooks, force_canonical=False #LINE# #TAB# #TAB# ) #LINE# #TAB# repositories.clear() #LINE# #TAB# return wsgi_app"
A three - locus heterogeneity penetrance model <code> def my_penetrance(geno): ,#LINE# #TAB# if sum(geno) < 2: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return sum(geno) * 0.1
"Gill - King algorithm for modified cholesky decomposition  <code> def gill_king(mat, eps=1e-16): ","#LINE# #TAB# if not scipy.sparse.issparse(mat): #LINE# #TAB# #TAB# mat = numpy.asfarray(mat) #LINE# #TAB# assert numpy.allclose(mat, mat.T) #LINE# #TAB# size = mat.shape[0] #LINE# #TAB# mat_diag = mat.diagonal() #LINE# #TAB# gamma = abs(mat_diag).max() #LINE# #TAB# off_diag = abs(mat - numpy.diag(mat_diag)).max() #LINE# #TAB# delta = eps*max(gamma + off_diag, 1) #LINE# #TAB# beta = numpy.sqrt(max(gamma, off_diag/size, eps)) #LINE# #TAB# lowtri = _gill_king(mat, beta, delta) #LINE# #TAB# return lowtri"
"Select only rows passing a set of cuts from catalog table <code> def select_sources(cat_table, cuts): ","#LINE# #TAB# nsrc = len(cat_table) #LINE# #TAB# full_mask = np.ones((nsrc), bool) #LINE# #TAB# for cut in cuts: #LINE# #TAB# #TAB# if cut == 'mask_extended': #LINE# #TAB# #TAB# #TAB# full_mask *= mask_extended(cat_table) #LINE# #TAB# #TAB# elif cut == 'select_extended': #LINE# #TAB# #TAB# #TAB# full_mask *= select_extended(cat_table) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# full_mask *= make_mask(cat_table, cut) #LINE# #TAB# lout = [src_name.strip() for src_name in cat_table['Source_Name'][full_mask]] #LINE# #TAB# return lout"
Exports the saved weight data as a CSV file <code> def export_csv(request): ,"#LINE# #TAB# response = HttpResponse(content_type='text/csv') #LINE# #TAB# writer = csv.writer(response) #LINE# #TAB# weights = WeightEntry.objects.filter(user=request.user) #LINE# #TAB# writer.writerow([_('Weight').encode('utf8'), _('Date').encode('utf8')]) #LINE# #TAB# for entry in weights: #LINE# #TAB# #TAB# writer.writerow([entry.weight, entry.date]) #LINE# #TAB# response['Content-Disposition'] = 'attachment; filename=Weightdata.csv' #LINE# #TAB# response['Content-Length'] = len(response.content) #LINE# #TAB# return response"
"Locate a cycle of ( least - preferable , second - choice ) pairs to be removed from the game  <code> def locate_all_or_nothing_cycle(player): ","#LINE# #TAB# lasts = [player] #LINE# #TAB# seconds = [] #LINE# #TAB# while True: #LINE# #TAB# #TAB# second_best = player.prefs[1] #LINE# #TAB# #TAB# their_worst = second_best.prefs[-1] #LINE# #TAB# #TAB# seconds.append(second_best) #LINE# #TAB# #TAB# lasts.append(their_worst) #LINE# #TAB# #TAB# player = their_worst #LINE# #TAB# #TAB# if lasts.count(player) > 1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# idx = lasts.index(player) #LINE# #TAB# cycle = zip(lasts[idx + 1:], seconds[idx:]) #LINE# #TAB# return cycle"
Generator classmethod that returns a configured Catalog for all valid combinations of Language and Topic  <code> def all_catalogs(cls): ,"#LINE# #TAB# for language, topic in product(Language, Topic): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield cls(language, topic) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# pass"
Returns the default start stop activation time threshold [ s ] . : return : Start - stop activation time threshold [ s ] . : rtype : float <code> def default_start_stop_activation_time(has_start_stop): ,#LINE# #TAB# if (not has_start_stop or dfl.functions.ENABLE_ALL_FUNCTIONS or dfl. #LINE# #TAB# #TAB# functions.default_start_stop_activation_time.ENABLE): #LINE# #TAB# #TAB# return dfl.functions.default_start_stop_activation_time.threshold #LINE# #TAB# return sh.NONE
"If an article is provided , then we select categories relating to it . Otherwise we select all article categories  <code> def related_categories(context, article=None, limit=None): ",#LINE# #TAB# if article is None: #LINE# #TAB# #TAB# categories = Category.objects.all() #LINE# #TAB# else: #LINE# #TAB# #TAB# categories = article.categories.all() #LINE# #TAB# context['category_list'] = categories #LINE# #TAB# return context
Compute the number of steps required to escape from a point on the complex plane <code> def compute_escape(pos): ,#LINE# #TAB# z = 0 + 0.0j #LINE# #TAB# for i in xrange(ESCAPE): #LINE# #TAB# #TAB# z = z ** 2 + pos #LINE# #TAB# #TAB# if abs(z) > 2: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return i
Return a new nurbs renderer for the system ( dereferences pointer ) <code> def glu_new_nurbs_renderer(baseFunction): ,#LINE# #TAB# newSet = baseFunction() #LINE# #TAB# new = newSet[0] #LINE# #TAB# return new
"Initialize a Rule instance from a YAML map : param name : rule name : param table : map of the table associated with the rule : param inobj : YAML map of the rule : return : Rule instance <code> def from_map(name, table, inobj): ","#LINE# #TAB# obj = Rule(name, table.schema, table.name, inobj.pop('description', #LINE# #TAB# #TAB# None), inobj.get('event'), inobj.pop('instead', False), inobj.pop( #LINE# #TAB# #TAB# 'actions', None), inobj.pop('condition', None)) #LINE# #TAB# obj.set_oldname(inobj) #LINE# #TAB# return obj"
"Generator for all child dx_nodes of dx_node that are part of dx_nodes : param dx_nodes : [ DiffxNode , ... ] : param dx_node : DiffxNode <code> def gen_child_nodes(dx_nodes, dx_node): ",#LINE# #TAB# for _dx_node in dx_nodes[dx_nodes.index(dx_node):]: #LINE# #TAB# #TAB# if _dx_node.xpath.find(dx_node.xpath) == 0: #LINE# #TAB# #TAB# #TAB# yield _dx_node
"Compute collective overlap between two lists of particles  <code> def collective_overlap(particle, other, a, side, normalize=True): ","#LINE# #TAB# x = numpy.array([p.position for p in particle]) #LINE# #TAB# y = numpy.array([p.position for p in other]) #LINE# #TAB# dr = numpy.ndarray(x.shape[0]) #LINE# #TAB# rij = numpy.asarray(y) #LINE# #TAB# q = 0 #LINE# #TAB# for i in range(y.shape[0]): #LINE# #TAB# #TAB# dr = x[:, :] - y[(i), :] #LINE# #TAB# #TAB# dr = dr - numpy.rint(dr / side) * side #LINE# #TAB# #TAB# dr = numpy.sum(dr ** 2, axis=1) #LINE# #TAB# #TAB# q += (dr < a ** 2).sum() #LINE# #TAB# if normalize: #LINE# #TAB# #TAB# q /= float(len(particle)) #LINE# #TAB# return q"
"Move a branch s HEAD to a new SHA  <code> def update_branch(profile, name, sha): ","#LINE# #TAB# ref = ""heads/"" + name #LINE# #TAB# data = refs.update_ref(profile, ref, sha) #LINE# #TAB# return data"
Convert a camel cased name to PEP8 style  <code> def camel_case_to_pep8(name): ,"#LINE# #TAB# converted = CAPITALS.sub(lambda m: '_' + m.groups()[0].lower(), name) #LINE# #TAB# if converted[0] == '_': #LINE# #TAB# #TAB# return converted[1:] #LINE# #TAB# else: #LINE# #TAB# #TAB# return converted"
"Return header object with ' Authorization ' for a given token <code> def get_auth_header(token: Optional[str]) ->Dict[str, str]: ",#LINE# #TAB# if token: #LINE# #TAB# #TAB# return {'Authorization': f'Bearer {token}'} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}
"Make an element 's name absolute to make it unique . Eg "" stock1 "" in model "" model "" -- > "" model.stock1 "" : param model_name : Name of model : param name : Element name : return : <code> def make_name_absolute(model_name, name): ",#LINE# #TAB# seperator = '.' #LINE# #TAB# if '.' not in name: #LINE# #TAB# #TAB# return model_name + seperator + name #LINE# #TAB# else: #LINE# #TAB# #TAB# return name
Replace all Capitalized tokens in ` x ` by their lower version and add ` TK_MAJ ` before  <code> def deal_caps(x: Collection[str]) ->Collection[str]: ,#LINE# #TAB# res = [] #LINE# #TAB# for t in x: #LINE# #TAB# #TAB# if t == '': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if t[0].isupper() and len(t) > 1 and t[1:].islower(): #LINE# #TAB# #TAB# #TAB# res.append(TK_MAJ) #LINE# #TAB# #TAB# res.append(t.lower()) #LINE# #TAB# return res
Generate a fake dataset that matches the dimensions of MNIST  <code> def fake_data(num_images): ,"#LINE# #TAB# data = np.ndarray(shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, #LINE# #TAB# #TAB# NUM_CHANNELS), dtype=np.float32) #LINE# #TAB# labels = np.zeros(shape=(num_images,), dtype=np.int64) #LINE# #TAB# for image in range(num_images): #LINE# #TAB# #TAB# label = image % 2 #LINE# #TAB# #TAB# data[(image), :, :, (0)] = label - 0.5 #LINE# #TAB# #TAB# labels[image] = label #LINE# #TAB# return data, labels"
"Read classifier from pickle file  <code> def read_pickle(cls, file_path: str): ","#LINE# #TAB# with open(file_path, 'rb') as fh: #LINE# #TAB# #TAB# clf = pickle.load(fh) #LINE# #TAB# _LOGGER.info('Loaded Moana classifier from ""%s"".', file_path) #LINE# #TAB# return clf"
"Represents value as a dict  <code> def _parse_dict(cls, value): ","#LINE# #TAB# #TAB# separator = '=' #LINE# #TAB# #TAB# result = {} #LINE# #TAB# #TAB# for line in cls._parse_list(value): #LINE# #TAB# #TAB# #TAB# key, sep, val = line.partition(separator) #LINE# #TAB# #TAB# #TAB# if sep != separator: #LINE# #TAB# #TAB# #TAB# #TAB# raise DistutilsOptionError( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'Unable to parse option value to dict: %s' % value) #LINE# #TAB# #TAB# #TAB# result[key.strip()] = val.strip() #LINE# #TAB# #TAB# return result"
"Takes a list containing identity names , emails , and ids , and returns a list of IdentityRefWithVote objects  <code> def resolve_reviewers_as_ids(reviewers, team_instance): ","#LINE# #TAB# resolved_reviewers = None #LINE# #TAB# if reviewers is not None and reviewers: #LINE# #TAB# #TAB# resolved_reviewers = [] #LINE# #TAB# #TAB# for reviewer in reviewers: #LINE# #TAB# #TAB# #TAB# resolved_reviewers.append(resolve_identity_as_id(reviewer, #LINE# #TAB# #TAB# #TAB# #TAB# team_instance)) #LINE# #TAB# return resolved_reviewers"
Returns the path to the location of the file associated with the resource with the given resource ID  <code> def get_filepath(resource_id): ,#LINE# #TAB# return '/var/lib/ckan/default/resources/' + resource_id[0:3 #LINE# #TAB# #TAB# ] + '/' + resource_id[3:6] + '/' + resource_id[6:]
hook : HOOK ATOM ATOM NEWLINE hook : HOOK ATOM ATOM atom_list NEWLINE <code> def p_hook(t): ,"#LINE# #TAB# if t[2] not in ('post-receive',): #LINE# #TAB# #TAB# raise ParseError('Hook %s not permitted' % t[2]) #LINE# #TAB# if len(t) == 5: #LINE# #TAB# #TAB# t[0] = Hook(hook=t[2], script=t[3], args=()) #LINE# #TAB# elif len(t) == 6: #LINE# #TAB# #TAB# t[0] = Hook(hook=t[2], script=t[3], args=tuple(t4)) #LINE# #TAB# else: #LINE# #TAB# #TAB# assert False"
Returns the displacement reduction factor of the foundation for a given corrected normalised rotation . : param cor_norm_rot : <code> def foundation_rotation_reduction_factor_millen(cor_norm_rot): ,#LINE# #TAB# dmf = np.sqrt(1.0 / (1.0 + 5.0 * (1 - np.exp(-0.15 * cor_norm_rot)))) #LINE# #TAB# return dmf
this is the getKey function that generates an encryption Key for you by passing your Secret Key as a parameter  <code> def get_key(secret_key): ,"#LINE# #TAB# hashed_secret_key = hashlib.md5(secret_key.encode('utf-8')).hexdigest() #LINE# #TAB# hashed_secret_key_last_12 = hashed_secret_key[-12:] #LINE# #TAB# secret_key_adjusted = secret_key.replace('FLWSECK-', '') #LINE# #TAB# secret_key_adjusted_first12 = secret_key_adjusted[:12] #LINE# #TAB# return secret_key_adjusted_first12 + hashed_secret_key_last_12"
Checks that the given packages are not older than any other existing package in the repos  <code> def cmd_check_upgrade(args): ,#LINE# #TAB# with dependency_analyzer_from_args(args) as analyzer: #LINE# #TAB# #TAB# problems = analyzer.find_upgrade_problems() #LINE# #TAB# if problems: #LINE# #TAB# #TAB# sys.stderr.write(u'Upgrade problems:\n') #LINE# #TAB# #TAB# sys.stderr.write(u'\n'.join(problems) + u'\n') #LINE# #TAB# #TAB# return 3 #LINE# #TAB# return 0
like mkdir - p <code> def mkdir_p(dir): ,"#LINE# #TAB# if not dir: #LINE# #TAB# #TAB# return #LINE# #TAB# if dir.endswith(""/"") or dir.endswith(""\\""): #LINE# #TAB# #TAB# mkdir_p(dir[:-1]) #LINE# #TAB# #TAB# return #LINE# #TAB# if os.path.isdir(dir): #LINE# #TAB# #TAB# return #LINE# #TAB# mkdir_p(os.path.dirname(dir)) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.mkdir(dir) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass"
"Create a customer with the name . Then attach the email and phone as contact methods <code> def create_customer(name, email, phone): ","#LINE# #TAB# Party = client.model('party.party') #LINE# #TAB# ContactMechanism = client.model('party.contact_mechanism') #LINE# #TAB# party, = Party.create([{'name': name}]) #LINE# #TAB# ContactMechanism.create([ #LINE# #TAB# #TAB# {'type': 'email', 'value': email, 'party': party}, #LINE# #TAB# #TAB# {'type': 'phone', 'value': phone, 'party': party}, #LINE# #TAB# ]) #LINE# #TAB# return party"
Estimates the number of FLOPs in conv layer .. warning : : Currently it ignore the padding : param node_string : an onnx node defining a convolutional layer : return : number of FLOPs : rtype : ` int ` <code> def count_convnd(node): ,"#LINE# #TAB# inp = string_to_shape(list(node.inputs())[0]) #LINE# #TAB# out = string_to_shape(list(node.outputs())[0]) #LINE# #TAB# bias = string_to_shape(list(node.inputs())[0], True) #LINE# #TAB# f_in = inp[1] #LINE# #TAB# kernel_size = node['kernel_shape'] #LINE# #TAB# kernel_ops = f_in #LINE# #TAB# for ks in kernel_size: #LINE# #TAB# #TAB# kernel_ops *= ks #LINE# #TAB# kernel_ops = kernel_ops // node['group'] #LINE# #TAB# bias_ops = 1 if bias is not None else 0 #LINE# #TAB# combined_ops = kernel_ops + bias_ops #LINE# #TAB# total_ops = combined_ops * reduce(lambda x, y: x * y, out) #LINE# #TAB# return total_ops"
Delete the desktop entry generated by create_desktop_entry  <code> def delete_desktop_entry(): ,"#LINE# #TAB# folder = os.path.expanduser(os.path.join('~', '.local', 'share', #LINE# #TAB# #TAB# 'applications')) #LINE# #TAB# for filename in ['com.sympathyfordata.SympathyForData.desktop', #LINE# #TAB# #TAB# 'com.sympathyfordata.SympathyForDataViewer.desktop']: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.unlink(os.path.join(folder, filename)) #LINE# #TAB# #TAB# except (IOError, OSError): #LINE# #TAB# #TAB# #TAB# pass"
set logger to used by the log function <code> def set_logger(logger): ,"#LINE# #TAB# if not enabled: #LINE# #TAB# #TAB# return False #LINE# #TAB# global gdb_logger #LINE# #TAB# result = False #LINE# #TAB# if isinstance(logger, logging.Logger): #LINE# #TAB# #TAB# gdb_logger = logger #LINE# #TAB# #TAB# result = True #LINE# #TAB# return result"
"Do a best effort attempt to conver input to a string . : param input : user supplied input . : type input : mixed . : return : String or None : rtype : string <code> def ensure_string(cls, data): ","#LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(data, string_types): #LINE# #TAB# #TAB# return data #LINE# #TAB# cls._logger.warning( #LINE# #TAB# #TAB# 'Supplied attribute is of type %s and should have been a string. ', #LINE# #TAB# #TAB# type(data)) #LINE# #TAB# try: #LINE# #TAB# #TAB# return json.dumps(data) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return None"
"Helper to match compatible architectures . If ' up ' is True , we return the included architectures , if False we return the including  <code> def compatible_archs(flags, up): ","#LINE# #TAB# flags = set(flags) #LINE# #TAB# matches = flags.issuperset if up else flags.issubset #LINE# #TAB# for name, available_flags in ARCH_DEFS.items(): #LINE# #TAB# #TAB# if matches(available_flags): #LINE# #TAB# #TAB# #TAB# yield name"
"Return the argument names from a Python function argument spec . > > > function_argument_names('foo = None , bar = baz(a , b=42 ) ' ) [ ' foo ' , ' bar ' ] <code> def function_argument_names(argument_spec): ",#LINE# #TAB# func_src = 'def f({spec}): pass'.format(spec=argument_spec) #LINE# #TAB# ast_module = ast.parse(func_src) #LINE# #TAB# ast_function = ast_module.body[0] #LINE# #TAB# assert ast_function.name == 'f' #LINE# #TAB# ast_arguments = ast_function.args #LINE# #TAB# ast_args = ast_arguments.args #LINE# #TAB# try: #LINE# #TAB# #TAB# arg_names = [arg.arg for arg in ast_args] #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# arg_names = [arg.id for arg in ast_args] #LINE# #TAB# return arg_names
Prepare a set of samples for parallel structural variant calling  <code> def batch_for_sv(samples): ,"#LINE# #TAB# samples = cwlutils.assign_complex_to_samples(samples) #LINE# #TAB# to_process, extras, background = _batch_split_by_sv(samples, ""standard"") #LINE# #TAB# out = [cwlutils.samples_to_records(xs) for xs in to_process.values()] + extras #LINE# #TAB# return out"
"Module function returning data as required by the configuration dialog . @return dictionary with key "" zzz_subversionPage "" containing the relevant data <code> def get_config_data(): ","#LINE# #TAB# return {'zzz_subversionPage': [QCoreApplication.translate( #LINE# #TAB# #TAB# 'VcsSubversionPlugin', 'Subversion'), os.path.join('VcsPlugins', #LINE# #TAB# #TAB# 'vcsSubversion', 'icons', 'preferences-subversion.svg'), #LINE# #TAB# #TAB# createConfigurationPage, 'vcsPage', None]}"
Translate ambiguous characters from curly bracket format to single letter format and remove spaces from sequences  <code> def translate_ambiguous(seq): ,"#LINE# #TAB# seq = seq.replace('{GT}', 'K') #LINE# #TAB# seq = seq.replace('{AC}', 'M') #LINE# #TAB# seq = seq.replace('{AG}', 'R') #LINE# #TAB# seq = seq.replace('{CT}', 'Y') #LINE# #TAB# seq = seq.replace('{CG}', 'S') #LINE# #TAB# seq = seq.replace('{AT}', 'W') #LINE# #TAB# seq = seq.replace('{CGT}', 'B') #LINE# #TAB# seq = seq.replace('{ACG}', 'V') #LINE# #TAB# seq = seq.replace('{ACT}', 'H') #LINE# #TAB# seq = seq.replace('{AGT}', 'D') #LINE# #TAB# seq = seq.replace('{GATC}', 'N') #LINE# #TAB# seq = seq.replace(' ', '') #LINE# #TAB# return seq"
"Discards every step - th item from ` iterator ` <code> def discard_nth_base(ctx, iterator, step): ","#LINE# #TAB# for i, value in enumerate(iterator): #LINE# #TAB# #TAB# if i % step != 0: #LINE# #TAB# #TAB# #TAB# yield value"
Normalize the JSON API data and convert to dataframe . Parameters ---------- data API response JSON data . Returns ------- pd . DataFrame A normalized dataframe  <code> def api_data_to_frame(data: list) ->pd.DataFrame: ,"#LINE# #TAB# df = pd.DataFrame(data) #LINE# #TAB# for col in df.columns.copy(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# expanded = pd.io.json.json_normalize(df[col], record_prefix=True) #LINE# #TAB# #TAB# #TAB# expanded.columns = [f'{col}.{x}' for x in expanded.columns] #LINE# #TAB# #TAB# #TAB# df = pd.concat([df, expanded], axis=1) #LINE# #TAB# #TAB# #TAB# df = df.drop(col, axis=1) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return df"
Load UCSC cytoBand table . Parameters ---------- filename : str path to cytoBand file Returns ------- df : pandas . DataFrame cytoBand table <code> def load_cytoband(filename): ,"#LINE# #TAB# df = pd.read_csv(filename, names=['chrom', 'start', 'end', 'name', #LINE# #TAB# #TAB# 'gie_stain'], sep='\t') #LINE# #TAB# df['chrom'] = df['chrom'].str[3:] #LINE# #TAB# return df"
Runs the main section of src / robotide / editor / tags.py  <code> def tags_test(ctx): ,"#LINE# #TAB# _set_development_path() #LINE# #TAB# try: #LINE# #TAB# #TAB# import subprocess #LINE# #TAB# #TAB# p = subprocess.Popen(['/usr/bin/python', #LINE# #TAB# #TAB# #TAB# '/home/helio/github/RIDE/src/robotide/editor/tags.py']) #LINE# #TAB# #TAB# p.communicate('') #LINE# #TAB# finally: #LINE# #TAB# #TAB# pass"
Reads the weird format of VarInt present in src / serialize.h of raven core and being used for storing data in the leveldb . This is not the VARINT format described for general ravencoin serialization use  <code> def read_varint(raw_hex): ,"#LINE# #TAB# n = 0 #LINE# #TAB# pos = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# data = raw_hex[pos] #LINE# #TAB# #TAB# pos += 1 #LINE# #TAB# #TAB# n = n << 7 | data & 127 #LINE# #TAB# #TAB# if data & 128 == 0: #LINE# #TAB# #TAB# #TAB# return n, pos #LINE# #TAB# #TAB# n += 1"
Return a string with any extra info to include for this object  <code> def extra_object_info(obj): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if type(obj).__name__ == 'DataFrame' and hasattr(obj, 'shape'): #LINE# #TAB# #TAB# #TAB# rows, cols = obj.shape #LINE# #TAB# #TAB# #TAB# return '{} rows x {} columns`\n\n`'.format(rows, cols) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logging.warning('Exception caught during extra_object_info: %s', e) #LINE# #TAB# return ''"
Removes the complex and complex complex suffixes from a medscan agent name so that it better corresponds with the grounding map  <code> def normalize_medscan_name(name): ,#LINE# #TAB# suffix = ' complex' #LINE# #TAB# for i in range(2): #LINE# #TAB# #TAB# if name.endswith(suffix): #LINE# #TAB# #TAB# #TAB# name = name[:-len(suffix)] #LINE# #TAB# return name
"A condition is a dictionary of { keypath : values } . Extract the keypath from the user . Check if the value is in the values list . If not , this condition fails  <code> def check_condition(condition, user): ","#LINE# #TAB# for keypath, values in condition.items(): #LINE# #TAB# #TAB# node = objectpath(user, keypath) #LINE# #TAB# #TAB# if node is None: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif isinstance(node, list): #LINE# #TAB# #TAB# #TAB# if not set(node) & values: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif node not in values: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Add dict_a into dict_b Merge values if possible else dict_a value replace dict_b value <code> def merge_dict(dict_a, dict_b): ","#LINE# #TAB# for key in dict_a: #LINE# #TAB# #TAB# if key in dict_b: #LINE# #TAB# #TAB# #TAB# if isinstance(dict_a[key], dict) and isinstance(dict_b[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# merge_dict(dict_a[key], dict_b[key]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dict_b[key] = dict_a[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dict_b[key] = dict_a[key] #LINE# #TAB# return dict_b"
"Parse string for time codes , titles and URL  <code> def parse_element_types(str_input): ","#LINE# #TAB# bitly_hash, rest = get_bitly_hash(str_input) #LINE# #TAB# time_codes, rest2 = get_time_codes(rest) #LINE# #TAB# titles, rest3 = get_titles(rest2) #LINE# #TAB# raw_tags = rest3.split('.') #LINE# #TAB# tags = [] #LINE# #TAB# for item in raw_tags: #LINE# #TAB# #TAB# if len(item) > 0: #LINE# #TAB# #TAB# #TAB# tags.append(item) #LINE# #TAB# return time_codes, titles, bitly_hash, tags"
Get the workid from an archiveofourown.org website url <code> def workid_from_url(url): ,#LINE# #TAB# split_url = url.split('/') #LINE# #TAB# try: #LINE# #TAB# #TAB# index = split_url.index('works') #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return #LINE# #TAB# if len(split_url) >= index + 1: #LINE# #TAB# #TAB# if split_url[index + 1].isdigit(): #LINE# #TAB# #TAB# #TAB# return int(split_url[index + 1]) #LINE# #TAB# return
"Calculates the Mututal Information between each feature in F and s. This function is for when |S| > 1 . s is the previously selected feature . We exploit the fact that this step is embarrassingly parallel  <code> def get_mi_vector(MI_FS, k, F, s, are_data_binned, n_jobs=1): ","#LINE# #TAB# if n_jobs <= 1: #LINE# #TAB# #TAB# MIs = [_get_mi(f, s, k, MI_FS, are_data_binned) for f in F] #LINE# #TAB# else: #LINE# #TAB# #TAB# MIs = Parallel(n_jobs=MI_FS.n_jobs)(delayed(_get_mi)(f, s, k, MI_FS, #LINE# #TAB# #TAB# #TAB# are_data_binned) for f in F) #LINE# #TAB# return MIs"
Check to prevent mutable default argument in sahara code . S360 <code> def no_mutable_default_args(logical_line): ,"#LINE# #TAB# msg = ""S360: Method's default argument shouldn't be mutable!"" #LINE# #TAB# if RE_MUTABLE_DEFAULT_ARGS.match(logical_line): #LINE# #TAB# #TAB# yield 0, msg"
Parse a list of degradations arguments Args : degradation_args ( list of string ) : Input degradations arguments Returns : ( list of Degradation ) : Degradation objects with specified params <code> def parse_degradations_args(degradations_args): ,#LINE# #TAB# degradations = [] #LINE# #TAB# for degradation_args in degradations_args: #LINE# #TAB# #TAB# degradation = ParametersParser.parse_degradation_args(degradation_args) #LINE# #TAB# #TAB# degradations.append(degradation) #LINE# #TAB# return degradations
Parse the references to Reference instances  <code> def parse_references(xml): ,"#LINE# #TAB# references = [] #LINE# #TAB# ref_finder = HTMLReferenceFinder(xml) #LINE# #TAB# for elm, uri_attr in ref_finder: #LINE# #TAB# #TAB# type_ = _discover_uri_type(elm.get(uri_attr)) #LINE# #TAB# #TAB# references.append(Reference(elm, type_, uri_attr)) #LINE# #TAB# return references"
"Return the figure size ( width , height ) following the golden ratio given the width figwidth ( default = 5 ) <code> def golden_ratio(figwidth=5): ","#LINE# #TAB# Phi = 1.61803 #LINE# #TAB# a = figwidth / Phi #LINE# #TAB# return [figwidth, a]"
"Get the ratio of ` iterable ` items that pass ` filter_function `  <code> def get_ratio(filter_function, iterable): ","#LINE# #TAB# if isinstance(filter_function, str): #LINE# #TAB# #TAB# attribute_name = filter_function #LINE# #TAB# #TAB# filter_function = lambda item: getattr(item, attribute_name, None) #LINE# #TAB# n_total_items = 0 #LINE# #TAB# n_passed_items = 0 #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# n_total_items += 1 #LINE# #TAB# #TAB# if filter_function(item): #LINE# #TAB# #TAB# #TAB# n_passed_items += 1 #LINE# #TAB# return n_passed_items / n_total_items"
Replace all strings in dictionary with compiled version of themselves and return dictionary  <code> def compile_patterns_in_dictionary(dictionary): ,"#LINE# #TAB# for key, value in dictionary.items(): #LINE# #TAB# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# #TAB# dictionary[key] = re.compile(value) #LINE# #TAB# #TAB# elif isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# compile_patterns_in_dictionary(value) #LINE# #TAB# return dictionary"
"Returns all disk images within a location with a given image name . The name must match exactly . The list may be empty  <code> def get_disk_image_by_name(pbclient, location, image_name): ","#LINE# #TAB# all_images = pbclient.list_images() #LINE# #TAB# matching = [i for i in all_images['items'] if #LINE# #TAB# #TAB# #TAB# #TAB# i['properties']['name'] == image_name and #LINE# #TAB# #TAB# #TAB# #TAB# i['properties']['imageType'] == ""HDD"" and #LINE# #TAB# #TAB# #TAB# #TAB# i['properties']['location'] == location] #LINE# #TAB# return matching"
"from quaternion=[vec , scalar]= [ sin angle / 2 ( unitvec(x , y , z ) ) , cos angle / 2 ] ie 4 elements returns rotation angles and axis longitude and latitude coordinates ( in degrees ) <code> def fromquat_to3rotangles(initquat): ","#LINE# #TAB# quat = tuple(np.array(initquat)) #LINE# #TAB# unitvec, rotangle = fromQuat_to_vecangle(quat) #LINE# #TAB# longit, lat = fromvec_to_directionangles(unitvec) #LINE# #TAB# return rotangle * 180.0 / np.pi, longit, lat"
"Generate Python code with numpy functions <code> def python_code(ss, arrays=None): ","#LINE# #TAB# sr = rename_function(ss, 'conjugate', 'numpy.conj') #LINE# #TAB# sr = rename_function(sr, 'exp', 'numpy.exp') #LINE# #TAB# if arrays is not None: #LINE# #TAB# #TAB# for ar in arrays: #LINE# #TAB# #TAB# #TAB# sr = fce2array(sr, ar) #LINE# #TAB# return sr"
Return authentication key for corosync <code> def get_auth_key(context): ,#LINE# #TAB# if os.path.exists('/var/lib/cloudOver/corosync_auth'): #LINE# #TAB# #TAB# return base64.b64encode(open('/var/lib/cloudOver/corosync_auth'). #LINE# #TAB# #TAB# #TAB# read(1024 * 1024 * 16)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
Returns keystone auth  <code> def create_auth(cloud_name): ,"#LINE# #TAB# cloud = os_client_config.OpenStackConfig().get_one_cloud(cloud_name) #LINE# #TAB# auth = v2.Password(auth_url=cloud.config['auth']['auth_url'], username= #LINE# #TAB# #TAB# cloud.config['auth']['username'], password=cloud.config['auth'][ #LINE# #TAB# #TAB# 'password'], tenant_name=cloud.config['auth']['project_name']) #LINE# #TAB# return auth"
"tries are_files then split_str use with multiple = True <code> def are_files_or_str_iter(ctx, param, s): ","#LINE# #TAB# files = [] #LINE# #TAB# for s2 in s: #LINE# #TAB# #TAB# files.append(are_files_or_str(ctx, param, s2)) #LINE# #TAB# return files"
"Pick biases such that neurons are active for a percentile of inputs  <code> def percentile_biases(encoders, trainX, percentile=50): ","#LINE# #TAB# trainX = trainX.reshape(trainX.shape[0], -1) #LINE# #TAB# H = np.dot(trainX, encoders.T) #LINE# #TAB# biases = np.percentile(H, percentile, axis=0) #LINE# #TAB# return biases"
ex_expression : OP_EXISTS cmp_expression | cmp_expression <code> def p_ex_expression(tok): ,"#LINE# #TAB# if len(tok) == 3: #LINE# #TAB# #TAB# tok[0] = UnaryOperationRule(tok[1], tok[2]) #LINE# #TAB# else: #LINE# #TAB# #TAB# tok[0] = tok[1]"
"Run linting for a single file  <code> def lint_file(filename, reg_score, reg_warning, reg_error): ","#LINE# #TAB# stdout, stderr = py_run('{} --extension-pkg-whitelist=numpy'.format( #LINE# #TAB# #TAB# filename), return_std=True) #LINE# #TAB# txt = stdout.read() #LINE# #TAB# warnings = [i for i in reg_warning.findall(txt)] #LINE# #TAB# errors = [i for i in reg_error.findall(txt)] #LINE# #TAB# result = reg_score.findall(txt) #LINE# #TAB# score, delta = process_result(result) #LINE# #TAB# stdout.close() #LINE# #TAB# stderr.close() #LINE# #TAB# return score, delta, warnings, errors"
"Create an image from a url , using a file cache  <code> def from_url_with_cache(cls, url, cache_dir='cache', filename=None): ","#LINE# #TAB# filepath = os.path.join(cache_dir, filename or url_to_filepath(url)) #LINE# #TAB# if os.path.isfile(filepath): #LINE# #TAB# #TAB# logger.debug('Loading cached image at {}'.format(filepath)) #LINE# #TAB# #TAB# img = Image.open(filepath) #LINE# #TAB# else: #LINE# #TAB# #TAB# img = cls.from_url(url, filepath) #LINE# #TAB# return img"
normalize the various options of hosts_and_ports int a list of host and port tuples <code> def normalize_hosts_and_ports(hosts_and_ports): ,"#LINE# #TAB# if isinstance(hosts_and_ports[0], list) or isinstance(hosts_and_ports[0 #LINE# #TAB# #TAB# ], tuple): #LINE# #TAB# #TAB# return hosts_and_ports #LINE# #TAB# return [hosts_and_ports]"
"build raster stack vrt file from in_file_list . : param in_file_list : : param out_file : output vrt file ( end with .vrt ) : return : <code> def build_stack_vrt(in_file_list, out_file): ","#LINE# #TAB# gdal_expression_01 = ( #LINE# #TAB# #TAB# 'gdalbuildvrt -separate -overwrite -input_file_list ""{}"" ""{}"" --config GDAL_CACHEMAX 2000' #LINE# #TAB# #TAB# .format(in_file_list, out_file)) #LINE# #TAB# subprocess.check_output(gdal_expression_01, shell=True) #LINE# #TAB# field_names = modify_vrt_xml(out_file) #LINE# #TAB# print(field_names) #LINE# #TAB# return field_names"
"Split the contents of a batch file into individual scenarios  <code> def split_scenarios(scentime, scencmd): ","#LINE# #TAB# start = 0 #LINE# #TAB# for i in range(1, len(scencmd) + 1): #LINE# #TAB# #TAB# if i == len(scencmd) or scencmd[i][:4] == 'SCEN': #LINE# #TAB# #TAB# #TAB# scenname = scencmd[start].split()[1].strip() #LINE# #TAB# #TAB# #TAB# yield dict(name=scenname, scentime=scentime[start:i], scencmd= #LINE# #TAB# #TAB# #TAB# #TAB# scencmd[start:i]) #LINE# #TAB# #TAB# #TAB# start = i"
Find all of the packages  <code> def find_packages(): ,"#LINE# #TAB# packages = [] #LINE# #TAB# for dir, subdirs, files in os.walk(name): #LINE# #TAB# #TAB# package = dir.replace(os.path.sep, '.') #LINE# #TAB# #TAB# if '__init__.py' not in files: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# packages.append(package) #LINE# #TAB# return packages"
"Compute the mean value of an diagonal observable  <code> def average_data(counts, observable): ","#LINE# #TAB# if not isinstance(observable, dict): #LINE# #TAB# #TAB# observable = make_dict_observable(observable) #LINE# #TAB# temp = 0 #LINE# #TAB# tot = sum(counts.values()) #LINE# #TAB# for key in counts: #LINE# #TAB# #TAB# if key in observable: #LINE# #TAB# #TAB# #TAB# temp += counts[key] * observable[key] / tot #LINE# #TAB# return temp"
simple method to determine if a url is relative or absolute <code> def is_relative_url(url): ,#LINE# #TAB# if url.startswith('#'): #LINE# #TAB# #TAB# return None #LINE# #TAB# if url.find('://') > 0 or url.startswith('//'): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Checks that the diff between consecutive sync pulses is below 150 PPM Returns True on a pass result ( all values below threshold ) <code> def check_diff_3b(sync): ,"#LINE# #TAB# THRESH_PPM = 150 #LINE# #TAB# d = np.diff(sync.times[sync.polarities == 1]) #LINE# #TAB# dt = np.median(d) #LINE# #TAB# qc_pass = np.all(np.abs((d - dt) / dt * 1000000.0) < THRESH_PPM) #LINE# #TAB# if not qc_pass: #LINE# #TAB# #TAB# _logger.error( #LINE# #TAB# #TAB# #TAB# f'Synchronizations bursts over {THRESH_PPM} ppm between sync pulses. Sync using ""exact"" match between pulses.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return qc_pass"
Validate node ip address . : param ip : An IPv4 address . : type ip : string : rtype : False : Is either a non valid IPv4 address or is an RFC1918-address . : rtype : string ( IP address ) <code> def is_valid_node_ip(ip): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# address = ipaddress.IPv4Address(ip) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if address.is_private: #LINE# #TAB# #TAB# if CONFIG.get('developer', 'developer_mode') is False: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return ip"
"Returns ` ` default ` ` if ` ` argument ` ` is ` ` None ` ` <code> def default_value(argument, default): ",#LINE# #TAB# if argument is None: #LINE# #TAB# #TAB# return default #LINE# #TAB# else: #LINE# #TAB# #TAB# return argument
Parse one line of the XFS info output  <code> def xfs_info_get_kv(serialized): ,"#LINE# #TAB# if serialized.startswith(""=""): #LINE# #TAB# #TAB# serialized = serialized[1:].strip() #LINE# #TAB# serialized = serialized.replace("" = "", ""=*** "").replace("" ="", ""="") #LINE# #TAB# opt = [] #LINE# #TAB# for tkn in serialized.split("" ""): #LINE# #TAB# #TAB# if not opt or ""="" in tkn: #LINE# #TAB# #TAB# #TAB# opt.append(tkn) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# opt[len(opt) - 1] = opt[len(opt) - 1] + "" "" + tkn #LINE# #TAB# return [tuple(items.split(""="")) for items in opt]"
"Converts labels from ( -1 , 1 ) to ( 0 , 1 ) : param y : label vector : return : new labels <code> def labels_to_01(y: np.ndarray) ->np.ndarray: ",#LINE# #TAB# if y is None: #LINE# #TAB# #TAB# raise ValueError('Labels must not be None!') #LINE# #TAB# return (y + 1) / 2
"create data - only container if it doesn t already exist  <code> def data_only_container(name, volumes): ","#LINE# #TAB# info = inspect_container(name) #LINE# #TAB# if info: #LINE# #TAB# #TAB# return #LINE# #TAB# c = _get_docker().create_container( #LINE# #TAB# #TAB# name=name, #LINE# #TAB# #TAB# image='datacats/postgres', #LINE# #TAB# #TAB# command='true', #LINE# #TAB# #TAB# volumes=volumes, #LINE# #TAB# #TAB# detach=True) #LINE# #TAB# return c"
Encode a value to utf-8 only if it 's unicode  <code> def encode_unicode_or_identity(value): ,"#LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# return encode_utf8(value) #LINE# #TAB# return value"
"gets the tempo for a song <code> def get_tempo(artist, title): ","#LINE# #TAB# ""gets the tempo for a song"" #LINE# #TAB# results = song.search(artist=artist, title=title, results=1, buckets=['audio_summary']) #LINE# #TAB# if len(results) > 0: #LINE# #TAB# #TAB# return results[0].audio_summary['tempo'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
get update query type <code> def get_query_type(key): ,#LINE# #TAB# if key == 'trialConcurrency': #LINE# #TAB# #TAB# return '?update_type=TRIAL_CONCURRENCY' #LINE# #TAB# if key == 'maxExecDuration': #LINE# #TAB# #TAB# return '?update_type=MAX_EXEC_DURATION' #LINE# #TAB# if key == 'searchSpace': #LINE# #TAB# #TAB# return '?update_type=SEARCH_SPACE' #LINE# #TAB# if key == 'maxTrialNum': #LINE# #TAB# #TAB# return '?update_type=MAX_TRIAL_NUM'
"Given a template name ( or list of them ) , returns the template names as a list , with each name prefixed with the device directory inserted into the front of the list  <code> def templates_for_host(request, templates): ","#LINE# #TAB# if not isinstance(templates, (list, tuple)): #LINE# #TAB# #TAB# templates = [templates] #LINE# #TAB# theme_dir = host_theme_path(request) #LINE# #TAB# host_templates = [] #LINE# #TAB# if theme_dir: #LINE# #TAB# #TAB# for template in templates: #LINE# #TAB# #TAB# #TAB# host_templates.append('%s/templates/%s' % (theme_dir, template)) #LINE# #TAB# #TAB# #TAB# host_templates.append(template) #LINE# #TAB# #TAB# return host_templates #LINE# #TAB# return templates"
Find all active carbon source reactions  <code> def find_carbon_sources(model): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# model.slim_optimize(error_value=None) #LINE# #TAB# except OptimizationError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# reactions = model.reactions.get_by_any(list(model.medium)) #LINE# #TAB# reactions_fluxes = [ #LINE# #TAB# #TAB# (rxn, total_components_flux(rxn.flux, reaction_elements(rxn), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# consumption=True)) for rxn in reactions] #LINE# #TAB# return [rxn for rxn, c_flux in reactions_fluxes if c_flux > 0]"
Searches for both btc - only and non - btc - only devices Returns : List of devices <code> def get_any_bitbox02_bootloaders() ->List[DeviceInfo]: ,#LINE# #TAB# devices = get_bitbox02multi_bootloaders() #LINE# #TAB# devices.extend(get_bitbox02btc_bootloaders()) #LINE# #TAB# return devices
getSEP([type ] ) ; Return the seperator specified by type <code> def get_sep(type=''): ,"#LINE# #TAB# if type in ['path', 'pathsep']: #LINE# #TAB# #TAB# return os.pathsep #LINE# #TAB# elif type in ['ext', 'extsep']: #LINE# #TAB# #TAB# return os.extsep #LINE# #TAB# elif type in ['line', 'linesep']: #LINE# #TAB# #TAB# return os.linesep #LINE# #TAB# elif type in ['alt', 'altsep']: #LINE# #TAB# #TAB# return os.altsep #LINE# #TAB# elif type not in ['', 'sep']: #LINE# #TAB# #TAB# if not type.endswith('sep'): #LINE# #TAB# #TAB# #TAB# type += 'sep' #LINE# #TAB# #TAB# print(""Error: 'os.%s' not found"" % type) #LINE# #TAB# #TAB# raise TypeError #LINE# #TAB# return os.sep"
Returns a list of favourite group the form of organization_list action function <code> def get_featured_groups(count=1): ,"#LINE# #TAB# config_groups = config.get('ckan.featured_groups', '').split() #LINE# #TAB# groups = featured_group_org(get_action='group_show', list_action= #LINE# #TAB# #TAB# 'group_list', count=count, items=config_groups) #LINE# #TAB# return groups"
The layer doesn t have an exposure class we need to add it  <code> def add_default_exposure_class(layer): ,"#LINE# #TAB# layer.startEditing() #LINE# #TAB# field = create_field_from_definition(exposure_class_field) #LINE# #TAB# layer.keywords['inasafe_fields'][exposure_class_field['key']] = ( #LINE# #TAB# #TAB# exposure_class_field['field_name']) #LINE# #TAB# layer.addAttribute(field) #LINE# #TAB# index = layer.fields().lookupField(exposure_class_field['field_name']) #LINE# #TAB# exposure = layer.keywords['exposure'] #LINE# #TAB# request = QgsFeatureRequest() #LINE# #TAB# request.setFlags(QgsFeatureRequest.NoGeometry) #LINE# #TAB# for feature in layer.getFeatures(request): #LINE# #TAB# #TAB# layer.changeAttributeValue(feature.id(), index, exposure) #LINE# #TAB# layer.commitChanges() #LINE# #TAB# return"
"Computes the Legendre symbol : param a : number : param p : prime number : return : Returns 1 if ` a ` has a square root modulo p , -1 otherwise  <code> def legendre_symbol(a, p): ","#LINE# #TAB# ls = pow(a, (p - 1) // 2, p) #LINE# #TAB# if ls == p - 1: #LINE# #TAB# #TAB# return -1 #LINE# #TAB# return ls"
Validate the given connectivity matrix  <code> def connectivity_matrix(cm): ,"#LINE# #TAB# if cm.size == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# if cm.ndim != 2: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be 2-dimensional.') #LINE# #TAB# if cm.shape[0] != cm.shape[1]: #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must be square.') #LINE# #TAB# if not np.all(np.logical_or(cm == 1, cm == 0)): #LINE# #TAB# #TAB# raise ValueError('Connectivity matrix must contain only binary values.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return True"
"Check whether broker action has completed yet  <code> def is_broker_action_done(action, rid=None, unit=None): ","#LINE# #TAB# rdata = relation_get(rid, unit) or {} #LINE# #TAB# broker_rsp = rdata.get(get_broker_rsp_key()) #LINE# #TAB# if not broker_rsp: #LINE# #TAB# #TAB# return False #LINE# #TAB# rsp = CephBrokerRsp(broker_rsp) #LINE# #TAB# unit_name = local_unit().partition('/')[2] #LINE# #TAB# key = ""unit_{}_ceph_broker_action.{}"".format(unit_name, action) #LINE# #TAB# kvstore = kv() #LINE# #TAB# val = kvstore.get(key=key) #LINE# #TAB# if val and val == rsp.request_id: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"Read the flags from a dictionary and return them in a usable form . Will return a list of ( flag , value ) for all flags in "" flag_dict "" matching the filter "" flag_filter ""  <code> def retrieve_flags(flag_dict, flag_filter): ","#LINE# #TAB# return [(flag, value) for flag, value in list(flag_dict.items()) if #LINE# #TAB# #TAB# isinstance(flag, (str, bytes)) and flag.startswith(flag_filter)]"
"Return a newly created stat as a dict with ' stat_id ' and ' values ' fields . The values are ordered consistenly with the item stat order in the d2s file  <code> def create_stat(func, stat, set_, val, param, min_, max_, rand=False): ","#LINE# #TAB# if rand: #LINE# #TAB# #TAB# logger.error('Random generation of stats not yet supported.') #LINE# #TAB# if func in [1, 3, 8]: #LINE# #TAB# #TAB# return {'stat': stat, 'values': [(min_ + max_) // 2]} #LINE# #TAB# if func == 21: #LINE# #TAB# #TAB# return {'stat': stat, 'values': [val, (min_ + max_) // 2]} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
Raise exception if the structure of the personnummer is incorrect  <code> def check_pnr_structure(pnr): ,#LINE# #TAB# splitpnr(pnr) #LINE# #TAB# date(pnr) #LINE# #TAB# check_parity(pnr) #LINE# #TAB# return True
Find locally installed tools : param str plugin_dir : : return : A list of importable packages : rtype : list <code> def find_local_modules(plugin_dir): ,"#LINE# #TAB# if plugin_dir is None or not os.path.isdir(plugin_dir): #LINE# #TAB# #TAB# return [] #LINE# #TAB# sys.path.append(plugin_dir) #LINE# #TAB# modules = [] #LINE# #TAB# for module_name in os.listdir(plugin_dir): #LINE# #TAB# #TAB# if os.path.isdir(os.path.join(plugin_dir, module_name)) and not str( #LINE# #TAB# #TAB# #TAB# module_name).startswith('__'): #LINE# #TAB# #TAB# #TAB# modules.append(module_name) #LINE# #TAB# return modules"
"Computes histogram ( density ) for a given vector of values  <code> def compute_histogram(values, edges, use_orig_distr=False): ","#LINE# #TAB# if use_orig_distr: #LINE# #TAB# #TAB# return values #LINE# #TAB# values = check_array(values).compressed() #LINE# #TAB# hist, bin_edges = np.histogram(values, bins=edges, density=True) #LINE# #TAB# hist = preprocess_histogram(hist, values, edges) #LINE# #TAB# return hist"
Convert camel - case names to underscore - delimited Parameters ---------- val : string The string to convert Returns ------- string <code> def to_underscore(val): ,"#LINE# #TAB# out = re.sub('([A-Z])', lambda x: '_' + x.group(1).lower(), val) #LINE# #TAB# if re.match('[A-Z]', val): #LINE# #TAB# #TAB# return out[1:] #LINE# #TAB# return out"
"Deprecated function for applying a seq on a spin group and retrieving the signal <code> def sim_single_spingroup_old(loc_ind, freq_offset, phantom, seq): ","#LINE# #TAB# sgloc = phantom.get_location(loc_ind) #LINE# #TAB# isc = sg.SpinGroup(loc=sgloc, pdt1t2=phantom.get_params(loc_ind), df= #LINE# #TAB# #TAB# freq_offset) #LINE# #TAB# signal = apply_pulseq_old(isc, seq) #LINE# #TAB# return signal"
"Gives hash parameters for the given set of words  <code> def hash_parameters(words, minimize_indices=False): ",#LINE# #TAB# words = tuple(words) #LINE# #TAB# return CzechHashBuilder(words).hash_info
"Deserializes retries from json . : param retries : Retries as json  <code> def deserialize_retries(retries: dict) ->Dict[Retry, RetryConfig]: ","#LINE# #TAB# return {Retry[key]: RetryConfigImpl(count=val['count'], seconds_between #LINE# #TAB# #TAB# =val['secondsBetween']) for key, val in retries.items() if key in #LINE# #TAB# #TAB# Retry.__members__}"
Verify that the file at path is the therapist hook and return the hash <code> def identify_hook(path): ,"#LINE# #TAB# with open(path, 'r') as f: #LINE# #TAB# #TAB# f.readline() #LINE# #TAB# #TAB# version_line = f.readline() #LINE# #TAB# #TAB# if version_line.startswith('# THERAPIST'): #LINE# #TAB# #TAB# #TAB# return version_line.split()[2]"
This is a fallback technique at best . I 'm not sure if using the registry for this guarantees us the correct answer for all CSIDL _ * names  <code> def get_win_folder_from_registry(csidl_name): ,"#LINE# #TAB# import _winreg #LINE# #TAB# shell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': #LINE# #TAB# #TAB# 'Common AppData', 'CSIDL_LOCAL_APPDATA': 'Local AppData'}[csidl_name] #LINE# #TAB# key = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER, #LINE# #TAB# #TAB# 'Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders' #LINE# #TAB# #TAB# ) #LINE# #TAB# dir, type = _winreg.QueryValueEx(key, shell_folder_name) #LINE# #TAB# return dir"
Sets environment internally for python <code> def is_in_path_env(path): ,"#LINE# #TAB# path_tokens = os.environ.get('PATH', '').split(os.pathsep) #LINE# #TAB# return path in path_tokens"
"iterate over a vcf - formatted file  <code> def iterate_from_vcf(infile, sample): ","#LINE# #TAB# vcf = pysam.VCF() #LINE# #TAB# vcf.connect(infile) #LINE# #TAB# if sample not in vcf.getsamples(): #LINE# #TAB# #TAB# raise KeyError(""sample %s not vcf file"") #LINE# #TAB# for row in vcf.fetch(): #LINE# #TAB# #TAB# result = vcf2pileup(row, sample) #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# yield result"
"Activate all the extensions for the given names . extension_names -- Sequence of extension names  <code> def activate_extensions(extension_names, verbose=False): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# for extension_name in extension_names: #LINE# #TAB# #TAB# #TAB# activate_extension(extension_name, verbose=verbose) #LINE# #TAB# except: #LINE# #TAB# #TAB# deactivate_extensions(get_active_extensions()) #LINE# #TAB# #TAB# raise"
"serializes a django queryset as netvis - graph : param qs : A django queryset : param limit : A integer to limit the number of objects : return : A netvis graph <code> def qs_as_graph(qs, limit=100): ",#LINE# #TAB# graphs = [as_graph(x) for x in qs[:limit]] #LINE# #TAB# graph = {'edges': []} #LINE# #TAB# nodes = [] #LINE# #TAB# for x in graphs: #LINE# #TAB# #TAB# for node in x['nodes']: #LINE# #TAB# #TAB# #TAB# nodes.append(node) #LINE# #TAB# #TAB# for edge in x['edges']: #LINE# #TAB# #TAB# #TAB# graph['edges'].append(edge) #LINE# #TAB# graph['nodes'] = list({v['id']: v for v in nodes}.values()) #LINE# #TAB# graph['types'] = graphs[0]['types'] #LINE# #TAB# return graph
"Gumbel width distribution Samples come from iid Gumbel distributions . Distributions are randomly inverted since Gumbels are skewed  <code> def width_gumbel(widths, num_samples): ","#LINE# #TAB# scales = widths[:, (None)] * np.sqrt(6) / np.pi #LINE# #TAB# draws = np.random.gumbel(-scales * np.euler_gamma, scales, (widths.size, #LINE# #TAB# #TAB# num_samples)) #LINE# #TAB# draws *= rand.randint(0, 2, (widths.size, 1)) * 2 - 1 #LINE# #TAB# return draws"
"Finds an unused node id in graph  <code> def get_unused_node_id(graph, initial_guess='unknown', _format='{}<%d>'): ",#LINE# #TAB# has_node = graph.has_node #LINE# #TAB# n = counter() #LINE# #TAB# node_id_format = _format.format(initial_guess) #LINE# #TAB# node_id = initial_guess #LINE# #TAB# while has_node(node_id): #LINE# #TAB# #TAB# node_id = node_id_format % n() #LINE# #TAB# return node_id
"F1-Fourier transform for N+P ( echo / antiecho ) 2D <code> def ft_n_p(data, axis='F1'): ",#LINE# #TAB# data.conv_n_p().ft_sh() #LINE# #TAB# return data
"Instantiate the collection from an existing environ  <code> def from_environ(cls, environ): ",#LINE# #TAB# fix_http_headers(environ) #LINE# #TAB# headers = cls(environ) #LINE# #TAB# headers.mutable = False #LINE# #TAB# return headers
"Build : class:`dtale.utils . JSONFormatter ` from : class:`pandas : pandas . DataFrame ` <code> def grid_formatter(col_types, nan_display='', overrides=None): ","#LINE# #TAB# f = JSONFormatter(nan_display) #LINE# #TAB# mappings = dict_merge(DF_MAPPINGS, overrides or {}) #LINE# #TAB# for i, ct in enumerate(col_types, 1): #LINE# #TAB# #TAB# c, dtype = map(ct.get, ['name', 'dtype']) #LINE# #TAB# #TAB# type_classification = classify_type(dtype) #LINE# #TAB# #TAB# mappings.get(type_classification, DF_MAPPINGS['S'])(f, i, c) #LINE# #TAB# return f"
Makes failure output for streams better by having key be the stream name <code> def transform_streams_for_comparison(outputs): ,"#LINE# #TAB# new_outputs = [] #LINE# #TAB# for output in outputs: #LINE# #TAB# #TAB# if (output.output_type == 'stream'): #LINE# #TAB# #TAB# #TAB# new_outputs.append({ #LINE# #TAB# #TAB# #TAB# #TAB# 'output_type': 'stream', #LINE# #TAB# #TAB# #TAB# #TAB# output.name: output.text, #LINE# #TAB# #TAB# #TAB# }) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new_outputs.append(output) #LINE# #TAB# return new_outputs"
"A version of xmlattr filter that sorts attributes  <code> def do_sorted_xmlattr(_environment, d, autospace=True): ","#LINE# #TAB# rv = ' '.join('%s=""%s""' % (escape(key), escape(value)) for key, value in #LINE# #TAB# #TAB# sorted(d.items()) if value is not None and not isinstance(value, #LINE# #TAB# #TAB# Undefined)) #LINE# #TAB# if autospace and rv: #LINE# #TAB# #TAB# rv = ' ' + rv #LINE# #TAB# if _environment.autoescape: #LINE# #TAB# #TAB# rv = Markup(rv) #LINE# #TAB# return rv"
"Create and show a ScrolledMessageDialog <code> def show_text_dialog(parent, text, caption): ","#LINE# #TAB# style = wx.CAPTION | wx.CLOSE_BOX | wx.RESIZE_BORDER | wx.SYSTEM_MENU #LINE# #TAB# dlg = ScrolledMessageDialog(parent, text, caption, style=style) #LINE# #TAB# font = wx.Font(12, wx.MODERN, wx.NORMAL, wx.NORMAL, False, 'Inconsolata') #LINE# #TAB# dlg.text.SetFont(font) #LINE# #TAB# n_lines = dlg.text.GetNumberOfLines() #LINE# #TAB# line_text = dlg.text.GetLineText(0) #LINE# #TAB# w, h = dlg.text.GetTextExtent(line_text) #LINE# #TAB# dlg.text.SetSize((w + 100, (h + 3) * n_lines + 50)) #LINE# #TAB# dlg.Fit() #LINE# #TAB# dlg.Show() #LINE# #TAB# return dlg"
Child - first discovery of ACEs for an object  <code> def iter_object_acl(root): ,"#LINE# #TAB# for obj in iter_object_graph(root): #LINE# #TAB# #TAB# for ace in parse_acl(getattr(obj, '__acl__', ())): #LINE# #TAB# #TAB# #TAB# yield ace"
"Lookup AQI database for station codes in a given city  <code> def find_station_codes_by_city(city_name, token): ","#LINE# #TAB# req = requests.get( #LINE# #TAB# #TAB# API_ENDPOINT_SEARCH, #LINE# #TAB# #TAB# params={ #LINE# #TAB# #TAB# #TAB# 'token': token, #LINE# #TAB# #TAB# #TAB# 'keyword': city_name #LINE# #TAB# #TAB# }) #LINE# #TAB# if req.status_code == 200 and req.json()[""status""] == ""ok"": #LINE# #TAB# #TAB# return [result[""uid""] for result in req.json()[""data""]] #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
"Generate N colors starting from color1 to color2 by linear interpolation HSV in or RGB spaces  <code> def make_palette(color1, color2, N, hsv=True): ","#LINE# #TAB# if hsv: #LINE# #TAB# #TAB# color1 = rgb2hsv(color1) #LINE# #TAB# #TAB# color2 = rgb2hsv(color2) #LINE# #TAB# c1 = np.array(getColor(color1)) #LINE# #TAB# c2 = np.array(getColor(color2)) #LINE# #TAB# cols = [] #LINE# #TAB# for f in np.linspace(0, 1, N - 1, endpoint=True): #LINE# #TAB# #TAB# c = c1 * (1 - f) + c2 * f #LINE# #TAB# #TAB# if hsv: #LINE# #TAB# #TAB# #TAB# c = np.array(hsv2rgb(c)) #LINE# #TAB# #TAB# cols.append(c) #LINE# #TAB# return cols"
"Set the repository to be used . The arg should be the normal /entry / view URL for a REPOSITORY entry <code> def set_repository(repository, should_list=False): ","#LINE# #TAB# Repository.theRepository = repository #LINE# #TAB# if should_list: #LINE# #TAB# #TAB# list_repository(Repository.theRepository.entry_id, repository) #LINE# #TAB# return Repository.theRepository"
"If the string is bytes , decode it to a string ( for python3 support ) <code> def convert_string_to_unicode(string): ","#LINE# #TAB# result = string #LINE# #TAB# try: #LINE# #TAB# #TAB# if string is not None and not isinstance(string, str): #LINE# #TAB# #TAB# #TAB# result = string.decode('utf-8') #LINE# #TAB# except (TypeError, UnicodeDecodeError, AttributeError): #LINE# #TAB# #TAB# pass #LINE# #TAB# return result"
"Merge ` ` right ` ` into ` ` left ` ` and return a new dictionary  <code> def recursive_dict_merge(left, right, create_copy=True): ","#LINE# #TAB# if create_copy is True: #LINE# #TAB# #TAB# left = deepcopy(left) #LINE# #TAB# for key in right: #LINE# #TAB# #TAB# if key in left: #LINE# #TAB# #TAB# #TAB# if isinstance(left[key], dict) and isinstance(right[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# recursive_dict_merge(left[key], right[key], False) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# left[key] = right[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# left[key] = right[key] #LINE# #TAB# return left"
Alias a Selectable if it has a group by clause <code> def alias_it(s): ,"#LINE# #TAB# if hasattr(s, '_group_by_clause' #LINE# #TAB# #TAB# ) and s._group_by_clause is not None and len(s._group_by_clause): #LINE# #TAB# #TAB# return s.alias(next(aliases)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return s"
Coerce ' value ' to an JSON - compatible representation  <code> def int_to_json(value): ,"#LINE# #TAB# if isinstance(value, int): #LINE# #TAB# #TAB# value = str(value) #LINE# #TAB# return value"
"Method for calculating the center of mass ( CM ) Parameters ---------- data : numpy array Numpy array of the vertices of shape ( N,2 ) Returns ------- cm : touple Touple of coordinates of the CM <code> def center_mass(data): ","#LINE# #TAB# n, _ = data.shape #LINE# #TAB# cm = np.sum(data, axis=0) / np.float(n) #LINE# #TAB# return cm"
"Remove the weak reference of the given component <code> def remove_component(cls, component): ","#LINE# #TAB# comp_ref = next((cr for cr in H2OConnectionMonitorMixin. #LINE# #TAB# #TAB# _h2o_components_refs if cr() is component), None) #LINE# #TAB# if comp_ref: #LINE# #TAB# #TAB# cls.remove_component_ref(comp_ref) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"Load any GLTF "" extras "" into scene.metadata['extras ' ] . Parameters -------------- header : dict GLTF header Returns ------------- kwargs : dict Includes metadata <code> def parse_extras(header): ","#LINE# #TAB# if 'extras' not in header: #LINE# #TAB# #TAB# return {} #LINE# #TAB# try: #LINE# #TAB# #TAB# return {'metadata': {'extras': dict(header['extras'])}} #LINE# #TAB# except BaseException: #LINE# #TAB# #TAB# log.warning('failed to load extras', exc_info=True) #LINE# #TAB# #TAB# return {}"
"Returns set of columns that are being searched and highlights <code> def get_search_fields(request, doc_types): ","#LINE# #TAB# fields = {'uuid'} #LINE# #TAB# highlights = {} #LINE# #TAB# types = request.registry[TYPES] #LINE# #TAB# for doc_type in doc_types: #LINE# #TAB# #TAB# type_info = types[doc_type] #LINE# #TAB# #TAB# for value in type_info.schema.get('boost_values', ()): #LINE# #TAB# #TAB# #TAB# fields.add('embedded.' + value) #LINE# #TAB# #TAB# #TAB# highlights['embedded.' + value] = {} #LINE# #TAB# return fields, highlights"
Generate a D - Optimal Design problem from LIBSVM datasets <code> def d_opt_libsvm(filename): ,"#LINE# #TAB# X, y = load_libsvm_file(filename) #LINE# #TAB# if X.shape[0] > X.shape[1]: #LINE# #TAB# #TAB# H = X.T.toarray('C') #LINE# #TAB# else: #LINE# #TAB# #TAB# H = X.toarray('C') #LINE# #TAB# n = H.shape[1] #LINE# #TAB# f = DOptimalObj(H) #LINE# #TAB# h = BurgEntropySimplex() #LINE# #TAB# L = 1.0 #LINE# #TAB# x0 = 1.0 / n * np.ones(n) #LINE# #TAB# return f, h, L, x0"
"Converts a version string to a tuple of the form ( major , minor , revision , prerelease ) Example : "" fw - v0.3.6 - 23 "" = > ( 0 , 3 , 6 , True ) <code> def version_str_to_tuple(version_string): ","#LINE# #TAB# regex = '.*v([0-9a-zA-Z]+).([0-9a-zA-Z]+).([0-9a-zA-Z]+)(.*)' #LINE# #TAB# return int(re.sub(regex, '\\1', version_string)), int(re.sub(regex, #LINE# #TAB# #TAB# '\\2', version_string)), int(re.sub(regex, '\\3', version_string) #LINE# #TAB# #TAB# ), re.sub(regex, '\\4', version_string) != ''"
Retrieves a logger configured for the application  <code> def get_logger(name): ,"#LINE# #TAB# logger = logging.get_logger(name) #LINE# #TAB# try: #LINE# #TAB# #TAB# logger.setLevel(os.getenv('LOG_LEVEL', 'INFO')) #LINE# #TAB# except Exception as error: #LINE# #TAB# #TAB# raise CouldNotSetLogLevel(error) #LINE# #TAB# try: #LINE# #TAB# #TAB# stream_handler = logging.StreamHandler(sys.stdout) #LINE# #TAB# #TAB# formatter = StackDriverJsonFormatter() #LINE# #TAB# #TAB# stream_handler.setFormatter(formatter) #LINE# #TAB# #TAB# logger.addHandler(stream_handler) #LINE# #TAB# except Exception as error: #LINE# #TAB# #TAB# raise CouldNotSetStreamHandler(error) #LINE# #TAB# return logger"
Check that the given folder exists and create it if not existing  <code> def folder_exists_or_create(f): ,#LINE# #TAB# if not exists(f): #LINE# #TAB# #TAB# makedirs(f) #LINE# #TAB# if not isdir(f): #LINE# #TAB# #TAB# raise ValueError('Target exists and is not a folder') #LINE# #TAB# return f
Convert a SeldonMessage proto to JSON Dict Parameters ---------- message_proto SeldonMessage proto Returns ------- JSON Dict <code> def seldon_message_to_json(message_proto: prediction_pb2.SeldonMessage) ->Dict: ,#LINE# #TAB# message_json = json_format.MessageToJson(message_proto) #LINE# #TAB# message_dict = json.loads(message_json) #LINE# #TAB# return message_dict
Yield a left->right traversal of an aggregation <code> def traverse_aggregation(agg): ,"#LINE# #TAB# if isinstance(agg, summary): #LINE# #TAB# #TAB# for a in agg.values: #LINE# #TAB# #TAB# #TAB# for a2 in traverse_aggregation(a): #LINE# #TAB# #TAB# #TAB# #TAB# yield a2 #LINE# #TAB# else: #LINE# #TAB# #TAB# yield agg"
"Execute given Vowpal Wabbit command , log stdout and stderr . Parameters ---------- cmd : str Given Vowpal Wabbit command to execute . Returns ------- str Stdout and stderr . Raises ------ subprocess . CalledProcessError If something goes wrong  <code> def run_vw_command(cmd): ","#LINE# #TAB# logger.info('Running Vowpal Wabbit command: %s', ' '.join(cmd)) #LINE# #TAB# proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess. #LINE# #TAB# #TAB# STDOUT) #LINE# #TAB# output = proc.communicate()[0].decode('utf-8') #LINE# #TAB# logger.debug('Vowpal Wabbit output: %s', output) #LINE# #TAB# if proc.returncode != 0: #LINE# #TAB# #TAB# raise subprocess.CalledProcessError(proc.returncode, ' '.join(cmd), #LINE# #TAB# #TAB# #TAB# output=output) #LINE# #TAB# return output"
"choose a random outcome , apply to the state , and return the new state <code> def resolve_outcomes(state, updates, dist): ","#LINE# #TAB# update = random_choice((u, p) for u, p in update_dist(state, updates, dist) #LINE# #TAB# #TAB# ) #LINE# #TAB# state = update_state(state, update, expected=False) #LINE# #TAB# if '~' in update: #LINE# #TAB# #TAB# state.update(update['~'](state)) #LINE# #TAB# return state"
"Builds and adds url for a given page / post to jinja context  <code> def add_url_to_context(jinja_context: dict, new_filename: str) ->dict: ","#LINE# #TAB# jinja_context['url' #LINE# #TAB# #TAB# ] = f""{BASE_URL}{new_filename.replace(f'{DIST_DIR}/', '')}"" #LINE# #TAB# return jinja_context"
"Tries to find a suitable log level based on input string . Input can be a level ( 0 = critical , 4 = debug ) , or a substring of debug , info , warning , error , or critical  <code> def log_level(lookfor): ","#LINE# #TAB# levels = {(0): CRITICAL, (1): ERROR, (2): WARNING, (3): INFO, (4): #LINE# #TAB# #TAB# DEBUG, 'critical': CRITICAL, 'error': ERROR, 'warning': WARNING, #LINE# #TAB# #TAB# 'info': INFO, 'debug': DEBUG} #LINE# #TAB# if lookfor.lower() in levels: #LINE# #TAB# #TAB# return levels[lookfor.lower()], lookfor.lower() #LINE# #TAB# else: #LINE# #TAB# #TAB# for k, v in six.iteritems(levels): #LINE# #TAB# #TAB# #TAB# l = len(str(k)) if len(str(k)) < len(lookfor) else len(lookfor) #LINE# #TAB# #TAB# #TAB# if str(k)[0:l].lower() == lookfor.lower(): #LINE# #TAB# #TAB# #TAB# #TAB# return v, k #LINE# #TAB# raise IndexError"
"converts rgb to a hex string <code> def int_hex(r, g, b): ",#LINE# #TAB# hex_value = '#' #LINE# #TAB# hexr = hex(r)[2:] #LINE# #TAB# if len(hexr) == 1: #LINE# #TAB# #TAB# hex_value += '0' + hexr #LINE# #TAB# else: #LINE# #TAB# #TAB# hex_value += hexr #LINE# #TAB# hexg = hex(g)[2:] #LINE# #TAB# if len(hexg) == 1: #LINE# #TAB# #TAB# hex_value += '0' + hexg #LINE# #TAB# else: #LINE# #TAB# #TAB# hex_value += hexg #LINE# #TAB# hexb = hex(b)[2:] #LINE# #TAB# if len(hexb) == 1: #LINE# #TAB# #TAB# hex_value += '0' + hexb #LINE# #TAB# else: #LINE# #TAB# #TAB# hex_value += hexb #LINE# #TAB# return hex_value
"Compute the balanced accuracy for binary classification  <code> def bac_metric(solution, prediction): ","#LINE# #TAB# [tn, fp, tp, fn] = acc_stat(solution, prediction) #LINE# #TAB# eps = 1e-15 #LINE# #TAB# tp = sp.maximum(eps, tp) #LINE# #TAB# pos_num = sp.maximum(eps, tp + fn) #LINE# #TAB# tpr = tp / pos_num #LINE# #TAB# tn = sp.maximum(eps, tn) #LINE# #TAB# neg_num = sp.maximum(eps, tn + fp) #LINE# #TAB# tnr = tn / neg_num #LINE# #TAB# bac = 0.5 * (tpr + tnr) #LINE# #TAB# return bac"
Calculate the memory limit in gigabytes Returns ------- int The memory limit ( in gigabytes ) <code> def memory_limit_in_gigabytes(): ,"#LINE# #TAB# _, used, _, _, _, available = free_gigabytes() #LINE# #TAB# return used + available - MEMORY_CUSHION_GB"
"Authenticate using OAuth refresh token . Raises GoogleAuthError if authentication fails . Returns access token string  <code> def auth_with_refresh_token(session, refresh_token): ","#LINE# #TAB# token_request_data = {'client_id': OAUTH2_CLIENT_ID, 'client_secret': #LINE# #TAB# #TAB# OAUTH2_CLIENT_SECRET, 'grant_type': 'refresh_token', #LINE# #TAB# #TAB# 'refresh_token': refresh_token} #LINE# #TAB# res = _make_token_request(session, token_request_data) #LINE# #TAB# return res['access_token']"
"Evaluate whether a given string can be rendered in ASCII Args : s ( str ) : the string to test Returns : bool : True if the string can be rendered in ASCII , False otherwise  <code> def is_ascii(s): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# s.encode('ascii') #LINE# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"Parse the results of an OBJECT command <code> def parse_object(response, infotype): ","#LINE# #TAB# if infotype in ('idletime', 'refcount'): #LINE# #TAB# #TAB# return int(response) #LINE# #TAB# return response"
creates an Program from a JSON dictionary structure Counterpart to Program.to_json ( )  <code> def parse_program(dictionary): ,"#LINE# #TAB# identifier = parse_identifier(dictionary['identifier']) #LINE# #TAB# name = dictionary['name'] #LINE# #TAB# version = dictionary['version'] #LINE# #TAB# description = dictionary['description'] #LINE# #TAB# res = Program(identifier, name, version, description) #LINE# #TAB# return res"
Reads in the prompt from a text file Returns string <code> def read_in_test_prompt(filename): ,#LINE# #TAB# prompt_string = open(filename).read() #LINE# #TAB# return prompt_string
"Get the style dictionary for matplotlib path objects <code> def get_path_style(path, fill=True): ",#LINE# #TAB# style = {} #LINE# #TAB# style['alpha'] = path.get_alpha() #LINE# #TAB# if style['alpha'] is None: #LINE# #TAB# #TAB# style['alpha'] = 1 #LINE# #TAB# style['edgecolor'] = color_to_hex(path.get_edgecolor()) #LINE# #TAB# if fill: #LINE# #TAB# #TAB# style['facecolor'] = color_to_hex(path.get_facecolor()) #LINE# #TAB# else: #LINE# #TAB# #TAB# style['facecolor'] = 'none' #LINE# #TAB# style['edgewidth'] = path.get_linewidth() #LINE# #TAB# style['dasharray'] = get_dasharray(path) #LINE# #TAB# style['zorder'] = path.get_zorder() #LINE# #TAB# return style
Method to determine where the users settings should be saved <code> def get_settings_folder(): ,#LINE# #TAB# if platform.system() == 'Windows': #LINE# #TAB# #TAB# folder = Path(environ['APPDATA']).joinpath('LastShout') #LINE# #TAB# else: #LINE# #TAB# #TAB# folder = Path.home() / '.LastShout' #LINE# #TAB# return folder
Default plot routine . : param obj : : return : <code> def plot_path(path): ,"#LINE# #TAB# if isinstance(path, Path): #LINE# #TAB# #TAB# for seg in path: #LINE# #TAB# #TAB# #TAB# for values in ZinglPlotter.plot_segment(seg): #LINE# #TAB# #TAB# #TAB# #TAB# yield values #LINE# #TAB# else: #LINE# #TAB# #TAB# for values in ZinglPlotter.plot_segment(path): #LINE# #TAB# #TAB# #TAB# yield values"
create a MinHash digest <code> def mh_digest (data): ,#LINE# #TAB# num_perm = 512 #LINE# #TAB# m = MinHash(num_perm) #LINE# #TAB# for d in data: #LINE# #TAB# #TAB# m.update(d.encode('utf8')) #LINE# #TAB# return m
"Verify the value of a cookie . This will fail if the cookie does not exist . Parameters : name : value value : value <code> def verify_cookie_value(name, value): ","#LINE# #TAB# message = ""Verify that cookie '{}' value is '{}'"".format(name, value) #LINE# #TAB# with _verify_step(message, take_screenshots=False) as s: #LINE# #TAB# #TAB# cookie = browser.get_browser().get_cookie(name) #LINE# #TAB# #TAB# s.error = ""Expected cookie '{}' value to be '{}' but was '{}'"".format( #LINE# #TAB# #TAB# #TAB# name, value, cookie['value']) #LINE# #TAB# #TAB# if not cookie: #LINE# #TAB# #TAB# #TAB# raise Exception(""Cookie '{}' was not found"".format(name)) #LINE# #TAB# #TAB# elif not 'value' in cookie: #LINE# #TAB# #TAB# #TAB# raise Exception(""Cookie '{}' did not have 'value' key"".format(name) #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# s.condition = cookie['value'] == value"
"dest should be a directory path that does not exist <code> def copy_directory(src, dest): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# shutil.copytree(src, dest) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOTDIR: #LINE# #TAB# #TAB# #TAB# shutil.copy(src, dest) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise Exception('Directory not copied') from e"
"Convert map coordinates to pixel coordinates based on geotransform Accepts float or NumPy arrays GDAL model used here - upper left corner of upper left pixel for mX , mY ( and in GeoTransform ) <code> def map_to_pixel(mX, mY, geoTransform): ","#LINE# #TAB# mX = np.asarray(mX) #LINE# #TAB# mY = np.asarray(mY) #LINE# #TAB# if geoTransform[2] + geoTransform[4] == 0: #LINE# #TAB# #TAB# pX = (mX - geoTransform[0]) / geoTransform[1] - 0.5 #LINE# #TAB# #TAB# pY = (mY - geoTransform[3]) / geoTransform[5] - 0.5 #LINE# #TAB# else: #LINE# #TAB# #TAB# pX, pY = applyGeoTransform(mX, mY, invertGeoTransform(geoTransform)) #LINE# #TAB# return pX, pY"
"Given an atom based on a built - in predicate , return an equivalent atom with the negation absorbed . If the atom is not based on a built - in predicate , return the atom unchanged  <code> def negate_builtin_atom(atom): ","#LINE# #TAB# from .formulas import Atom #LINE# #TAB# if isinstance(atom, Atom) and atom.predicate.builtin: #LINE# #TAB# #TAB# pred = atom.predicate #LINE# #TAB# #TAB# return create_atom(pred.language, pred.symbol.complement(), *atom. #LINE# #TAB# #TAB# #TAB# subterms) #LINE# #TAB# return atom"
Resets the clipboard  <code> def reset_clipboard(request): ,#LINE# #TAB# if 'clipboard' in request.session.keys(): #LINE# #TAB# #TAB# del request.session['clipboard'] #LINE# #TAB# if 'clipboard_action' in request.session.keys(): #LINE# #TAB# #TAB# del request.session['clipboard_action']
Sets the mode of the executor . params : passwd - None ( no password ) or string ( set password ) <code> def set_mode(passwd): ,#LINE# #TAB# module = sys.modules[__name__] #LINE# #TAB# if passwd: #LINE# #TAB# #TAB# module.password = passwd #LINE# #TAB# #TAB# module.execute_as_root_simple = _execute_with_password_simple #LINE# #TAB# #TAB# module.execute_as_root = _execute_with_password #LINE# #TAB# else: #LINE# #TAB# #TAB# module.execute_as_root_simple = _execute_without_password_simple #LINE# #TAB# #TAB# module.execute_as_root = _execute_without_password #LINE# #TAB# module.exec_log = P2PEM.loggs.used_logger
"Returns True if the string argument appears to be a Bluetooth address ( strictly speaking , a MAC address ) arguments : address - string MAC address returns : boolean - True if it appears to be a bluetooth address , otherwise false <code> def bluetooth_validate(address): ",#LINE# #TAB# import re #LINE# #TAB# validMAC = re.compile('^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$') #LINE# #TAB# if validMAC.match(address): #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Compute the log of the lower incomplete gamma function  <code> def log_lower_gamma(a, x): ","#LINE# #TAB# lx = math.log(x) #LINE# #TAB# ls = None #LINE# #TAB# for n in xrange(2000): #LINE# #TAB# #TAB# if ls is None: #LINE# #TAB# #TAB# #TAB# ls = n * lx + logGamma(a) - logGamma(a + n + 1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# v = n * lx + logGamma(a) - logGamma(a + n + 1) #LINE# #TAB# #TAB# #TAB# if ls - v > 50: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# ls = logAdd(ls, v) #LINE# #TAB# return a * lx + ls - x"
Reload the router : param ctx : return : nothing <code> def install_activate_reload(ctx): ,"#LINE# #TAB# message = 'Waiting the {} operation to continue'.format('reload') #LINE# #TAB# ctx.info(message) #LINE# #TAB# ctx.post_status(message) #LINE# #TAB# if not ctx.reload(reload_timeout=1200, no_reload_cmd=True): #LINE# #TAB# #TAB# ctx.error('Encountered error when attempting to reload device.') #LINE# #TAB# success = wait_for_reload(ctx) #LINE# #TAB# if not success: #LINE# #TAB# #TAB# ctx.error('Reload or boot failure') #LINE# #TAB# #TAB# return #LINE# #TAB# ctx.info('Operation reload finished successfully') #LINE# #TAB# return"
Deserialize a bookmark string to a place marker . : param bookmark : A string in the format produced by : func:`serialize_bookmark ` . : returns : A marker pair as described in : func:`serialize_bookmark `  <code> def unserialize_bookmark(bookmark): ,"#LINE# #TAB# if not bookmark: #LINE# #TAB# #TAB# return None, False #LINE# #TAB# direction = bookmark[0] #LINE# #TAB# if direction not in ('>', '<'): #LINE# #TAB# #TAB# raise ValueError #LINE# #TAB# backwards = direction == '<' #LINE# #TAB# cells = s.unserialize_values(bookmark[1:]) #LINE# #TAB# return cells, backwards"
Compare results stored in multiple files with data in tabular format  <code> def abicomp_data(options): ,#LINE# #TAB# plotter = GenericDataFilesPlotter.from_files(options.paths) #LINE# #TAB# print(plotter.to_string(verbose=options.verbose)) #LINE# #TAB# plotter.plot(use_index=options.use_index) #LINE# #TAB# return 0
"Deregisters a path specification resolver helper . Args : resolver_helper ( ResolverHelper ) : resolver helper . Raises : KeyError : if resolver helper object is not set for the corresponding type indicator  <code> def deregister_helper(cls, resolver_helper): ",#LINE# #TAB# if resolver_helper.type_indicator not in cls._resolver_helpers: #LINE# #TAB# #TAB# raise KeyError( #LINE# #TAB# #TAB# #TAB# 'Resolver helper object not set for type indicator: {0:s}.'. #LINE# #TAB# #TAB# #TAB# format(resolver_helper.type_indicator)) #LINE# #TAB# del cls._resolver_helpers[resolver_helper.type_indicator]
"randomly subset fastq files <code> def sub_fq(R1, R2, percent): ","#LINE# #TAB# pool = [(1) for i in range(0, percent)] + [(0) for i in range(0, 100 - #LINE# #TAB# #TAB# percent)] #LINE# #TAB# for r1, r2 in zip(parse_fq(R1), parse_fq(R2)): #LINE# #TAB# #TAB# if random.choice(pool) == 1: #LINE# #TAB# #TAB# #TAB# yield r1 #LINE# #TAB# #TAB# #TAB# yield r2"
Returns true if any networks or ports exist for a tenant  <code> def tenant_provisioned(tenant_id): ,"#LINE# #TAB# session = db.get_reader_session() #LINE# #TAB# with session.begin(): #LINE# #TAB# #TAB# res = any( #LINE# #TAB# #TAB# #TAB# session.query(m).filter(m.tenant_id == tenant_id).count() #LINE# #TAB# #TAB# #TAB# for m in [models_v2.Network, models_v2.Port] #LINE# #TAB# #TAB# ) #LINE# #TAB# return res"
r Runs a task that takes some time <code> def expensive_task_gen(num=8700): ,"#LINE# #TAB# import utool as ut #LINE# #TAB# for x in range(0, num): #LINE# #TAB# #TAB# with ut.Timer(verbose=False) as t: #LINE# #TAB# #TAB# #TAB# ut.is_prime(x) #LINE# #TAB# #TAB# yield t.ellapsed"
Score C to 5 risk - classes : param score_c : Score C : return : Risk - class <code> def score_c_to_5_classes(score_c): ,#LINE# #TAB# if score_c == 1: #LINE# #TAB# #TAB# ret = 0 #LINE# #TAB# elif 2 <= score_c <= 3: #LINE# #TAB# #TAB# ret = 1 #LINE# #TAB# elif 4 <= score_c <= 7: #LINE# #TAB# #TAB# ret = 2 #LINE# #TAB# elif 8 <= score_c <= 10: #LINE# #TAB# #TAB# ret = 3 #LINE# #TAB# else: #LINE# #TAB# #TAB# ret = 4 #LINE# #TAB# return ret
Set a custom template renderer  <code> def template_renderer(f): ,#LINE# #TAB# global _render_string #LINE# #TAB# _render_string = f #LINE# #TAB# return f
"Checks if a point ( y , z ) is inside a line given by ( y1,z1 ) and ( y2,z2 ) . Also check if the point i d matches with the ids from the line . Returns ------- Boolean <code> def no_overlap(y, z, y1, z1, y2, z2, pt_id, pt_1_id, pt_2_id): ","#LINE# #TAB# if y < min(y1, y2) or y > max(y1, y2) or z < min(z1, z2) or z > max(z1, z2 #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif pt_id == pt_1_id or pt_id == pt_2_id: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"Return Window object if title or its part found in visible windows titles else return None <code> def get_window(title, exact=False): ","#LINE# #TAB# titles = get_windows() #LINE# #TAB# hwnd = titles.get(title, None) #LINE# #TAB# if not hwnd and not exact: #LINE# #TAB# #TAB# for k, v in titles.items(): #LINE# #TAB# #TAB# #TAB# if title in k: #LINE# #TAB# #TAB# #TAB# #TAB# hwnd = v #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# if hwnd: #LINE# #TAB# #TAB# return Window(hwnd) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
Query CERN Resources to get user info and groups  <code> def get_resource(remote): ,"#LINE# #TAB# cached_resource = session.pop('cern_resource', None) #LINE# #TAB# if cached_resource: #LINE# #TAB# #TAB# return cached_resource #LINE# #TAB# response = remote.get(REMOTE_APP_RESOURCE_API_URL) #LINE# #TAB# dict_response = get_dict_from_response(response) #LINE# #TAB# session['cern_resource'] = dict_response #LINE# #TAB# return dict_response"
"Expand environment variables and user home ( ~ ) in the log.file and return as relative path  <code> def fix_logging_path(config, main_section): ","#LINE# #TAB# log_file = config.get(main_section, 'log.file') #LINE# #TAB# if log_file: #LINE# #TAB# #TAB# log_file = os.path.expanduser(os.path.expandvars(log_file)) #LINE# #TAB# #TAB# if os.path.isabs(log_file): #LINE# #TAB# #TAB# #TAB# log_file = os.path.relpath(log_file) #LINE# #TAB# return log_file"
Navigates up the given node and returns the root node  <code> def get_root(node): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# while node.parent: #LINE# #TAB# #TAB# #TAB# node = node.parent #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return node
"Return the quantum fidelity between pure states  <code> def state_fidelity(state0: State, state1: State) -> bk.BKTensor: ","#LINE# #TAB# assert state0.qubits == state1.qubits #LINE# #TAB# tensor = bk.absolute(bk.inner(state0.tensor, state1.tensor))**bk.fcast(2) #LINE# #TAB# return tensor"
"Test if points p1 and p2 lie on the same side of line a - b  <code> def closest_point(p, a, b): ","#LINE# #TAB# vector_ab = [(y - x) for x, y in zip(a, b)] #LINE# #TAB# vector_ap = [(y - x) for x, y in zip(a, p)] #LINE# #TAB# dot_ap_ab = sum(x * y for x, y in zip(vector_ap, vector_ab)) #LINE# #TAB# dot_ab_ab = sum(x * y for x, y in zip(vector_ab, vector_ab)) #LINE# #TAB# t = max(0.0, min(dot_ap_ab / dot_ab_ab, 1.0)) #LINE# #TAB# return a[0] + vector_ab[0] * t, a[1] + vector_ab[1] * t"
Converts an object into a request body . If it s None we ll return an empty string if it s one of our objects it ll convert it to XML and return it . Otherwise we just use the object directly <code> def get_request_body(request_body): ,"#LINE# #TAB# if request_body is None: #LINE# #TAB# #TAB# return b'' #LINE# #TAB# if isinstance(request_body, bytes) or isinstance(request_body, IOBase): #LINE# #TAB# #TAB# return request_body #LINE# #TAB# if isinstance(request_body, _unicode_type): #LINE# #TAB# #TAB# return request_body.encode('utf-8') #LINE# #TAB# request_body = str(request_body) #LINE# #TAB# if isinstance(request_body, _unicode_type): #LINE# #TAB# #TAB# return request_body.encode('utf-8') #LINE# #TAB# return request_body"
"Transliterate to 8 bit using only single letter replacements  <code> def single_encode(input, errors='strict'): ","#LINE# #TAB# if not isinstance(input, text_type): #LINE# #TAB# #TAB# input = text_type(input, sys.getdefaultencoding(), errors) #LINE# #TAB# length = len(input) #LINE# #TAB# input = unicodedata.normalize('NFKC', input) #LINE# #TAB# return input.translate(single_table), length"
Test if a module has tests . Module format : homeassistant.components.hue Test if exists : tests / components / hue <code> def has_tests(module: str): ,"#LINE# #TAB# path = Path(module.replace('.', '/').replace('homeassistant', 'tests')) #LINE# #TAB# if not path.exists(): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not path.is_dir(): #LINE# #TAB# #TAB# return True #LINE# #TAB# content = [f.name for f in path.glob('*')] #LINE# #TAB# return content != ['__pycache__']"
"Return the nearest xterm 256 color code from rgb input  <code> def from_rgb(r, g=None, b=None): ","#LINE# #TAB# c = r if isinstance(r, list) else [r, g, b] #LINE# #TAB# best = {} #LINE# #TAB# for index, item in enumerate(colors): #LINE# #TAB# #TAB# d = __distance(item, c) #LINE# #TAB# #TAB# if(not best or d <= best['distance']): #LINE# #TAB# #TAB# #TAB# best = {'distance': d, 'index': index} #LINE# #TAB# if 'index' in best: #LINE# #TAB# #TAB# return best['index'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1"
Get IP address for the docker host <code> def get_ip(): ,"#LINE# #TAB# cmd_netstat = ['netstat', '-nr'] #LINE# #TAB# p1 = subprocess.Popen(cmd_netstat, stdout=subprocess.PIPE) #LINE# #TAB# cmd_grep = ['grep', '^0\.0\.0\.0'] #LINE# #TAB# p2 = subprocess.Popen(cmd_grep, stdin=p1.stdout, stdout=subprocess.PIPE) #LINE# #TAB# cmd_awk = ['awk', '{ print $2 }'] #LINE# #TAB# p3 = subprocess.Popen(cmd_awk, stdin=p2.stdout, stdout=subprocess.PIPE) #LINE# #TAB# galaxy_ip = p3.stdout.read() #LINE# #TAB# log.debug('Host IP determined to be %s', galaxy_ip) #LINE# #TAB# return galaxy_ip"
"Calculates the autocorr of mag series for specific lag  <code> def autocorr_func1(mags, lag, maglen, magmed, magstd): ","#LINE# #TAB# lagindex = nparange(1,maglen-lag) #LINE# #TAB# products = (mags[lagindex] - magmed) * (mags[lagindex+lag] - magmed) #LINE# #TAB# acorr = (1.0/((maglen - lag)*magstd)) * npsum(products) #LINE# #TAB# return acorr"
"Returns the mode flags to use for a modifier symbol . : param Xlib.display . Display display : The * X * display . : param str symbol : The name of the symbol . : return : the modifier mask <code> def find_mask(display, symbol): ","#LINE# #TAB# modifier_keycode = display.keysym_to_keycode(Xlib.XK.string_to_keysym( #LINE# #TAB# #TAB# symbol)) #LINE# #TAB# for index, keycodes in enumerate(display.get_modifier_mapping()): #LINE# #TAB# #TAB# for keycode in keycodes: #LINE# #TAB# #TAB# #TAB# if keycode == modifier_keycode: #LINE# #TAB# #TAB# #TAB# #TAB# return 1 << index #LINE# #TAB# return 0"
""" Detrend iq along axial direction then use hilbert transform to get envelope <code> def iq_to_envelope(iq_array: np.ndarray) ->np.ndarray: ","#LINE# #TAB# env = np.abs(iq_array) #LINE# #TAB# env_detrended = detrend_along_dimension(env, 1) #LINE# #TAB# return env_detrended"
This function decorator may be used to mark a method as usable for intercepting request head content . handles_request_head will accept an HttpRequest object and implement the logic that will define the FilterActions to be applied to the request <code> def handles_request_head(request_func): ,#LINE# #TAB# request_func._handles_request_head = True #LINE# #TAB# return request_func
"Try to convert all possible dependencies to processes <code> def proc_depends_setter(this, value): ","#LINE# #TAB# from .pyppl import _anything2procs #LINE# #TAB# depends = _anything2procs(value, procset='ends') #LINE# #TAB# try: #LINE# #TAB# #TAB# prev_depends = this._depends #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# prev_depends = [] #LINE# #TAB# for prevdep in prev_depends: #LINE# #TAB# #TAB# prevdep.nexts.remove(this) #LINE# #TAB# for depend in depends: #LINE# #TAB# #TAB# depend.nexts.append(this) #LINE# #TAB# return depends"
build a dictionary mapping assembly keys to file paths with reference sequence . This method takes a hierachical or no - hierarchical dictionary and builds reduces it to a non - hierarchical dictionary  <code> def build_reference_fasta_map(reference_fasta_map_param): ,"#LINE# #TAB# if reference_fasta_map_param is None: #LINE# #TAB# #TAB# return reference_fasta_map_param #LINE# #TAB# if not isinstance(reference_fasta_map_param, collections.Mapping): #LINE# #TAB# #TAB# raise ValueError('expected a dictionary') #LINE# #TAB# if IOTools.is_nested(reference_fasta_map_param): #LINE# #TAB# #TAB# return dict([(x, y['path']) for x, y in reference_fasta_map_param. #LINE# #TAB# #TAB# #TAB# items()]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return reference_fasta_map_param"
Fallback implementation of glibc_version_string using ctypes  <code> def glibc_version_string_ctypes(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# import ctypes #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None #LINE# #TAB# process_namespace = ctypes.CDLL(None) #LINE# #TAB# try: #LINE# #TAB# #TAB# gnu_get_libc_version = process_namespace.gnu_get_libc_version #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# gnu_get_libc_version.restype = ctypes.c_char_p #LINE# #TAB# version_str = gnu_get_libc_version() #LINE# #TAB# if not isinstance(version_str, str): #LINE# #TAB# #TAB# version_str = version_str.decode('ascii') #LINE# #TAB# return version_str"
"wrapper to get the module for a script <code> def get_script_module(script_information, package='pylabcontrol', verbose=False): ","#LINE# #TAB# #TAB# module, _, _, _, _, _, _ = Script.get_script_information(script_information=script_information, package=package, verbose=verbose) #LINE# #TAB# #TAB# return module"
"Assign list of permissions . Example : : { % flow_perms request.user task as task_perms % } <code> def flow_perms(user, task): ","#LINE# #TAB# result = [] #LINE# #TAB# if hasattr(task.flow_task, 'can_execute') and task.flow_task.can_execute( #LINE# #TAB# #TAB# user, task): #LINE# #TAB# #TAB# result.append('can_execute') #LINE# #TAB# if hasattr(task.flow_task, 'can_assign') and task.flow_task.can_assign(user #LINE# #TAB# #TAB# , task): #LINE# #TAB# #TAB# result.append('can_assign') #LINE# #TAB# if hasattr(task.flow_task, 'can_view') and task.flow_task.can_view(user, #LINE# #TAB# #TAB# task): #LINE# #TAB# #TAB# result.append('can_view') #LINE# #TAB# return result"
Create a complete graph from the list of node ids  <code> def create_complete_graph(node_ids): ,"#LINE# #TAB# g = nx.Graph() #LINE# #TAB# g.add_nodes_from(node_ids) #LINE# #TAB# for (i, j) in combinations(node_ids, 2): #LINE# #TAB# #TAB# g.add_edge(i, j) #LINE# #TAB# return g"
Check given seq is two - dimensional . Raise error if ca n't be easily transformed  <code> def check_2d(seq: Sequence) ->Sequence: ,"#LINE# #TAB# for ndx, el in enumerate(seq): #LINE# #TAB# #TAB# if not isinstance(el, (list, tuple, np.ndarray, pd.Series)): #LINE# #TAB# #TAB# #TAB# raise ValueError('Too few dimensions.') #LINE# #TAB# #TAB# seq[ndx] = check_1d(el) #LINE# #TAB# return seq"
"get overridden method if any <code> def overridden_method(klass, name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# parent = next(klass.local_attr_ancestors(name)) #LINE# #TAB# except (StopIteration, KeyError): #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# meth_node = parent[name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if isinstance(meth_node, astroid.FunctionDef): #LINE# #TAB# #TAB# return meth_node #LINE# #TAB# return None"
Process assignment function definition  <code> def math_funcdef_handle(tokens): ,"#LINE# #TAB# internal_assert(len(tokens) == 2, ""invalid assignment function definition tokens"", tokens) #LINE# #TAB# return tokens[0] + ("""" if tokens[1].startswith(""\n"") else "" "") + tokens[1]"
"Determine whether it is possible for this endpoint to return an iterated response  <code> def iterable_method(method, params): ","#LINE# #TAB# required_params = 'limit', 'page_num' #LINE# #TAB# params_present = all(param in params for param in required_params) #LINE# #TAB# return method.lower() == 'get' and params_present"
"Return real value of CDATA section <code> def get_rawvalue_value(data, encoding=None): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# if encoding is None or encoding == '': #LINE# #TAB# #TAB# #TAB# return data #LINE# #TAB# #TAB# elif encoding == 'base64': #LINE# #TAB# #TAB# #TAB# return base64.b64decode(data) #LINE# #TAB# #TAB# return base64.b64decode(data) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return data
"Generate a set of random SDR s <code> def generate_random_sdr(numSDR, numDims, numActiveInputBits, seed=42): ","#LINE# randomSDRs = np.zeros((numSDR, numDims), dtype=uintType) #LINE# indices = np.array(range(numDims)) #LINE# np.random.seed(seed) #LINE# for i in range(numSDR): #LINE# #TAB# randomIndices = np.random.permutation(indices) #LINE# #TAB# activeBits = randomIndices[:numActiveInputBits] #LINE# #TAB# randomSDRs[i, activeBits] = 1 #LINE# return randomSDRs"
Get redis db handler <code> def get_store(cls): ,"#LINE# #TAB# if not cls.redis_store: #LINE# #TAB# #TAB# cls.redis_store = redis.StrictRedis(host='localhost', port=6379) #LINE# #TAB# return cls.redis_store"
"Read CSV at ` location ` , return a list of ordered dictionaries , one for each row  <code> def load_csv(location): ","#LINE# #TAB# results = [] #LINE# #TAB# with codecs.open(location, mode='rb', encoding='utf-8', errors='ignore' #LINE# #TAB# #TAB# ) as csvfile: #LINE# #TAB# #TAB# for row in csv.DictReader(csvfile): #LINE# #TAB# #TAB# #TAB# updated_row = OrderedDict([(key.lower(), value) for key, value in #LINE# #TAB# #TAB# #TAB# #TAB# row.items()]) #LINE# #TAB# #TAB# #TAB# results.append(updated_row) #LINE# #TAB# return results"
"Reads int property from the HID device  <code> def get_device_int_property(dev_ref, key): ","#LINE# cf_key = CFStr(key) #LINE# type_ref = iokit.IOHIDDeviceGetProperty(dev_ref, cf_key) #LINE# cf.CFRelease(cf_key) #LINE# if not type_ref: #LINE# #TAB# return None #LINE# if cf.CFGetTypeID(type_ref) != cf.CFNumberGetTypeID(): #LINE# #TAB# raise errors.OsHidError('Expected number type, got {}'.format( #LINE# #TAB# #TAB# cf.CFGetTypeID(type_ref))) #LINE# out = ctypes.c_int32() #LINE# ret = cf.CFNumberGetValue(type_ref, K_CF_NUMBER_SINT32_TYPE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ctypes.byref(out)) #LINE# if not ret: #LINE# #TAB# return None #LINE# return out.value"
Get the indeces of gates which commute with CZ <code> def get_commuters(unitaries): ,"#LINE# #TAB# commuters = qi.id, qi.pz, qi.ph, qi.hermitian_conjugate(qi.ph) #LINE# #TAB# return [find_clifford(u, unitaries) for u in commuters]"
"Show detailed information about a chute in the store . NAME must be the name of a chute in the store  <code> def describe_chute(ctx, name): ",#LINE# #TAB# client = ControllerClient() #LINE# #TAB# result = client.find_chute(name) #LINE# #TAB# click.echo(util.format_result(result)) #LINE# #TAB# return result
"Returns True if task is chief task in the corresponding run <code> def is_chief(task: backend.Task, run_name: str): ","#LINE# global run_task_dict #LINE# if run_name not in run_task_dict: #LINE# #TAB# return True #LINE# task_list = run_task_dict[run_name] #LINE# assert task in task_list, f""Task {task.name} doesn't belong to run {run_name}"" #LINE# return task_list[0] == task"
"Calculates the individual and combined scaling factors for both provided models <code> def build_scaling_factors(S12, model_one, model_two): ","#LINE# #TAB# model_one.omatrix_scaling[:] = np.amax(model_one.omatrix, axis=(2, 3)) #LINE# #TAB# model_two.omatrix_scaling[:] = np.amax(model_two.omatrix, axis=(2, 3)) #LINE# #TAB# S12[:] = np.maximum(model_one.omatrix_scaling, model_two.omatrix_scaling) #LINE# #TAB# return"
"Numpy aware comparison between two values  <code> def is_different(old_value, new_value): ","#LINE# #TAB# if opt.has_numpy: #LINE# #TAB# #TAB# return not opt.np.array_equal(old_value, new_value) #LINE# #TAB# else: #LINE# #TAB# #TAB# return old_value != new_value"
"Return true if two wildcard patterns can match the same string  <code> def wildcards_overlap(name1, name2): ","#LINE# #TAB# if not name1 and not name2: #LINE# #TAB# #TAB# return True #LINE# #TAB# if not name1 or not name2: #LINE# #TAB# #TAB# return False #LINE# #TAB# for matched1, matched2 in _character_matches(name1, name2): #LINE# #TAB# #TAB# if wildcards_overlap(name1[matched1:], name2[matched2:]): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Validate the config file and command line parameters  <code> def validate_config(config: Dict) ->Dict: ,#LINE# #TAB# try: #LINE# #TAB# #TAB# schema: ConfigSchema = ConfigSchema(strict=True) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# schema = ConfigSchema() #LINE# #TAB# try: #LINE# #TAB# #TAB# loaded: Dict = schema.load(config) #LINE# #TAB# except ValidationError as err: #LINE# #TAB# #TAB# raise ConfigLoadError(construct_config_error_msg(err.messages)) #LINE# #TAB# try: #LINE# #TAB# #TAB# validated = loaded.data #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# validated = loaded #LINE# #TAB# if get_display_protocol() == DisplayProtocol.X11: #LINE# #TAB# #TAB# purge_invalid_wayland_rules(validated) #LINE# #TAB# return validated
": type database indexdigest.database . Database : type table_name str : type column indexdigest.Column.column : rtype : dict <code> def get_boundary_times(database, table_name, column): ","#LINE# #TAB# query = ( #LINE# #TAB# #TAB# 'SELECT /* index-digest */ UNIX_TIMESTAMP(MIN(`{column}`)) as `min`, UNIX_TIMESTAMP(MAX(`{column}`)) as `max` FROM `{table}`' #LINE# #TAB# #TAB# .format(column=column.name, table=table_name)) #LINE# #TAB# timestamps = database.query_dict_row(query) #LINE# #TAB# return timestamps if timestamps.get('min') and timestamps.get('max' #LINE# #TAB# #TAB# ) else None"
"Annotates the provided DOM tree with XMLNS attributes and adds XMLNS prefixes to the tags of the tree nodes  <code> def annotate_with_xmlns(tree, prefix, URI): ","#LINE# #TAB# if not ET.iselement(tree): #LINE# #TAB# #TAB# tree = tree.getroot() #LINE# #TAB# tree.attrib['xmlns:' + prefix] = URI #LINE# #TAB# iterator = tree.iter() #LINE# #TAB# next(iterator) #LINE# #TAB# for e in iterator: #LINE# #TAB# #TAB# e.tag = prefix + "":"" + e.tag"
"Return a concise human - readable description for this plugin . Subclasses should set the class attribute DESCRIPTION , which is returned by the default implementation  <code> def get_description(cls): ",#LINE# #TAB# if not cls.DESCRIPTION: #LINE# #TAB# #TAB# raise NotImplementedError #LINE# #TAB# return cls.DESCRIPTION
Convert a bytecode file path to a source path ( if possible ) . This function exists purely for backwards - compatibility for PyImport_ExecCodeModuleWithFilenames ( ) in the C API  <code> def get_sourcefile(bytecode_path): ,"#LINE# #TAB# if len(bytecode_path) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# rest, _, extension = bytecode_path.rpartition('.') #LINE# #TAB# if not rest or extension.lower()[-3:-1] != 'py': #LINE# #TAB# #TAB# return bytecode_path #LINE# #TAB# try: #LINE# #TAB# #TAB# source_path = source_from_cache(bytecode_path) #LINE# #TAB# except (NotImplementedError, ValueError): #LINE# #TAB# #TAB# source_path = bytecode_path[:-1] #LINE# #TAB# return source_path if _path_isfile(source_path) else bytecode_path"
MSOA data contains individual level demographic characteristics including : - HID - Household ID - Area ID - HID - Household ID <code> def read_oa_data(): ,"#LINE# #TAB# for filename in oa_year_files.values(): #LINE# #TAB# #TAB# with open(filename, 'r') as year_file: #LINE# #TAB# #TAB# #TAB# year_reader = csv.reader(year_file) #LINE# #TAB# #TAB# #TAB# next(year_reader, None) #LINE# #TAB# #TAB# #TAB# for line in year_reader: #LINE# #TAB# #TAB# #TAB# #TAB# OA_data.append({'HID': line[0], 'OA': line[1], 'SES': line[ #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 12], 'year': filename[-8:-4]}) #LINE# #TAB# return OA_data"
Does this line start with a number ? <code> def is_numline(i): ,"#LINE# #TAB# l = lines[i] #LINE# #TAB# line_start = 0 #LINE# #TAB# if l.startswith(', '): #LINE# #TAB# #TAB# line_start += 2 #LINE# #TAB# line_end = len(l) - 1 #LINE# #TAB# line_end_new = l.find(' ', line_start) #LINE# #TAB# if line_end_new != -1: #LINE# #TAB# #TAB# line_end = line_end_new #LINE# #TAB# try: #LINE# #TAB# #TAB# float(l[line_start:line_end]) #LINE# #TAB# #TAB# return True #LINE# #TAB# except: #LINE# #TAB# #TAB# return False"
Loads the objective function from a . json file  <code> def load_objective(config): ,"#LINE# #TAB# assert 'prjpath' in config #LINE# #TAB# assert 'main-file' in config, ""The problem file ('main-file') is missing!"" #LINE# #TAB# os.chdir(config['prjpath']) #LINE# #TAB# if config['language'].lower()=='python': #LINE# #TAB# #TAB# assert config['main-file'].endswith('.py'), 'The python problem file has to end with .py!' #LINE# #TAB# #TAB# import imp #LINE# #TAB# #TAB# m = imp.load_source(config['main-file'][:-3], os.path.join(config['prjpath'],config['main-file'])) #LINE# #TAB# #TAB# func = m.__dict__[config['main-file'][:-3]] #LINE# #TAB# return func"
If maybe_dttm is a datetime instance convert to a STIX - compliant string representation . Otherwise return the value unchanged  <code> def ensure_datetime_to_string(maybe_dttm): ,"#LINE# #TAB# if isinstance(maybe_dttm, datetime.datetime): #LINE# #TAB# #TAB# maybe_dttm = _format_datetime(maybe_dttm) #LINE# #TAB# return maybe_dttm"
"make index the first item of the list whilst preserving the order of elements <code> def reindex_lst(lst, index) ->List: ","#LINE# #TAB# pos = lst.index(index) #LINE# #TAB# start, tail = lst[:pos], lst[pos:] #LINE# #TAB# tail.extend(start) #LINE# #TAB# assert len(lst) == len(tail) #LINE# #TAB# return tail"
"Given a valid user access token , it will return the user 's email  <code> def get_user_email(user_access_token): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# resp = requests.get( #LINE# #TAB# #TAB# #TAB# f'{settings.API_BASE_URL}/me?access_token={user_access_token}&fields=email' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# raise exceptions.FacebookNetworkException() #LINE# #TAB# resp = resp.json() #LINE# #TAB# if 'error' in resp: #LINE# #TAB# #TAB# raise exceptions.FacebookRequestException(resp['error']) #LINE# #TAB# email = resp.get('email', None) #LINE# #TAB# if email in [None, '']: #LINE# #TAB# #TAB# raise exceptions.UserEmailException() #LINE# #TAB# return email"
Show button for the creation of sections ( if experiment type is multivariate ) <code> def add_section(obj): ,"#LINE# #TAB# if obj.experiment_type == 'Multivariate': #LINE# #TAB# #TAB# admin_path = reverse('admin:gwo_gwosection_add') #LINE# #TAB# #TAB# return ''.join(['<a href=""%s' % admin_path, '?gwo_experiment=%s"" ' % #LINE# #TAB# #TAB# #TAB# obj.id, 'class=""linkbutton"">Add section</a>']) #LINE# #TAB# return ''"
"Replace nulls with the mode on dataframe[colname ] <code> def null_removal_mode(dataframe, colname): ","#LINE# #TAB# col = dataframe[colname] #LINE# #TAB# dataframe[colname] = col.fillna(col.mode().get(0, None)) #LINE# #TAB# return dataframe"
"Accumulate random bit string and remove \ 0 bytes until the needed length is obtained  <code> def get_nonzero_random_bytes(length, rnd=default_crypto_random): ","#LINE# #TAB# result = [] #LINE# #TAB# i = 0 #LINE# #TAB# while i < length: #LINE# #TAB# #TAB# rnd = rnd.getrandbits(12*length) #LINE# #TAB# #TAB# s = i2osp(rnd, 3*length) #LINE# #TAB# #TAB# s = s.replace('\x00', '') #LINE# #TAB# #TAB# result.append(s) #LINE# #TAB# #TAB# i += len(s) #LINE# #TAB# return (''.join(result))[:length]"
"Takes a pandas series of dates as inputs , calculates windows , and returns a series of which windows each observation are in . To be used with groupby.transform ( ) <code> def window_mapping(time, col, method='between'): ","#LINE# #TAB# windows = create_windows(col, time, method=method) #LINE# #TAB# return [n for i in range(len(col.index)) for n, window in enumerate( #LINE# #TAB# #TAB# windows) if i in window]"
"Construct actual key for password identification <code> def format_http_key(url, username): ","#LINE# #TAB# key = '%s@@%s' % (meu.pycompat.sysstr(username), meu.pycompat.sysstr(url)) #LINE# #TAB# return key"
Get the current LAN ip address of this device : rtype : str <code> def get_ip_address(): ,"#LINE# #TAB# ip = None #LINE# #TAB# interfaces = get_interfaces() #LINE# #TAB# for interface in interfaces: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ip = ni.ifaddresses(interface)[ni.AF_INET][0]['addr'] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# except (KeyError, Exception): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return ip"
"Return a new shard tail that follows this shard body  <code> def shard_body_tail(num_rows, sbody): ","#LINE# #TAB# shard_tail = [] #LINE# #TAB# col_gap = 0 #LINE# #TAB# done_rows = 0 #LINE# #TAB# for done_rows, content_iter, cview in sbody: #LINE# #TAB# #TAB# cols, rows = cview[2:4] #LINE# #TAB# #TAB# done_rows += num_rows #LINE# #TAB# #TAB# if done_rows == rows: #LINE# #TAB# #TAB# #TAB# col_gap += cols #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# shard_tail.append((col_gap, done_rows, content_iter, cview)) #LINE# #TAB# #TAB# col_gap = 0 #LINE# #TAB# return shard_tail"
Flatten the nested json structure returned by the Explorer  <code> def flatten_vertex(vertex_json): ,"#LINE# #TAB# vertex = {'id': vertex_json['id'], 'properties': {}} #LINE# #TAB# for prop in vertex_json['properties']: #LINE# #TAB# #TAB# if prop['cardinality'] == 'single': #LINE# #TAB# #TAB# #TAB# vals = prop['values'][0]['value'] #LINE# #TAB# #TAB# elif prop['cardinality'] == 'set': #LINE# #TAB# #TAB# #TAB# vals = {v['value'] for v in prop['values']} #LINE# #TAB# #TAB# elif prop['cardinality'] == 'list': #LINE# #TAB# #TAB# #TAB# vals = [v['value'] for v in prop['values']] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise RuntimeError('Undefined property cardinality') #LINE# #TAB# #TAB# vertex['properties'][prop['key']] = vals #LINE# #TAB# return vertex"
"Dungeon map generator based on square room grid  <code> def dungeon_grid_simple(w, h, room_size=4, delete_chance=0.33): ","#LINE# #TAB# M = Map(w, h, fill_cell=cell_dungeon_floor) #LINE# #TAB# _create_room_grid(M, room_size=room_size) #LINE# #TAB# _create_doors(M, room_size=room_size) #LINE# #TAB# _crush_walls(M, room_size=room_size, delete_chance=delete_chance) #LINE# #TAB# _clear_wall_points(M) #LINE# #TAB# return M"
"Use only the smallest group with a certain amount of observations <code> def min_n_obs_shrinkage(group_sizes: list, min_n_obs) ->np.ndarray: ",#LINE# #TAB# if min_n_obs > max(group_sizes): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'There is no group with size greater than or equal to {min_n_obs}' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# res = np.zeros(len(group_sizes)) #LINE# #TAB# res[np.argmin(np.array(group_sizes) >= min_n_obs) - 1] = 1 #LINE# #TAB# return res
"os.walk with filter to only return directories . : param empty_only : filter that specifies to only return empty directories . The empty_only use case represents removal of directories in OwnCloud not found in Git  <code> def get_directories(root_dir, empty_only=False): ",#LINE# #TAB# dirs = [] #LINE# #TAB# for dir_data in os.walk(root_dir): #LINE# #TAB# #TAB# dirpath = dir_data[0][len(root_dir) + 1:] #LINE# #TAB# #TAB# if empty_only is False or not dir_data[2] and dirpath: #LINE# #TAB# #TAB# #TAB# dirs.append(dirpath) #LINE# #TAB# return dirs
Randomly shuffle the training data . Parameters ---------- model : alphapy . Model The model object describing the data . Returns ------- model : alphapy . Model The model object with the shuffled data  <code> def shuffle_data(model): ,#LINE# #TAB# seed = model.specs['seed'] #LINE# #TAB# shuffle = model.specs['shuffle'] #LINE# #TAB# X_train = model.X_train #LINE# #TAB# y_train = model.y_train #LINE# #TAB# if shuffle: #LINE# #TAB# #TAB# logger.info('Shuffling Training Data') #LINE# #TAB# #TAB# np.random.seed(seed) #LINE# #TAB# #TAB# new_indices = np.random.permutation(y_train.size) #LINE# #TAB# #TAB# model.X_train = X_train[new_indices] #LINE# #TAB# #TAB# model.y_train = y_train[new_indices] #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.info('Skipping Shuffling') #LINE# #TAB# return model
"Guesses whether a set of Axes is home to a colorbar Parameters ---------- ax Axes instance Returns ------- True if the x xor y axis satisfies all of the following and thus looks like a colorbar : No ticks , no tick labels , no axis label <code> def is_color_bar(ax): ",#LINE# #TAB# xcb = len(ax.get_xticks()) == 0 and len(ax.get_xticklabels()) == 0 and len( #LINE# #TAB# #TAB# ax.get_xlabel()) == 0 #LINE# #TAB# ycb = len(ax.get_yticks()) == 0 and len(ax.get_yticklabels()) == 0 and len( #LINE# #TAB# #TAB# ax.get_ylabel()) == 0 #LINE# #TAB# return xcb != ycb
Helper to open also compressed files <code> def open_any(filename): ,#LINE# #TAB# if filename.endswith('.gz'): #LINE# #TAB# #TAB# return gzip.open #LINE# #TAB# if filename.endswith('.bz2'): #LINE# #TAB# #TAB# return bz2.BZ2File #LINE# #TAB# return open
Reject duplicate keys  <code> def dict_raise_on_duplicates(ordered_pairs): ,"#LINE# #TAB# d = {} #LINE# #TAB# for k, v in ordered_pairs: #LINE# #TAB# #TAB# if k in d: #LINE# #TAB# #TAB# raise ValueError(""duplicate key: %r"" % (k,)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# d[k] = v #LINE# #TAB# return d"
Creates a MainWindow using 75 % of the available screen resolution  <code> def create_main_window(): ,"#LINE# #TAB# main_win = MainWindow() #LINE# #TAB# main_windows.append(main_win) #LINE# #TAB# available_geometry = app.desktop().availableGeometry(main_win) #LINE# #TAB# main_win.resize(available_geometry.width() * 2 / 3, available_geometry. #LINE# #TAB# #TAB# height() * 2 / 3) #LINE# #TAB# main_win.show() #LINE# #TAB# return main_win"
"Returns uniform model noisy value  <code> def f_uniform(ftrue, alpha, beta): ","#LINE# #TAB# popsi = np.shape(ftrue) #LINE# #TAB# fval = _rand(popsi) ** beta * ftrue * np.maximum(1.0, (1000000000.0 / ( #LINE# #TAB# #TAB# ftrue + 1e-99)) ** (alpha * _rand(popsi))) #LINE# #TAB# tol = 1e-08 #LINE# #TAB# fval = fval + 1.01 * tol #LINE# #TAB# idx = ftrue < tol #LINE# #TAB# try: #LINE# #TAB# #TAB# fval[idx] = ftrue[idx] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# if idx: #LINE# #TAB# #TAB# #TAB# fval = ftrue #LINE# #TAB# return fval"
Collect some information about the machine this experiment runs on  <code> def get_host_info(): ,"#LINE# #TAB# host_info = {} #LINE# #TAB# for k, v in host_info_gatherers.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# host_info[k] = v() #LINE# #TAB# #TAB# except IgnoreHostInfo: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return host_info"
Parse out information from sacct status output  <code> def parse_sacct(sacct_stream): ,#LINE# #TAB# rows = (line.split() for line in sacct_stream) #LINE# #TAB# relevant_rows = (row for row in rows if row[0].isdigit()) #LINE# #TAB# jobs = [convert_job(row) for row in relevant_rows] #LINE# #TAB# return jobs
"call a faker if value is None uses : { { myint|or_fake:'randomInt ' } } <code> def do_or_fake_filter(value, formatter): ",#LINE# #TAB# if not value: #LINE# #TAB# #TAB# value = Faker.getGenerator().format(formatter) #LINE# #TAB# return value
Compute the peaks profile for a set of CW peaks and add into the yc array <code> def compute_pwdr_prof_cw(profList): ,"#LINE# #TAB# for pos, refl, iBeg, iFin, kRatio in profList: #LINE# #TAB# #TAB# yc[iBeg:iFin] += refl[11 + im] * refl[9 + im #LINE# #TAB# #TAB# #TAB# ] * kRatio * G2pwd.getFCJVoigt3(pos, refl[6 + im], refl[7 + im], #LINE# #TAB# #TAB# #TAB# shl, x[iBeg:iFin]) #LINE# #TAB# return yc"
"Loads custom / pypi / map.txt and builds a dict where map[package_name ] = url : return : dict , urls <code> def get_url_map(): ","#LINE# #TAB# map = {} #LINE# #TAB# path = os.path.join(os.path.dirname(os.path.realpath(__file__)), #LINE# #TAB# #TAB# 'custom', 'pypi', 'map.txt') #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# package, url = line.strip().split(': ') #LINE# #TAB# #TAB# #TAB# map[package] = url #LINE# #TAB# return map"
Get the list of tags for output : param doc : a mapping from HTTP verb to the properties for serialization : return : a list of string containing tags as described by the openapi 3.0 spec <code> def tags_for(doc: List[str]) ->Iterable[List[str]]: ,#LINE# #TAB# tags = [] #LINE# #TAB# for name in doc['tags']: #LINE# #TAB# #TAB# tags.append(name) #LINE# #TAB# return tags
"Load template by it 's name . Appends ' .html ' or ' .htm ' extension if needed  <code> def direct_to_disk_template(request, template_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return render_to_response(['auto_urls/' + template_name, #LINE# #TAB# #TAB# #TAB# 'auto_urls/' + template_name + '.html', 'auto_urls/' + #LINE# #TAB# #TAB# #TAB# template_name + '.htm', template_name, template_name + '.html', #LINE# #TAB# #TAB# #TAB# template_name + '.htm'], {}, RequestContext(request)) #LINE# #TAB# except TemplateDoesNotExist: #LINE# #TAB# #TAB# raise Http404"
Ensure string is a unicode string . If it is n't it assumed it is utf-8 and decodes it to a unicode string  <code> def ensure_unicode(s): ,"#LINE# #TAB# if isinstance(s, bytes): #LINE# #TAB# #TAB# s = s.decode('utf-8', 'replace') #LINE# #TAB# return s"
Sets / Returns the background color for text elements : return : ( str ) - color string of the text background color currently in use : rtype : ( str ) <code> def theme_text_element_background_color(color=None): ,#LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# set_options(text_element_background_color=color) #LINE# #TAB# return DEFAULT_TEXT_ELEMENT_BACKGROUND_COLOR
Get ignore commits regex patterns . : return : <code> def get_ignore_commits_regex_patterns() ->List[str]: ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return ['{}'.format(_s) for _s in CONFIG['IgnoreCommits']['regex']. #LINE# #TAB# #TAB# #TAB# split('\n')] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return []
"Collects PSMs with the highes precursor quant values adds sum of the top 3 of these to a protein table <code> def add_ms1_quant_from_top3_mzidtsv(proteins, psms, headerfields, protcol): ","#LINE# #TAB# if not protcol: #LINE# #TAB# #TAB# protcol = mzidtsvdata.HEADER_MASTER_PROT #LINE# #TAB# top_ms1_psms = generate_top_psms(psms, protcol) #LINE# #TAB# for protein in proteins: #LINE# #TAB# #TAB# prot_acc = protein[prottabledata.HEADER_PROTEIN] #LINE# #TAB# #TAB# prec_area = calculate_protein_precursor_quant(top_ms1_psms, prot_acc) #LINE# #TAB# #TAB# outprotein = {k: v for k, v in protein.items()} #LINE# #TAB# #TAB# outprotein[headerfields['precursorquant'][ #LINE# #TAB# #TAB# #TAB# prottabledata.HEADER_AREA][None]] = str(prec_area) #LINE# #TAB# #TAB# yield outprotein"
"Sort the individual lines of a block of text . Args : dedupe ( bool ) : Remove duplicate lines with the same text . Useful for dealing with import statements in templates  <code> def sort_lines(text: str, dedupe: bool=True) ->str: ",#LINE# #TAB# leading = '\n' if text.startswith('\n') else '' #LINE# #TAB# trailing = '\n' if text.endswith('\n') else '' #LINE# #TAB# lines: Iterable[str] = (i for i in text.strip().split('\n') if i.strip()) #LINE# #TAB# if dedupe: #LINE# #TAB# #TAB# lines = set(lines) #LINE# #TAB# answer = '\n'.join(sorted(lines)) #LINE# #TAB# return f'{leading}{answer}{trailing}'
Yield range of consecutive numbers  <code> def find_ranges(iterable): ,"#LINE# #TAB# for group in consecutive_groups(iterable): #LINE# #TAB# #TAB# group = list(group) #LINE# #TAB# #TAB# if len(group) == 1: #LINE# #TAB# #TAB# #TAB# yield group[0], group[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield group[0], group[-1]"
scroll_offset + = 1 <code> def scroll_one_line_down(event): ,"#LINE# #TAB# w = find_window_for_buffer_name(event.cli, event.cli.current_buffer_name) #LINE# #TAB# b = event.cli.current_buffer #LINE# #TAB# if w: #LINE# #TAB# #TAB# if w.render_info: #LINE# #TAB# #TAB# #TAB# info = w.render_info #LINE# #TAB# #TAB# #TAB# if w.vertical_scroll < info.content_height - info.window_height: #LINE# #TAB# #TAB# #TAB# #TAB# if (info.cursor_position.y <= info. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# configured_scroll_offsets.top): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# b.cursor_position += b.document.get_cursor_down_position() #LINE# #TAB# #TAB# #TAB# #TAB# w.vertical_scroll += 1"
repository_fullname = : owener/:name <code> def get_repository_fullname_from_url(url): ,"#LINE# #TAB# m = github_rx.search(url) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return m.group(1).replace('.git', '') #LINE# #TAB# return None"
"return ( calc_dtype , res_dtype ) suitable for BLAS calculations  <code> def find_calc_dtype(a_dtype, b_dtype): ","#LINE# #TAB# res_dtype = np.find_common_type([a_dtype, b_dtype], []) #LINE# #TAB# _, calc_dtype, _ = BLAS.find_best_blas_type(dtype=res_dtype) #LINE# #TAB# return calc_dtype, res_dtype"
Given a Status contents in HTML converts it into lines of plain text  <code> def format_content(content): ,"#LINE# #TAB# paragraphs = parse_html(content) #LINE# #TAB# first = True #LINE# #TAB# for paragraph in paragraphs: #LINE# #TAB# #TAB# if not first: #LINE# #TAB# #TAB# #TAB# yield """" #LINE# #TAB# #TAB# for line in paragraph: #LINE# #TAB# #TAB# #TAB# yield line #LINE# #TAB# #TAB# first = False"
Iterator that yields all cells in a notebook nbformat version independent <code> def yield_code_cells(nb): ,#LINE# #TAB# if nb.nbformat >= 4: #LINE# #TAB# #TAB# for cell in nb['cells']: #LINE# #TAB# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# yield cell #LINE# #TAB# elif nb.nbformat == 3: #LINE# #TAB# #TAB# for ws in nb['worksheets']: #LINE# #TAB# #TAB# #TAB# for cell in ws['cells']: #LINE# #TAB# #TAB# #TAB# #TAB# if cell['cell_type'] == 'code': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield cell
Retains only cannonical chromosomes in bed - like df  <code> def filter_cannonical(df): ,"#LINE# #TAB# df = df[df.loc[:, (0)].str.match('chr[0-9|X|Y]+[a|b]?$')] #LINE# #TAB# return df"
"Return the media that was registered in the request object . .. note : : The output of plugins is typically cached . Changes the the registered media only show up after flushing the cache , or re - saving the items ( which flushes the cache )  <code> def get_frontend_media(request): ","#LINE# #TAB# return getattr(request, '_fluent_contents_frontend_media', None #LINE# #TAB# #TAB# ) or ImmutableMedia.empty_instance"
CWL target to retrieve a list of callable regions for parallelization  <code> def get_parallel_regions(batch): ,"#LINE# #TAB# samples = [utils.to_single_data(d) for d in batch] #LINE# #TAB# regions = _get_parallel_regions(samples[0]) #LINE# #TAB# return [{""region"": ""%s:%s-%s"" % (c, s, e)} for c, s, e in regions]"
Return all index as ` { ' items ' : [ list of indexes ] } ` <code> def get_all(cls): ,#LINE# #TAB# return {'items': [columns[0] for columns in db.session.query(cls.index) #LINE# #TAB# #TAB# .all()]}
Generate seed for random number generator <code> def generate_seed(seed): ,"#LINE# #TAB# if seed is None: #LINE# #TAB# #TAB# random.seed() #LINE# #TAB# #TAB# seed = random.randint(0, sys.maxsize) #LINE# #TAB# random.seed(a=seed) #LINE# #TAB# return seed"
"Converts extended representation to x , y Accepts also projective representation . : param P : : return : <code> def conv_ext_to_xy(P): ","#LINE# #TAB# x, y, z = P[0], P[1], P[2] #LINE# #TAB# zi = inv(z) #LINE# #TAB# x = x * zi % q #LINE# #TAB# y = y * zi % q #LINE# #TAB# return x, y"
"Returns a new State that follows the given State and also has the resource gain of the given pickup : param state : : param pickup : : return : <code> def state_with_pickup(state: State, pickup: PickupEntry) ->State: ","#LINE# #TAB# new_state = state.copy() #LINE# #TAB# new_state.previous_state = state #LINE# #TAB# add_pickup_to_state(new_state, pickup) #LINE# #TAB# return new_state"
Opens connection to S3 returning bucket and key <code> def open_s3(bucket): ,"#LINE# #TAB# conn = boto.connect_s3(options.paved.s3.access_id, options.paved.s3.secret) #LINE# #TAB# try: #LINE# #TAB# #TAB# bucket = conn.get_bucket(bucket) #LINE# #TAB# except boto.exception.S3ResponseError: #LINE# #TAB# #TAB# bucket = conn.create_bucket(bucket) #LINE# #TAB# return bucket"
"It adds data to the plot grid  <code> def add_to_grid(x, y, marker, color): ","#LINE# #TAB# h.xplot = [] #LINE# #TAB# h.yplot = [] #LINE# #TAB# for i in range(len(x)): #LINE# #TAB# #TAB# c = int((x[i] - h.xmin) / h.dx) #LINE# #TAB# #TAB# r = int((y[i] - h.ymin) / h.dy) #LINE# #TAB# #TAB# if 0 <= r < h.rows and 0 <= c < h.cols: #LINE# #TAB# #TAB# #TAB# h.xplot.append(c) #LINE# #TAB# #TAB# #TAB# h.yplot.append(r) #LINE# #TAB# #TAB# #TAB# h.grid[r][c] = set_color(marker, color) #LINE# #TAB# return h.grid"
"Using a predecessor matrix from scipy.csgraph.shortest_paths , get all indices on the path from a root node to a target node  <code> def get_path(root, target, pred): ",#LINE# #TAB# path = [target] #LINE# #TAB# p = target #LINE# #TAB# while p != root: #LINE# #TAB# #TAB# p = pred[p] #LINE# #TAB# #TAB# path.append(p) #LINE# #TAB# path.reverse() #LINE# #TAB# return path
sensitivity in dB re 1V / Pa Returns the transfer - factor in mV / Pa <code> def microphone_transferfactor(sensitivity): ,#LINE# #TAB# a = db2amp(sensitivity) #LINE# #TAB# return a * 1000
We have a list of terms with which we return facets <code> def process_facet_terms(facet_terms): ,"#LINE# #TAB# elastic_facets = {} #LINE# #TAB# for facet in facet_terms: #LINE# #TAB# #TAB# facet_term = {""field"": facet} #LINE# #TAB# #TAB# if facet_terms[facet]: #LINE# #TAB# #TAB# #TAB# for facet_option in facet_terms[facet]: #LINE# #TAB# #TAB# #TAB# #TAB# facet_term[facet_option] = facet_terms[facet][facet_option] #LINE# #TAB# #TAB# elastic_facets[facet] = { #LINE# #TAB# #TAB# #TAB# ""terms"": facet_term #LINE# #TAB# #TAB# } #LINE# #TAB# return elastic_facets"
"Prepares a configuration element containing its URI for a version creation document containing the specified configuration , if any <code> def prepare_configuration_uri(application, configuration): ","#LINE# #TAB# if None == configuration: #LINE# #TAB# #TAB# return None #LINE# #TAB# result = ET.Element('configuration') #LINE# #TAB# config = application.find('configurations/configuration/[name=""' + #LINE# #TAB# #TAB# configuration + '""]') #LINE# #TAB# if None == config: #LINE# #TAB# #TAB# raise FatalError('Configuration ""' + configuration + #LINE# #TAB# #TAB# #TAB# '"" is not available from ' + application.find('uri').text, 1, #LINE# #TAB# #TAB# #TAB# ET.tostring(application)) #LINE# #TAB# result.text = config.find('uri').text #LINE# #TAB# return result"
Return the Pyramid application registry associated to the current running application : returns : A Pyramid registry instance <code> def get_registry(): ,#LINE# #TAB# from pyramid_celery import celery_app #LINE# #TAB# return celery_app.conf['PYRAMID_REGISTRY']
Improves the docstrings of application models when called at the bottom of the respective module  <code> def autodoc_applicationmodel(module): ,#LINE# #TAB# autodoc_tuple2doc(module) #LINE# #TAB# name_applicationmodel = module.__name__ #LINE# #TAB# name_basemodel = name_applicationmodel.split('_')[0] #LINE# #TAB# module_basemodel = importlib.import_module(name_basemodel) #LINE# #TAB# substituter = Substituter(module_basemodel.substituter) #LINE# #TAB# substituter.add_module(module) #LINE# #TAB# substituter.update_masters() #LINE# #TAB# module.substituter = substituter
convert secret to DES key used by bsdi_crypt <code> def bsdi_secret_to_key(secret): ,"#LINE# #TAB# key_value = _crypt_secret_to_key(secret) #LINE# #TAB# idx = 8 #LINE# #TAB# end = len(secret) #LINE# #TAB# while idx < end: #LINE# #TAB# #TAB# next = idx + 8 #LINE# #TAB# #TAB# tmp_value = _crypt_secret_to_key(secret[idx:next]) #LINE# #TAB# #TAB# key_value = des_encrypt_int_block(key_value, key_value) ^ tmp_value #LINE# #TAB# #TAB# idx = next #LINE# #TAB# return key_value"
Returns context dict containing a single quote . This is chosen from amongst quotes flagged as public  <code> def random_public_quote(request): ,#LINE# #TAB# quote = Quote.objects.get_random() #LINE# #TAB# return {'quotidian_quote': quote}
Checks if the graph ` G ` has isolated nodes <code> def has_isolated_nodes(G): ,#LINE# #TAB# if len(list(nx.isolates(G))) > 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Return the appropriate line index , depending on ` ` line_or_func ` ` which can be either a function , a positive or negative int , or None  <code> def get_line_index(line_or_func, lines): ","#LINE# #TAB# if hasattr(line_or_func, '__call__'): #LINE# #TAB# #TAB# return line_or_func(lines) #LINE# #TAB# elif line_or_func: #LINE# #TAB# #TAB# if line_or_func >= 0: #LINE# #TAB# #TAB# #TAB# return line_or_func #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# n_lines = sum(1 for line in lines) #LINE# #TAB# #TAB# #TAB# return n_lines + line_or_func #LINE# #TAB# else: #LINE# #TAB# #TAB# return line_or_func"
Convert to string all values in ` data ` . Parameters ---------- data : dict[str]->object Returns ------- string_data : dict[str]->str <code> def to_string(data): ,"#LINE# #TAB# sdata = data.copy() #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if isinstance(v, datetime): #LINE# #TAB# #TAB# #TAB# sdata[k] = timestamp_to_date_str(v) #LINE# #TAB# #TAB# elif not isinstance(v, (string_types, float, int)): #LINE# #TAB# #TAB# #TAB# sdata[k] = str(v) #LINE# #TAB# return sdata"
Returns md5sum of file <code> def get_md5sum(src_file): ,"#LINE# #TAB# with open(src_file, 'r') as src_data: #LINE# #TAB# #TAB# src_content = src_data.read() #LINE# #TAB# #TAB# if sys.version_info[0] == 3: #LINE# #TAB# #TAB# #TAB# src_content = src_content.encode('utf-8') #LINE# #TAB# #TAB# src_md5 = hashlib.md5(src_content).hexdigest() #LINE# #TAB# return src_md5"
Function to decide which key function to use . Defaults to ` ` default_key_func ` `  <code> def get_key_func(key_func): ,"#LINE# #TAB# if key_func is not None: #LINE# #TAB# #TAB# if callable(key_func): #LINE# #TAB# #TAB# #TAB# return key_func #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# key_func_module_path, key_func_name = key_func.rsplit('.', 1) #LINE# #TAB# #TAB# #TAB# key_func_module = import_module(key_func_module_path) #LINE# #TAB# #TAB# #TAB# return getattr(key_func_module, key_func_name) #LINE# #TAB# return default_key_func"
"Add BDDs from JSON ` file_name ` to ` bdd ` . @param load_order : if ` True ` , then load variable order from ` file_name `  <code> def load_json(file_name, bdd, load_order=False): ","#LINE# #TAB# tmp_fname = os.path.join(SHELVE_DIR, 'temporary_shelf') #LINE# #TAB# os.makedirs(SHELVE_DIR) #LINE# #TAB# try: #LINE# #TAB# #TAB# cache = shelve.open(tmp_fname) #LINE# #TAB# #TAB# with open(file_name, 'r') as f: #LINE# #TAB# #TAB# #TAB# nodes = _load_json(f, bdd, load_order, cache) #LINE# #TAB# #TAB# cache.close() #LINE# #TAB# finally: #LINE# #TAB# #TAB# shutil.rmtree(SHELVE_DIR) #LINE# #TAB# return nodes"
Infer language pair from filename : < split>.<lang1>-<lang2>.( ... ).idx <code> def infer_language_pair(path): ,"#LINE# #TAB# src, dst = None, None #LINE# #TAB# for filename in os.listdir(path): #LINE# #TAB# #TAB# parts = filename.split('.') #LINE# #TAB# #TAB# if len(parts) >= 3 and len(parts[1].split('-')) == 2: #LINE# #TAB# #TAB# #TAB# return parts[1].split('-') #LINE# #TAB# return src, dst"
"return ids that match this resource type 's i d format  <code> def match_ids(cls, ids): ","#LINE# #TAB# id_prefix = getattr(cls.get_model(), 'id_prefix', None) #LINE# #TAB# if id_prefix is not None: #LINE# #TAB# #TAB# return [i for i in ids if i.startswith(id_prefix)] #LINE# #TAB# return ids"
"Remove interactiontion if one of these components is not in complexes dataframe <code> def filter_interactions_by_complexes(interactions: pd.DataFrame, complexes: ","#LINE# #TAB# pd.DataFrame) ->pd.DataFrame: #LINE# #TAB# complex_ids = complexes['complex_multidata_id'].tolist() #LINE# #TAB# interactions_filtered = interactions[interactions.apply(lambda #LINE# #TAB# #TAB# interaction: interaction['multidata_1_id'] in complex_ids or #LINE# #TAB# #TAB# interaction['multidata_2_id'] in complex_ids, axis=1)].copy() #LINE# #TAB# interactions_filtered.drop_duplicates('id_cp_interaction', inplace=True) #LINE# #TAB# return interactions_filtered"
"Compute the tensor- , index- , and total shape of an expr . Returns ( shape , size , index_shape , index_size , total_shape , total_size )  <code> def compute_all_shapes(v): ","#LINE# #TAB# shape = v.ufl_shape #LINE# #TAB# index_shape = v.ufl_index_dimensions #LINE# #TAB# total_shape = shape + index_shape #LINE# #TAB# return shape, index_shape, total_shape"
Helper method for multiprocessing of apply_transformation . Must not be in the class so that it can be pickled  <code> def apply_transformation(inputs): ,"#LINE# #TAB# ts, transformation, extend_collection, clear_redo = inputs #LINE# #TAB# new = ts.append_transformation(transformation, extend_collection, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# clear_redo=clear_redo) #LINE# #TAB# o = [ts] #LINE# #TAB# if new: #LINE# #TAB# #TAB# o.extend(new) #LINE# #TAB# return o"
Return the names of all imported snp sets <code> def get_snpsets_list(): ,#LINE# #TAB# import rabaDB.filters as rfilt #LINE# #TAB# f = rfilt.RabaQuery(SNPMaster) #LINE# #TAB# names = [] #LINE# #TAB# for g in f.run(gen=True): #LINE# #TAB# #TAB# names.append(g.setName) #LINE# #TAB# return names
"Performs ` ` nexus3 script delete ` ` <code> def cmd_delete(nexus_client, name): ",#LINE# #TAB# nexus_client.scripts.delete(name) #LINE# #TAB# return exception.CliReturnCode.SUCCESS.value
"Setup logging via CLI options <code> def _cli(cls, opts): ","#LINE# #TAB# #TAB# if opts.background: #LINE# #TAB# #TAB# #TAB# logging.getLogger().setLevel(logging.INFO) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# if opts.logdir: #LINE# #TAB# #TAB# #TAB# logging.basicConfig( #LINE# #TAB# #TAB# #TAB# #TAB# level=logging.INFO, #LINE# #TAB# #TAB# #TAB# #TAB# format=cls._log_format, #LINE# #TAB# #TAB# #TAB# #TAB# filename=os.path.join(opts.logdir, ""luigi-server.log"")) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False"
"Get compose v3 compatibility from --compatibility option or from COMPOSE_COMPATIBILITY environment variable  <code> def compatibility_from_options(working_dir, options=None, environment=None): ",#LINE# #TAB# compatibility_option = options.get('--compatibility') #LINE# #TAB# compatibility_environment = environment.get_boolean('COMPOSE_COMPATIBILITY' #LINE# #TAB# #TAB# ) #LINE# #TAB# return compatibility_option or compatibility_environment
"Split an iterator at ` ` size ` ` item intervals <code> def chunk_sequence(data, size): ","#LINE# #TAB# for offset in range(0, len(data), size): #LINE# #TAB# #TAB# yield data[offset:offset + size]"
validates the connection object is capable of read access to rethink <code> def validate_get_dbs(connection): ,#LINE# #TAB# remote_dbs = set(rethinkdb.db_list().run(connection)) #LINE# #TAB# assert remote_dbs #LINE# #TAB# return remote_dbs
Return the server defaults for the given environment variables  <code> def get_server_defaults(): ,"#LINE# #TAB# default_address = os.environ.get(ENVIRON_ADDRESS_NAME, socket. #LINE# #TAB# #TAB# gethostbyname(socket.gethostname())) #LINE# #TAB# default_port = int(os.environ.get(ENVIRON_PORT_NAME, '54212')) #LINE# #TAB# return default_address, default_port"
"Perform put operation . This is used in the distributed wrapper <code> def do_put(content_path, content): ","#LINE# #TAB# content_hash = hashlib.sha1(content).hexdigest() #LINE# #TAB# content_dirname = join(content_path, content_hash[:2]) #LINE# #TAB# if not isdir(content_dirname): #LINE# #TAB# #TAB# os.makedirs(content_dirname) #LINE# #TAB# content_filename = join(content_dirname, content_hash[2:]) #LINE# #TAB# if not isfile(content_filename): #LINE# #TAB# #TAB# with safeopen.std_open(content_filename, 'wb') as content_file: #LINE# #TAB# #TAB# #TAB# content_file.write(content) #LINE# #TAB# return content_hash"
Escape all special characters  <code> def glob_escape(pathname): ,"#LINE# #TAB# drive, pathname = os.path.splitdrive(pathname) #LINE# #TAB# pathname = _magic_check.sub('[\\1]', pathname) #LINE# #TAB# return drive + pathname"
After request handler  <code> def after_request(response): ,"#LINE# #TAB# service_log.info('{0} {1} {2} {3} {4}'.format(request.remote_addr, #LINE# #TAB# #TAB# request.method, request.scheme, request.full_path, response.status)) #LINE# #TAB# return response"
"Turn an Astropy Quantity object into something we can write out to an HDF5 file  <code> def quantity_to_hdf5(f, key, q): ","#LINE# #TAB# if hasattr(q, 'unit'): #LINE# #TAB# #TAB# f[key] = q.value #LINE# #TAB# #TAB# f[key].attrs['unit'] = str(q.unit) #LINE# #TAB# else: #LINE# #TAB# #TAB# f[key] = q #LINE# #TAB# #TAB# f[key].attrs['unit'] = """""
"pick url querystring to params <code> def add_querystr_to_params(url, params): ","#LINE# #TAB# sch, net, path, par, query, fra = urlparse.urlparse(url) #LINE# #TAB# query_params = urlparse.parse_qsl(query, keep_blank_values=True) #LINE# #TAB# params.update(query_params) #LINE# #TAB# uri = urlparse.urlunparse((sch, net, path, par, '', fra)) #LINE# #TAB# return uri, params"
Get the area of a polygon which is represented by a 2D array of points . Area is computed using the Shoelace Algorithm . Args : polygon : 2D array of points  <code> def polygon_area(polygon): ,"#LINE# #TAB# x = polygon[:, (0)] #LINE# #TAB# y = polygon[:, (1)] #LINE# #TAB# area = np.dot(x, np.roll(y, -1)) - np.dot(np.roll(x, -1), y) #LINE# #TAB# return np.abs(area) / 2"
Return the config data as a dictionary Parameters ---------- file : str path to the configuration file <code> def grab_config_data_from_data_file(file): ,"#LINE# #TAB# config = file #LINE# #TAB# sections = config.sections() #LINE# #TAB# data = {} #LINE# #TAB# if config.error: #LINE# #TAB# #TAB# logger.info( #LINE# #TAB# #TAB# #TAB# 'Unable to open %s with configparser because %s. The data will not be stored in the meta file' #LINE# #TAB# #TAB# #TAB# % (config.path_to_file, config.error)) #LINE# #TAB# if sections != []: #LINE# #TAB# #TAB# for i in sections: #LINE# #TAB# #TAB# #TAB# data[i] = {} #LINE# #TAB# #TAB# #TAB# for key in config['%s' % i]: #LINE# #TAB# #TAB# #TAB# #TAB# data[i][key] = config['%s' % i]['%s' % key] #LINE# #TAB# return data"
Branching order of a tree section <code> def strahler_order(section): ,#LINE# #TAB# if section.children: #LINE# #TAB# #TAB# child_orders = [strahler_order(child) for child in section.children] #LINE# #TAB# #TAB# max_so_children = max(child_orders) #LINE# #TAB# #TAB# it = iter(co == max_so_children for co in child_orders) #LINE# #TAB# #TAB# any(it) #LINE# #TAB# #TAB# if any(it): #LINE# #TAB# #TAB# #TAB# return max_so_children + 1 #LINE# #TAB# #TAB# return max_so_children #LINE# #TAB# return 1
"Render an HTML element(tree ) and optionally pretty - print it  <code> def et_tostring(elem, pretty_print=False): ","#LINE# #TAB# text = ET.tostring(elem, 'utf-8' if version < '3.0' else 'unicode') #LINE# #TAB# if pretty_print: #LINE# #TAB# #TAB# from xml.dom import minidom #LINE# #TAB# #TAB# import re #LINE# #TAB# #TAB# declaration_len = len(minidom.Document().toxml()) #LINE# #TAB# #TAB# reparsed = minidom.parseString(text) #LINE# #TAB# #TAB# text = reparsed.toprettyxml(indent=' ')[declaration_len:] #LINE# #TAB# #TAB# text_re = re.compile('>\\n\\s+([^<>\\s].*?)\\n\\s+</', re.DOTALL) #LINE# #TAB# #TAB# text = text_re.sub('>\\g<1></', text) #LINE# #TAB# return text"
"Create an object key for storage  <code> def make_key(table_name, objid): ",#LINE# #TAB# key = datastore.Key() #LINE# #TAB# path = key.path_element.add() #LINE# #TAB# path.kind = table_name #LINE# #TAB# path.name = str(objid) #LINE# #TAB# return key
Calculates the centroid of a polygon with paired x - y values  <code> def get_centroid(points): ,"#LINE# #TAB# xs = numpy.array([x for x, y in points] + [points[0][0]]) #LINE# #TAB# ys = numpy.array([y for x, y in points] + [points[0][1]]) #LINE# #TAB# a = xs[:-1] * ys[1:] #LINE# #TAB# b = ys[:-1] * xs[1:] #LINE# #TAB# A = numpy.sum(a - b) / 2.0 #LINE# #TAB# cx = xs[:-1] + xs[1:] #LINE# #TAB# cy = ys[:-1] + ys[1:] #LINE# #TAB# Cx = numpy.sum(cx * (a - b)) / (6.0 * A) #LINE# #TAB# Cy = numpy.sum(cy * (a - b)) / (6.0 * A) #LINE# #TAB# return Cx, Cy"
Get name of this resource : return : name of this resource : rtype : str <code> def get_name(friendly=False): ,#LINE# #TAB# if friendly: #LINE# #TAB# #TAB# return 'Check result log' #LINE# #TAB# return 'logcheckresult'
"Given a string and index , return ( line , column ) <code> def find_position(string, index, line_offset): ","#LINE# #TAB# leading = string[:index].splitlines() #LINE# #TAB# return len(leading) + line_offset, len(leading[-1]) + 1"
"Assemble a multiline message from a sequence of NMEA messages . : param messages : Sequence of NMEA messages : return : Single message <code> def assemble_from_iterable(cls, messages: Sequence): ",#LINE# #TAB# raw = b'' #LINE# #TAB# data = b'' #LINE# #TAB# bit_array = bitarray() #LINE# #TAB# for msg in messages: #LINE# #TAB# #TAB# raw += msg.raw #LINE# #TAB# #TAB# data += msg.data #LINE# #TAB# #TAB# bit_array += msg.bit_array #LINE# #TAB# messages[0].raw = raw #LINE# #TAB# messages[0].data = data #LINE# #TAB# messages[0].bit_array = bit_array #LINE# #TAB# return messages[0]
"Get a list of skil.resources.base . Resource objects by type ( ' compute ' or ' storage ' )  <code> def get_resources_by_type(skil, resource_type): ","#LINE# #TAB# resource_list = skil.api.get_resource_by_type(resource_type) #LINE# #TAB# resource_ids = [resource.resource_id for resource in resource_list] #LINE# #TAB# return [get_resource_by_id(skil, resource_id) for resource_id in #LINE# #TAB# #TAB# resource_ids]"
Preprocessor to create contour from an earthquake <code> def earthquake_contour_preprocessor(impact_function): ,"#LINE# #TAB# contour_path = create_smooth_contour(impact_function.hazard) #LINE# #TAB# if os.path.exists(contour_path): #LINE# #TAB# #TAB# from safe.gis.tools import load_layer #LINE# #TAB# #TAB# return load_layer(contour_path, tr('Contour'), 'ogr')[0]"
"Create string from object , concatenate if list  <code> def format_to_str(obj): ","#LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# res = '' #LINE# #TAB# if isinstance(obj, list): #LINE# #TAB# #TAB# res = ' '.join(obj) #LINE# #TAB# else: #LINE# #TAB# #TAB# res = obj #LINE# #TAB# return res"
Enable or disable the plugin <code> def set_enabled(enabled=True): ,#LINE# #TAB# onCreate.enabled = enabled #LINE# #TAB# onResize.enabled = enabled #LINE# #TAB# focusChanged.enabled = enabled
"Returns a code describing the capitalization of the word : lower , title , other or non - alpha ( numbers and other tokens that ca n't be capitalized )  <code> def get_capitalization(word): ",#LINE# #TAB# if word == WD.padding_left or word == WD.padding_right: #LINE# #TAB# #TAB# return Caps.padding #LINE# #TAB# if not any(c.isalpha() for c in word): #LINE# #TAB# #TAB# return Caps.non_alpha #LINE# #TAB# if word.islower(): #LINE# #TAB# #TAB# return Caps.lower #LINE# #TAB# if len(word) == 1: #LINE# #TAB# #TAB# return Caps.title #LINE# #TAB# elif word[0].isupper() and word[1:].islower(): #LINE# #TAB# #TAB# return Caps.title #LINE# #TAB# return Caps.other
Ignore any exception via isinstance on Python 3  <code> def safe_isclass(obj): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return inspect.isclass(obj) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False
Function to return a list of unique field names from the config file <code> def extract_fieldnames(config): ,#LINE# #TAB# fields = [] #LINE# #TAB# for x in get_fields(config): #LINE# #TAB# #TAB# if x in fields: #LINE# #TAB# #TAB# #TAB# fields.append(x + '_' + str(fields.count(x) + 1)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# fields.append(x) #LINE# #TAB# return fields
Function to derive address family from a junos table name . : params table : The name of the routing table : returns : address family <code> def get_address_family(table): ,"#LINE# #TAB# address_family_mapping = {'inet': 'ipv4', 'inet6': 'ipv6', 'inetflow': #LINE# #TAB# #TAB# 'flow'} #LINE# #TAB# family = table.split('.')[-2] #LINE# #TAB# try: #LINE# #TAB# #TAB# address_family = address_family_mapping[family] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# address_family = family #LINE# #TAB# return address_family"
"Return string representing given number in levenshtein coding . n : int , number to convert to levenshtein coding  <code> def levenshtein_coding(n: int) ->str: ","#LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return '0' #LINE# #TAB# c, code = 1, '' #LINE# #TAB# while True: #LINE# #TAB# #TAB# code = bin(n)[3:] + code #LINE# #TAB# #TAB# n = ceil(log2(n + 1)) - 1 #LINE# #TAB# #TAB# if n == 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# c += 1 #LINE# #TAB# return '1' * c + '0' + code"
Serialize datetime objects as string for JSON  <code> def datetime_serializer(obj: Any) ->str: ,#LINE# #TAB# if type(obj) is datetime.datetime: #LINE# #TAB# #TAB# return obj.strftime('%Y-%m-%dT%H:%M:%S') #LINE# #TAB# elif type(obj) is datetime.date: #LINE# #TAB# #TAB# return obj.strftime('%Y-%m-%d') #LINE# #TAB# elif type(obj) is datetime.time: #LINE# #TAB# #TAB# return obj.strftime('%H:%M:%S') #LINE# #TAB# raise NotImplementedError
"On the assumption that all the leaves of a nested dictionary ( tree ) structure are in some way equivalent , this is a quick method of returning the first such leaf without knowing the specific keys used to construct the nested dict  <code> def first_leaf(nested_dict): ",#LINE# #TAB# partial_dict = nested_dict #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# partial_dict = partial_dict[list(partial_dict.keys())[0]] #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# partial_dict = list(partial_dict) #LINE# #TAB# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# #TAB# return partial_dict #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# partial_dict = partial_dict[0]
Transform text describing a SpaceWorld into a 2D array of fields  <code> def parse_space_world(description): ,#LINE# #TAB# description = ''.join(description.split()) #LINE# #TAB# rows = description.split('||') #LINE# #TAB# tokenized_rows = [row.strip('|').split('|') for row in rows] #LINE# #TAB# fields = [[parse_field(field) for field in row] for row in tokenized_rows] #LINE# #TAB# return fields
get an singleton_instance of the client class singleton  <code> def get_client(): ,#LINE# #TAB# client = Client.singleton_instance #LINE# #TAB# if not client: #LINE# #TAB# #TAB# raise Exception('Please manually instantiate the client.') #LINE# #TAB# return client
"relevant IDL typecodes and corresponding Numpy Codes Most specify a byte size , but not all <code> def idl_typecode(i): ","#LINE# #TAB# typecode = {(0): 'V', (1): 'B', (2): 'i2', (3): 'i4', (4): 'f4', (5): #LINE# #TAB# #TAB# 'f8', (6): 'c4', (7): 'S', (8): 'O', (9): 'c8', (10): 'i8', (11): #LINE# #TAB# #TAB# 'O', (12): 'u2', (13): 'u4', (14): 'i8', (15): 'u8'} #LINE# #TAB# return typecode[i]"
"Task files ( logs , etc . ) are saved in TASK_DIR in following structure based on task_id : TASK_DIR / millions / tens_of_thousands / task_id/ * <code> def get_task_dir(cls, task_id, create=False): ","#LINE# #TAB# task_id = int(task_id) #LINE# #TAB# third = task_id #LINE# #TAB# second = task_id // 10000 * 10000 #LINE# #TAB# first = task_id // 1000000 * 1000000 #LINE# #TAB# task_dir = os.path.abspath(settings.TASK_DIR) #LINE# #TAB# path = os.path.join(task_dir, str(first), str(second), str(third)) #LINE# #TAB# path = os.path.abspath(path) #LINE# #TAB# if not path.startswith(task_dir): #LINE# #TAB# #TAB# raise Exception('Possible hack, trying to read path ""%s""' % path) #LINE# #TAB# if create and not os.path.isdir(path): #LINE# #TAB# #TAB# os.makedirs(path, mode=493) #LINE# #TAB# return path"
"Return an attribute for the LDAP entry specified by dn and ensure it only occurs once  <code> def get_one_dn_attribute(conn, dn, filtr, attr): ","#LINE# #TAB# l = get_dn_attribute(conn, dn, filtr, attr) #LINE# #TAB# if not len(l) == 1: #LINE# #TAB# #TAB# sys.exit('{} does not have exactly one {} attribute!'.format(dn, attr)) #LINE# #TAB# return l[0]"
"Get stream version from bookmark <code> def get_stream_version(tap_stream_id, state): ","#LINE# #TAB# stream_version = singer.get_bookmark(state, tap_stream_id, 'version') #LINE# #TAB# if stream_version is None: #LINE# #TAB# #TAB# stream_version = int(time.time() * 1000) #LINE# #TAB# return stream_version"
"Efficiently parses the root element of a * raw * XML document , returning a tuple of its qualified name and attribute dictionary  <code> def parse_root(raw): ","#LINE# #TAB# if sys.version < '3': #LINE# #TAB# #TAB# fp = StringIO(raw) #LINE# #TAB# else: #LINE# #TAB# #TAB# fp = BytesIO(raw.encode('UTF-8')) #LINE# #TAB# for event, element in etree.iterparse(fp, events=('start',)): #LINE# #TAB# #TAB# return element.tag, element.attrib"
Check whether an unparsed string is a numeric value <code> def is_numeric(val: str) ->bool: ,#LINE# #TAB# if val in MISSING_VALUES: #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# float(val) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"Remove the highlight from the given edges or all edges if none given  <code> def remove_highlight_edges(graph: BELGraph, edges=None): ","#LINE# #TAB# for u, v, k, _ in graph.edges(keys=True, data=True) if edges is None else edges: #LINE# #TAB# #TAB# if is_edge_highlighted(graph, u, v, k): #LINE# #TAB# #TAB# #TAB# del graph[u][v][k][EDGE_HIGHLIGHT]"
Truncate the file and return the filename  <code> def prepare_file(filename): ,"#LINE# #TAB# directory = os.path.join(utils.get_project_root(), ""analyzation/"") #LINE# #TAB# if not os.path.exists(directory): #LINE# #TAB# #TAB# os.makedirs(directory) #LINE# #TAB# workfilename = os.path.join(directory, filename) #LINE# #TAB# open(workfilename, 'w').close() #LINE# #TAB# return workfilename"
Assigns a color to the direction ( dynamic - defined colors ) Parameters -------------- dir Direction Returns -------------- col Color <code> def give_color_to_direction_dynamic(dir): ,"#LINE# #TAB# norm = mpl.colors.Normalize(vmin=-1, vmax=1) #LINE# #TAB# cmap = cm.plasma #LINE# #TAB# m = cm.ScalarMappable(norm=norm, cmap=cmap) #LINE# #TAB# rgba = m.to_rgba(dir) #LINE# #TAB# r = get_string_from_int_below_255(math.ceil(rgba[0] * 255.0)) #LINE# #TAB# g = get_string_from_int_below_255(math.ceil(rgba[1] * 255.0)) #LINE# #TAB# b = get_string_from_int_below_255(math.ceil(rgba[2] * 255.0)) #LINE# #TAB# return '#' + r + g + b"
"Try passwordless login with shell ssh command  <code> def try_passwordless_openssh(server, keyfile): ","#LINE# #TAB# if pexpect is None: #LINE# #TAB# #TAB# raise ImportError('pexpect unavailable, use paramiko') #LINE# #TAB# cmd = 'ssh -f ' + server #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# cmd += ' -i ' + keyfile #LINE# #TAB# cmd += ' exit' #LINE# #TAB# p = pexpect.spawn(cmd) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p.expect('[Pp]assword:', timeout=0.1) #LINE# #TAB# #TAB# except pexpect.TIMEOUT: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# except pexpect.EOF: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False"
Return standard deviation  <code> def standard_deviation(numbers): ,#LINE# #TAB# numbers = list(numbers) #LINE# #TAB# if not numbers: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# mean = sum(numbers) / len(numbers) #LINE# #TAB# return (sum((n - mean) ** 2 for n in numbers) / #LINE# #TAB# #TAB# #TAB# len(numbers)) ** .5
"Visual - related features for a single span  <code> def vizlib_unary_features(span: SpanMention) ->Iterator[Tuple[str, int]]: ","#LINE# #TAB# if not span.sentence.is_visual(): #LINE# #TAB# #TAB# return #LINE# #TAB# for f in get_visual_aligned_lemmas(span): #LINE# #TAB# #TAB# yield f'ALIGNED_{f}', DEF_VALUE #LINE# #TAB# for page in set(span.get_attrib_tokens('page')): #LINE# #TAB# #TAB# yield f'PAGE_[{page}]', DEF_VALUE"
"Cast a string to a strictly positive integer  <code> def positive_int(integer_string, strict=False, cutoff=None): ","#LINE# #TAB# if integer_string: #LINE# #TAB# #TAB# ret = int(integer_string) #LINE# #TAB# else: #LINE# #TAB# #TAB# return integer_string #LINE# #TAB# if ret < 0 or ret == 0 and strict: #LINE# #TAB# #TAB# raise ValueError() #LINE# #TAB# if cutoff: #LINE# #TAB# #TAB# return min(ret, cutoff) #LINE# #TAB# return ret"
The number of residues per turn at each Monomer in the Polymer  <code> def residues_per_turn(p): ,"#LINE# #TAB# cas = p.get_reference_coords() #LINE# #TAB# prim_cas = p.primitive.coordinates #LINE# #TAB# dhs = [abs(dihedral(cas[i], prim_cas[i], prim_cas[i + 1], cas[i + 1])) #LINE# #TAB# #TAB# for i in range(len(prim_cas) - 1)] #LINE# #TAB# rpts = [360.0 / dh for dh in dhs] #LINE# #TAB# rpts.append(None) #LINE# #TAB# return rpts"
"Add ( ` ` x ` ` , ` ` y ` ` ) to current mouse location  <code> def move_mouse(x, y): ","#LINE# #TAB# current_location = Mouse.Instance.Location #LINE# #TAB# point = Point(int(x) + current_location.X, int(y) + current_location.Y) #LINE# #TAB# Mouse.Instance.Location = point"
"Given a tuple of genes ( one from each strain ) identify and return the two which are from strain pair  <code> def get_genes_from_strain_t(geneT, strainT, genesO): ",#LINE# #TAB# genesInStrainTL = [] #LINE# #TAB# for geneNum in geneT: #LINE# #TAB# #TAB# if genesO.numToStrainName(geneNum) in strainT: #LINE# #TAB# #TAB# #TAB# genesInStrainTL.append(geneNum) #LINE# #TAB# return genesInStrainTL
Return known information about references held by the given object  <code> def annotated_references(obj): ,"#LINE# #TAB# references = KeyTransformDict(transform=id, default_factory=list) #LINE# #TAB# for type_ in type(obj).__mro__: #LINE# #TAB# #TAB# if type_ in type_based_references: #LINE# #TAB# #TAB# #TAB# type_based_references[type_](obj, references) #LINE# #TAB# add_attr(obj, ""__dict__"", references) #LINE# #TAB# add_attr(obj, ""__class__"", references) #LINE# #TAB# if isinstance(obj, type): #LINE# #TAB# #TAB# add_attr(obj, ""__mro__"", references) #LINE# #TAB# return references"
Merge entities into a single token  <code> def merge_entities(doc): ,"#LINE# #TAB# with doc.retokenize() as retokenizer: #LINE# #TAB# #TAB# for ent in doc.ents: #LINE# #TAB# #TAB# #TAB# attrs = {""tag"": ent.root.tag, ""dep"": ent.root.dep, ""ent_type"": ent.label} #LINE# #TAB# #TAB# #TAB# retokenizer.merge(ent, attrs=attrs) #LINE# #TAB# return doc"
Returns the Monday of the given week  <code> def get_week_start(day=None): ,#LINE# #TAB# day = add_timezone(day or datetime.date.today()) #LINE# #TAB# days_since_monday = day.weekday() #LINE# #TAB# if days_since_monday != 0: #LINE# #TAB# #TAB# day = day - relativedelta(days=days_since_monday) #LINE# #TAB# return day
Read URL list from config dictionary <code> def create_mapping(config): ,"#LINE# #TAB# LOG.debug(config) #LINE# #TAB# endpoint_list = [] #LINE# #TAB# for e in config: #LINE# #TAB# #TAB# snapshot_url = e['endpoint']['snapshot'] #LINE# #TAB# #TAB# update_url = '' #LINE# #TAB# #TAB# if 'update' in e['endpoint']: #LINE# #TAB# #TAB# #TAB# update_url = e['endpoint']['update'] #LINE# #TAB# #TAB# if update_url != '': #LINE# #TAB# #TAB# #TAB# endpoint_list.append((snapshot_url, update_url)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# endpoint_list.append(snapshot_url) #LINE# #TAB# LOG.info('Finished reading endpoints file') #LINE# #TAB# return endpoint_list"
"given an address , get decrypted hex private key from DB <code> def get_privkey(address): ","#LINE# #TAB# entry = registrar_addresses.find_one({'address': address}) #LINE# #TAB# if entry is None: #LINE# #TAB# #TAB# log.debug(""Address not found in DB, can't fetch privkey"") #LINE# #TAB# #TAB# return None #LINE# #TAB# encrypted_privkey = entry['encrypted_privkey'] #LINE# #TAB# hex_privkey = aes_decrypt(encrypted_privkey, SECRET_KEY) #LINE# #TAB# return hex_privkey"
Delete index infromation from params <code> def split_index(params): ,"#LINE# #TAB# if isinstance(params, list): #LINE# #TAB# #TAB# return [params[0], split_index(params[1])] #LINE# #TAB# elif isinstance(params, dict): #LINE# #TAB# #TAB# if INDEX in params.keys(): #LINE# #TAB# #TAB# #TAB# return split_index(params[VALUE]) #LINE# #TAB# #TAB# result = dict() #LINE# #TAB# #TAB# for key in params: #LINE# #TAB# #TAB# #TAB# result[key] = split_index(params[key]) #LINE# #TAB# #TAB# return result #LINE# #TAB# else: #LINE# #TAB# #TAB# return params"
"Return random unit vector from an equal distribution on a sphere . References : Cook 1957 , Marsaglia 1972 <code> def random_unit_vector(): ","#LINE# #TAB# while True: #LINE# #TAB# #TAB# x0 = 2 * random.random() - 1 #LINE# #TAB# #TAB# x1 = 2 * random.random() - 1 #LINE# #TAB# #TAB# x2 = 2 * random.random() - 1 #LINE# #TAB# #TAB# x3 = 2 * random.random() - 1 #LINE# #TAB# #TAB# r = x0 ** 2 + x1 ** 2 + x2 ** 2 + x3 ** 2 #LINE# #TAB# #TAB# if r < 1: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return numpy.array([(x1 * x3 + x0 * x2) * 2, (x2 * x3 - x0 * x1) * 2, #LINE# #TAB# #TAB# x0 ** 2 + x3 ** 2 - x1 ** 2 - x2 ** 2]) / r"
Returns a list of parameter names for the given pytest Function node . parameterization marks containing several names are split : param parentnode : : return : <code> def get_param_names(fnode): ,#LINE# #TAB# p_markers = get_parametrization_markers(fnode) #LINE# #TAB# param_names = [] #LINE# #TAB# for paramz_mark in p_markers: #LINE# #TAB# #TAB# param_names += get_param_argnames_as_list(paramz_mark.args[0]) #LINE# #TAB# return param_names
Reads letter aliases from a text file created by GoDepth1LettersWr  <code> def read_d1_letter(fin_txt): ,"#LINE# #TAB# go2letter = {} #LINE# #TAB# re_goid = re.compile(r""(GO:\d{7})"") #LINE# #TAB# with open(fin_txt) as ifstrm: #LINE# #TAB# #TAB# for line in ifstrm: #LINE# #TAB# #TAB# #TAB# mtch = re_goid.search(line) #LINE# #TAB# #TAB# #TAB# if mtch and line[:1] != ' ': #LINE# #TAB# #TAB# #TAB# #TAB# go2letter[mtch.group(1)] = line[:1] #LINE# #TAB# return go2letter"
"Determine the RealMe authentication strength of this session . : param request : a Django request : return : the AuthStrength of the RealMe session , or None if not authenticated with RealMe <code> def realme_auth_strength(request): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return AuthStrength[request.session.get('realme_strength')] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return None
"Creates an Input device instance based on an InputTemplate . : param dev_name : The name of the new device : param dev_type : The ID of the template to base the device off of : return : Input device instance <code> def create_from_template(dev_name: str, dev_type: int) ->Input: ","#LINE# #TAB# dev_template = get_template(dev_type) #LINE# #TAB# obj = Input(dev_name=dev_name, dev_type=dev_template.type, uses_clk= #LINE# #TAB# #TAB# dev_template.uses_clk, parameters=dev_template.parameters.copy()) #LINE# #TAB# for name, pin in dev_template.pins.items(): #LINE# #TAB# #TAB# obj.pins[name] = pin.copy(obj) #LINE# #TAB# obj.template = dev_template #LINE# #TAB# return obj"
"Some sources from apis return lowerCased where as describe calls always return TitleCase , this function turns the former to the later <code> def camel_resource(obj): ","#LINE# #TAB# if not isinstance(obj, dict): #LINE# #TAB# #TAB# return obj #LINE# #TAB# for k in list(obj.keys()): #LINE# #TAB# #TAB# v = obj.pop(k) #LINE# #TAB# #TAB# obj['%s%s' % (k[0].upper(), k[1:])] = v #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# camel_resource(v) #LINE# #TAB# #TAB# elif isinstance(v, list): #LINE# #TAB# #TAB# #TAB# list(map(camel_resource, v)) #LINE# #TAB# return obj"
Identifies what the type of file is  <code> def file_type(path): ,"#LINE# #TAB# signature = {'\x1f\x8b\x08': 'gz', 'BZh': 'bz2', 'PK\x03\x04': 'zip'} #LINE# #TAB# with open(path) as f: #LINE# #TAB# #TAB# for sig, f_type in signature.items(): #LINE# #TAB# #TAB# #TAB# if f.read(4).startswith(sig): #LINE# #TAB# #TAB# #TAB# #TAB# return f_type"
Apply the last saved configuration to a device  <code> def attach_to(device): ,#LINE# #TAB# if not _configuration: #LINE# #TAB# #TAB# _load() #LINE# #TAB# persister = _device_entry(device) #LINE# #TAB# for s in device.settings: #LINE# #TAB# #TAB# if s.persister is None: #LINE# #TAB# #TAB# #TAB# s.persister = persister #LINE# #TAB# #TAB# assert s.persister == persister
"Converts current year , month , weekday and week number into the day of month <code> def weekday_and_week_to_day(year: int, month: int, week: int, weekday: int): ","#LINE# #TAB# dt_first = datetime(year, month, 1) #LINE# #TAB# dt_first_weekday = dt_first.weekday() #LINE# #TAB# result = week * 7 + weekday - dt_first_weekday + 1 #LINE# #TAB# if result < 1 or result > num_days_in_month(year, month): #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return result"
"Return whether the URL blacklisted or not . Using BLACKLIST_URLS methods against the URLs . : param url : url string : return : True if URL is blacklisted , else False <code> def is_blacklisted_url(url): ","#LINE# #TAB# url = urllib.parse.urlparse(url).netloc #LINE# #TAB# for method in WHITELIST_URL: #LINE# #TAB# #TAB# for whitelist_url in WHITELIST_URL[method]: #LINE# #TAB# #TAB# #TAB# if method(url, whitelist_url): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# for method in BLACKLIST_URLS: #LINE# #TAB# #TAB# for blacklist_url in BLACKLIST_URLS[method]: #LINE# #TAB# #TAB# #TAB# if method(url, blacklist_url): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Checks if given value is boolean <code> def is_valid_boolean(val): ,"#LINE# #TAB# values = [True, False] #LINE# #TAB# return val in values"
Returns a single row from a set of results or raises an error <code> def single_row(results): ,"#LINE# #TAB# if not results: #LINE# #TAB# #TAB# raise chef.exceptions.ChefServerNotFoundError('No matching rows found') #LINE# #TAB# elif len(results) > 1: #LINE# #TAB# #TAB# raise chef.exceptions.ChefServerNotFoundError( #LINE# #TAB# #TAB# #TAB# 'Multiple rows returned, expected one') #LINE# #TAB# return results[0]"
"Given a dataset object and data in the appropriate format for the interface , return a simple scalar  <code> def unpack_scalar(cls, dataset, data): ","#LINE# #TAB# if data.shape == (1, 1): #LINE# #TAB# #TAB# return data[0, 0] #LINE# #TAB# return data"
"Test if x is a correctly shaped point array in R^d  <code> def is_valid_input_array(x, ndim=None): ",#LINE# #TAB# x = np.asarray(x) #LINE# #TAB# if ndim is None or ndim == 1: #LINE# #TAB# #TAB# return x.ndim == 1 and x.size > 1 or x.ndim == 2 and x.shape[0] == 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return x.ndim == 2 and x.shape[0] == ndim
Create the specified path making all intermediate - level directories needed to contain the leaf directory . Ignore any error that would occur if the leaf directory already exists  <code> def make_directory_if_not_exists(path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError, error: #LINE# #TAB# #TAB# if error.errno <> errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise error"
"Takes a list of sentences in french and preprocesses them  <code> def preprocess_french(trans, fr_nlp, remove_brackets_content=True): ","#LINE# #TAB# if remove_brackets_content: #LINE# #TAB# #TAB# trans = pangloss.remove_content_in_brackets(trans, '[]') #LINE# #TAB# trans = fr_nlp(' '.join(trans.split()[:])) #LINE# #TAB# trans = ' '.join([token.lower_ for token in trans if not token.is_punct]) #LINE# #TAB# return trans"
Check if file name have valid suffix for formatting . if have suffix return it else return False  <code> def get_suffix(name): ,"#LINE# #TAB# a = name.count(""."") #LINE# #TAB# if a: #LINE# #TAB# #TAB# ext = name.split(""."")[-1] #LINE# #TAB# #TAB# if ext in LANGS.keys(): #LINE# #TAB# #TAB# #TAB# return ext #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"Build the main content of the entry , which is written in markdown at the moment <code> def compile_markdown(entry): ","#LINE# #TAB# md_out = markdown.markdown(entry.raw_content, extensions=[ #LINE# #TAB# #TAB# 'markdown.extensions.extra', 'pymdownx.tilde', 'pymdownx.magiclink', #LINE# #TAB# #TAB# 'pymdownx.arithmatex'], output_format='html5') #LINE# #TAB# entry.compile_output = md_out"
Format the name of the field Args : value : The string to format <code> def format_name(value: str): ,#LINE# #TAB# if get_setting('camelize'): #LINE# #TAB# #TAB# components = value.split('_') #LINE# #TAB# #TAB# return components[0] + ''.join(x.title() for x in components[1:]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return value
Find the current state of the lpar . State is converted to the appropriate nova.compute.power_state : return : The appropriate integer state value from power_state <code> def translate_vm_state(pvm_state): ,#LINE# #TAB# if pvm_state is None: #LINE# #TAB# #TAB# return power_state.NOSTATE #LINE# #TAB# try: #LINE# #TAB# #TAB# nova_state = POWERVM_TO_NOVA_STATE[pvm_state.lower()] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# nova_state = power_state.NOSTATE #LINE# #TAB# return nova_state
"definition : func(x1 , x2 , y1 , y2 ) { ... } objective : returns the slope ( m ) from the slope formula m = y2 - y1 / x2 - x1 <code> def slope_formula(x1, x2, y1, y2): ",#LINE# #TAB# slope = (y2 - y1) / (x2 - x1) #LINE# #TAB# return slope
Return True if data container has multiple timesteps  <code> def has_multiple_timesteps(data): ,"#LINE# #TAB# if ""timestep"" in data.keys(): #LINE# #TAB# #TAB# if len(np.unique(data[""timestep""])) > 1: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Get the number of ISO - weeks in this year <code> def iso_num_weeks(iso_year): ,"#LINE# #TAB# ""Get the number of ISO-weeks in this year"" #LINE# #TAB# year_start = _iso_year_start(iso_year) #LINE# #TAB# next_year_start = _iso_year_start(iso_year+1) #LINE# #TAB# year_num_weeks = ((next_year_start - year_start).days) // 7 #LINE# #TAB# return year_num_weeks"
\?[a - zA - Z_][a - zA - Z_0 - 9 ] * : <code> def t_opt_keyword(t): ,#LINE# #TAB# t.value = Token(t) #LINE# #TAB# return t
"Check MIC ICV & return the data from a decrypted TKIP packet <code> def check_mic_icv(data, mic_key, source, dest): ","#LINE# #TAB# assert len(data) > 12 #LINE# #TAB# ICV = data[-4:] #LINE# #TAB# MIC = data[-12:-4] #LINE# #TAB# data_clear = data[:-12] #LINE# #TAB# expected_ICV = pack(""<I"", crc32(data_clear + MIC) & 0xFFFFFFFF) #LINE# #TAB# if expected_ICV != ICV: #LINE# #TAB# #TAB# raise ICVError() #LINE# #TAB# sa = mac2str(source) #LINE# #TAB# da = mac2str(dest) #LINE# #TAB# expected_MIC = michael(mic_key, da + sa + b""\x00"" * 4 + data_clear) #LINE# #TAB# if expected_MIC != MIC: #LINE# #TAB# #TAB# raise MICError() #LINE# #TAB# return data_clear"
Compute the spectral radius for the coefficient matrix <code> def var_specrad(a: np.ndarray) ->float: ,"#LINE# #TAB# n_vars, _, lags = a.shape #LINE# #TAB# pn1 = (lags - 1) * n_vars #LINE# #TAB# a_ = np.vstack((a.reshape(n_vars, -1), np.hstack((np.eye(pn1), np.zeros #LINE# #TAB# #TAB# ((pn1, n_vars)))))) #LINE# #TAB# w, _ = np.linalg.eig(a_) #LINE# #TAB# rho = np.max(np.abs(w)) #LINE# #TAB# return rho"
"Given a ssa in the format produced by get_ssa ( ) , returns a map from versioned blob into the operator index that produces that version of the blob . A versioned blob is a tuple ( blob_name , version )  <code> def get_output_producers(ssa): ","#LINE# #TAB# producers = {} #LINE# #TAB# for i, (_inputs, outputs) in enumerate(ssa): #LINE# #TAB# #TAB# for o in outputs: #LINE# #TAB# #TAB# #TAB# producers[o] = i #LINE# #TAB# return producers"
"Returns the ( x , y ) tuple of the point that has progressed a proportion n along the line defined by the two x , y coordinates  <code> def get_point_on_line(x1, y1, x2, y2, n): ","#LINE# #TAB# x = (x2 - x1) * n + x1 #LINE# #TAB# y = (y2 - y1) * n + y1 #LINE# #TAB# return x, y"
"duck type implementation for checking if object is ClusterNode or Hypothesis_Node , more or less <code> def is_tree(pObj): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# get_data(pObj) #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False
If the second return value is not None then it is an Exception encountered during parsing . The first return value will be the XML string  <code> def xml_records(filename): ,"#LINE# #TAB# with Evtx(filename) as evtx: #LINE# #TAB# #TAB# for xml, record in evtx_file_xml_view(evtx.get_file_header()): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield to_lxml(xml), None #LINE# #TAB# #TAB# #TAB# except etree.XMLSyntaxError as e: #LINE# #TAB# #TAB# #TAB# #TAB# yield xml, e"
Monkey - patch urllib3 with PyOpenSSL - backed SSL - support  <code> def inject_into_urllib3(): ,#LINE# #TAB# connection.ssl_wrap_socket = ssl_wrap_socket #LINE# #TAB# util.HAS_SNI = HAS_SNI #LINE# #TAB# util.IS_PYOPENSSL = True
Return True if ` value ` is a valid IMDB ID for a person  <code> def is_valid_imdb_person_id(value): ,"#LINE# #TAB# if not isinstance(value, str): #LINE# #TAB# #TAB# raise TypeError('is_valid_imdb_person_id expects a string but got {0}' #LINE# #TAB# #TAB# #TAB# .format(type(value))) #LINE# #TAB# return re.match('nm\\d{7,8}', value) is not None"
"A generator that stops on a KeyboardInterrupt <code> def catch_keyboard_interrupt(gen, logger=None): ",#LINE# #TAB# running = True #LINE# #TAB# gen = iter(gen) #LINE# #TAB# while running: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(gen) #LINE# #TAB# #TAB# except KeyboardInterrupt: #LINE# #TAB# #TAB# #TAB# if logger is not None: #LINE# #TAB# #TAB# #TAB# #TAB# logger.warn('KeyboardInterrupt') #LINE# #TAB# #TAB# #TAB# running = False #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# running = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pass
Converts value in int or float if possible <code> def guess_type(value: str): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# return float(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return value
"Case when node have two children , one of which is operator , other not . In that case choice operator node as top node . Example : T->[a , * ->[a ] ] to * ->[a , a ] <code> def case_1(node): ","#LINE# #TAB# op_node = node.find_op_child() #LINE# #TAB# unop_node = node.find_unop_child() #LINE# #TAB# op_node.add_child(unop_node) #LINE# #TAB# parent = node.parent #LINE# #TAB# if parent is not None: #LINE# #TAB# #TAB# parent.replace_child(node, op_node) #LINE# #TAB# #TAB# return parent #LINE# #TAB# else: #LINE# #TAB# #TAB# op_node.parent = parent #LINE# #TAB# #TAB# return op_node"
returns the first ' size ' dataframe : param size : ( optional ) the size of the sample . If None then all the names are returned : return : the mapping DataFrame <code> def companies_inc5000(size: int=None) ->pd.DataFrame: ,"#LINE# #TAB# _path = Path(AbstractSample._full_path('map_companies_inc5000.csv')) #LINE# #TAB# df = pd.read_csv(_path, encoding='latin1') #LINE# #TAB# return df.iloc[:size]"
Get the source data for a particular GW catalog <code> def get_source(source): ,"#LINE# #TAB# if source == 'gwtc-1': #LINE# #TAB# #TAB# fname = download_file(gwtc1_url, cache=True) #LINE# #TAB# #TAB# data = json.load(open(fname, 'r')) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('Unkown catalog source {}'.format(source)) #LINE# #TAB# return data['data']"
"Remove a place from a Petri net Parameters ------------- net Petri net place Place to remove Returns ------------- net Petri net <code> def remove_place(net, place): ",#LINE# #TAB# if place in net.places: #LINE# #TAB# #TAB# in_arcs = place.in_arcs #LINE# #TAB# #TAB# for arc in in_arcs: #LINE# #TAB# #TAB# #TAB# trans = arc.source #LINE# #TAB# #TAB# #TAB# trans.out_arcs.remove(arc) #LINE# #TAB# #TAB# #TAB# net.arcs.remove(arc) #LINE# #TAB# #TAB# out_arcs = place.out_arcs #LINE# #TAB# #TAB# for arc in out_arcs: #LINE# #TAB# #TAB# #TAB# trans = arc.target #LINE# #TAB# #TAB# #TAB# trans.in_arcs.remove(arc) #LINE# #TAB# #TAB# #TAB# net.arcs.remove(arc) #LINE# #TAB# #TAB# net.places.remove(place) #LINE# #TAB# return net
"Makes a normalized Suite from a Cdf object  <code> def make_suite_from_cdf(cdf, name=None): ","#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# name = cdf.name #LINE# #TAB# suite = Suite(name=name) #LINE# #TAB# prev = 0.0 #LINE# #TAB# for val, prob in cdf.Items(): #LINE# #TAB# #TAB# suite.Incr(val, prob - prev) #LINE# #TAB# #TAB# prev = prob #LINE# #TAB# return suite"
return string of parameters comments and exposure time found in header in marccd image file .mccd - print allsentences displays the header - use allsentences.split('\n ' ) to get a list <code> def read_header_marccd2(filename): ,"#LINE# #TAB# f = open(filename, 'rb') #LINE# #TAB# f.seek(3072) #LINE# #TAB# tt = f.read(512) #LINE# #TAB# dataset_comments = tt.strip('\x00') #LINE# #TAB# f.seek(1024 + 2 * 256 + 128 + 12) #LINE# #TAB# s = struct.Struct('I I I') #LINE# #TAB# unpacked_data = s.unpack(f.read(3 * 4)) #LINE# #TAB# _, expo_time, _ = unpacked_data #LINE# #TAB# f.close() #LINE# #TAB# return dataset_comments, expo_time"
Read and flatten json <code> def read_json(json_fn): ,"#LINE# #TAB# with open(json_fn) as file: #LINE# #TAB# #TAB# struct_json = json.load(file) #LINE# #TAB# bfr_json_flat = flatten_json(struct_json) #LINE# #TAB# keys = bfr_json_flat.keys() #LINE# #TAB# key_keys = [] #LINE# #TAB# for key in keys: #LINE# #TAB# #TAB# if 'key' in key: #LINE# #TAB# #TAB# #TAB# key_keys.append(key[:-4]) #LINE# #TAB# return bfr_json_flat, key_keys"
"Get a list of node colors by binning some continuous - variable attribute into quantiles  <code> def get_node_colors_by_attr(G, attr, num_bins=None, cmap='viridis', start=0, stop=1, na_color='none'): ","#LINE# #TAB# if num_bins is None: #LINE# #TAB# #TAB# num_bins=len(G.nodes()) #LINE# #TAB# bin_labels = range(num_bins) #LINE# #TAB# attr_values = pd.Series([data[attr] for node, data in G.nodes(data=True)]) #LINE# #TAB# cats = pd.qcut(x=attr_values, q=num_bins, labels=bin_labels) #LINE# #TAB# colors = get_colors(num_bins, cmap, start, stop) #LINE# #TAB# node_colors = [colors[int(cat)] if pd.notnull(cat) else na_color for cat in cats] #LINE# #TAB# return node_colors"
Get index prefix for a given schema  <code> def schema_prefix(schema): ,"#LINE# #TAB# if not schema: #LINE# #TAB# #TAB# return None #LINE# #TAB# index, doctype = schema_to_index(schema, index_names=current_search. #LINE# #TAB# #TAB# mappings.keys()) #LINE# #TAB# return index.split('-')[0]"
"Return Protection Group Volume or None <code> def get_pgroupvolume(module, array): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# pgroup = array.get_pgroup(module.params['name']) #LINE# #TAB# #TAB# for volume in pgroup['volumes']: #LINE# #TAB# #TAB# #TAB# if volume == module.params['restore']: #LINE# #TAB# #TAB# #TAB# #TAB# return volume #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return None
Returns a functions to search sender \ s name or it \ s part  <code> def contains_sender_names(sender): ,"#LINE# #TAB# names = '( |$)|'.join(flatten_list([[e, e.capitalize()] #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# for e in extract_names(sender)])) #LINE# #TAB# names = names or sender #LINE# #TAB# if names != '': #LINE# #TAB# #TAB# return binary_regex_search(re.compile(names)) #LINE# #TAB# return lambda s: 0"
"The file_access_waiting state global faw_CACHE_DESTROY event handler <code> def faw_cache_destroy(cache, e): ","#LINE# #TAB# payload = CacheWritePayload(json=json.dumps({'time_out_in_minutes': 0}), #LINE# #TAB# #TAB# file_name=cache.file_name) #LINE# #TAB# cache.post_fifo(Event(signal=signals.cache_file_write), payload=payload) #LINE# #TAB# return return_status.HANDLED"
""" c - a : indium chloride [ PM73 ]  <code> def bc_in_cl_pm73(T, P): ","#LINE# #TAB# b0 = -1.68 * 2 / 3 #LINE# #TAB# b1 = -3.85 * 2 / 3 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = 0.0 * 2 / 3 ** (3 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['In'] * i2c['Cl']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"Find a single HTML element using jQuery - style selectors  <code> def find_element_by_jquery(browser, selector): ","#LINE# #TAB# elements = find_elements_by_jquery(browser, selector) #LINE# #TAB# if not elements: #LINE# #TAB# #TAB# raise AssertionError('No matching element found.') #LINE# #TAB# if len(elements) > 1: #LINE# #TAB# #TAB# raise AssertionError('Multiple matching elements found.') #LINE# #TAB# return elements[0]"
"Group an array by its unique values  <code> def unique_value_groups(ar, sort=True): ","#LINE# #TAB# inverse, values = pd.factorize(ar, sort=sort) #LINE# #TAB# groups = [[] for _ in range(len(values))] #LINE# #TAB# for n, g in enumerate(inverse): #LINE# #TAB# #TAB# if g >= 0: #LINE# #TAB# #TAB# #TAB# groups[g].append(n) #LINE# #TAB# return values, groups"
"Force the noupdate value of an xmlid  <code> def force_noupdate(cr, xmlid, noupdate=True, warn=False): ","#LINE# #TAB# if '.' not in xmlid: #LINE# #TAB# #TAB# raise ValueError('Please use fully qualified name <module>.<name>') #LINE# #TAB# module, name = xmlid.split('.') #LINE# #TAB# _query = sql.SQL( #LINE# #TAB# #TAB# ) #LINE# #TAB# cr.execute(_query, locals()) #LINE# #TAB# if noupdate is False and cr.rowcount and warn: #LINE# #TAB# #TAB# _logger.warning('Customizations on `%s` might be lost!', xmlid) #LINE# #TAB# return cr.rowcount"
"Convert a string into a list whose elements length do not exceed a given width  <code> def str_format(string, width): ","#LINE# #TAB# output = [] #LINE# #TAB# for seq in string.split('\n'): #LINE# #TAB# #TAB# tmp = textwrap.wrap(seq, width) #LINE# #TAB# #TAB# for split in tmp: #LINE# #TAB# #TAB# #TAB# output.append(split) #LINE# #TAB# return output"
"Compute specified symmetric confidence interval for empirical sample  <code> def empirical_confidence_interval(sample, interval=0.95): ","#LINE# #TAB# sample = np.sort(sample) #LINE# #TAB# N = len(sample) #LINE# #TAB# low_index = int(np.round((N-1) * (0.5 - interval/2))) + 1 #LINE# #TAB# high_index = int(np.round((N-1) * (0.5 + interval/2))) + 1 #LINE# #TAB# low = sample[low_index] #LINE# #TAB# high = sample[high_index] #LINE# #TAB# return [low, high]"
Checker for the nearby preprocessor  <code> def check_nearby_preprocessor(impact_function): ,#LINE# #TAB# hazard_key = layer_purpose_hazard['key'] #LINE# #TAB# earthquake_key = hazard_earthquake['key'] #LINE# #TAB# exposure_key = layer_purpose_exposure['key'] #LINE# #TAB# place_key = exposure_place['key'] #LINE# #TAB# if impact_function.hazard.keywords.get(hazard_key) == earthquake_key: #LINE# #TAB# #TAB# if impact_function.exposure.keywords.get(exposure_key) == place_key: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
Checks if a file is locked by another process  <code> def is_locked(filepath): ,"#LINE# #TAB# import msvcrt #LINE# #TAB# try: #LINE# #TAB# #TAB# fd = os.open(filepath, os.O_APPEND | os.O_EXCL | os.O_RDWR) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return True #LINE# #TAB# try: #LINE# #TAB# #TAB# filesize = os.path.getsize(filepath) #LINE# #TAB# #TAB# msvcrt.locking(fd, msvcrt.LK_NBLCK, filesize) #LINE# #TAB# #TAB# msvcrt.locking(fd, msvcrt.LK_UNLCK, filesize) #LINE# #TAB# #TAB# os.close(fd) #LINE# #TAB# #TAB# return False #LINE# #TAB# except (OSError, IOError): #LINE# #TAB# #TAB# os.close(fd) #LINE# #TAB# #TAB# return True"
The maximum distance between parallel tangents to the projection area of the contour : param contour : : return : <code> def max_feret(contour): ,"#LINE# #TAB# feret, _ = Contours._max_min_feret(contour) #LINE# #TAB# return feret"
Apply format_value_for_spreadsheet to every value of a dict <code> def format_data_dicts_records_for_spreadsheet(data_dicts): ,"#LINE# #TAB# return [{key: format_value_for_spreadsheet(value) for key, value in #LINE# #TAB# #TAB# data_dict.items()} for data_dict in data_dicts]"
Get the data for this key from the rundata <code> def get_from_currentdata(key): ,#LINE# #TAB# data = None #LINE# #TAB# currdata = get_currentdata() #LINE# #TAB# if key and key in currdata: #LINE# #TAB# #TAB# data = currdata[key] #LINE# #TAB# return data
Base method to send GET request and retrieve text from response . Parameters ---------- url : str Returns ------- string text Raises ------ Exception Client or Server Error can be raised  <code> def make_get_text_request(url): ,#LINE# #TAB# response = requests.get(url) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# return response.text
"Run sst algorithm with svd  <code> def sst_svd(X_test, X_history, n_components): ","#LINE# #TAB# U_test, _, _ = np.linalg.svd(X_test, full_matrices=False) #LINE# #TAB# U_history, _, _ = np.linalg.svd(X_history, full_matrices=False) #LINE# #TAB# _, s, _ = np.linalg.svd(U_test[:, :n_components].T @ U_history[:, : #LINE# #TAB# #TAB# n_components], full_matrices=False) #LINE# #TAB# return 1 - s[0]"
"Create the order for samples between X and genotypes  <code> def generate_sample_order(x_samples, geno_samples): ","#LINE# #TAB# geno_order = np.array([i for i, s in enumerate(geno_samples) if s in #LINE# #TAB# #TAB# x_samples], dtype=int) #LINE# #TAB# x_order = geno_samples.values[geno_order] #LINE# #TAB# if x_samples.duplicated().any(): #LINE# #TAB# #TAB# logger.debug('Duplicated samples found') #LINE# #TAB# #TAB# counts = x_samples.value_counts() #LINE# #TAB# #TAB# geno_order = np.array(list(chain(*[([geno_samples.get_loc(s)] * #LINE# #TAB# #TAB# #TAB# counts[s]) for s in x_order])), dtype=int) #LINE# #TAB# return geno_order, x_order"
Remove a directory . Suppress error if the directory does not exist  <code> def remove_dir(dirpath): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# rmdir(dirpath) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# pass
"init a socks proxy socket  <code> def init_socks(proxy, timeout): ","#LINE# #TAB# map_to_type = {'v4': socks.SOCKS4, 'v5': socks.SOCKS5, 'http': socks.HTTP} #LINE# #TAB# ssock = socks.socksocket() #LINE# #TAB# if isinstance(timeout, (int, float)): #LINE# #TAB# #TAB# ssock.settimeout(timeout) #LINE# #TAB# ssock.set_proxy(proxy_type=map_to_type[proxy.get('socks_type', 'v5')], #LINE# #TAB# #TAB# rdns=proxy.get('rdns', True), addr=proxy.get('host'), port=proxy. #LINE# #TAB# #TAB# get('port'), username=proxy.get('username'), password=proxy.get( #LINE# #TAB# #TAB# 'password')) #LINE# #TAB# return ssock"
"Detect if the field is a boolean or a dict  <code> def boolean_or_dict_field(object_dict, parent_object_dict): ","#LINE# #TAB# if isinstance(object_dict, dict): #LINE# #TAB# #TAB# return fields.Dict #LINE# #TAB# return fields.Bool"
"Enable integration with a given GUI <code> def enable_gui(gui, kernel=None): ","#LINE# #TAB# if gui not in loop_map: #LINE# #TAB# #TAB# raise ValueError(""GUI %r not supported"" % gui) #LINE# #TAB# if kernel is None: #LINE# #TAB# #TAB# if Application.initialized(): #LINE# #TAB# #TAB# #TAB# kernel = getattr(Application.instance(), 'kernel', None) #LINE# #TAB# #TAB# if kernel is None: #LINE# #TAB# #TAB# #TAB# raise RuntimeError(""You didn't specify a kernel,"" #LINE# #TAB# #TAB# #TAB# #TAB# "" and no IPython Application with a kernel appears to be running."" #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# loop = loop_map[gui] #LINE# #TAB# if kernel.eventloop is not None and kernel.eventloop is not loop: #LINE# #TAB# #TAB# raise RuntimeError(""Cannot activate multiple GUI eventloops"") #LINE# #TAB# kernel.eventloop = loop"
"Add one column to the left vector ( MPS version ) <code> def eig_leftvec_add_mps(lv, lt1, lt2): ","#LINE# #TAB# lv = np.tensordot(lv, lt1.conj(), axes=(0, 0)) #LINE# #TAB# lv = np.tensordot(lv, lt2, axes=((0, 1), (0, 1))) #LINE# #TAB# return lv"
"inner covariance matrix for HAC for panel data no denominator nobs used no reference for this , just accounting for time indices <code> def s_nw_panel(xw, weights, groupidx): ","#LINE# #TAB# nlags = len(weights) - 1 #LINE# #TAB# S = weights[0] * np.dot(xw.T, xw) #LINE# #TAB# for lag in range(1, nlags + 1): #LINE# #TAB# #TAB# xw0, xwlag = lagged_groups(xw, lag, groupidx) #LINE# #TAB# #TAB# s = np.dot(xw0.T, xwlag) #LINE# #TAB# #TAB# S += weights[lag] * (s + s.T) #LINE# #TAB# return S"
"Construct a PauliTerm operator by taking the non - identity single - qubit operator at each qubit position  <code> def max_weight_operator(ops: Iterable[PauliTerm]) -> Union[None, PauliTerm]: ","#LINE# #TAB# mapping = dict() #LINE# #TAB# for op in ops: #LINE# #TAB# #TAB# for idx, op_str in op: #LINE# #TAB# #TAB# #TAB# if idx in mapping: #LINE# #TAB# #TAB# #TAB# #TAB# if mapping[idx] != op_str: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# mapping[idx] = op_str #LINE# #TAB# op = functools.reduce(mul, (PauliTerm(op, q) for q, op in mapping.items()), sI()) #LINE# #TAB# return op"
Returns an internal sequence dictionary update  <code> def dict_to_sequence(d): ,"#LINE# #TAB# if hasattr(d, 'items'): #LINE# #TAB# #TAB# d = d.items() #LINE# #TAB# return d"
"Get the latest block hash for a given coin <code> def get_latest_block_hash(coin_symbol='btc', api_key=None): ","#LINE# #TAB# return get_blockchain_overview(coin_symbol=coin_symbol, api_key=api_key)[ #LINE# #TAB# #TAB# 'hash']"
Calculate hash of image read in from stream  <code> def get_hash_from_image(image_file): ,"#LINE# #TAB# image_hash = hashlib.sha256() #LINE# #TAB# with open(image_file, 'rb') as f: #LINE# #TAB# #TAB# for byte_block in iter(lambda : f.read(4096), b''): #LINE# #TAB# #TAB# #TAB# image_hash.update(byte_block) #LINE# #TAB# return image_hash"
input : String | Generator outs : Generator reads firstline as the headers and converts input into a stream of dicts <code> def to_csv(file_path_or_generator): ,"#LINE# #TAB# if isinstance(file_path_or_generator, types.GeneratorType): #LINE# #TAB# #TAB# f = io.StringIO(''.join(list(file_path_or_generator))) #LINE# #TAB# elif isinstance(file_path_or_generator, str): #LINE# #TAB# #TAB# f = open(file_path_or_generator, mode='r', encoding='utf-8', errors #LINE# #TAB# #TAB# #TAB# ='replace') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise ValueError('to_csv accepts Strings or Generators') #LINE# #TAB# with f: #LINE# #TAB# #TAB# headers = clean_headers(f.readline()) #LINE# #TAB# #TAB# for row in csv.DictReader(f, fieldnames=headers): #LINE# #TAB# #TAB# #TAB# yield row"
Return a list of email text bodies from a list of email objects <code> def email_bodies(emails): ,#LINE# #TAB# body_texts = [] #LINE# #TAB# for eml in emails: #LINE# #TAB# #TAB# body_texts.extend(list(eml.walk())[1:]) #LINE# #TAB# return body_texts
Convert E(eV ) < > nu(nm )  <code> def conv_ev_nm(data): ,#LINE# #TAB# h = scipy.constants.h #LINE# #TAB# c = scipy.constants.c #LINE# #TAB# evJ = scipy.constants.physical_constants['electron volt-joule relationship' #LINE# #TAB# #TAB# ][0] #LINE# #TAB# waveConv = 1e-09 #LINE# #TAB# dataOut = h * c / (data * evJ) / waveConv #LINE# #TAB# return dataOut
"Filter contaminants , as marked by the search engine Looking for a contaminant tag in the leading_protein column <code> def filter_contaminant(df, config, _filter): ","#LINE# #TAB# CON_TAG = _filter['tag'] #LINE# #TAB# filter_con = df['proteins'].str.contains(CON_TAG) #LINE# #TAB# filter_con[pd.isnull(filter_con)] = False #LINE# #TAB# logger.info('Filtering out {} PSMs as contaminants with tag ""{}""'. #LINE# #TAB# #TAB# format(np.sum(filter_con), CON_TAG)) #LINE# #TAB# return filter_con"
"Manage file handles to keep files open during demultiplexing  <code> def get_files_handles(output_dir, barcodes, overwrite, prefix): ","#LINE# #TAB# return {cell: {'r1': create_output_handle(output_dir, cell, overwrite, #LINE# #TAB# #TAB# prefix, 1), 'r2': create_output_handle(output_dir, cell, overwrite, #LINE# #TAB# #TAB# prefix, 2)} for cell in barcodes.keys()}"
"write code into database , szsec and shsec <code> def write_code(): ","#LINE# #TAB# sz_stock, sh_stock = get_code() #LINE# #TAB# if sz_stock is not None and sh_stock is not None: #LINE# #TAB# #TAB# sz_stock.to_sql('code', sz_engine, if_exists='replace', index=False) #LINE# #TAB# #TAB# sh_stock.to_sql('code', sh_engine, if_exists='replace', index=False) #LINE# #TAB# #TAB# return sz_stock['code'], sh_stock['code'] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
Simple funcion to eliminate consecutive duplicates in a list or arrays or in a list of numbers . : param input_list : list with possible consecutive duplicates . : return : input_list with no consecutive duplicates  <code> def eliminate_consecutive_duplicates(input_list): ,"#LINE# #TAB# if isinstance(input_list[0], np.ndarray): #LINE# #TAB# #TAB# output_list = [input_list[0]] #LINE# #TAB# #TAB# for k in input_list[1:]: #LINE# #TAB# #TAB# #TAB# if not list(k) == list(output_list[-1]): #LINE# #TAB# #TAB# #TAB# #TAB# output_list.append(k) #LINE# #TAB# #TAB# return output_list #LINE# #TAB# else: #LINE# #TAB# #TAB# output_list = [input_list[0]] #LINE# #TAB# #TAB# for i in range(1, len(input_list)): #LINE# #TAB# #TAB# #TAB# if not input_list[i] == input_list[i - 1]: #LINE# #TAB# #TAB# #TAB# #TAB# output_list.append(input_list[i]) #LINE# #TAB# #TAB# return output_list"
"Plots outputs of compute_cap_exposures as line graphs <code> def plot_cap_exposures_net(net_exposures, ax=None): ","#LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# ax = plt.gca() #LINE# #TAB# color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5)) #LINE# #TAB# cap_names = CAP_BUCKETS.keys() #LINE# #TAB# for i in range(len(net_exposures)): #LINE# #TAB# #TAB# ax.plot(net_exposures[i], color=color_list[i], alpha=0.8, #LINE# #TAB# #TAB# #TAB# #TAB# label=cap_names[i]) #LINE# #TAB# ax.axhline(0, color='k', linestyle='-') #LINE# #TAB# ax.set(title='Net exposure to market caps', #LINE# #TAB# #TAB# ylabel='Proportion of net exposure \n in market cap buckets') #LINE# #TAB# return ax"
Allocate a buffer on the host using pinned memory with specified number of bytes  <code> def alloc_pinned(num_bytes): ,"#LINE# #TAB# ptr = c_void_ptr_t(0) #LINE# #TAB# c_num_bytes = c_dim_t(num_bytes) #LINE# #TAB# safe_call(backend.get().af_alloc_pinned(c_pointer(ptr), c_num_bytes)) #LINE# #TAB# return ptr.value"
Parses all the current RFC titles from the rfc-index.txt file allowing the title to be written to the database easily . : return list of RFC title information <code> def get_title_list(): ,"#LINE# #TAB# list_of_titles = [] #LINE# #TAB# with open(os.path.join(Config.STORAGE_PATH, 'rfc-index.txt'), 'r') as f: #LINE# #TAB# #TAB# f = f.read().strip() #LINE# #TAB# #TAB# search_regex = '^([\\d{1,4}])([^.]*).' #LINE# #TAB# #TAB# result = re.finditer(search_regex, f, re.M) #LINE# #TAB# #TAB# for title in result: #LINE# #TAB# #TAB# #TAB# list_of_titles.append(title[0]) #LINE# #TAB# return list_of_titles"
Convert the given Enaml Font into a QFont . Parameters ---------- font : Font The Enaml Font object . Returns ------- result : QFont The QFont instance for the given Enaml font  <code> def qfont_from_font(font): ,"#LINE# #TAB# qfont = QFont(font.family, font.pointsize, font.weight) #LINE# #TAB# qfont.setStyle(FONT_STYLES[font.style]) #LINE# #TAB# qfont.setCapitalization(FONT_CAPS[font.caps]) #LINE# #TAB# qfont.setStretch(FONT_STRETCH[font.stretch]) #LINE# #TAB# return qfont"
"Wraps the sink socket , which receives results in chunks , to produce a generator of single results  <code> def sink_wrapper(receiver): ",#LINE# #TAB# while True: #LINE# #TAB# #TAB# message = receiver.recv() #LINE# #TAB# #TAB# decoded = unpackb(message) #LINE# #TAB# #TAB# for result in decoded: #LINE# #TAB# #TAB# #TAB# yield result
Generate a synthetic person : param fake : : return : <code> def get_person(fake) ->dict: ,"#LINE# #TAB# return {'id': '{} {}'.format(fake.first_name(), fake.last_name()), #LINE# #TAB# #TAB# 'context': DOMAIN_CONCEPTS.PERSON, 'title': DOMAIN_CONCEPTS.PERSON}"
Gives the path to our module : return : path to our module <code> def get_module_dir() ->os.PathLike: ,#LINE# #TAB# spec = find_spec(MODULE_NAME) #LINE# #TAB# return spec.submodule_search_locations[0]
Load individual columns into named tuples using the ORM  <code> def test_orm_columns(n): ,"#LINE# #TAB# sess = Session(engine) #LINE# #TAB# for row in sess.query(Customer.id, Customer.name, Customer.description #LINE# #TAB# #TAB# ).yield_per(10000).limit(n): #LINE# #TAB# #TAB# pass"
"Currently assumes your on Middle C. Could potentially take into account n1 as a way to know how to handle the irregularities . Such as E - F being 1 half step , but G - A being 2  <code> def finger_distance(f1, f2): ","#LINE# #TAB# key = '%d,%d' % (f1, f2) #LINE# #TAB# return FINGER_DISTANCE[key]"
Ensure the given item is an returned as an iterable . Parameters ---------- item : object The item to ensure is returned as an iterable  <code> def ensure_iterable(item): ,"#LINE# #TAB# if item is None: #LINE# #TAB# #TAB# item = list() #LINE# #TAB# if not hasattr(item, '__iter__') or isinstance(item, string_types): #LINE# #TAB# #TAB# item = [item] #LINE# #TAB# item = list(item) #LINE# #TAB# return item"
Make a sumple list of routing rules . : param app : Flask application : returns : list of routes <code> def get_routing_list(app): ,"#LINE# #TAB# routes = [] #LINE# #TAB# for route in app.url_map.iter_rules(): #LINE# #TAB# #TAB# routes.append({'uri': route.rule, 'methods': sorted(list(route. #LINE# #TAB# #TAB# #TAB# methods)), 'endpoint': route.endpoint}) #LINE# #TAB# routes.sort(key=lambda d: d['uri']) #LINE# #TAB# return routes"
"Return a list of character codes consisting of pairs [ code1a , code1b , code2a , code2b , ... ] which cover all the characters in |s|  <code> def chars_to_ranges(s): ",#LINE# #TAB# char_list = list(s) #LINE# #TAB# char_list.sort() #LINE# #TAB# i = 0 #LINE# #TAB# n = len(char_list) #LINE# #TAB# result = [] #LINE# #TAB# while i < n: #LINE# #TAB# #TAB# code1 = ord(char_list[i]) #LINE# #TAB# #TAB# code2 = code1 + 1 #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# while i < n and code2 >= ord(char_list[i]): #LINE# #TAB# #TAB# #TAB# code2 += 1 #LINE# #TAB# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# result.append(code1) #LINE# #TAB# #TAB# result.append(code2) #LINE# #TAB# return result
"Create a graphene type for ( non - specific ) django model . Used for Orderables and other foreign keys  <code> def register_django_model(cls: Type[models.Model], type_prefix: str): ","#LINE# #TAB# if cls in registry.django_models: #LINE# #TAB# #TAB# return #LINE# #TAB# django_node_type = build_node_type(cls, type_prefix, None) #LINE# #TAB# if django_node_type: #LINE# #TAB# #TAB# registry.django_models[cls] = django_node_type"
"Converts a N - dimensional array to blob proto . If diff is given , also convert the diff . You need to make sure that arr and diff have the same shape , and this function does not do sanity check  <code> def array_to_blobproto(arr, diff=None): ",#LINE# #TAB# blob = caffe_pb2.BlobProto() #LINE# #TAB# blob.shape.dim.extend(arr.shape) #LINE# #TAB# blob.data.extend(arr.astype(float).flat) #LINE# #TAB# if diff is not None: #LINE# #TAB# #TAB# blob.diff.extend(diff.astype(float).flat) #LINE# #TAB# return blob
Finds a selected nav_extender node <code> def find_selected(nodes): ,"#LINE# #TAB# for node in nodes: #LINE# #TAB# #TAB# if hasattr(node, 'selected'): #LINE# #TAB# #TAB# #TAB# return node #LINE# #TAB# #TAB# elif hasattr(node, 'ancestor'): #LINE# #TAB# #TAB# #TAB# result = find_selected(node.children) #LINE# #TAB# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# #TAB# return result"
"Helper for getting incremental state for an nn . Module  <code> def get_incremental_state(module, incremental_state, key): ","#LINE# #TAB# full_key = _get_full_incremental_state_key(module, key) #LINE# #TAB# if incremental_state is None or full_key not in incremental_state: #LINE# #TAB# #TAB# return None #LINE# #TAB# return incremental_state[full_key]"
Get config file specified in environment  <code> def config_file_env(): ,#LINE# #TAB# if CONFIG_FILE_ENV and os.path.exists(CONFIG_FILE_ENV): #LINE# #TAB# #TAB# return CONFIG_FILE_ENV
"Reverse namespace lookup . Note that returned namespace may not be unique : param uri : namespace URI : return : namespace <code> def namespace_for(uri: Union[URIRef, Namespace, str]) ->str: ","#LINE# #TAB# uri = str(uri) #LINE# #TAB# if uri not in namespaces.values(): #LINE# #TAB# #TAB# namespaces[AnonNS().ns] = uri #LINE# #TAB# return [k for k, v in namespaces.items() if uri == v][0]"
"Generate the user+password portion of a URL . : param str user : the user name or : data:`None ` : param str password : the password or : data:`None ` <code> def create_url_identifier(user, password): ","#LINE# #TAB# if user is not None: #LINE# #TAB# #TAB# user = parse.quote(user.encode('utf-8'), safe=USERINFO_SAFE_CHARS) #LINE# #TAB# #TAB# if password: #LINE# #TAB# #TAB# #TAB# password = parse.quote(password.encode('utf-8'), safe= #LINE# #TAB# #TAB# #TAB# #TAB# USERINFO_SAFE_CHARS) #LINE# #TAB# #TAB# #TAB# return '{0}:{1}'.format(user, password) #LINE# #TAB# #TAB# return user #LINE# #TAB# return None"
Parse the chain definitions  <code> def parse_chains(data): ,"#LINE# #TAB# chains = odict() #LINE# #TAB# for line in data.splitlines(True): #LINE# #TAB# #TAB# m = re_chain.match(line) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# policy = None #LINE# #TAB# #TAB# #TAB# if m.group(2) != '-': #LINE# #TAB# #TAB# #TAB# #TAB# policy = m.group(2) #LINE# #TAB# #TAB# #TAB# chains[m.group(1)] = {'policy': policy, 'packets': int(m.group( #LINE# #TAB# #TAB# #TAB# #TAB# 3)), 'bytes': int(m.group(4))} #LINE# #TAB# return chains"
raw_dict : raw_dict COMMA raw_dict <code> def p_raw_dict_chain(t): ,#LINE# #TAB# t[1].update(t[3]) #LINE# #TAB# t[0] = t[1]
Convert file 's open ( ) flags into a readable string . Used by Process.open_files ( )  <code> def file_flags_to_mode(flags): ,"#LINE# #TAB# modes_map = {os.O_RDONLY: 'r', os.O_WRONLY: 'w', os.O_RDWR: 'w+'} #LINE# #TAB# mode = modes_map[flags & (os.O_RDONLY | os.O_WRONLY | os.O_RDWR)] #LINE# #TAB# if flags & os.O_APPEND: #LINE# #TAB# #TAB# mode = mode.replace('w', 'a', 1) #LINE# #TAB# mode = mode.replace('w+', 'r+') #LINE# #TAB# return mode"
Clean from the tree all knows problems . : param penn_tree : the plain text tree : return : cleaned tree <code> def clean_penn_tree(penn_tree): ,#LINE# #TAB# penn_tree = penn_tree.strip() #LINE# #TAB# return penn_tree
Convert DataFrame index from datetime to clocktime ( seconds past midnight ) Parameters -------------- index : pandas Index DataFrame index in datetime Returns ---------- pandas Index DataFrame index in clocktime <code> def datetime_to_clocktime(index): ,#LINE# #TAB# clocktime = (index.hour * 3600 + index.minute * 60 + index.second + #LINE# #TAB# #TAB# index.microsecond / 1000000.0) #LINE# #TAB# return clocktime
"Helper used by both resolve_configuration_dict and get_configuration <code> def resolve_configuration_dict(ch, service_name, config): ","#LINE# #TAB# if _has_connections(config): #LINE# #TAB# #TAB# rels = _get_relationships_from_consul(ch, service_name) #LINE# #TAB# #TAB# connection_types = _get_connection_types(config) #LINE# #TAB# #TAB# connection_names = _resolve_connection_types(service_name, #LINE# #TAB# #TAB# #TAB# connection_types, rels) #LINE# #TAB# #TAB# for key, conn in [(key, [_resolve_name(partial(_lookup_with_consul, #LINE# #TAB# #TAB# #TAB# ch), name)[0] for name in names]) for key, names in #LINE# #TAB# #TAB# #TAB# connection_names]: #LINE# #TAB# #TAB# #TAB# config = util.update_json(config, key, conn) #LINE# #TAB# _logger.info('Generated config: {0}'.format(config)) #LINE# #TAB# return config"
Get the current vi mode for display  <code> def get_vi_mode(): ,"#LINE# #TAB# return {InputMode.INSERT: 'I', InputMode.NAVIGATION: 'N', InputMode. #LINE# #TAB# #TAB# REPLACE: 'R', InputMode.INSERT_MULTIPLE: 'M'}[get_app().vi_state. #LINE# #TAB# #TAB# input_mode]"
Returns the NetDumplings configuration data . : param file : Path to config ; ` ` None ` ` falls back on default config . : return : A dict of configuration data  <code> def get_config(file=None): ,"#LINE# #TAB# config_file = get_config_file() if not file else file #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(config_file) as f: #LINE# #TAB# #TAB# #TAB# config = json.load(f) #LINE# #TAB# except (IOError, json.decoder.JSONDecodeError) as e: #LINE# #TAB# #TAB# raise NetDumplingsError('Error loading config from {0}: {1}'.format #LINE# #TAB# #TAB# #TAB# (config_file, e)) #LINE# #TAB# return config"
Check if ` youtube - dl ` is available in the installation  <code> def youtube_dl_available(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# import youtube_dl #LINE# #TAB# #TAB# return True #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
"Generate a symmetric random matrix with zeros along the diagonal  <code> def fully_random_weights(n_features, lam_scale, prng): ","#LINE# #TAB# weights = np.zeros((n_features, n_features)) #LINE# #TAB# n_off_diag = int((n_features ** 2 - n_features) / 2) #LINE# #TAB# weights[np.triu_indices(n_features, k=1)] = 0.1 * lam_scale * prng.randn( #LINE# #TAB# #TAB# n_off_diag #LINE# #TAB# ) + (0.25 * lam_scale) #LINE# #TAB# weights[weights < 0] = 0 #LINE# #TAB# weights = weights + weights.T #LINE# #TAB# return weights"
"Takes large data array and sigma clips it to find noise per bl for input to detect_bispectra . Takes mean across pols and channels for now , as in detect_bispectra  <code> def estimate_noiseperbl(data): ",#LINE# #TAB# datamean = data.mean(axis=2).imag #LINE# #TAB# noiseperbl = datamean.std() #LINE# #TAB# logger.debug('Measured noise per baseline of {0:.3f}'.format(noiseperbl)) #LINE# #TAB# return noiseperbl
"get reactions whose flux is restricted to zero based on FVA solution Keyword arguments : fvaMinmax -- dict { reaction : flux minimum , flux maximum } Returns list of reactions ( in arbitrary order ) <code> def get_blocked_reactions(fvaMinmax): ","#LINE# #TAB# blockedReactions = [] #LINE# #TAB# for name in fvaMinmax: #LINE# #TAB# #TAB# mini, maxi = fvaMinmax[name] #LINE# #TAB# #TAB# if mini == maxi == 0.0: #LINE# #TAB# #TAB# #TAB# blockedReactions.append(name) #LINE# #TAB# return blockedReactions"
Converts the vehicle 's OBD standards this vehicle conforms to : return : the current engine value <code> def obd_standards(value): ,#LINE# #TAB# return OBD_STANDARDS[__digit(value)] if len(OBD_STANDARDS) >= __digit(value #LINE# #TAB# #TAB# ) else None
"Checks if the given path is a tdt block . Parameters ---------- block : str Block path Returns ------- is_block : bool True if ` path ` is a tdt block , else False <code> def is_block(path): ",#LINE# #TAB# if not os.path.isdir(path): #LINE# #TAB# #TAB# return False #LINE# #TAB# fnames = os.listdir(path) #LINE# #TAB# if any('.tsq' in fname for fname in fnames): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Truncates floats that are integers to their integer representation . That is converts 1 . 0 to 1 etc . Otherwise returns the starting value . Will raise an exception if the input cannot be converted to int  <code> def trunc_if_integer(n: Any) -> Any: ,#LINE# #TAB# if n == int(n): #LINE# #TAB# #TAB# return int(n) #LINE# #TAB# return n
"Display all sub objects of the object with given slug <code> def objects_by_slug(context, slug, limit=5, title=True, text=False): ","#LINE# #TAB# request = context.get('request') #LINE# #TAB# try: #LINE# #TAB# #TAB# obj = lfc.utils.traverse_object(request, slug) #LINE# #TAB# except Http404: #LINE# #TAB# #TAB# return {'objs': []} #LINE# #TAB# objs = obj.children.all().order_by('-publication_date')[:limit] #LINE# #TAB# result = [] #LINE# #TAB# for obj in objs: #LINE# #TAB# #TAB# result.append(obj.get_content_object()) #LINE# #TAB# return {'objs': result, 'title': title, 'text': text}"
Builds a new array with valid values for the MessageEntity constructor . : return : new array with valid values : rtype : dict <code> def validate_array(array): ,"#LINE# #TAB# assert_type_or_raise(array, dict, parameter_name='array') #LINE# #TAB# from pytgbot.api_types.receivable.peer import User #LINE# #TAB# data = Result.validate_array(array) #LINE# #TAB# data['type'] = u(array.get('type')) #LINE# #TAB# data['offset'] = int(array.get('offset')) #LINE# #TAB# data['length'] = int(array.get('length')) #LINE# #TAB# data['url'] = u(array.get('url')) if array.get('url') is not None else None #LINE# #TAB# data['user'] = User.from_array(array.get('user')) if array.get('user' #LINE# #TAB# #TAB# ) is not None else None #LINE# #TAB# data['language'] = u(array.get('language')) if array.get('language' #LINE# #TAB# #TAB# ) is not None else None #LINE# #TAB# return data"
"Returns the years interval from start year <code> def copyright_years(startyear, sep='-'): ",#LINE# #TAB# syear = int(startyear) #LINE# #TAB# cyear = date.today().year #LINE# #TAB# ret = str(syear) #LINE# #TAB# if cyear != syear: #LINE# #TAB# #TAB# ret += sep + str(cyear) #LINE# #TAB# return ret
Retrieves the PIL image from the given array of bytes . : param image_bytes : bytes to convert into a PIL image . : return : PIL image  <code> def get_image(image_bytes): ,"#LINE# #TAB# with io.BytesIO(image_bytes) as bytes_io, Image.open(bytes_io) as im: #LINE# #TAB# #TAB# image = im.convert('RGB') #LINE# #TAB# return image"
"set_get ( + Set , -Value ) <code> def builtin_set_get(g, rt): ","#LINE# #TAB# rt._trace('CALLED BUILTIN set_get', g) #LINE# #TAB# pred = g.terms[g.inx] #LINE# #TAB# args = pred.args #LINE# #TAB# if len(args) != 2: #LINE# #TAB# #TAB# raise PrologRuntimeError('set_get: 2 args (+Set, -Value) expected.', #LINE# #TAB# #TAB# #TAB# g.location) #LINE# #TAB# arg_set = rt.prolog_get_set(args[0], g.env, g.location) #LINE# #TAB# arg_val = rt.prolog_get_variable(args[1], g.env, g.location) #LINE# #TAB# res = [] #LINE# #TAB# for v in arg_set.s: #LINE# #TAB# #TAB# res.append({arg_val: v}) #LINE# #TAB# return res"
returns a list of tools to install @return list of tools <code> def windows_default_tools_list(): ,"#LINE# #TAB# return ['7z', 'scite', 'putty', 'mingw', 'SQLiteSpy', 'r', 'vs', #LINE# #TAB# #TAB# 'julia', 'graphviz', 'tdm', 'pandoc', 'jdk', 'jenkins', 'miktex', #LINE# #TAB# #TAB# 'inkscape', 'git', 'python']"
Take an ElementTree and extract a software release from it . : param root : ElementTree we 're barking up . : type root : xml.etree . ElementTree . ElementTree <code> def sr_lookup_extractor(root): ,"#LINE# #TAB# reg = re.compile('(\\d{1,4}\\.)(\\d{1,4}\\.)(\\d{1,4}\\.)(\\d{1,4})') #LINE# #TAB# packages = root.findall('./data/content/') #LINE# #TAB# for package in packages: #LINE# #TAB# #TAB# if package.text is not None: #LINE# #TAB# #TAB# #TAB# match = reg.match(package.text) #LINE# #TAB# #TAB# #TAB# packtext = package.text if match else 'SR not in system' #LINE# #TAB# #TAB# #TAB# return packtext"
"Parse provided hostname and extract port number <code> def _parse_host(cls, host='localhost', port=0): ","#LINE# #TAB# #TAB# if not port and (host.find(':') == host.rfind(':')): #LINE# #TAB# #TAB# #TAB# i = host.rfind(':') #LINE# #TAB# #TAB# #TAB# if i >= 0: #LINE# #TAB# #TAB# #TAB# #TAB# host, port = host[:i], host[i + 1:] #LINE# #TAB# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# port = int(port) #LINE# #TAB# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# raise OSError('nonnumeric port') #LINE# #TAB# #TAB# return host, port"
Converts value into map object or returns empty map when conversion is not possible : param value : the value to convert . : return : map object or empty map when conversion is not supported  <code> def to_map(value): ,#LINE# #TAB# result = RecursiveMapConverter.to_nullable_map(value) #LINE# #TAB# return result if result != None else {}
Unfortunately facebook returns 500s which mean they are down Or 500s with a nice error message because you use open graph wrong So we have to figure out which is which :) <code> def is_json(content): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# json.loads(content) #LINE# #TAB# #TAB# is_json = True #LINE# #TAB# except: #LINE# #TAB# #TAB# is_json = False #LINE# #TAB# return is_json
"Estimate inverse covariance via scikit - learn GraphLassoCV class  <code> def graph_lasso(X, num_folds): ","#LINE# #TAB# print(""GraphLasso (sklearn)"") #LINE# #TAB# model = GraphLassoCV(cv=num_folds) #LINE# #TAB# model.fit(X) #LINE# #TAB# print("" lam_: {}"".format(model.alpha_)) #LINE# #TAB# return model.covariance_, model.precision_, model.alpha_"
"Do PCA analysis on 4D image to turn it into a 3D image suitable for SLIC <code> def preprocess_pca(img, ncomp, normalise_input_image=True, norm_type='perc'): ","#LINE# #TAB# baseline = np.mean(img[:, :, :, :3], axis=-1) #LINE# #TAB# img = img - np.tile(np.expand_dims(baseline, axis=-1), (1, 1, 1, img. #LINE# #TAB# #TAB# shape[-1])) #LINE# #TAB# pca = PcaFeatReduce(n_components=ncomp, norm_modes=True, norm_input= #LINE# #TAB# #TAB# normalise_input_image, norm_type=norm_type) #LINE# #TAB# feat_image = pca.get_training_features(img, smooth_timeseries=2.0, #LINE# #TAB# #TAB# feature_volume=True) #LINE# #TAB# return feat_image"
"true if user has ' add ' , ' change ' or ' delete ' permission on this model <code> def user_can_edit_snippet_type(user, model): ","#LINE# #TAB# for action in ('add', 'change', 'delete'): #LINE# #TAB# #TAB# if user.has_perm(get_permission_name(action, model)): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Get data from an image or FITS file <code> def get_img_data(toyz_settings, tid, params): ","#LINE# #TAB# import toyz.web.viewer as viewer #LINE# #TAB# core.check4keys(params, ['data_type', 'file_info', 'img_info']) #LINE# #TAB# response = viewer.get_img_data(**params) #LINE# #TAB# return response"
"Inverse CDF for a pure power - law <code> def pl_inv(P, xm, a): ",#LINE# #TAB# x = (1 - P) ** (1 / (1 - a)) * xm #LINE# #TAB# return x
"Attempt to find featured artists in the item 's artist fields and return the results . Returns None if no featured artist found  <code> def find_feat_part(artist, albumartist): ","#LINE# #TAB# albumartist_split = artist.split(albumartist, 1) #LINE# #TAB# if len(albumartist_split) <= 1: #LINE# #TAB# #TAB# return None #LINE# #TAB# elif albumartist_split[1] != '': #LINE# #TAB# #TAB# _, feat_part = split_on_feat(albumartist_split[1]) #LINE# #TAB# #TAB# return feat_part #LINE# #TAB# else: #LINE# #TAB# #TAB# lhs, rhs = split_on_feat(albumartist_split[0]) #LINE# #TAB# #TAB# if lhs: #LINE# #TAB# #TAB# #TAB# return lhs #LINE# #TAB# return None"
"Create a list of Bars from a dic . Parameters ---------- dic : dict Keys are attribute names of Bar , values are corresponding values . Returns ------- bar : Bar <code> def create_from_dict(cls, dic): ",#LINE# #TAB# bar = cls() #LINE# #TAB# bar.__dict__.update(dic) #LINE# #TAB# return bar
"Get parser epilog . We parse this as reStructuredText , allowing users to embed rich information in their help messages if they so choose  <code> def format_epilog(parser): ","#LINE# #TAB# for line in statemachine.string2lines(parser.epilog, tab_width=4, #LINE# #TAB# #TAB# convert_whitespace=True): #LINE# #TAB# #TAB# yield line"
Create the cell model <code> def create_cell(add_synapses=True): ,#LINE# #TAB# neuron.h.load_file('morphology.hoc') #LINE# #TAB# neuron.h.load_file('biophysics.hoc') #LINE# #TAB# neuron.h.load_file('template.hoc') #LINE# #TAB# print('Loading cell bAC217_L5_DBC_4765d943f4') #LINE# #TAB# cell = neuron.h.bAC217_L5_DBC_4765d943f4(1 if add_synapses else 0) #LINE# #TAB# return cell
"This function finds the first set of values for each model and scenario and subtracts them from all values to remove the offset  <code> def remove_t0_from_wide_db(times_needed, _db): ","#LINE# #TAB# for model, scenario in set(zip(_db.index.get_level_values('model'), _db #LINE# #TAB# #TAB# .index.get_level_values('scenario'))): #LINE# #TAB# #TAB# offset = _db.loc[model, scenario, min(times_needed)].copy( #LINE# #TAB# #TAB# #TAB# ).values.squeeze() #LINE# #TAB# #TAB# for time in times_needed: #LINE# #TAB# #TAB# #TAB# _db.loc[model, scenario, time] = _db.loc[model, scenario, time #LINE# #TAB# #TAB# #TAB# #TAB# ] - offset"
"Filter certain superflous parameter name suffixes from argument names . : param str prefix : The potential prefix that will be filtered . : param str name : The arg name to be prefixed . : returns : Combined name with prefix  <code> def join_prefix(prefix, name): ",#LINE# #TAB# if prefix.endswith('_specification'): #LINE# #TAB# #TAB# return prefix[:-14] + '_' + name #LINE# #TAB# if prefix.endswith('_patch_parameter'): #LINE# #TAB# #TAB# return prefix[:-16] + '_' + name #LINE# #TAB# if prefix.endswith('_update_parameter'): #LINE# #TAB# #TAB# return prefix[:-17] + '_' + name #LINE# #TAB# return prefix + '_' + name
Make directories leading to ' dst ' if they do n't exist yet <code> def mkdirs_thread_safe(dst: DirPath) ->None: ,"#LINE# #TAB# if dst == '' or os.path.exists(dst): #LINE# #TAB# #TAB# return #LINE# #TAB# head, _ = os.path.split(dst) #LINE# #TAB# if os.sep == ':' and not ':' in head: #LINE# #TAB# #TAB# head = head + ':' #LINE# #TAB# mkdirs_thread_safe(DirPath(head)) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.mkdir(dst, 511) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# if err.errno != 17: #LINE# #TAB# #TAB# #TAB# raise"
"Sets the upper bound timestamp  <code> def SetUpperTimestamp(cls, timestamp): ","#LINE# #TAB# if not hasattr(cls, '_upper'): #LINE# #TAB# cls._upper = timestamp #LINE# #TAB# return #LINE# #TAB# if timestamp > cls._upper: #LINE# #TAB# cls._upper = timestamp"
"Load a texture from a file into a PIL image  <code> def load_texture(file_name, resolver): ",#LINE# #TAB# file_data = resolver.get(file_name) #LINE# #TAB# image = PIL.Image.open(util.wrap_as_stream(file_data)) #LINE# #TAB# return image
"Performs attention on the input . : param attention_input : The input tensor for attention layer . : param attention_mask : A tensor to mask the invalid values . : return : The masked output tensor  <code> def attention_layer(cls, attention_input: typing.Any, attention_mask: ","#LINE# #TAB# typing.Any=None) ->keras.layers.Layer: #LINE# #TAB# dense_input = keras.layers.Dense(1, use_bias=False)(attention_input) #LINE# #TAB# if attention_mask is not None: #LINE# #TAB# #TAB# dense_input = keras.layers.Lambda(lambda x: x + (1.0 - #LINE# #TAB# #TAB# #TAB# attention_mask) * -10000.0, name='attention_mask')(dense_input) #LINE# #TAB# attention_probs = keras.layers.Lambda(lambda x: tf.nn.softmax(x, axis=1 #LINE# #TAB# #TAB# ), output_shape=lambda s: (s[0], s[1], s[2]), name='attention_probs')( #LINE# #TAB# #TAB# dense_input) #LINE# #TAB# return attention_probs"
"Download an HTML page using the requests session and return the final URL after following redirects  <code> def get_page_and_url(session, url): ","#LINE# #TAB# reply = get_reply(session, url) #LINE# #TAB# return reply.text, reply.url"
"Compute the checksum of a full line <code> def _checksum(cls, line): ","#LINE# #TAB# #TAB# tr_table = str.maketrans({c: None for c in ascii_uppercase + ""+ .""}) #LINE# #TAB# #TAB# no_letters = line[:68].translate(tr_table).replace(""-"", ""1"") #LINE# #TAB# #TAB# return sum([int(l) for l in no_letters]) % 10"
Decompose number into the primes <code> def decompose_to_primes(max_prime): ,"#LINE# #TAB# for prime in SIEVE_PRIMES(list(range(2, max_prime))): #LINE# #TAB# #TAB# if prime * prime > max_prime: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# while max_prime % prime == 0: #LINE# #TAB# #TAB# #TAB# yield prime #LINE# #TAB# #TAB# #TAB# max_prime //= prime #LINE# #TAB# if max_prime > 1: #LINE# #TAB# #TAB# yield max_prime"
"Searches for a predefined unit with the given ` ` name ` ` and returns a : class:`Units ` object representing it . If no such unit is found a : class:`CellMLError ` is raised  <code> def find_units(cls, name): ","#LINE# #TAB# if name == 'celsius': #LINE# #TAB# #TAB# raise UnsupportedUnitsError('celsius') #LINE# #TAB# obj = cls._si_unit_objects.get(name, None) #LINE# #TAB# if obj is None: #LINE# #TAB# #TAB# myokit_unit = cls._si_units.get(name, None) #LINE# #TAB# #TAB# if myokit_unit is None: #LINE# #TAB# #TAB# #TAB# raise CellMLError('Unknown units name ""' + str(name) + '"".') #LINE# #TAB# #TAB# obj = cls(name, myokit_unit, predefined=True) #LINE# #TAB# #TAB# cls._si_unit_objects[name] = obj #LINE# #TAB# return obj"
"Split list items into ` ( matching , non_matching ) ` by ` cond(item ) ` callable  <code> def split_list(iterable, condition): ","#LINE# #TAB# matching = [] #LINE# #TAB# non_matching = [] #LINE# #TAB# for item in iterable: #LINE# #TAB# #TAB# if condition(item): #LINE# #TAB# #TAB# #TAB# matching.append(item) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# non_matching.append(item) #LINE# #TAB# return matching, non_matching"
Convert Checks API request object into a dict  <code> def to_gh_query(req): ,"#LINE# #TAB# return {k: conditional_to_gh_query(v) for k, v in attr.asdict(req). #LINE# #TAB# #TAB# items() if v is not None or isinstance(v, (list, dict)) and not v}"
"r Return the area of a triangle  <code> def triangle_area(pt1, pt2, pt3): ",#LINE# #TAB# a = 0.0 #LINE# #TAB# a += pt1[0] * pt2[1] - pt2[0] * pt1[1] #LINE# #TAB# a += pt2[0] * pt3[1] - pt3[0] * pt2[1] #LINE# #TAB# a += pt3[0] * pt1[1] - pt1[0] * pt3[1] #LINE# #TAB# return abs(a) / 2
Default thread creation - used to create threads when the client does n't want to provide their own thread creation . : param function callback : the callback function provided to threading . Thread <code> def default_create_thread(callback): ,"#LINE# #TAB# thread = threading.Thread(None, callback) #LINE# #TAB# thread.daemon = True #LINE# #TAB# thread.start() #LINE# #TAB# return thread"
set up a proxy that store too long warnings in a separate map <code> def set_proxy_filter(warningstuple): ,"#LINE# #TAB# if len(warningstuple) > 5: #LINE# #TAB# #TAB# key = len(_proxy_map) + 1 #LINE# #TAB# #TAB# _proxy_map[key] = warningstuple #LINE# #TAB# #TAB# return 'custom', re_matchall, ProxyWarning, re_matchall, key #LINE# #TAB# else: #LINE# #TAB# #TAB# return warningstuple"
Generates a serial number request packet . Returns : packet : A request packet  <code> def get_sn(): ,#LINE# #TAB# packet = p.Packet(MsgType.Base) #LINE# #TAB# packet.add_subpacket(p.NoPayload(BaseMsgCode.GetSn)) #LINE# #TAB# return packet
"Generates a topography mask from an oceanographic transect taking the deepest CTD scan as the depth of each station  <code> def gen_topomask(h, lon, lat, dx=1.0, kind=""linear"", plot=False): ","#LINE# #TAB# import gsw #LINE# #TAB# from scipy.interpolate import interp1d #LINE# #TAB# h, lon, lat = list(map(np.asanyarray, (h, lon, lat))) #LINE# #TAB# x = np.append(0, np.cumsum(gsw.distance(lon, lat)[0] / 1e3)) #LINE# #TAB# h = -gsw.z_from_p(h, lat.mean()) #LINE# #TAB# Ih = interp1d(x, h, kind=kind, bounds_error=False, fill_value=h[-1]) #LINE# #TAB# xm = np.arange(0, x.max() + dx, dx) #LINE# #TAB# hm = Ih(xm) #LINE# #TAB# return xm, hm"
Return the first task instance that is a module node  <code> def get_module_task_instance_id(task_instances): ,#LINE# #TAB# for id in task_instances: #LINE# #TAB# #TAB# if task_instances[id] == 'module_node': #LINE# #TAB# #TAB# #TAB# return id #LINE# #TAB# return None
List regions for the service <code> def list_regions(service): ,"#LINE# #TAB# for region in service.regions(): #LINE# #TAB# #TAB# print '%(name)s: %(endpoint)s' % { #LINE# #TAB# #TAB# #TAB# 'name': region.name, #LINE# #TAB# #TAB# #TAB# 'endpoint': region.endpoint, #LINE# #TAB# #TAB# }"
"Set the host and the scheduled_at field of a group . : returns : A Group with the updated fields set properly  <code> def generic_group_update_db(context, group, host, cluster_name): ","#LINE# #TAB# group.update({'host': host, 'updated_at': timeutils.utcnow(), #LINE# #TAB# #TAB# 'cluster_name': cluster_name}) #LINE# #TAB# group.save() #LINE# #TAB# return group"
"a - a ' : chloride hydroxide [ MP98 ]  <code> def theta_cl_oh_mp98(T, P): ","#LINE# #TAB# theta = -0.05 + (T - 298.15) * 0.0003125 + (T - 298.15) ** 2 * -8.362e-06 #LINE# #TAB# valid = logical_and(T >= 273.15, T <= 323.15) #LINE# #TAB# return theta, valid"
"For a given opcode op in opcode module opc return the size in bytes of an op instruction  <code> def instruction_size(op, opc): ",#LINE# #TAB# if op < opc.HAVE_ARGUMENT: #LINE# #TAB# #TAB# return 2 if opc.version >= 3.6 else 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 2 if opc.version >= 3.6 else 3
"if ` test is None ` do a straight equality test <code> def all_equal(iterator, test=None): ",#LINE# #TAB# it = iter(iterator) #LINE# #TAB# if test is None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# target = next(it) #LINE# #TAB# #TAB# #TAB# test = lambda x: x == target #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# for item in it: #LINE# #TAB# #TAB# if not test(item): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"Transposes an image by either rotating or flipping it  <code> def transpose_cmd(images, rotate, flip): ","#LINE# #TAB# for image in images: #LINE# #TAB# #TAB# if rotate is not None: #LINE# #TAB# #TAB# #TAB# mode, degrees = rotate #LINE# #TAB# #TAB# #TAB# click.echo('Rotate ""%s"" by %ddeg' % (image.filename, degrees)) #LINE# #TAB# #TAB# #TAB# image = copy_filename(image.transpose(mode), image) #LINE# #TAB# #TAB# if flip is not None: #LINE# #TAB# #TAB# #TAB# mode, direction = flip #LINE# #TAB# #TAB# #TAB# click.echo('Flip ""%s"" %s' % (image.filename, direction)) #LINE# #TAB# #TAB# #TAB# image = copy_filename(image.transpose(mode), image) #LINE# #TAB# #TAB# yield image"
"Split a song into two parts one starting at start1 the other at start2 <code> def split_song(songToSplit, start1, start2): ","#LINE# #TAB# print ""start1 "" + str(start1) #LINE# #TAB# print ""start2 "" + str(start2) #LINE# #TAB# songs = [songToSplit[:start1], songToSplit[start2:]] #LINE# #TAB# return songs"
Load IMDb data and augment with hashed n - gram features  <code> def load_and_preprocess_imdb_data(n_gram=None): ,"#LINE# #TAB# X_train, y_train, X_test, y_test = tl.files.load_imdb_dataset(nb_words=VOCAB_SIZE) #LINE# #TAB# if n_gram is not None: #LINE# #TAB# #TAB# X_train = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_train]) #LINE# #TAB# #TAB# X_test = np.array([augment_with_ngrams(x, VOCAB_SIZE, N_BUCKETS, n=n_gram) for x in X_test]) #LINE# #TAB# return X_train, y_train, X_test, y_test"
Class method to get a reference to the speed dial . @return reference to the speed dial ( SpeedDial ) <code> def speed_dial(cls): ,#LINE# #TAB# if cls._speed_dial is None: #LINE# #TAB# #TAB# from .SpeedDial.SpeedDial import SpeedDial #LINE# #TAB# #TAB# cls._speed_dial = SpeedDial() #LINE# #TAB# return cls._speed_dial
Get the peers relation id if a peers relation has been joined else None  <code> def peer_relation_id(): ,#LINE# #TAB# md = metadata() #LINE# #TAB# section = md.get('peers') #LINE# #TAB# if section: #LINE# #TAB# #TAB# for key in section: #LINE# #TAB# #TAB# #TAB# relids = relation_ids(key) #LINE# #TAB# #TAB# #TAB# if relids: #LINE# #TAB# #TAB# #TAB# #TAB# return relids[0] #LINE# #TAB# return None
"Maps the vars_list into a list of axis indices corresponding to the fluent scope  <code> def _varslist2axis(cls, fluent: 'TensorFluent', vars_list: List[str]) -> List[int]: ",#LINE# #TAB# #TAB# axis = [] #LINE# #TAB# #TAB# for var in vars_list: #LINE# #TAB# #TAB# #TAB# if var in fluent.scope.as_list(): #LINE# #TAB# #TAB# #TAB# #TAB# ax = fluent.scope.index(var) #LINE# #TAB# #TAB# #TAB# #TAB# if fluent.batch: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ax += 1 #LINE# #TAB# #TAB# #TAB# #TAB# axis.append(ax) #LINE# #TAB# #TAB# return axis
"Ensure a configuration file exists in given configuration directory . Creating a default one if needed . Return path to the configuration file  <code> def ensure_config_exists(config_dir: str, detect_location: bool=True) ->str: ","#LINE# #TAB# config_path = find_config_file(config_dir) #LINE# #TAB# if config_path is None: #LINE# #TAB# #TAB# print('Unable to find configuration. Creating default one in', #LINE# #TAB# #TAB# #TAB# config_dir) #LINE# #TAB# #TAB# config_path = create_default_config(config_dir, detect_location) #LINE# #TAB# return config_path"
Return ContextService arguments for su as a become method  <code> def connect_su(spec): ,"#LINE# #TAB# return {'method': 'su', 'enable_lru': True, 'kwargs': {'username': spec #LINE# #TAB# #TAB# .become_user(), 'password': spec.become_pass(), 'python_path': spec #LINE# #TAB# #TAB# .python_path(), 'su_path': spec.become_exe(), 'connect_timeout': #LINE# #TAB# #TAB# spec.timeout(), 'remote_name': get_remote_name(spec)}}"
Read 4 bytes as BE integer in file f <code> def read_long(f): ,"#LINE# #TAB# read_bytes = f.read(4) #LINE# #TAB# return struct.unpack('>l', read_bytes)[0]"
"Get one object by primary_key value . : type engine_or_session : Union[Engine , Session ] <code> def by_id(cls, _id, engine_or_session): ","#LINE# #TAB# ses, auto_close = ensure_session(engine_or_session) #LINE# #TAB# obj = ses.query(cls).get(_id) #LINE# #TAB# if auto_close: #LINE# #TAB# #TAB# ses.close() #LINE# #TAB# return obj"
Returns a list of n - grams read from the file at path  <code> def get_ngrams(path): ,"#LINE# #TAB# with open(path, encoding='utf-8') as fh: #LINE# #TAB# #TAB# ngrams = [ngram.strip() for ngram in fh.readlines()] #LINE# #TAB# return ngrams"
Matches task details emitted  <code> def task_matcher(details): ,#LINE# #TAB# if not details: #LINE# #TAB# #TAB# return False #LINE# #TAB# if 'task_name' in details and 'task_uuid' in details: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
initiating all elemental materials in csv files in path <code> def init_group(path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open('data', 'rb') as fp: #LINE# #TAB# #TAB# #TAB# data = load(fp) #LINE# #TAB# #TAB# return data #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# data = {} #LINE# #TAB# #TAB# for file_ in os.listdir(path): #LINE# #TAB# #TAB# #TAB# if file_.endswith('.csv'): #LINE# #TAB# #TAB# #TAB# #TAB# mat = Material(path + '/' + file_) #LINE# #TAB# #TAB# #TAB# #TAB# if mat is None: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# print('{} failed to initialize'.format(file_)) #LINE# #TAB# #TAB# #TAB# #TAB# data[mat.uid] = mat #LINE# #TAB# return data"
Accessor for factory instance <code> def get_rpc_collections(): ,#LINE# #TAB# if not __rpc_collections: #LINE# #TAB# #TAB# raise RuntimeError('RPCCollectionFactory must be instantiated!') #LINE# #TAB# return __rpc_collections
Return a DOCTYPE declaration : param doc_type : doc type string must be in ` ` page . DOC_TYPES ` ` : type doc_type : str : return : DOCTYPE declaration : rtype : str <code> def get_doc_type(doc_type): ,"#LINE# #TAB# if doc_type not in DOC_TYPES: #LINE# #TAB# #TAB# raise ValueError('Invalid DOCTYPE %s available values are %s' % ( #LINE# #TAB# #TAB# #TAB# doc_type, DOC_TYPES.keys())) #LINE# #TAB# return DOC_TYPES[doc_type]"
"Calculate normals from vertices and faces . Parameters ---------- verices : ndarray faces : ndarray Returns ------- normals : ndarray Shape same as vertices <code> def normals_from_v_f(vertices, faces): ","#LINE# #TAB# norm = np.zeros(vertices.shape, dtype=vertices.dtype) #LINE# #TAB# tris = vertices[faces] #LINE# #TAB# n = np.cross(tris[:, (1)] - tris[:, (0)], tris[:, (2)] - tris[:, (0)]) #LINE# #TAB# normalize_v3(n) #LINE# #TAB# norm[faces[:, (0)]] += n #LINE# #TAB# norm[faces[:, (1)]] += n #LINE# #TAB# norm[faces[:, (2)]] += n #LINE# #TAB# normalize_v3(norm) #LINE# #TAB# return norm"
"Calculate the w field of a quaternion  <code> def quat_from_qx_qy_qz(qx, qy, qz): ","#LINE# #TAB# qw = 1.0 - (qx * qx + qy * qy + qz * qz) #LINE# #TAB# if qw < 0.0: #LINE# #TAB# #TAB# qw = 0.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# qw = -math.sqrt(qw) #LINE# #TAB# return qx, qy, qz, qw"
"Return list of chunks <code> def chunk_range(size, chunk_size): ",#LINE# #TAB# if chunk_size is None: #LINE# #TAB# #TAB# return [size] #LINE# #TAB# crange = [chunk_size] * (size // chunk_size) + [size - chunk_size * ( #LINE# #TAB# #TAB# size // chunk_size)] #LINE# #TAB# crange = [x for x in crange if x != 0] #LINE# #TAB# return crange
Get all direct dependencies based on requirements.in file and generated Pipfile.lock from it  <code> def get_direct_dependencies_requirements() ->set: ,#LINE# #TAB# with open('requirements.in') as requirements_in_file: #LINE# #TAB# #TAB# content = requirements_in_file.read() #LINE# #TAB# direct_dependencies = set() #LINE# #TAB# for line in content.splitlines(): #LINE# #TAB# #TAB# if line.strip().startswith('#'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# package_name = _RE_VERSION_DELIMITER.split(line)[0] #LINE# #TAB# #TAB# direct_dependencies.add(package_name.lower()) #LINE# #TAB# return direct_dependencies
Returns True if a 2-nested list contains at least one truthy value . : param l : a nested list : return : <code> def has_values(l): ,#LINE# #TAB# for row in l: #LINE# #TAB# #TAB# for value in row: #LINE# #TAB# #TAB# #TAB# if value: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"Find file by name , None if none or multiple  <code> def search_file_by_name(name: str, path: Path) ->Optional[Path]: ",#LINE# #TAB# results = tuple(path.glob(f'**/{name}')) #LINE# #TAB# if len(results) == 1: #LINE# #TAB# #TAB# return results[0] #LINE# #TAB# return None
"Read lines from f until a blank line is encountered  <code> def skip_to_blank(f, spacegroup, setting): ","#LINE# #TAB# while True: #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# #TAB# if not line: #LINE# #TAB# #TAB# #TAB# raise SpacegroupNotFoundError( #LINE# #TAB# #TAB# #TAB# #TAB# 'invalid spacegroup %s, setting %i not found in data base' % #LINE# #TAB# #TAB# #TAB# #TAB# (spacegroup, setting)) #LINE# #TAB# #TAB# if not line.strip(): #LINE# #TAB# #TAB# #TAB# break"
Return True for likely dict object via duck typing  <code> def infer_dict(obj): ,"#LINE# #TAB# for attrs in (('items', 'keys', 'values'), ('iteritems', 'iterkeys', #LINE# #TAB# #TAB# 'itervalues')): #LINE# #TAB# #TAB# attrs += '__len__', 'get', 'has_key' #LINE# #TAB# #TAB# if all(_callable(getattr(obj, a, None)) for a in attrs): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Returns true if the specified object property at given path satisfies the given predicate ; false otherwise <code> def path_satisfies(predicate, path, value): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return predicate(_path(path, value)) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return False"
"Cache known ActivityType 's to remove useless latency . : param domain : : type domain : : param name : : type name : : param version : : type version : : return : : rtype : <code> def get_activity_type(cls, domain, name, version): ","#LINE# #TAB# key = domain.name, name, version #LINE# #TAB# if key not in cls.cached_models: #LINE# #TAB# #TAB# cls.cached_models[key] = swf.models.ActivityType(domain, name, #LINE# #TAB# #TAB# #TAB# version=version) #LINE# #TAB# return cls.cached_models[key]"
"Return number of all intercepted commands filtered by ' which ' field . Args : cmds_file : Path to the txt file with intercepted commands . which_list : A list of strings to filter command by ' which ' field  <code> def number_of_cmds_by_which(cmds_file, which_list): ","#LINE# #TAB# i = 0 #LINE# #TAB# for _ in iter_cmds_by_which(cmds_file, which_list): #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# return i"
"Evaluate the time / memory requests for the grid job , allowing for ints or formulas <code> def evaluate_resource_requests(time, mem): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# time = eval(str(time)) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# raise TypeError('Unable to evaluate time request for task: ' + time) #LINE# #TAB# try: #LINE# #TAB# #TAB# mem = eval(str(mem)) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# raise TypeError('Unable to evaluate memory request for task: ' + mem) #LINE# #TAB# return time, mem"
"Get the length of the string representation of the key . If several keys of the same name are present , the maximum length is returned . : param bytes key : the keyword to get the string representation size of . : rtype : int <code> def codes_get_string_length(handle, key): ","#LINE# #TAB# size = ffi.new('size_t *') #LINE# #TAB# _codes_get_length(handle, key.encode(ENC), size) #LINE# #TAB# return size[0]"
Parse the mana cost of a card <code> def parse_mana(mana_div) ->str: ,#LINE# #TAB# mana_string = '' #LINE# #TAB# for mana_content in mana_div.children: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# mana_string += card_py_bot.config.MANA_DICT[mana_content['alt'] #LINE# #TAB# #TAB# #TAB# #TAB# ].strip() + ' ' #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return mana_string
"Scale a folded site frequency spectrum  <code> def scale_sfs_folded(s, n): ",#LINE# #TAB# k = np.arange(s.shape[0]) #LINE# #TAB# out = s * k * (n - k) / n #LINE# #TAB# return out
"Return keyword arguments for PIP install  <code> def pip_kwargs(config_dir: Optional[str]) ->Dict[str, Any]: ","#LINE# #TAB# is_docker = pkg_util.is_docker_env() #LINE# #TAB# kwargs = {'constraints': os.path.join(os.path.dirname(__file__), #LINE# #TAB# #TAB# CONSTRAINT_FILE), 'no_cache_dir': is_docker} #LINE# #TAB# if 'WHEELS_LINKS' in os.environ: #LINE# #TAB# #TAB# kwargs['find_links'] = os.environ['WHEELS_LINKS'] #LINE# #TAB# if not (config_dir is None or pkg_util.is_virtual_env()) and not is_docker: #LINE# #TAB# #TAB# kwargs['target'] = os.path.join(config_dir, 'deps') #LINE# #TAB# return kwargs"
List vm snapshots . Args : vm_name : The virtual machine name . Returns : Info obtained from remote hyper - v host  <code> def list_vm_snaps(vm_name: str) ->Response: ,"#LINE# #TAB# ps_script = ( #LINE# #TAB# #TAB# 'Get-VMSnapshot -VMName ""{}"" | Select Name,ParentSnapshotName,CreationTime,ParentSnapshotId,Id | ConvertTo-Json' #LINE# #TAB# #TAB# .format(vm_name)) #LINE# #TAB# rs = run_ps(ps_script) #LINE# #TAB# return rs"
"Returns all cleaving contexts stored in the broker . : param broker : : return : list of tuples of ( CleavingContext , timestamp ) <code> def load_all(cls, broker): ","#LINE# #TAB# brokers = broker.get_brokers() #LINE# #TAB# sysmeta = brokers[-1].get_sharding_sysmeta_with_timestamps() #LINE# #TAB# for key, (val, timestamp) in sysmeta.items(): #LINE# #TAB# #TAB# if key.startswith('Context-') and len(val) > 0: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# yield cls(**json.loads(val)), timestamp #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# continue"
Check to see if a file exists or is empty  <code> def is_valid(file_path): ,"#LINE# #TAB# from os import path, stat #LINE# #TAB# can_open = False #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(file_path) as fp: #LINE# #TAB# #TAB# #TAB# can_open = True #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# return False #LINE# #TAB# is_file = path.isfile(file_path) #LINE# #TAB# return path.exists(file_path) and is_file and stat(file_path).st_size > 0"
"Look for a favicon under the domain <code> def get_favicon(url, soup=None): ",#LINE# #TAB# parsed_uri = urlparse(url) #LINE# #TAB# icon_url = '{uri.scheme}://{uri.netloc}/favicon.ico'.format(uri=parsed_uri) #LINE# #TAB# res = requests.head(icon_url) #LINE# #TAB# if res.status_code != 200: #LINE# #TAB# #TAB# return None #LINE# #TAB# return icon_url
Validates a Finnish bank account number  <code> def validate_account_number(number): ,#LINE# #TAB# number = BBAN.expand_account_number(number) #LINE# #TAB# if not number: #LINE# #TAB# #TAB# return False #LINE# #TAB# checksum = BBAN.calculate_account_number_checksum(number) #LINE# #TAB# if checksum is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# return number[-1] == checksum
"Prompts user for username and password gets API key from server if not provided  <code> def login_uname_pwd(server, api_key=None): ","#LINE# #TAB# username = click.prompt(""Please enter your One Codex (email)"") #LINE# #TAB# if api_key is not None: #LINE# #TAB# #TAB# return username, api_key #LINE# #TAB# password = click.prompt(""Please enter your password (typing will be hidden)"", hide_input=True) #LINE# #TAB# api_key = fetch_api_key_from_uname(username, password, server) #LINE# #TAB# return username, api_key"
"requires that value is already in tree with node as the root <code> def find_node(value, node): ","#LINE# #TAB# if value > node.value and node.right: #LINE# #TAB# #TAB# return BST.find_node(value, node.right) #LINE# #TAB# elif value < node.value and node.left: #LINE# #TAB# #TAB# return BST.find_node(value, node.left) #LINE# #TAB# if value == node.value: #LINE# #TAB# #TAB# return node"
Calculate the CRC over a range of bytes using the CCITT polynomial  <code> def crc_ccitt(data): ,#LINE# #TAB# crc = 0 #LINE# #TAB# if not have_py_3: #LINE# #TAB# #TAB# for x in data: #LINE# #TAB# #TAB# #TAB# crc = crc_ccitt_table[ord(x) ^ ((crc >> 8) & 0xFF)] ^ ((crc << 8) & 0xFF00) #LINE# #TAB# else: #LINE# #TAB# #TAB# mv = memoryview(data) #LINE# #TAB# #TAB# for x in mv.tobytes(): #LINE# #TAB# #TAB# #TAB# crc = crc_ccitt_table[x ^ ((crc >> 8) & 0xFF)] ^ ((crc << 8) & 0xFF00) #LINE# #TAB# return crc
Decorator for all search filter options <code> def filter_opts(fun): ,#LINE# #TAB# for o in _filter_opts: #LINE# #TAB# #TAB# fun = o(fun) #LINE# #TAB# return fun
"Returns copy of DataFrame with stretched DateTimeIndex  <code> def stretch_signals(source, factor, start_time=None): ","#LINE# #TAB# df = source.copy() #LINE# #TAB# if start_time is None: #LINE# #TAB# #TAB# start_time = df.index.min() #LINE# #TAB# logger.debug('Use start time: {}'.format(start_time)) #LINE# #TAB# timedelta = df.index - start_time #LINE# #TAB# new_index = timedelta * factor + start_time #LINE# #TAB# df.set_index(new_index, inplace=True, verify_integrity=True) #LINE# #TAB# return df"
"Do n't mangle the wrong org by accident <code> def validate_master_id(org_client, spec): ","#LINE# #TAB# master_account_id = org_client.describe_organization()['Organization'][ #LINE# #TAB# #TAB# 'MasterAccountId'] #LINE# #TAB# if master_account_id != spec['master_account_id']: #LINE# #TAB# #TAB# errmsg = ( #LINE# #TAB# #TAB# #TAB# ""The Organization Master Account Id '%s' does not match the 'master_account_id' set in the spec-file"" #LINE# #TAB# #TAB# #TAB# % master_account_id) #LINE# #TAB# #TAB# raise RuntimeError(errmsg) #LINE# #TAB# return"
Load Manifold from Stanford PLY file  <code> def ply_load(fn): ,"#LINE# #TAB# m = Manifold() #LINE# #TAB# s = ct.c_char_p(fn.encode('utf-8')) #LINE# #TAB# if lib_py_gel.ply_load(s, m.obj): #LINE# #TAB# #TAB# return m #LINE# #TAB# return None"
"Get "" remote "" info for a single block of given backup , this is for "" Data "" surface  <code> def get_remote_data_array(backupID, blockNum): ",#LINE# #TAB# customer_idurl = packetid.CustomerIDURL(backupID) #LINE# #TAB# if backupID not in remote_files(): #LINE# #TAB# #TAB# return [0] * contactsdb.num_suppliers(customer_idurl=customer_idurl) #LINE# #TAB# if blockNum not in remote_files()[backupID]: #LINE# #TAB# #TAB# return [0] * contactsdb.num_suppliers(customer_idurl=customer_idurl) #LINE# #TAB# return remote_files()[backupID][blockNum]['D']
"Makes a Flask response with a XML encoded body <code> def output_xml(data, code, headers=None): ","#LINE# #TAB# resp = make_response(dumps({'response' :data}), code) #LINE# #TAB# resp.headers.extend(headers or {}) #LINE# #TAB# return resp"
"Format binary data into a string for debug purpose <code> def get_log_buffer(prefix, buff): ",#LINE# #TAB# log = prefix #LINE# #TAB# for i in buff: #LINE# #TAB# #TAB# log += str(ord(i) if PY2 else i) + '-' #LINE# #TAB# return log[:-1]
Helper method for screening keyword arguments <code> def ok_kwarg(val): ,#LINE# #TAB# #TAB# import keyword #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return str.isidentifier(val) and not keyword.iskeyword(val) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# return False
This Context Manager redirects STDOUT to a StringIO objects which is returned from the Context . On exit STDOUT is restored  <code> def capture_stdout(): ,#LINE# #TAB# stdout = sys.stdout #LINE# #TAB# try: #LINE# #TAB# #TAB# capture_out = StringIO() #LINE# #TAB# #TAB# sys.stdout = capture_out #LINE# #TAB# #TAB# yield capture_out #LINE# #TAB# finally: #LINE# #TAB# #TAB# sys.stdout = stdout
Find scene texture dependencies of the current scene that are not referenced : return : set < str > <code> def find_scene_textures(): ,"#LINE# #TAB# paths = set() #LINE# #TAB# for f in maya.cmds.ls(l=True, type='file'): #LINE# #TAB# #TAB# if maya.cmds.referenceQuery(f, isNodeReferenced=True): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# texture_path = maya.cmds.getAttr(os.path.normpath('.'.join([f, #LINE# #TAB# #TAB# #TAB# 'fileTextureName']))) #LINE# #TAB# #TAB# if texture_path: #LINE# #TAB# #TAB# #TAB# paths.add(texture_path) #LINE# #TAB# return paths"
"Returns a tuple with the file path and format found , or ( None , None ) <code> def find_file_format(file_name): ","#LINE# #TAB# for file_format in Format.ALLOWED: #LINE# #TAB# #TAB# file_path = '.'.join((file_name, file_format)) #LINE# #TAB# #TAB# if os.path.exists(file_path): #LINE# #TAB# #TAB# #TAB# return file_path, file_format #LINE# #TAB# return None, None"
Create the results DataFrame  <code> def initialise_commodity_sources(): ,"#LINE# #TAB# cols = pd.MultiIndex(levels=[[], []], codes=[[], []], names=['', '']) #LINE# #TAB# src = pd.DataFrame(columns=cols, index=range(1990, 2017)) #LINE# #TAB# return src"
"Sort the keys in a JSON - RPC response object  <code> def sort_response(response: Dict[str, Any]) -> OrderedDict: ","#LINE# #TAB# root_order = [""jsonrpc"", ""result"", ""error"", ""id""] #LINE# #TAB# error_order = [""code"", ""message"", ""data""] #LINE# #TAB# req = OrderedDict(sorted(response.items(), key=lambda k: root_order.index(k[0]))) #LINE# #TAB# if ""error"" in response: #LINE# #TAB# #TAB# req[""error""] = OrderedDict( #LINE# #TAB# #TAB# #TAB# sorted(response[""error""].items(), key=lambda k: error_order.index(k[0])) #LINE# #TAB# #TAB# ) #LINE# #TAB# return req"
"Generate the AWS4 auth string to sign for the request  <code> def get_sig_string(req, cano_req, scope): ","#LINE# #TAB# #TAB# amz_date = req.headers['x-amz-date'] #LINE# #TAB# #TAB# hsh = hashlib.sha256(cano_req.encode()) #LINE# #TAB# #TAB# sig_items = ['AWS4-HMAC-SHA256', amz_date, scope, hsh.hexdigest()] #LINE# #TAB# #TAB# sig_string = '\n'.join(sig_items) #LINE# #TAB# #TAB# return sig_string"
Get position in byte stream  <code> def cio_tell(cio): ,#LINE# #TAB# OPENJPEG.cio_tell.argtypes = [ctypes.POINTER(CioType)] #LINE# #TAB# OPENJPEG.cio_tell.restype = ctypes.c_int #LINE# #TAB# pos = OPENJPEG.cio_tell(cio) #LINE# #TAB# return pos
"Shrink the bounding box bb by factor in order to prevent unneeded work due to rounding  <code> def shrink_bb(bb, factor=_EPS): ","#LINE# #TAB# p = bb.get_points() #LINE# #TAB# p += factor * (np.diff(p) * np.array([1, -1])).T #LINE# #TAB# bb.set_points(p) #LINE# #TAB# return bb"
Get chrom as key  <code> def get_chr_key(chr_str: str): ,#LINE# #TAB# chr_str = chr_str.strip('chr') #LINE# #TAB# try: #LINE# #TAB# #TAB# chrom = int(chr_str) #LINE# #TAB# #TAB# chrom = '{:03d}'.format(chrom) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# chrom = chr_str #LINE# #TAB# return chrom
"Returns True if the given path resolves against the default URL resolver , False otherwise . This is a convenience method to make working with "" is this a match ? "" cases easier , avoiding unnecessarily indented try ... except blocks  <code> def is_valid_path(path, urlconf=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# resolve(path, urlconf) #LINE# #TAB# #TAB# return True #LINE# #TAB# except Resolver404: #LINE# #TAB# #TAB# return False"
Return whether or not this function is executed in a notebook environment  <code> def is_notebook(): ,#LINE# #TAB# if not IPython: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# shell = IPython.get_ipython().__class__.__name__ #LINE# #TAB# #TAB# if shell == 'ZMQInteractiveShell': #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif shell == 'TerminalInteractiveShell': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# except NameError: #LINE# #TAB# #TAB# return False
"Check and parse thickness specs as either a single [ s ] or a list of [ s , s , s , ... ] <code> def check_thickness(s): ","#LINE# #TAB# s = check_1d(s, 'thickness') #LINE# #TAB# if any(map(lambda d: d <= 0, s)): #LINE# #TAB# #TAB# raise Exception('Thickness cannot be 0 or negative') #LINE# #TAB# return s"
"Remove the nexus nve binding  <code> def remove_nexusnve_binding(vni, switch_ip, device_id): ","#LINE# #TAB# LOG.debug(""remove_nexusnve_binding() called"") #LINE# #TAB# session = bc.get_writer_session() #LINE# #TAB# binding = (session.query(nexus_models_v2.NexusNVEBinding). #LINE# #TAB# #TAB# #TAB# filter_by(vni=vni, switch_ip=switch_ip, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# device_id=device_id).one()) #LINE# #TAB# if binding: #LINE# #TAB# #TAB# session.delete(binding) #LINE# #TAB# #TAB# session.flush() #LINE# #TAB# #TAB# return binding"
"Tries to make a time out of the value . If impossible , returns None . : param value : A value of some ilk . : return : Date : rtype : datetime.date <code> def try_parse_date(value): ",#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# return parse_date(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None
"reverse the shuffled list with the original key : param lst : shuffled list : param key : shuffle key : return : original list <code> def shuffle_reverse(lst, key): ","#LINE# #TAB# random.seed(key) #LINE# #TAB# swap_list = [random.randint(0, len(lst) - 1) for _ in range(len(lst))] #LINE# #TAB# for i in range(len(lst) - 1, -1, -1): #LINE# #TAB# #TAB# to_swap = swap_list[i] #LINE# #TAB# #TAB# lst[i], lst[to_swap] = lst[to_swap], lst[i] #LINE# #TAB# return lst"
returns all the repositories in the database <code> def get_repositories(request): ,"#LINE# #TAB# return [{'id': repo.id, 'name': repo.name, 'linux_path': repo. #LINE# #TAB# #TAB# linux_path, 'osx_path': repo.osx_path, 'windows_path': repo. #LINE# #TAB# #TAB# windows_path} for repo in Repository.query.all()]"
Finds the most recently saved operative_config in a directory  <code> def get_latest_operative_config(restore_dir): ,"#LINE# #TAB# file_paths = tf.io.gfile.glob(os.path.join(restore_dir, #LINE# #TAB# #TAB# 'operative_config*')) #LINE# #TAB# get_iter = lambda file_path: int(file_path.split('-')[-1].split('.gin')[0]) #LINE# #TAB# return max(file_paths, key=get_iter) if file_paths else ''"
"For a given trace , return a mapping from activity to frequency in trace . Parameters ---------- trace : ` tuple ` of ` str ` a trace as a tuple of activities Returns ------- ` dict ` mapping from activity to frequency in trace <code> def activity_2_freq(trace): ",#LINE# #TAB# d = {} #LINE# #TAB# for a in trace: #LINE# #TAB# #TAB# if a not in d: #LINE# #TAB# #TAB# #TAB# d[a] = 0 #LINE# #TAB# #TAB# d[a] += 1 #LINE# #TAB# return d
Return a PEP 440-compliant version number from VERSION  <code> def get_version(version): ,"#LINE# #TAB# version = get_complete_version(version) #LINE# #TAB# main = get_main_version(version) #LINE# #TAB# sub = '' #LINE# #TAB# if version[3] != 'final': #LINE# #TAB# #TAB# mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc', 'dev': '.dev'} #LINE# #TAB# #TAB# sub = mapping[version[3]] + str(version[4]) #LINE# #TAB# return main + sub"
Show subcommand . Parameters : api : the API instance to use . Returns : int : always 0  <code> def subcommand_show(api: API) ->int: ,"#LINE# #TAB# downloads = api.get_downloads() #LINE# #TAB# print( #LINE# #TAB# #TAB# f""{'GID':<17} {'STATUS':<9} {'PROGRESS':>8} {'DOWN_SPEED':>12} {'UP_SPEED':>12} {'ETA':>8} NAME"" #LINE# #TAB# #TAB# ) #LINE# #TAB# for download in downloads: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# f'{download.gid:<17} {download.status:<9} {download.progress_string():>8} {download.download_speed_string():>12} {download.upload_speed_string():>12} {download.eta_string():>8} {download.name}' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return 0"
Returns a list of header strings that could be passed to a compiler <code> def pkg_config_header_strings(pkg_libraries): ,"#LINE# #TAB# _, _, header_dirs = pkg_config(pkg_libraries) #LINE# #TAB# header_strings = [] #LINE# #TAB# for header_dir in header_dirs: #LINE# #TAB# #TAB# header_strings.append('-I' + header_dir) #LINE# #TAB# return header_strings"
Parse the given rgb . txt file into a Python dict  <code> def parse_rgb_txt_file(path): ,"#LINE# #TAB# color_dict = {} #LINE# #TAB# with open(path, 'r') as rgb_txt: #LINE# #TAB# #TAB# for line in rgb_txt: #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line or line.startswith('!'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# parts = line.split() #LINE# #TAB# #TAB# #TAB# color_dict["" "".join(parts[3:])] = (int(parts[0]), int(parts[1]), int(parts[2])) #LINE# #TAB# return color_dict"
"Move the relationships to appropriate location in the props <code> def to_rest_rels(model, props): ",#LINE# #TAB# props['to_many'] = {} #LINE# #TAB# props['to_one'] = {} #LINE# #TAB# for key in model.to_one: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# props['to_one'][key] = props.pop(key) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# for key in model.to_many: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# props['to_many'][key] = props.pop(key) #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# continue
List of maps available in * * escher * * Returns : list : map names <code> def list_escher_maps(): ,#LINE# #TAB# import escher #LINE# #TAB# maps = escher.list_available_maps() #LINE# #TAB# return [entry['map_name'] for entry in maps]
Build a wallet configuration dictionary ( postgres specific )  <code> def wallet_config(wallet_name): ,#LINE# #TAB# storage_config = settings.INDY_CONFIG['storage_config'] #LINE# #TAB# wallet_config = settings.INDY_CONFIG['wallet_config'] #LINE# #TAB# wallet_config['id'] = wallet_name #LINE# #TAB# wallet_config['storage_config'] = storage_config #LINE# #TAB# wallet_config_json = json.dumps(wallet_config) #LINE# #TAB# return wallet_config_json
Determine if value is an integer  <code> def is_number(s): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# int(s) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"Return the flux command as string : param ` flux_path ` : the full path to the flux bin : param ` no_errors ` : a flag to determine if this a test run to ignore errors <code> def get_flux_cmd(flux_path, no_errors=False): ","#LINE# #TAB# flux_cmd = 'flux mini run' #LINE# #TAB# flux_ver = get_flux_version(flux_path, no_errors=no_errors) #LINE# #TAB# vers = [int(n) for n in flux_ver.split('.')] #LINE# #TAB# if vers[0] == 0 and vers[1] < 13: #LINE# #TAB# #TAB# flux_cmd = 'flux wreckrun' #LINE# #TAB# return flux_cmd"
Utility method to retrieve a list of server groups  <code> def server_group_list(request): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return api.nova.server_group_list(request) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# exceptions.handle(request, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# _('Unable to retrieve Nova server groups.')) #LINE# #TAB# #TAB# return []"
"Load a json file and add is parent dir to resulting dict  <code> def json_add_collection_dir(file_name, force=True): ","#LINE# #TAB# loaded = _json_from_file(file_name) #LINE# #TAB# set_func = loaded.__setitem__ if force else loaded.setdefault #LINE# #TAB# dir_path = os.path.dirname(file_name) #LINE# #TAB# set_func('absolute_path', dir_path) #LINE# #TAB# set_func('relative_path', os.path.basename(dir_path)) #LINE# #TAB# return loaded"
"Merge a dictionary ( b ) into another ( a ) and override any field in a which is also present in b : param a : dict to be updated : param b : dict with values to update : param path : : return : dict a <code> def override_dict(a, b, path=None): ","#LINE# #TAB# if path is None: #LINE# #TAB# #TAB# path = [] #LINE# #TAB# for key in b: #LINE# #TAB# #TAB# if key in a: #LINE# #TAB# #TAB# #TAB# if isinstance(a[key], dict) and isinstance(b[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# override_dict(a[key], b[key], path + [str(key)]) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# a[key] = b[key] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# a[key] = b[key] #LINE# #TAB# return a"
Decorate a delta and check that the file ids in it are unique . : return : A generator over delta  <code> def check_delta_unique_ids(delta): ,"#LINE# #TAB# ids = set() #LINE# #TAB# for item in delta: #LINE# #TAB# #TAB# length = len(ids) + 1 #LINE# #TAB# #TAB# ids.add(item[2]) #LINE# #TAB# #TAB# if len(ids) != length: #LINE# #TAB# #TAB# #TAB# raise errors.InconsistentDelta(item[0] or item[1], item[2], #LINE# #TAB# #TAB# #TAB# #TAB# 'repeated file_id') #LINE# #TAB# #TAB# yield item"
"Return a dict of units that match the filter  <code> def get_units(obs, filter_fn=None, owner=None, unit_type=None, tag=None): ","#LINE# #TAB# if unit_type and not isinstance(unit_type, (list, tuple)): #LINE# #TAB# #TAB# unit_type = unit_type, #LINE# #TAB# return {u.tag: u for u in obs.observation.raw_data.units if (filter_fn is #LINE# #TAB# #TAB# None or filter_fn(u)) and (owner is None or u.owner == owner) and ( #LINE# #TAB# #TAB# unit_type is None or u.unit_type in unit_type) and (tag is None or #LINE# #TAB# #TAB# u.tag == tag)}"
Return True if the element is a physical entity <code> def is_physical_entity(pe): ,"#LINE# #TAB# val = isinstance(pe, _bp('PhysicalEntity')) or \ #LINE# #TAB# #TAB# #TAB# isinstance(pe, _bpimpl('PhysicalEntity')) #LINE# #TAB# return val"
Given a SQLAlchemy Table object return a Shelf description suitable for passing to Shelf . from_config  <code> def introspect_table(table): ,"#LINE# #TAB# d = {} #LINE# #TAB# for c in table.columns: #LINE# #TAB# #TAB# if isinstance(c.type, String): #LINE# #TAB# #TAB# #TAB# d[c.name] = {'kind': 'Dimension', 'field': c.name} #LINE# #TAB# #TAB# if isinstance(c.type, (Integer, Float)): #LINE# #TAB# #TAB# #TAB# d[c.name] = {'kind': 'Metric', 'field': c.name} #LINE# #TAB# return d"
checks if a file is stored securely <code> def assert_secure_file(file): ,"#LINE# #TAB# if not is_secure_file(file): #LINE# #TAB# #TAB# msg = """""" #LINE# #TAB# #TAB# File {0} can be read by other users. #LINE# #TAB# #TAB# This is not secure. Please run 'chmod 600 ""{0}""'"""""" #LINE# #TAB# #TAB# raise SecurityError(dedent(msg).replace('\n', ' ').format(file)) #LINE# #TAB# return True"
"Parameter : String ( unicode or bytes ) . Returns : The ` text ` , with each instance of "" -- "" translated to an en - dash character , and each "" --- "" translated to an em - dash character  <code> def educate_dashes_old_school(text): ","#LINE# #TAB# text = re.sub('---', smartchars.emdash, text) #LINE# #TAB# text = re.sub('--', smartchars.endash, text) #LINE# #TAB# return text"
"Determine whether the given response indicates that the token is expired  <code> def _is_expired_token_response(cls, response): ","#LINE# #TAB# #TAB# EXPIRED_MESSAGE = ""Expired oauth2 access token"" #LINE# #TAB# #TAB# INVALID_MESSAGE = ""Invalid oauth2 access token"" #LINE# #TAB# #TAB# if response.status_code == 400: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# body = response.json() #LINE# #TAB# #TAB# #TAB# #TAB# if str(body.get('error_description')) in [EXPIRED_MESSAGE, INVALID_MESSAGE]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return False"
Encodes a path value  <code> def encode_path(s): ,"#LINE# #TAB# if isinstance(s, text_type): #LINE# #TAB# #TAB# s = s.encode('utf-8') #LINE# #TAB# if NUL in s: #LINE# #TAB# #TAB# raise TypeError('Null bytes are not allowed in paths') #LINE# #TAB# return s"
"Parse a list for non - matches to a regex  <code> def remove_regex(urls, regex): ","#LINE# #TAB# if not regex: #LINE# #TAB# #TAB# return urls #LINE# #TAB# if not isinstance(urls, (list, set, tuple)): #LINE# #TAB# #TAB# urls = [urls] #LINE# #TAB# try: #LINE# #TAB# #TAB# non_matching_urls = [url for url in urls if not re.search(regex, url)] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return [] #LINE# #TAB# return non_matching_urls"
"Check if criterias for Likely Benign are fullfilled <code> def is_likely_benign(bs_terms, bp_terms): ",#LINE# #TAB# if bs_terms: #LINE# #TAB# #TAB# if bp_terms: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# if len(bp_terms) >= 2: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Set all variables to live setting <code> def set_live(cls): ,"#LINE# #TAB# cls.is_test = False #LINE# #TAB# cls.course_ids = [] #LINE# #TAB# cls.load_section_ids() #LINE# #TAB# print(' '.join([' LIVE ' for _ in range(0, 5)])) #LINE# #TAB# CanvasHacks.testglobals.TEST = False"
"Burst , emit from center , velocity in fixed angle and random speed <code> def emitter_9(): ","#LINE# #TAB# e = arcadeplus.Emitter(center_xy=CENTER_POS, emit_controller=arcadeplus #LINE# #TAB# #TAB# .EmitBurst(BURST_PARTICLE_COUNT // 4), particle_factory=lambda #LINE# #TAB# #TAB# emitter: arcadeplus.LifetimeParticle(filename_or_texture=TEXTURE, #LINE# #TAB# #TAB# change_xy=arcadeplus.rand_vec_magnitude(45, 1.0, 4.0), lifetime= #LINE# #TAB# #TAB# DEFAULT_PARTICLE_LIFETIME, scale=DEFAULT_SCALE, alpha=DEFAULT_ALPHA)) #LINE# #TAB# return emitter_9.__doc__, e"
"Recursively finds the most recent timestamp in the given directory  <code> def get_last_modified_timestamp(path, ignore=None): ","#LINE# #TAB# ignore = ignore or [] #LINE# #TAB# if not isinstance(path, six.string_types): #LINE# #TAB# #TAB# return #LINE# #TAB# ignore_str = '' #LINE# #TAB# if ignore: #LINE# #TAB# #TAB# assert isinstance(ignore, (tuple, list)) #LINE# #TAB# #TAB# ignore_str = ' '.join(""! -name '%s'"" % _ for _ in ignore) #LINE# #TAB# cmd = 'find ""'+path+'"" ' + ignore_str + ' -type f -printf ""%T@ %p\n"" | sort -n | tail -1 | cut -f 1 -d "" ""' #LINE# #TAB# ret = subprocess.check_output(cmd, shell=True) #LINE# #TAB# try: #LINE# #TAB# #TAB# ret = round(float(ret), 2) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return #LINE# #TAB# return ret"
"Clean input toml , increasing the chance for beautiful output <code> def clean_toml_text(input_toml: str) ->str: ","#LINE# #TAB# cleaned = re.sub('[\\r\\n][\\r\\n]{2,}', '\n\n', input_toml) #LINE# #TAB# return '\n' + cleaned.strip() + '\n'"
Construct an indented name for the node given its path  <code> def index_to_nice_name(index): ,#LINE# #TAB# if index: #LINE# #TAB# #TAB# return '&nbsp;' * 4 * (len(index) - 1) + index[-1] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Everything'
"Creates a magnitude spectrum from image params : image : A numpy ndarray , which has 2 or 3 dimensions ( BGR ) return : A numpy ndarray , which has 2 dimensions <code> def create_magnitude_spectrum(image): ","#LINE# #TAB# image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #LINE# #TAB# dft = cv2.dft(np.float32(image), flags=cv2.DFT_COMPLEX_OUTPUT) #LINE# #TAB# dft_shift = np.fft.fftshift(dft) #LINE# #TAB# magnitude_spectrum = 20 * np.log(cv2.magnitude(dft_shift[:, :, (0)], #LINE# #TAB# #TAB# dft_shift[:, :, (1)])) #LINE# #TAB# return magnitude_spectrum"
"Return True if the xlog is a .history file , False otherwise It supports either a full file path or a simple file name . : param str path : the file name to test : rtype : bool <code> def is_history_file(path): ",#LINE# #TAB# match = _xlog_re.search(os.path.basename(path)) #LINE# #TAB# if match and match.group(0).endswith('.history'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Translate a CommException into something more useful to the user <code> def translate_exception(exp: Exception) ->Exception: ,"#LINE# #TAB# string_err = str(exp) #LINE# #TAB# if 'No dongle found' in string_err: #LINE# #TAB# #TAB# return Exception('Ledger Nano not found') #LINE# #TAB# elif '6804' in string_err: #LINE# #TAB# #TAB# return Exception('Ledger appears to be locked?') #LINE# #TAB# elif any([(x in string_err) for x in ['6700', '6d00']]): #LINE# #TAB# #TAB# return Exception('Please open the Ethereum app on your Ledger device') #LINE# #TAB# elif '6a80' in string_err: #LINE# #TAB# #TAB# return Exception( #LINE# #TAB# #TAB# #TAB# 'General failure: Invalid transaction, contract data not allowed in settings, or something else?' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# return exp"
Unlock a locked user <code> def users_unlock(login_or_id): ,"#LINE# #TAB# okta_manager.call_okta(f'/users/{login_or_id}/lifecycle/unlock', REST.post) #LINE# #TAB# return f""User '{login_or_id}' unlocked."""
This function extracts temporal derivative features which are first and second derivatives  <code> def extract_derivative_feature(feature): ,"#LINE# #TAB# first_derivative_feature = processing.derivative_extraction( #LINE# #TAB# #TAB# feature, DeltaWindows=2) #LINE# #TAB# second_derivative_feature = processing.derivative_extraction( #LINE# #TAB# #TAB# first_derivative_feature, DeltaWindows=2) #LINE# #TAB# feature_cube = np.concatenate( #LINE# #TAB# #TAB# (feature[:, :, None], first_derivative_feature[:, :, None], #LINE# #TAB# #TAB# second_derivative_feature[:, :, None]), #LINE# #TAB# #TAB# axis=2) #LINE# #TAB# return feature_cube"
Retrieve the bounding box of a volume in millimetres  <code> def get_bounding_box(in_file): ,"#LINE# #TAB# from itertools import product #LINE# #TAB# import nibabel as nib #LINE# #TAB# import numpy as np #LINE# #TAB# img = nib.load(in_file) #LINE# #TAB# corners = np.array(list(product([0, 1], repeat=3))) #LINE# #TAB# corners = corners * (np.array(img.shape[:3]) - 1) #LINE# #TAB# corners = img.affine.dot(np.hstack([corners, np.ones((8, 1))]).T).T[:, :3] #LINE# #TAB# low_corner = np.min(corners, axis=0) #LINE# #TAB# high_corner = np.max(corners, axis=0) #LINE# #TAB# return [low_corner.tolist(), high_corner.tolist()]"
This function converts the ' yes ' and ' no ' YAML values to traditional Boolean values  <code> def convert_yaml_to_bool(_yaml_bool_value): ,"#LINE# #TAB# true_values = ['yes', 'true'] #LINE# #TAB# if _yaml_bool_value.lower() in true_values: #LINE# #TAB# #TAB# _bool_value = True #LINE# #TAB# else: #LINE# #TAB# #TAB# _bool_value = False #LINE# #TAB# return _bool_value"
"registers the end of a request <code> def end_request(req, collector_addr='tcp://127.0.0.2:2345', prefix='my_app'): ","#LINE# #TAB# req_end = time() #LINE# #TAB# hreq = hash(req) #LINE# #TAB# if hreq in requests: #LINE# #TAB# #TAB# req_time = req_end - requests[hreq] #LINE# #TAB# #TAB# req_time *= 1000 #LINE# #TAB# #TAB# del requests[hreq] #LINE# #TAB# #TAB# collector = get_context().socket(zmq.PUSH) #LINE# #TAB# #TAB# collector.connect(collector_addr) #LINE# #TAB# #TAB# collector.send_multipart([prefix, str(req_time)]) #LINE# #TAB# #TAB# collector.close() #LINE# #TAB# #TAB# return req_time"
"Return the min - isr for topic or None if not specified <code> def get_min_isr(zk, topic): ",#LINE# #TAB# ISR_CONF_NAME = 'min.insync.replicas' #LINE# #TAB# try: #LINE# #TAB# #TAB# config = zk.get_topic_config(topic) #LINE# #TAB# except NoNodeError: #LINE# #TAB# #TAB# return None #LINE# #TAB# if ISR_CONF_NAME in config['config']: #LINE# #TAB# #TAB# return int(config['config'][ISR_CONF_NAME]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
Calculate min and max of array with guards for nan and inf  <code> def safe_minmax(values): ,"#LINE# #TAB# isfinite = np.isfinite(values) #LINE# #TAB# if np.any(isfinite): #LINE# #TAB# #TAB# values = values[isfinite] #LINE# #TAB# minval = np.min(values) #LINE# #TAB# maxval = np.max(values) #LINE# #TAB# return minval, maxval"
