nl,code
Return True if the specified file is small enough to process in memory  <code> def is_tiny(path): ,#LINE# #TAB# size_in_mib = os.stat(path).st_size / 1000000.0 #LINE# #TAB# return _is_gzipped(path) and size_in_mib < 10 or size_in_mib < 100
"show code of func or module <code> def show_code(func, get_txt=False): ",#LINE# #TAB# import inspect #LINE# #TAB# txt = inspect.getsource(func) #LINE# #TAB# if not get_txt: #LINE# #TAB# #TAB# print(txt) #LINE# #TAB# else: #LINE# #TAB# #TAB# return txt
"Check the given path , printing out any warnings detected . @return : the number of warnings printed <code> def check_path(filename): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return check(open(filename, 'rb').read() + b'\n', filename) #LINE# #TAB# except IOError as msg: #LINE# #TAB# #TAB# print('%s: %s' % (filename, msg.args[1]), file=sys.stderr) #LINE# #TAB# #TAB# return 1"
"Normalize the datatype if the type information is in the comm  <code> def normalize_datatype(normalizer, value, instance, schema): ","#LINE# #TAB# if isinstance(instance, dict) and 'datatype' not in instance: #LINE# #TAB# #TAB# type_keys = list(metaschema.get_metaschema()['properties'].keys()) #LINE# #TAB# #TAB# datatype = {} #LINE# #TAB# #TAB# for k in type_keys: #LINE# #TAB# #TAB# #TAB# if k in instance: #LINE# #TAB# #TAB# #TAB# #TAB# datatype[k] = instance.pop(k) #LINE# #TAB# #TAB# if datatype: #LINE# #TAB# #TAB# #TAB# instance['datatype'] = datatype #LINE# #TAB# return instance"
"Convert a file path into a more readable title  <code> def filename_to_title(filename, split_char='_'): ",#LINE# #TAB# filename = Path(filename).with_suffix('').name #LINE# #TAB# filename_parts = filename.split(split_char) #LINE# #TAB# try: #LINE# #TAB# #TAB# int(filename_parts[0]) #LINE# #TAB# #TAB# if len(filename_parts) > 1: #LINE# #TAB# #TAB# #TAB# filename_parts = filename_parts[1:] #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# title = ' '.join(ii.capitalize() for ii in filename_parts) #LINE# #TAB# return title
"Generate a unique |state id| for the current state of the given object . Parameters ---------- obj The object for which to compute the state sid . debug If ` True ` , produce some debug output . Returns ------- The generated state i d  <code> def generate_sid(obj, debug=False): ","#LINE# #TAB# sid_generator = _SIDGenerator() #LINE# #TAB# return sid_generator.generate(obj, debug, ())[0]"
Checks for undeclared file conflicts in the given packages  <code> def cmd_check_conflicts(args): ,#LINE# #TAB# with dependency_analyzer_from_args(args) as analyzer: #LINE# #TAB# #TAB# conflicts = analyzer.find_conflicts() #LINE# #TAB# if conflicts: #LINE# #TAB# #TAB# sys.stderr.write(u'Undeclared file conflicts:\n') #LINE# #TAB# #TAB# sys.stderr.write(u'\n'.join(conflicts) + u'\n') #LINE# #TAB# #TAB# return 3 #LINE# #TAB# return 0
Extracts the name which may be embedded for a Jinja2 filter node <code> def grok_filter_name(element): ,"#LINE# #TAB# e_name = None #LINE# #TAB# if element.name == 'default': #LINE# #TAB# #TAB# if isinstance(element.node, jinja2.nodes.Getattr): #LINE# #TAB# #TAB# #TAB# e_name = element.node.node.name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# e_name = element.node.name #LINE# #TAB# return e_name"
"Finds the ` xref ` attribute value of each article if it exists <code> def find_references(generator: ArticlesGenerator) ->Dict[str, Xref]: ","#LINE# #TAB# references = dict() #LINE# #TAB# article: Article #LINE# #TAB# for article in generator.articles: #LINE# #TAB# #TAB# if hasattr(article, 'xref'): #LINE# #TAB# #TAB# #TAB# references[article.xref] = Xref(article.url, 'published', #LINE# #TAB# #TAB# #TAB# #TAB# article.title) #LINE# #TAB# draft: Article #LINE# #TAB# for draft in generator.drafts: #LINE# #TAB# #TAB# if hasattr(draft, 'xref'): #LINE# #TAB# #TAB# #TAB# references[draft.xref] = Xref(draft.url, 'draft', draft.title) #LINE# #TAB# return references"
"count the number of terms in the study group <code> def count_terms(geneset, assoc, obo_dag): ",#LINE# #TAB# term_cnt = Counter() #LINE# #TAB# for gene in (g for g in geneset if g in assoc): #LINE# #TAB# #TAB# for goid in assoc[gene]: #LINE# #TAB# #TAB# #TAB# if goid in obo_dag: #LINE# #TAB# #TAB# #TAB# #TAB# term_cnt[obo_dag[goid].id] += 1 #LINE# #TAB# return term_cnt
Clean a string for addition into the translation dictionary . Leading and trailing whitespace is removed . Newlines are converted to the UNIX style . Args : text ( str ) : String to be cleaned . Returns : A cleaned string with number removed  <code> def clean_string(text): ,"#LINE# #TAB# text = text.strip() #LINE# #TAB# text = text.replace('\r\n', '\n') #LINE# #TAB# text = text.replace('\r', '\n') #LINE# #TAB# text = space_newline_fix(text) #LINE# #TAB# text = newline_space_fix(text) #LINE# #TAB# text = space_space_fix(text) #LINE# #TAB# return text"
generate instance_id to instance_name map . If an instance has no name it will have value ' unknown '  <code> def get_instance_id_to_name_map(instance_info): ,#LINE# #TAB# instance_id_to_name = {} #LINE# #TAB# for instance_id in instance_info: #LINE# #TAB# #TAB# instance = instance_info[instance_id] #LINE# #TAB# #TAB# instance_name = 'unnamed' #LINE# #TAB# #TAB# if 'Tags' in instance: #LINE# #TAB# #TAB# #TAB# for tag in instance['Tags']: #LINE# #TAB# #TAB# #TAB# #TAB# if tag['Key'] == 'Name': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# instance_name = tag['Value'] #LINE# #TAB# #TAB# instance_id_to_name[instance['InstanceId']] = instance_name #LINE# #TAB# return instance_id_to_name
"for use when calling command - line scripts from withing a program . if a variable is present add its proper command_line flag . return a string  <code> def add_flag(var, flag): ","#LINE# #TAB# if var: #LINE# #TAB# #TAB# var = flag + "" "" + str(var) #LINE# #TAB# else: #LINE# #TAB# #TAB# var = """" #LINE# #TAB# return var"
"Helper for iterating only nonempty lines without line breaks <code> def iter_lines(file_like: Iterable[str]) ->Generator[str, None, None]: ",#LINE# #TAB# for line in file_like: #LINE# #TAB# #TAB# line = line.rstrip('\r\n') #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# yield line
"( item1 , deps , func ) , ( item1 , ... ) -- > { item1 , ( set(deps ) , set(funcs ) ) } <code> def consolidate_relations(relations): ","#LINE# #TAB# rels = defaultdict() #LINE# #TAB# rels.default_factory = lambda : (set(), set()) #LINE# #TAB# for item, deps, func in relations: #LINE# #TAB# #TAB# pdes, pfuncs = rels[item[_root_len:]] #LINE# #TAB# #TAB# pdes.update([d[_root_len:] for d in deps]) #LINE# #TAB# #TAB# pfuncs.add(func) #LINE# #TAB# return rels"
Checks if the response is an error frame  <code> def is_error(response): ,#LINE# #TAB# if response[0][PN532_FRAME_POSITION_LENGTH] == 1: #LINE# #TAB# #TAB# if response[0][PN532_FRAME_POSITION_LENGTH_CHECKSUM] == 255: #LINE# #TAB# #TAB# #TAB# if response[0][PN532_FRAME_POSITION_FRAME_IDENTIFIER] == 127: #LINE# #TAB# #TAB# #TAB# #TAB# if response[0][PN532_FRAME_POSITION_DATA_START] == 129: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
Strip leading and trailing angle brackets  <code> def strip_brackets(s): ,#LINE# #TAB# if s.startswith('<'): #LINE# #TAB# #TAB# s = s[1:] #LINE# #TAB# if s.endswith('>'): #LINE# #TAB# #TAB# s = s[:-1] #LINE# #TAB# return s
Set up command line parsing  <code> def parse_command_line_arguments(): ,"#LINE# #TAB# parser = argparse.ArgumentParser(description= #LINE# #TAB# #TAB# 'Calculate parallax error for given G and (V-I)') #LINE# #TAB# parser.add_argument('gmag', help='G-band magnitude of source', type=float) #LINE# #TAB# parser.add_argument('vmini', help='(V-I) colour of source', type=float) #LINE# #TAB# args = vars(parser.parse_args()) #LINE# #TAB# return args"
Renders the app list for the admin dropdown menu navigation  <code> def admin_dropdown_menu(context): ,#LINE# #TAB# user = context['request'].user #LINE# #TAB# if user.is_staff: #LINE# #TAB# #TAB# context['dropdown_menu_app_list'] = admin_app_list(context['request']) #LINE# #TAB# #TAB# if user.is_superuser: #LINE# #TAB# #TAB# #TAB# sites = Site.objects.all() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sites = user.sitepermissions.get().sites.all() #LINE# #TAB# #TAB# context['dropdown_menu_sites'] = list(sites) #LINE# #TAB# #TAB# context['dropdown_menu_selected_site_id'] = current_site_id() #LINE# #TAB# #TAB# return context
Adjust the vendor field to make it human readable <code> def adjust_raw_vendor(info): ,#LINE# #TAB# if 'CPU implementer' not in info: #LINE# #TAB# #TAB# return #LINE# #TAB# arm_vendors = TARGETS_JSON['conversions']['arm_vendors'] #LINE# #TAB# arm_code = info['CPU implementer'] #LINE# #TAB# if arm_code in arm_vendors: #LINE# #TAB# #TAB# info['CPU implementer'] = arm_vendors[arm_code]
Splits the str given and returns a properly stripped list of the comma separated values  <code> def list_from_csv(comma_separated_str): ,"#LINE# #TAB# if comma_separated_str: #LINE# #TAB# #TAB# return [v.strip() for v in comma_separated_str.split(',') if v.strip()] #LINE# #TAB# return []"
Separate tiles by suits and count them : param tiles_34 : array of tiles to count : return : dict <code> def count_tiles_by_suits(tiles_34): ,"#LINE# #TAB# suits = [{'count': 0, 'name': 'sou', 'function': is_sou}, {'count': 0, #LINE# #TAB# #TAB# 'name': 'man', 'function': is_man}, {'count': 0, 'name': 'pin', #LINE# #TAB# #TAB# 'function': is_pin}, {'count': 0, 'name': 'honor', 'function': #LINE# #TAB# #TAB# is_honor}] #LINE# #TAB# for x in range(0, 34): #LINE# #TAB# #TAB# tile = tiles_34[x] #LINE# #TAB# #TAB# if not tile: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for item in suits: #LINE# #TAB# #TAB# #TAB# if item['function'](x): #LINE# #TAB# #TAB# #TAB# #TAB# item['count'] += tile #LINE# #TAB# return suits"
"Get the human readable value for the DB value from a choice tuple . Args : choices ( tuple ) : The choice tuple given in the model . db_val : The respective DB value . Returns : The matching human readable value  <code> def get_hr_val(choices, db_val): ",#LINE# #TAB# for pair in choices: #LINE# #TAB# #TAB# if pair[0] == db_val: #LINE# #TAB# #TAB# #TAB# return pair[1] #LINE# #TAB# return None
"Return list of dates within a specified range inclusive  <code> def date_range(start, end, boo): ","#LINE# earliest = datetime.strptime(start.replace('-', ' '), '%Y %m %d') #LINE# latest = datetime.strptime(end.replace('-', ' '), '%Y %m %d') #LINE# num_days = (latest - earliest).days + 1 #LINE# all_days = [latest - timedelta(days=x) for x in range(num_days)] #LINE# all_days.reverse() #LINE# output = [] #LINE# if boo: #LINE# #TAB# for d in all_days: #LINE# #TAB# output.append(int(str(d).replace('-', '')[:8])) #LINE# else: #LINE# #TAB# for d in all_days: #LINE# #TAB# output.append(str(d)[:10]) #LINE# return output"
return a random element from a list <code> def get_random_val_from_list(ls): ,"#LINE# #TAB# col_len = len(ls) #LINE# #TAB# index = randint(0, col_len - 1) #LINE# #TAB# return ls[index]"
"Adds a QLabel contaning text to the given menu <code> def add_menu_label(menu, text): ","#LINE# #TAB# qaw = QWidgetAction(menu) #LINE# #TAB# lab = QLabel(text, menu) #LINE# #TAB# qaw.setDefaultWidget(lab) #LINE# #TAB# lab.setAlignment(Qt.AlignCenter) #LINE# #TAB# lab.setFrameShape(QFrame.StyledPanel) #LINE# #TAB# lab.setFrameShadow(QFrame.Sunken) #LINE# #TAB# menu.addAction(qaw) #LINE# #TAB# return lab"
"So we can support potential circular import problems , by using normal import_string import specification  <code> def load_target_class(cls, class_definition): ","#LINE# #TAB# associated_class = super(Factory, cls).load_target_class(class_definition) #LINE# #TAB# if isinstance(associated_class, string_types) and '.' in associated_class: #LINE# #TAB# #TAB# if cls._associated_model is None: #LINE# #TAB# #TAB# #TAB# cls._associated_model = import_string(associated_class) #LINE# #TAB# #TAB# #TAB# cls._associated_model.FACTORY_CLASS = cls #LINE# #TAB# #TAB# return cls._associated_model #LINE# #TAB# if associated_class and associated_class.FACTORY_CLASS is None: #LINE# #TAB# #TAB# associated_class.FACTORY_CLASS = cls #LINE# #TAB# return associated_class"
Yes / no dialog with message <code> def yesno_box(message): ,"#LINE# #TAB# return ctypes.windll.user32.MessageBoxA(0, str(message), 'Question', 1 #LINE# #TAB# #TAB# ) == 1"
Create the set of pencils over a domain . Parameters ---------- domain : domain object Problem domain Returns ------- pencils : list Pencil objects <code> def build_pencils(domain): ,"#LINE# #TAB# trans_shape = domain.local_coeff_shape[:-1] #LINE# #TAB# indices = np.ndindex(*trans_shape) #LINE# #TAB# pencils = [] #LINE# #TAB# scales = domain.remedy_scales(1) #LINE# #TAB# start = domain.distributor.coeff_layout.start(scales)[:-1] #LINE# #TAB# for index in indices: #LINE# #TAB# #TAB# pencils.append(Pencil(domain, index, start + index)) #LINE# #TAB# return pencils"
"Return formatted sample tag from filename using regex  <code> def get_tag(filename, tag_expr, tag_format): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return tag_expr.search(filename).expand(tag_format) #LINE# #TAB# except: #LINE# #TAB# #TAB# return filename
get a resource group name from a VMSS ID string <code> def get_rg_from_id(vmss_id): ,"#LINE# #TAB# rgname = re.search('Groups/(.+?)/providers', vmss_id).group(1) #LINE# #TAB# print('Resource group: ' + rgname) #LINE# #TAB# return rgname"
"Given a value , determine if it can represent BED block sizes . : param value : a value to check : type value : str : return : if the specified value can represent BED block sizes : rtype : bool <code> def is_block_sizes(value): ","#LINE# #TAB# size_values = value.split(',') #LINE# #TAB# if not all([autosql.is_int(x) for x in size_values]): #LINE# #TAB# #TAB# return False #LINE# #TAB# size_numbers = [int(x) for x in size_values] #LINE# #TAB# for value in size_numbers: #LINE# #TAB# #TAB# if not value > 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"a - a ' : chloride sulfate [ PP87ii ]  <code> def theta_cl_so4_pp87ii(T, P): ","#LINE# #TAB# theta = 0.03 #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return theta, valid"
"Returns a list of videos pageNo : page number , start from 0 pageSize : number of videos per page <code> def all_list(): ","#LINE# #TAB# pageNo = request.json['pageNo'] #LINE# #TAB# pageSize = request.json['pageSize'] #LINE# #TAB# if pageSize > 50: #LINE# #TAB# #TAB# pageSize = 50 #LINE# #TAB# offset = pageNo * pageSize #LINE# #TAB# video_list = db.session.query(Video).filter_by(is_public=1).order_by(Video #LINE# #TAB# #TAB# .upload_time.desc()).limit(pageSize).offset(offset).all() #LINE# #TAB# video_list = list(map(lambda v: v.to_dict(), video_list)) #LINE# #TAB# return {'video_list': video_list}"
"Given a docname path , pick apart and return name of parent <code> def parse_parent(docname): ",#LINE# #TAB# lineage = docname.split('/') #LINE# #TAB# lineage_count = len(lineage) #LINE# #TAB# if docname == 'index': #LINE# #TAB# #TAB# parent = None #LINE# #TAB# elif lineage_count == 1: #LINE# #TAB# #TAB# parent = 'index' #LINE# #TAB# elif lineage_count == 2 and lineage[-1] == 'index': #LINE# #TAB# #TAB# parent = 'index' #LINE# #TAB# elif lineage_count == 2: #LINE# #TAB# #TAB# parent = lineage[0] + '/index' #LINE# #TAB# elif lineage[-1] == 'index': #LINE# #TAB# #TAB# parent = '/'.join(lineage[:-2]) + '/index' #LINE# #TAB# else: #LINE# #TAB# #TAB# parent = '/'.join(lineage[:-1]) + '/index' #LINE# #TAB# return parent
Returns the node name  <code> def ast_name(node: ast.AST) ->str: ,"#LINE# #TAB# match = re.match(AST_PATTERN, str(node)) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return match.group(1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Unknown'"
"Merges two python dicts by making a copy of first then updating with second . Returns a copy  <code> def merge_two_dicts(first_dict, second_dict): ",#LINE# #TAB# return_dict = first_dict.copy() #LINE# #TAB# return_dict.update(second_dict) #LINE# #TAB# return return_dict
Parse configuration and add Rflink sensor devices  <code> def devices_from_config(domain_config): ,"#LINE# #TAB# devices = [] #LINE# #TAB# for device_id, config in domain_config[CONF_DEVICES].items(): #LINE# #TAB# #TAB# device = RflinkBinarySensor(device_id, **config) #LINE# #TAB# #TAB# devices.append(device) #LINE# #TAB# return devices"
Read Synchronization File and return sample stamp and time <code> def read_snc(snc_file): ,"#LINE# #TAB# snc_raw_dtype = dtype([('sampleStamp', '<i'), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# ('sampleTime', '<q')]) #LINE# #TAB# with snc_file.open('rb') as f: #LINE# #TAB# #TAB# f.seek(352) #LINE# #TAB# #TAB# snc_raw = fromfile(f, dtype=snc_raw_dtype) #LINE# #TAB# sampleStamp = snc_raw['sampleStamp'] #LINE# #TAB# sampleTime = asarray([_filetime_to_dt(x) for x in snc_raw['sampleTime']]) #LINE# #TAB# return sampleStamp, sampleTime"
"Returns a unique service status <code> def get_svc(rcd, service_status): ","#LINE# #TAB# ena = None #LINE# #TAB# lines = __salt__['cmd.run']('{0} rcvar'.format(rcd)).splitlines() #LINE# #TAB# for rcvar in lines: #LINE# #TAB# #TAB# if rcvar.startswith('$') and '={0}'.format(service_status) in rcvar: #LINE# #TAB# #TAB# #TAB# ena = 'yes' #LINE# #TAB# #TAB# elif rcvar.startswith('#'): #LINE# #TAB# #TAB# #TAB# svc = rcvar.split(' ', 1)[1] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# if ena and svc: #LINE# #TAB# #TAB# return svc #LINE# #TAB# return None"
get iterator for the reaction participants ( reactants + products ) @type reaction : libsbml . Reaction @rtype : Iterator <code> def get_participants(reaction): ,#LINE# #TAB# for s in reaction.getListOfReactants(): #LINE# #TAB# #TAB# yield s #LINE# #TAB# for s in reaction.getListOfProducts(): #LINE# #TAB# #TAB# yield s
"A function to calculate a hash value of multiple integer values , not used at the moment Parameters ---------- input_list Returns ------- <code> def int_array_hash(input_list): ","#LINE# #TAB# list_len = len(input_list) #LINE# #TAB# arr_len = len(input_list[0]) #LINE# #TAB# mult_arr = np.full(arr_len, 1000003, dtype=np.long) #LINE# #TAB# value_arr = np.full(arr_len, 3430008, dtype=np.long) #LINE# #TAB# for i, current_arr in enumerate(input_list): #LINE# #TAB# #TAB# index = list_len - i - 1 #LINE# #TAB# #TAB# value_arr ^= current_arr #LINE# #TAB# #TAB# value_arr *= mult_arr #LINE# #TAB# #TAB# mult_arr += 82520 + index + index #LINE# #TAB# value_arr += 97531 #LINE# #TAB# result_carray = bcolz.carray(value_arr) #LINE# #TAB# del value_arr #LINE# #TAB# return result_carray"
"Checks configuration and if current machine is not specified , tries to set it . Returns ------- str Current machine  <code> def find_if_current_machine(): ","#LINE# #TAB# current_machine = ConfigBuild.current_machine #LINE# #TAB# if not isinstance(current_machine, bool): #LINE# #TAB# #TAB# from os import getcwd #LINE# #TAB# #TAB# from os.path import basename #LINE# #TAB# #TAB# return basename(getcwd()).startswith('pip-\u200c\u200b') #LINE# #TAB# return current_machine"
Walk an unpacked egg s contents skipping the metadata directory <code> def walk_egg(egg_dir): ,"#LINE# #TAB# walker = sorted_walk(egg_dir) #LINE# #TAB# base, dirs, files = next(walker) #LINE# #TAB# if 'EGG-INFO' in dirs: #LINE# #TAB# #TAB# dirs.remove('EGG-INFO') #LINE# #TAB# yield base, dirs, files #LINE# #TAB# for bdf in walker: #LINE# #TAB# #TAB# yield bdf"
"Detect whether given attribute field Parameters ---------- gff_line ffs Returns ------- <code> def guess_kind_of_gff(gff_line: str, ffs=['gff3', 'gtf']): ","#LINE# #TAB# if type(gff_line) != GffLine: #LINE# #TAB# #TAB# gff_line = GffLine(gff_line) #LINE# #TAB# attrib = gff_line.attributes #LINE# #TAB# ffs = ffs.copy() #LINE# #TAB# try: #LINE# #TAB# #TAB# ff = ffs.pop() #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# raise UnsupportedFile(""This file doesn't look like gtf/gff"") #LINE# #TAB# try: #LINE# #TAB# #TAB# attributes_parser(attrib, ff) #LINE# #TAB# except ParseError: #LINE# #TAB# #TAB# ff = guess_kind_of_gff(gff_line, ffs) #LINE# #TAB# return ff"
"Ensure setting match minimum resources required for used programs  <code> def ensure_min_resources(progs, cores, memory, min_memory): ","#LINE# #TAB# for p in progs: #LINE# #TAB# #TAB# if p in min_memory: #LINE# #TAB# #TAB# #TAB# if not memory or cores * memory < min_memory[p]: #LINE# #TAB# #TAB# #TAB# #TAB# memory = float(min_memory[p]) / cores #LINE# #TAB# return cores, memory"
"Get the content of a file Return string content <code> def file_get_contents(filename, mode='r'): ","#LINE# #TAB# content = '' #LINE# #TAB# if os.path.exists(filename): #LINE# #TAB# #TAB# fp = open(filename, mode) #LINE# #TAB# #TAB# content = fp.read() #LINE# #TAB# #TAB# fp.close() #LINE# #TAB# return content"
"Delete the function that is associated with ` name ` from all instances of this class Args : name ( str ) : A name corresponding to the desired function . Raises : KeyError : If no function corresponding to ` name ` exists  <code> def remove_function(cls, name): ","#LINE# #TAB# fun_name, length = cls._convert_name(name) #LINE# #TAB# if not hasattr(cls, fun_name): #LINE# #TAB# #TAB# raise KeyError('A function corresponding to the name `' + name + #LINE# #TAB# #TAB# #TAB# '` does not exists') #LINE# #TAB# delattr(cls, fun_name) #LINE# #TAB# if cls.static_longest_name == length: #LINE# #TAB# #TAB# cls.static_longest_name = cls._compute_static_longest_name() #LINE# #TAB# return cls"
Simple function to test if the JIP software bibaries are present . Parameters ---------- jip_dir : str the JIP binaries directory . Returns ------- status : bool the JIP installation status  <code> def check_jip_install(jipdir): ,"#LINE# #TAB# status = True #LINE# #TAB# for name in ('jip', 'align'): #LINE# #TAB# #TAB# binary_path = os.path.join(jipdir, name) #LINE# #TAB# #TAB# if not os.path.isfile(binary_path): #LINE# #TAB# #TAB# #TAB# status = False #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return status"
"Compute checksum by given image path and algorithm  <code> def compute_image_checksum(image_path, algorithm='md5'): ","#LINE# #TAB# time_start = time.time() #LINE# #TAB# LOG.debug('Start computing %(algo)s checksum for image %(image)s.', { #LINE# #TAB# #TAB# 'algo': algorithm, 'image': image_path}) #LINE# #TAB# checksum = fileutils.compute_file_checksum(image_path, algorithm=algorithm) #LINE# #TAB# time_elapsed = time.time() - time_start #LINE# #TAB# LOG.debug( #LINE# #TAB# #TAB# 'Computed %(algo)s checksum for image %(image)s in %(delta).2f seconds, checksum value: %(checksum)s.' #LINE# #TAB# #TAB# , {'algo': algorithm, 'image': image_path, 'delta': time_elapsed, #LINE# #TAB# #TAB# 'checksum': checksum}) #LINE# #TAB# return checksum"
Create the entire database . Parameters ---------- pth : str Path where the database is created  <code> def db_create(pth): ,"#LINE# #TAB# for pragma in PRAGMAS: #LINE# #TAB# #TAB# db_execute_general(pth, pragma) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'pressure_mode'}) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'pressure_unit'}) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'adsorbate_mode'}) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'adsorbate_unit'}) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'loading_mode'}) #LINE# #TAB# pygaps.db_upload_isotherm_property_type(pth, {'type': 'loading_unit'}) #LINE# #TAB# return"
Returns primary screen width and height in pixels <code> def get_screen_dimensions(): ,"#LINE# #TAB# import mss #LINE# #TAB# with mss.mss() as sct: #LINE# #TAB# #TAB# for monitor in sct.monitors: #LINE# #TAB# #TAB# #TAB# if monitor['left'] == 0 and monitor['top'] == 0: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return monitor['width'], monitor['height']"
"For the given graph , return the cut value for all binary assignments of the graph . Taken from QuantumFlow <code> def graph_cuts(graph: nx.Graph) ->np.ndarray: ","#LINE# #TAB# N = len(graph) #LINE# #TAB# diag_hamiltonian = np.zeros(shape=[2] * N, dtype=np.double) #LINE# #TAB# for q0, q1 in graph.edges(): #LINE# #TAB# #TAB# for index, _ in np.ndenumerate(diag_hamiltonian): #LINE# #TAB# #TAB# #TAB# if index[q0] != index[q1]: #LINE# #TAB# #TAB# #TAB# #TAB# weight = graph[q0][q1].get('weight', 1) #LINE# #TAB# #TAB# #TAB# #TAB# diag_hamiltonian[index] += weight #LINE# #TAB# return diag_hamiltonian"
"Perform find or findall on given xml with string from param  <code> def xml_search_helper(data, param): ","#LINE# #TAB# if isinstance(param, (tuple, list)) and param[1]: #LINE# #TAB# #TAB# result = data.findall(param[0]) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = data.find(param) #LINE# #TAB# return result"
"Checks whether the given i d is an ObjectId instance , and if not wraps it  <code> def ensure_object_id(cls, id): ","#LINE# #TAB# if isinstance(id, ObjectId): #LINE# #TAB# #TAB# return id #LINE# #TAB# if isinstance(id, str) and OBJECTIDEXPR.match(id): #LINE# #TAB# #TAB# return ObjectId(id) #LINE# #TAB# return id"
"Abort the execution of the current step or loop and yield an warning message ` msg ` if ` expr ` is False <code> def stop_if(expr, msg='', no_output=False): ","#LINE# #TAB# if expr: #LINE# #TAB# #TAB# raise StopInputGroup(msg=msg, keep_output=not no_output) #LINE# #TAB# return 0"
"Return hostname from host mor  <code> def get_hostname_for_host_mor(session, host_mor): ","#LINE# #TAB# if host_mor: #LINE# #TAB# #TAB# hostname = session._call_method(vim_util, 'get_dynamic_property', #LINE# #TAB# #TAB# #TAB# host_mor, 'HostSystem', 'name') #LINE# #TAB# #TAB# return hostname #LINE# #TAB# return None"
"Takes a N length list of q - points and returns an N - 2 length list of booleans indicating whether the direction has changed between each pair of q - points <code> def direction_changed(qpts, tolerance=5e-06): ","#LINE# #TAB# delta = np.diff(qpts, axis=0) #LINE# #TAB# dot = np.einsum('ij,ij->i', delta[1:, :], delta[:-1, :]) #LINE# #TAB# modq = np.linalg.norm(delta, axis=1) #LINE# #TAB# direction_changed = np.abs(np.abs(dot) - modq[1:] * modq[:-1]) > tolerance #LINE# #TAB# return direction_changed"
"Make the cache control headers based on a previous request 's response headers <code> def make_headers(headers: typing.Dict) ->typing.Dict[str, str]: ",#LINE# #TAB# out = {} #LINE# #TAB# if 'etag' in headers: #LINE# #TAB# #TAB# out['if-none-match'] = headers['etag'] #LINE# #TAB# if 'last-modified' in headers: #LINE# #TAB# #TAB# out['if-modified-since'] = headers['last-modified'] #LINE# #TAB# return out
Determine whether the supplied filename looks like a possible name of python . : param str name : The name of the provided file . : return : Whether the provided name looks like python . : rtype : bool <code> def looks_like_python(name): ,"#LINE# #TAB# if not any(name.lower().startswith(py_name) for py_name in #LINE# #TAB# #TAB# PYTHON_IMPLEMENTATIONS): #LINE# #TAB# #TAB# return False #LINE# #TAB# match = RE_MATCHER.match(name) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return any(fnmatch(name, rule) for rule in MATCH_RULES) #LINE# #TAB# return False"
Returns the ASCII decoded version of the given HTML string . This does NOT remove normal HTML tags like < p >  <code> def html_decode(s): ,"#LINE# #TAB# html_codes = (""'"", '&#39;'), ('""', '&quot;'), ('>', '&gt;'), ('<', '&lt;' #LINE# #TAB# #TAB# ), ('&', '&amp;') #LINE# #TAB# for code in html_codes: #LINE# #TAB# #TAB# s = s.replace(code[1], code[0]) #LINE# #TAB# return s"
Returns the proper log level core based on a given string <code> def log_level_from_string(str_level): ,"#LINE# #TAB# levels = { #LINE# #TAB# #TAB# 'CRITICAL': logging.CRITICAL, #LINE# #TAB# #TAB# 'ERROR': logging.ERROR, #LINE# #TAB# #TAB# 'WARNING': logging.WARNING, #LINE# #TAB# #TAB# 'INFO': logging.INFO, #LINE# #TAB# #TAB# 'DEBUG': logging.DEBUG, #LINE# #TAB# } #LINE# #TAB# try: #LINE# #TAB# #TAB# return levels[str_level.upper()] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# if str_level in [logging.DEBUG, logging.INFO, logging.WARNING, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# logging.ERROR, logging.CRITICAL]: #LINE# #TAB# #TAB# #TAB# return str_level #LINE# #TAB# return logging.NOTSET"
"compare Wyckoff positions Parameters ---------- pos1 , pos2 : tuple tuples with Wyckoff label and optional parameters <code> def pos_eq(pos1, pos2): ","#LINE# #TAB# if pos1[0] != pos2[0]: #LINE# #TAB# #TAB# return False #LINE# #TAB# if pos1[1] == pos2[1]: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# for f1, f2 in zip(pos1[1], pos2[1]): #LINE# #TAB# #TAB# #TAB# if not numpy.isclose(f1 % 1, f2 % 1, atol=1e-05): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"same , but put back together <code> def wrap_text3(text, size=defaultsize): ","#LINE# #TAB# lines = wrapText2(text, size) #LINE# #TAB# return '\n'.join(lines) + '\n'"
Get an example signal of a STEM detector Example ------- > > > import atomap.api as am > > > s = am.example_data.get_detector_image_signal ( ) <code> def get_detector_image_signal(): ,"#LINE# #TAB# global example_detector_image #LINE# #TAB# if example_detector_image is None: #LINE# #TAB# #TAB# path = os.path.join(my_path, 'example_data', #LINE# #TAB# #TAB# #TAB# 'example_detector_image.hspy') #LINE# #TAB# #TAB# example_detector_image = load(path) #LINE# #TAB# s = example_detector_image.deepcopy() #LINE# #TAB# return s"
Return the file extension to use for format <code> def format_to_ext(format): ,"#LINE# #TAB# return {constants.JPEG: 'jpg', constants.PNG: 'png', constants.SVG: 'svg'}[ #LINE# #TAB# #TAB# format]"
"Return the parameters of the simulator , excluding start , end , and delta_t  <code> def parameter_names(cls): ","#LINE# #TAB# excluded = ['start_time', 'end_time', 'delta_t'] #LINE# #TAB# return [f.name for f in dataclasses.fields(cls) if f.name not in excluded]"
Determine an output filename based on the ` Content - Disposition ` header  <code> def get_filename_from_content_disposition(content_disposition): ,"#LINE# #TAB# params = value, params = cgi.parse_header(content_disposition) #LINE# #TAB# if 'filename*' in params: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# charset, lang, filename = params['filename*'].split(""'"", 2) #LINE# #TAB# #TAB# #TAB# filename = urlparse.unquote(filename) #LINE# #TAB# #TAB# #TAB# filename = filename.encode('iso-8859-1').decode(charset) #LINE# #TAB# #TAB# #TAB# return _safe_filename(filename) #LINE# #TAB# #TAB# except (ValueError, LookupError): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# if 'filename' in params: #LINE# #TAB# #TAB# filename = params['filename'] #LINE# #TAB# #TAB# return _safe_filename(filename) #LINE# #TAB# return None"
Getter for the asconv headers ( asci header info stored in the dicom ) <code> def get_asconv_headers(mosaic): ,"#LINE# #TAB# asconv_headers = re.findall('### ASCCONV BEGIN(.*)### ASCCONV END ###', #LINE# #TAB# #TAB# mosaic[Tag(41, 4128)].value.decode(encoding='ISO-8859-1'), re.DOTALL)[0 #LINE# #TAB# #TAB# ] #LINE# #TAB# return asconv_headers"
Try to find user and project name from git remote output @param [ String ] output of git remote command @return [ Array ] user and project <code> def user_project_from_remote(remote): ,"#LINE# #TAB# regex1 = ( #LINE# #TAB# #TAB# b'.*(?:[:/])(?P<user>(-|\\w|\\.)*)/(?P<project>(-|\\w|\\.)*)(\\.git).*' #LINE# #TAB# #TAB# ) #LINE# #TAB# match = re.match(regex1, remote) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return match.group('user'), match.group('project') #LINE# #TAB# regex2 = '.*/((?:-|\\w|\\.)*)/((?:-|\\w|\\.)*).*' #LINE# #TAB# match = re.match(regex2, remote) #LINE# #TAB# if match: #LINE# #TAB# #TAB# return match.group('user'), match.group('project') #LINE# #TAB# return None, None"
Get a logger instance . : param identifier : identifier of logger as addressed in configuration . : return : the requested logger  <code> def get_instance(identifier): ,#LINE# #TAB# logger = logging.getLogger(identifier) #LINE# #TAB# return logger
Check if LIBSVM label is formatted like so : < label > if just label < label>:<instance_weight > if label and instance weight both exist : param libsvm_label : <code> def is_valid_libsvm_label(libsvm_label): ,#LINE# #TAB# split_label = libsvm_label.split(':') #LINE# #TAB# if len(split_label) <= 2: #LINE# #TAB# #TAB# for label_part in split_label: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# float(label_part) #LINE# #TAB# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"Traverse the tree to merge quotations , given a quotationList ( computed with findQuotation ) . Fill quoteIndexToNode ( map from the index of the beginning of the quote to the node  <code> def quotation_traversal(t, quotationList, quoteIndexToNode): ","#LINE# #TAB# childCopy = list(t.child) #LINE# #TAB# for c in childCopy: #LINE# #TAB# #TAB# quotation_traversal(c, quotationList, quoteIndexToNode) #LINE# #TAB# quote = matchingQuote(t.wordList, quotationList) #LINE# #TAB# if not quote: #LINE# #TAB# #TAB# return #LINE# #TAB# if not quote[0] in quoteIndexToNode: #LINE# #TAB# #TAB# quoteIndexToNode[quote[0]] = t #LINE# #TAB# childCopy = list(t.child) #LINE# #TAB# for c in childCopy: #LINE# #TAB# #TAB# if matchingQuote(c.wordList, quotationList) == quote: #LINE# #TAB# #TAB# #TAB# t.merge(c, True) #LINE# #TAB# #TAB# #TAB# quoteIndexToNode[quote[0]] = t"
Iterate over frames from a dataset stored in disk <code> def frames_iter_dataset(filename): ,"#LINE# #TAB# filename = pathlib.Path(filename) #LINE# #TAB# if filename.is_dir(): #LINE# #TAB# #TAB# cache_directory = filename #LINE# #TAB# elif filename.suffix == '.h5': #LINE# #TAB# #TAB# cache_directory = filename.parent / '__cache__' #LINE# #TAB# #TAB# if not cache_directory.is_dir(): #LINE# #TAB# #TAB# #TAB# _build_dataset_cache(filename, cache_directory) #LINE# #TAB# for fname in cache_directory.iterdir(): #LINE# #TAB# #TAB# if fname.suffix != '.pickle': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# with open(fname, 'rb') as fobj: #LINE# #TAB# #TAB# #TAB# dataset = pickle.load(fobj) #LINE# #TAB# #TAB# for frame in dataset: #LINE# #TAB# #TAB# #TAB# yield frame"
Returns True if the given configuration file uses EPD_username  <code> def is_using_epd_username(filename_or_fp): ,#LINE# #TAB# data = parse_assignments(filename_or_fp) #LINE# #TAB# return 'EPD_username' in data and 'EPD_auth' not in data
returns the order of vertices to triangulate the polygon <code> def get_vertex_order(coord_list): ,"#LINE# #TAB# vertices = [coord_list[i:i + 3] for i in range(0, len(coord_list), 3)] #LINE# #TAB# minimalAxes = find_dominant_axis(vertices) #LINE# #TAB# for i in vertices: #LINE# #TAB# #TAB# i.pop(minimalAxes) #LINE# #TAB# poly = [j for i in vertices for j in i] #LINE# #TAB# vertex_order = earcut(poly) #LINE# #TAB# return vertex_order"
"Build padding params with given constraints . Next formula can clarify what frame is : padding_length = data_length MOD frame <code> def with_constraints(cls, frame, frame_max): ","#LINE# #TAB# inst = cls.__new__(cls) #LINE# #TAB# inst._lib_vscf_padding_params = VscfPaddingParams() #LINE# #TAB# inst.ctx = (inst._lib_vscf_padding_params. #LINE# #TAB# #TAB# vscf_padding_params_new_with_constraints(frame, frame_max)) #LINE# #TAB# return inst"
Automatically creates dispatches for messages without them  <code> def prepare_dispatches(): ,"#LINE# #TAB# dispatches = [] #LINE# #TAB# target_messages = Message.get_without_dispatches() #LINE# #TAB# cache = {} #LINE# #TAB# for message_model in target_messages: #LINE# #TAB# #TAB# if message_model.cls not in cache: #LINE# #TAB# #TAB# #TAB# message_cls = get_registered_message_type(message_model.cls) #LINE# #TAB# #TAB# #TAB# subscribers = message_cls.get_subscribers() #LINE# #TAB# #TAB# #TAB# cache[message_model.cls] = (message_cls, subscribers) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# message_cls, subscribers = cache[message_model.cls] #LINE# #TAB# #TAB# dispatches.extend(message_cls.prepare_dispatches(message_model)) #LINE# #TAB# return dispatches"
"Accepts string and searches for it in conlang words list and English words list . If word exists in database , returns True , otherwise returns False  <code> def word_exists(english=None, conlang=None): ",#LINE# #TAB# if conlang is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# findConWord(conlang) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except LookupError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# if english is not None: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# findEnglishWord(english) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# except LookupError: #LINE# #TAB# #TAB# #TAB# return False
"Sorts a DataFrame according to its nullity in either ascending or descending order  <code> def nullity_sort(df, sort=None): ","#LINE# #TAB# if sort == 'ascending': #LINE# #TAB# #TAB# return df.iloc[np.argsort(df.count(axis='columns').values), :] #LINE# #TAB# elif sort == 'descending': #LINE# #TAB# #TAB# return df.iloc[np.flipud(np.argsort(df.count(axis='columns').values)), :] #LINE# #TAB# else: #LINE# #TAB# #TAB# return df"
"Returns a list of the DROP INDEX SQL statements for all models in the given app  <code> def sql_destroy_indexes(app, style, connection): ","#LINE# #TAB# output = [] #LINE# #TAB# for model in models.get_models(app, include_auto_created=True): #LINE# #TAB# #TAB# output.extend(connection.creation.sql_destroy_indexes_for_model( #LINE# #TAB# #TAB# #TAB# model, style)) #LINE# #TAB# return output"
Strips the tensor name to reflect the op name  <code> def format_tensor_name(name): ,"#LINE# #TAB# if name.startswith('^'): #LINE# #TAB# #TAB# name_old = name #LINE# #TAB# #TAB# name = name.strip('^') #LINE# #TAB# #TAB# log.warning('Changing ""{}"" to ""{}""'.format(name_old, name)) #LINE# #TAB# return name.split(':')[0]"
"Add the student 's information to the rubric file This help for collecting the marks after marking done <code> def add_information_to_rubric(username, student, rubric_file_name): ","#LINE# #TAB# message = '{}\n{}\n{}\n'.format(comment_wrap('PUT MARK HERE'), #LINE# #TAB# #TAB# comment_wrap(student['id']), comment_wrap(username)) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(student['repo_path'] + '/' + rubric_file_name, 'r+') as file: #LINE# #TAB# #TAB# #TAB# file.write(message) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# print('Unable to find the file!') #LINE# #TAB# #TAB# raise"
Skip comments and empty lines : param f : the tsv file handler : return : the first not empty line with no comments <code> def skip_comments_and_empty(f): ,#LINE# #TAB# line = f.readline() #LINE# #TAB# while line.startswith('#') or line == '\n': #LINE# #TAB# #TAB# line = f.readline() #LINE# #TAB# return line
Tests if obj is a class instance of any type . Parameters ---------- obj : any Input object Returns ------- out : bool Flag if obj is class instance or not <code> def is_instance(obj): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# _ = obj.__dict__ #LINE# #TAB# #TAB# return True #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return False
"Try and parse the contents of cea/__init__.py for the version . return "" 0.1 "" if nothing is found <code> def read_version(commit_id): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# init_py = subprocess.check_output( #LINE# #TAB# #TAB# #TAB# 'git show {commit_id}:cea/__init__.py'.format(commit_id=commit_id)) #LINE# #TAB# #TAB# match = re.search('__version__\\s+=\\s+""([^""]+)""', init_py) #LINE# #TAB# #TAB# if not match: #LINE# #TAB# #TAB# #TAB# return '0.1' #LINE# #TAB# #TAB# return match.group(1) #LINE# #TAB# except: #LINE# #TAB# #TAB# return '<unversioned>'"
Returns True when the Proxy component is in the Bohrium backend <code> def is_proxy_in_stack(): ,#LINE# #TAB# global _proxy_is_in_stack #LINE# #TAB# if _proxy_is_in_stack is None: #LINE# #TAB# #TAB# _proxy_is_in_stack = 'Proxy' in messaging.runtime_info() #LINE# #TAB# return _proxy_is_in_stack
"Given a path to the config file , load its contents and assign it to the config file as appropriate  <code> def resolve_info(nexus, path): ","#LINE# #TAB# if not os.path.isfile(path): #LINE# #TAB# #TAB# createDefaultInfo(path) #LINE# #TAB# contents = pdosq.read_yaml_file(path) #LINE# #TAB# if not validateInfo(contents): #LINE# #TAB# #TAB# output.out.err('Saved configuration data invalid, destroying it.') #LINE# #TAB# #TAB# os.remove(path) #LINE# #TAB# #TAB# createDefaultInfo(path) #LINE# #TAB# #TAB# contents = pdosq.read_yaml_file(path, default={}) #LINE# #TAB# #TAB# writeYaml(contents, path) #LINE# #TAB# nexus.info.pdid = contents['pdid'] #LINE# #TAB# nexus.info.version = contents['version'] #LINE# #TAB# nexus.info.pdserver = contents['pdserver'] #LINE# #TAB# nexus.info.wampRouter = contents['wampRouter']"
RE parser for classify.oclc service ( publisher and year )  <code> def parser_pub(htmlthing): ,"#LINE# #TAB# match = RE_PUB.search(u(htmlthing)) #LINE# #TAB# if match: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# buf = match.group() #LINE# #TAB# #TAB# #TAB# flds = RE_FP.findall(buf) #LINE# #TAB# #TAB# #TAB# vals = RE_VP.findall(buf) #LINE# #TAB# #TAB# #TAB# return dict(zip(flds, vals)) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return None"
Get the first accent from the right of a string  <code> def get_accent_string(string): ,"#LINE# #TAB# accents = list(filter(lambda accent: accent != Accent.NONE, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# map(get_accent_char, string))) #LINE# #TAB# return accents[-1] if accents else Accent.NONE"
Formats the AutoPatchingSettings object removing arguments that are empty <code> def format_auto_patching_settings(result): ,#LINE# #TAB# from collections import OrderedDict #LINE# #TAB# order_dict = OrderedDict() #LINE# #TAB# if result.enable is not None: #LINE# #TAB# #TAB# order_dict['enable'] = result.enable #LINE# #TAB# if result.day_of_week is not None: #LINE# #TAB# #TAB# order_dict['dayOfWeek'] = result.day_of_week #LINE# #TAB# if result.maintenance_window_starting_hour is not None: #LINE# #TAB# #TAB# order_dict['maintenanceWindowStartingHour'] = result.maintenance_window_starting_hour #LINE# #TAB# if result.maintenance_window_duration is not None: #LINE# #TAB# #TAB# order_dict['maintenanceWindowDuration'] = result.maintenance_window_duration #LINE# #TAB# return order_dict
"Checks whether a given model is registered . This is used to only register models if they are n't overridden by the application <code> def is_model_registered(app_label, model_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# apps.get_registered_model(app_label, model_name) #LINE# #TAB# except LookupError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
"The entropy of the user s contacts  <code> def entropy_of_contacts(records, normalize=False): ",#LINE# #TAB# counter = Counter(r.correspondent_id for r in records) #LINE# #TAB# raw_entropy = entropy(counter.values()) #LINE# #TAB# n = len(counter) #LINE# #TAB# if normalize and n > 1: #LINE# #TAB# #TAB# return raw_entropy / math.log(n) #LINE# #TAB# else: #LINE# #TAB# #TAB# return raw_entropy
Extract all imports from a python script <code> def extract_imports(script): ,#LINE# #TAB# if not os.path.isfile(script): #LINE# #TAB# #TAB# raise ValueError('Not a file: %s' % script) #LINE# #TAB# parse_tree = parse_python(script) #LINE# #TAB# result = find_imports(parse_tree) #LINE# #TAB# result.path = script #LINE# #TAB# return result
Generator to extract operators and operands in pairs <code> def operator_operands(tokenlist): ,"#LINE# #TAB# iterator = iter(tokenlist) #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# yield next(iterator), next(iterator) #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# break"
"Run a program the "" hard way "" so we do n't lose our UID  <code> def run_task(Args): ","#LINE# #TAB# Read, Write = os.pipe() #LINE# #TAB# if not os.fork(): #LINE# #TAB# #TAB# os.close(Read) #LINE# #TAB# #TAB# os.dup2(Write, 1) #LINE# #TAB# #TAB# os.execv(Args[0], Args) #LINE# #TAB# os.close(Write) #LINE# #TAB# Read = os.fdopen(Read) #LINE# #TAB# RetVal = Read.readlines() #LINE# #TAB# Read.close() #LINE# #TAB# return RetVal"
"Explicit integration of the covariance term , using scipy 's sparse matrix for Csurf surf - density function along the surface Csurf - transition matrix x - grid <code> def advance_surf_cov(surf, Csurf, x): ","#LINE# #TAB# surf = (Csurf * surf.reshape(len(x) ** 2)).reshape(len(x), len(x)) #LINE# #TAB# return surf"
"Get the list of all available roles <code> def get_roles(token, service_url=None): ","#LINE# #TAB# if not service_url: #LINE# #TAB# #TAB# service_url = config_service_url() #LINE# #TAB# username = get_username(token) #LINE# #TAB# url = '{}/aws-account-roles/{}'.format(service_url, username) #LINE# #TAB# response = requests.get(url, headers={'Authorization': 'Bearer {}'. #LINE# #TAB# #TAB# format(token)}, timeout=30) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# roles = response.json()['account_roles'] #LINE# #TAB# return [AWSRole(role['account_id'], role['account_name'], role[ #LINE# #TAB# #TAB# 'role_name']) for role in roles]"
Returns the GeoQuerySet spatial reference identifier  <code> def get_srid(queryset): ,#LINE# #TAB# srid = queryset.query.get_context('transformed_srid') #LINE# #TAB# return srid or geo_field(queryset).srid
Returns DBH24 TST names <code> def get_dbh24_tst(name): ,#LINE# #TAB# assert name in dbh24_reaction_list #LINE# #TAB# d = dbh24_reaction_list[name] #LINE# #TAB# tst = d['tst'] #LINE# #TAB# return tst
Django doesn t particularly understand REST . In case we send data over PUT Django won t actually look at the data and load it . We need to twist its arm here  <code> def coerce_put_post(request): ,"#LINE# #TAB# if request.method == ""PUT"": #LINE# #TAB# #TAB# if hasattr(request, '_post'): #LINE# #TAB# #TAB# #TAB# del request._post #LINE# #TAB# #TAB# #TAB# del request._files #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# request.method = ""POST"" #LINE# #TAB# #TAB# #TAB# request._load_post_and_files() #LINE# #TAB# #TAB# #TAB# request.method = ""PUT"" #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# request.META['REQUEST_METHOD'] = 'POST' #LINE# #TAB# #TAB# #TAB# request._load_post_and_files() #LINE# #TAB# #TAB# #TAB# request.META['REQUEST_METHOD'] = 'PUT' #LINE# #TAB# #TAB# request.PUT = request.POST"
r Avoid extraneous whitespace around an operator  <code> def whitespace_around_operator(logical_line): ,"#LINE# #TAB# for match in OPERATOR_REGEX.finditer(logical_line): #LINE# #TAB# #TAB# before, after = match.groups() #LINE# #TAB# #TAB# if '\t' in before: #LINE# #TAB# #TAB# #TAB# yield match.start(1), ""E223 tab before operator"" #LINE# #TAB# #TAB# elif len(before) > 1: #LINE# #TAB# #TAB# #TAB# yield match.start(1), ""E221 multiple spaces before operator"" #LINE# #TAB# #TAB# if '\t' in after: #LINE# #TAB# #TAB# #TAB# yield match.start(2), ""E224 tab after operator"" #LINE# #TAB# #TAB# elif len(after) > 1: #LINE# #TAB# #TAB# #TAB# yield match.start(2), ""E222 multiple spaces after operator"""
"In the IOU fungible the supply is set by Issuer who issue funds  <code> def issue_funds(ctx, amount='uint256', rtgs_hash='bytes32', returns=STATUS): ","#LINE# #TAB# #TAB# ""In the IOU fungible the supply is set by Issuer, who issue funds."" #LINE# #TAB# #TAB# ctx.accounts[ctx.msg_sender] += amount #LINE# #TAB# #TAB# ctx.issued_amounts[ctx.msg_sender] += amount #LINE# #TAB# #TAB# ctx.Issuance(ctx.msg_sender, rtgs_hash, amount) #LINE# #TAB# #TAB# return OK"
"[ 0 - 9]{1,3}:[0 - 9]{1,2}:[0 - 9]{1,2 } <code> def t_timestamp(t): ","#LINE# #TAB# m, s, f = t.value.split(':') #LINE# #TAB# t.value = int(m) * 75 * 60 + int(s) * 75 + int(f) #LINE# #TAB# return t"
Returns Extended Resource for service type management  <code> def get_resources(cls): ,"#LINE# #TAB# resource_attributes = RESOURCE_ATTRIBUTE_MAP[SEGMENTS] #LINE# #TAB# controller = base.create_resource(SEGMENTS, SEGMENT, directory. #LINE# #TAB# #TAB# get_plugin(SEGMENTS), resource_attributes, allow_pagination=True, #LINE# #TAB# #TAB# allow_sorting=True) #LINE# #TAB# return [extensions.ResourceExtension(SEGMENTS, controller, attr_map= #LINE# #TAB# #TAB# resource_attributes)]"
"Get an array of blocks at the specified height  <code> def get_block_height(height, api_code=None): ",#LINE# #TAB# resource = 'block-height/{0}?format=json'.format(height) #LINE# #TAB# if api_code is not None: #LINE# #TAB# #TAB# resource += '&api_code=' + api_code #LINE# #TAB# response = util.call_api(resource) #LINE# #TAB# json_response = json.loads(response) #LINE# #TAB# return [Block(b) for b in json_response['blocks']]
"safe version of add <code> def safe_add(a, b): ","#LINE# #TAB# if isinstance(a, str) and isinstance(b, str) and len(a) + len(b) > MAX_STR_LEN: #LINE# #TAB# #TAB# raise RuntimeError(""String length exceeded, max string length is {}"".format(MAX_STR_LEN)) #LINE# #TAB# return a + b"
Return new RackSpace CloudFiles Connection <code> def get_conn(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = cloudfiles.get_connection(USERNAME, API_KEY) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logging.error(e) #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# return conn"
Obtain Cromer - Mann formula and coefficients for a specified element . * symbol * : string symbol of an element Return instance of CromerMannFormula  <code> def get_cmformula(symbol): ,#LINE# #TAB# if not _cmformulas: #LINE# #TAB# #TAB# _update_cmformulas() #LINE# #TAB# return _cmformulas[symbol]
Shortcut function to prepare molecule to draw . Overwrite this function for customized appearance . It is recommended to clone the molecule before draw because all the methods above are destructive  <code> def ready_to_draw(mol): ,#LINE# #TAB# copied = molutil.clone(mol) #LINE# #TAB# equalize_terminal_double_bond(copied) #LINE# #TAB# scale_and_center(copied) #LINE# #TAB# format_ring_double_bond(copied) #LINE# #TAB# return copied
"Returns if obj1 aspects obj2 within its orb , considering a list of possible aspect types  <code> def is_aspecting(obj1, obj2, aspList): ","#LINE# #TAB# aspDict = _aspectDict(obj1, obj2, aspList) #LINE# #TAB# if aspDict: #LINE# #TAB# #TAB# return aspDict['orb'] < obj1.orb() #LINE# #TAB# return False"
Finds out whether this is an ordered or unordered list : type style : dict : rtype : str <code> def google_list_style(style): ,"#LINE# #TAB# if 'list-style-type' in style: #LINE# #TAB# #TAB# list_style = style['list-style-type'] #LINE# #TAB# #TAB# if list_style in ['disc', 'circle', 'square', 'none']: #LINE# #TAB# #TAB# #TAB# return 'ul' #LINE# #TAB# return 'ol'"
"Returns a VLCPanel class <code> def vlcpanel_factory(filepath, volume=None): ",#LINE# #TAB# vlc_panel_cls = VLCPanel #LINE# #TAB# VLCPanel.filepath = filepath #LINE# #TAB# if volume is not None: #LINE# #TAB# #TAB# VLCPanel.volume = volume #LINE# #TAB# return vlc_panel_cls
Test if a element is iterable Parameters ---------- el : object Returns ------- iterable : boolean if True then then el is iterable if Fales then not <code> def element_iterable(el): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# el[0] #LINE# #TAB# #TAB# iterable = True #LINE# #TAB# except (TypeError, IndexError): #LINE# #TAB# #TAB# iterable = False #LINE# #TAB# return iterable"
"Determine if a given comment token is an end - of - line comment or not  <code> def is_eol(token: tokenize.TokenInfo, path_to_anchor, before_anchor) ->bool: ","#LINE# #TAB# if before_anchor: #LINE# #TAB# #TAB# return False #LINE# #TAB# assert path_to_anchor #LINE# #TAB# anchor = path_to_anchor[-1] #LINE# #TAB# assert isinstance(anchor.field, str), type(anchor.field) #LINE# #TAB# node = getattr(anchor.node, anchor.field) #LINE# #TAB# if anchor.index is not None: #LINE# #TAB# #TAB# node = node[anchor.index] #LINE# #TAB# if not hasattr(node, 'lineno'): #LINE# #TAB# #TAB# raise ValueError('anchor node {} must have ""lineno"" attribute'. #LINE# #TAB# #TAB# #TAB# format(typed_ast.ast3.dump(node, include_attributes=True))) #LINE# #TAB# return node.lineno == token.start[0] and node.lineno == token.end[0]"
"Calculate labels and cost function given a matrix of points and a list of centroids for the k - modes algorithm  <code> def labels_cost(X, centroids): ","#LINE# #TAB# npoints = X.shape[0] #LINE# #TAB# cost = 0.0 #LINE# #TAB# labels = np.empty(npoints, dtype='int64') #LINE# #TAB# for ipoint, curpoint in enumerate(X): #LINE# #TAB# #TAB# diss = matching_dissim(centroids, curpoint) #LINE# #TAB# #TAB# clust = np.argmin(diss) #LINE# #TAB# #TAB# labels[ipoint] = clust #LINE# #TAB# #TAB# cost += diss[clust] #LINE# #TAB# return labels, cost"
"Return an iterator to a file - like object that yields fixed size chunks : param file_object : a file - like object : param file_chunk_size : maximum size of chunk <code> def file_chunk_iter(file_object, file_chunk_size=65536): ",#LINE# #TAB# while True: #LINE# #TAB# #TAB# chunk = file_object.read(file_chunk_size) #LINE# #TAB# #TAB# if chunk: #LINE# #TAB# #TAB# #TAB# yield chunk #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# break
Get language code from region  <code> def get_lang(region) ->str: ,#LINE# #TAB# regions = COUNTRIES #LINE# #TAB# lang = regions[region] #LINE# #TAB# lang = lang.split('/') #LINE# #TAB# lang = lang[0] #LINE# #TAB# assert lang in TYPE_LIST.keys() #LINE# #TAB# return lang
"@param d : { long | str}private exponent @param n : { long | str}modulus <code> def private_key(d, n): ","#LINE# #TAB# if isinstance(d, bytes): #LINE# #TAB# #TAB# d = rsa.transform.bytes2int(d) #LINE# #TAB# if isinstance(n, bytes): #LINE# #TAB# #TAB# n = rsa.transform.bytes2int(n) #LINE# #TAB# return {'d': d, 'n': n}"
Get the reason code from the API response : param response : Response object in JSON : return : String - reason code <code> def get_reason_code(response): ,#LINE# #TAB# resp_dict = json.loads(response.text) #LINE# #TAB# try: #LINE# #TAB# #TAB# reason = resp_dict['reason_code'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# print('Retrieval unsuccessful.') #LINE# #TAB# #TAB# return None #LINE# #TAB# return reason
"Create a structing element composed of the origin and another pixel x y - x and y offsets of the other pixel returns a structuring element <code> def strel_pair(x, y): ","#LINE# #TAB# x_center = int(np.abs(x)) #LINE# #TAB# y_center = int(np.abs(y)) #LINE# #TAB# result = np.zeros((y_center * 2 + 1, x_center * 2 + 1), bool) #LINE# #TAB# result[y_center, x_center] = True #LINE# #TAB# result[y_center + int(y), x_center + int(x)] = True #LINE# #TAB# return result"
UCS2 text encoding algorithm Encodes the specified text string into UCS2-encoded bytes . : param text : the text string to encode : return : A bytearray containing the string encoded in UCS2 encoding : rtype : bytearray <code> def encode_ucs2(text): ,"#LINE# #TAB# result = bytearray() #LINE# #TAB# for b in map(ord, text): #LINE# #TAB# #TAB# result.append(b >> 8) #LINE# #TAB# #TAB# result.append(b & 255) #LINE# #TAB# return result"
"Get the IP that this machine uses to contact the internet . If behind a NAT , this will still be this computer 's IP , and not the router 's  <code> def get_public_ip(): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# with closing(socket.socket(socket.AF_INET, socket.SOCK_DGRAM)) as sock: #LINE# #TAB# #TAB# #TAB# sock.connect(('203.0.113.1', 1)) #LINE# #TAB# #TAB# #TAB# ip = sock.getsockname()[0] #LINE# #TAB# #TAB# return ip #LINE# #TAB# except: #LINE# #TAB# #TAB# return '127.0.0.1'"
Parse * .cov file  <code> def parse_cov(cov_file): ,"#LINE# #TAB# data = [] #LINE# #TAB# covered_amplicons = 0 #LINE# #TAB# with open(cov_file, 'r') as cfile: #LINE# #TAB# #TAB# for row in csv.reader(cfile, delimiter='\t'): #LINE# #TAB# #TAB# #TAB# data.append(row) #LINE# #TAB# #TAB# #TAB# if float(row[8]) >= 1: #LINE# #TAB# #TAB# #TAB# #TAB# covered_amplicons += 1 #LINE# #TAB# return data, covered_amplicons"
"Return the path to the stats_name file . : raises : ResourceNotFoundError if the file does n't exist and create is false  <code> def get_stats_file(stats_name=None, create=True): ","#LINE# #TAB# if not stats_name: #LINE# #TAB# #TAB# stats_name = 'stats' #LINE# #TAB# stats_dir = get_stats_dir(create) #LINE# #TAB# stats_file = '{}/{}'.format(stats_dir, stats_name) #LINE# #TAB# if not os.path.exists(stats_file): #LINE# #TAB# #TAB# if create: #LINE# #TAB# #TAB# #TAB# with open(stats_file, 'w'): #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ResourceNotFoundError(stats_file) #LINE# #TAB# return stats_file"
Add shared arguments to the convert or mogrify parser  <code> def build_parser(parser): ,"#LINE# #TAB# add_options(parser) #LINE# #TAB# parser.add_argument('source_file', type=common.FileType('rt'), help= #LINE# #TAB# #TAB# 'Input sequence file') #LINE# #TAB# parser.add_argument('dest_file', help='Output file') #LINE# #TAB# return parser"
"Group comments by the round to which they belong <code> def group_comments_by_round(comments, ranking=0): ","#LINE# #TAB# comment_rounds = {} #LINE# #TAB# ordered_comment_round_names = [] #LINE# #TAB# for comment in comments: #LINE# #TAB# #TAB# comment_round_name = ranking and comment[11] or comment[7] #LINE# #TAB# #TAB# if comment_round_name not in comment_rounds: #LINE# #TAB# #TAB# #TAB# comment_rounds[comment_round_name] = [] #LINE# #TAB# #TAB# #TAB# ordered_comment_round_names.append(comment_round_name) #LINE# #TAB# #TAB# comment_rounds[comment_round_name].append(comment) #LINE# #TAB# return [(comment_round_name, comment_rounds[comment_round_name]) #LINE# #TAB# #TAB# #TAB# for comment_round_name in ordered_comment_round_names]"
"Allocate alignment score / edit matrix . m is len(genomic ) , n is len(flowdna ) . Return old_score_column , score_column , traceback_matrix  <code> def allocate_flow_alignment_matrices(m, n): ","#LINE# #TAB# score_dtype = np.int32 #LINE# #TAB# score_column = np.empty(m + 1, dtype=score_dtype) #LINE# #TAB# old_score_column = np.empty_like(score_column) #LINE# #TAB# traceback_matrix = np.empty((n + 1, m + 1), dtype=np.int32) #LINE# #TAB# return old_score_column, score_column, traceback_matrix"
"Save content to the given file . If file_name is an array , join it with os.path.sep <code> def save_to_output(content, file_name): ","#LINE# #TAB# file_name = make_path(file_name) #LINE# #TAB# os.makedirs(os.path.dirname(os.path.join(settings.output_dir, file_name #LINE# #TAB# #TAB# )), exist_ok=True) #LINE# #TAB# with open(settings.output_dir + os.path.sep + file_name, 'w') as f: #LINE# #TAB# #TAB# f.write(content) #LINE# #TAB# return file_name"
Determines whether the string is a gitignore glob expression . : param line : Given string . : return : True if the string is a glob expression . False otherwise  <code> def is_glob_exp(line): ,"#LINE# #TAB# results = unescaped_search_for('[*!?\\[\\]]', line, use_regex=True) #LINE# #TAB# return sum(1 for x in results) != 0"
Load the config yaml file into a dictionary  <code> def load_config(config_file: Path) ->Dict: ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with config_file.open('r') as f: #LINE# #TAB# #TAB# #TAB# config: Dict = yaml.load(f, Loader=yaml.FullLoader) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# raise ConfigLoadError(f'Config file does not exist: {config_file}') #LINE# #TAB# except (ScannerError, ParserError) as e: #LINE# #TAB# #TAB# logging.error(str(e)) #LINE# #TAB# #TAB# raise ConfigLoadError( #LINE# #TAB# #TAB# #TAB# 'Error encountered in config file. Check that your config file is formatted correctly.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# dehyphen(config) #LINE# #TAB# return config"
"Returns the HasProperties class to use for deserialization <code> def _deserialize_class(cls, input_cls_name, trusted, strict): ","#LINE# #TAB# #TAB# if not input_cls_name or input_cls_name == cls.__name__: #LINE# #TAB# #TAB# #TAB# return cls #LINE# #TAB# #TAB# if trusted and input_cls_name in cls._REGISTRY: #LINE# #TAB# #TAB# #TAB# return cls._REGISTRY[input_cls_name] #LINE# #TAB# #TAB# if strict: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Class name {} from deserialization input dictionary does ' #LINE# #TAB# #TAB# #TAB# #TAB# 'not match input class {}'.format(input_cls_name, cls.__name__) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return cls"
Retrieve the Android Version of the given recipe <code> def get_android_version(recipe): ,"#LINE# #TAB# data = _get_recipe_item_data(recipe, 'ova') #LINE# #TAB# return data['android_version'] if data else 'Unknown'"
"Check if sdcard is a valid path  <code> def is_valid_sdcard(parser, path_to_sdcard): ","#LINE# #TAB# if not os.path.exists(path_to_sdcard): #LINE# #TAB# #TAB# parser.error(""The path '%s' does not exist."" % path_to_sdcard) #LINE# #TAB# if not os.access(path_to_sdcard, os.W_OK): #LINE# #TAB# #TAB# parser.error(""The path '%s' is not writable"" % path_to_sdcard) #LINE# #TAB# else: #LINE# #TAB# #TAB# return path_to_sdcard"
Get preview file as active record  <code> def get_preview_file_raw(preview_file_id): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# preview_file = PreviewFile.get(preview_file_id) #LINE# #TAB# except StatementError: #LINE# #TAB# #TAB# raise PreviewFileNotFoundException() #LINE# #TAB# if preview_file is None: #LINE# #TAB# #TAB# raise PreviewFileNotFoundException() #LINE# #TAB# return preview_file
"Return True if the given control name exists in the control dictionary and has a suitable value such as TRUE or ON  <code> def boolean_control(controlName, controlDict=None): ","#LINE# #TAB# if controlDict == None: #LINE# #TAB# #TAB# controlDict = Controls #LINE# #TAB# if controlName not in controlDict: #LINE# #TAB# #TAB# return False #LINE# #TAB# result = booleanValue(controlDict[controlName]) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# logging.error(""Unknown value for controlname '"" + controlName + #LINE# #TAB# #TAB# #TAB# ""' = '"" + str(controlDict[controlName]) + ""'"") #LINE# #TAB# return result"
"Provide GC annotated intervals for error correction during panels and denoising  <code> def annotate_intervals(target_file, data): ","#LINE# #TAB# out_file = ""%s-gcannotated.tsv"" % utils.splitext_plus(target_file)[0] #LINE# #TAB# if not utils.file_uptodate(out_file, target_file): #LINE# #TAB# #TAB# with file_transaction(data, out_file) as tx_out_file: #LINE# #TAB# #TAB# #TAB# params = [""-T"", ""AnnotateIntervals"", ""-R"", dd.get_ref_file(data), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ""-L"", target_file, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ""--interval-merging-rule"", ""OVERLAPPING_ONLY"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ""-O"", tx_out_file] #LINE# #TAB# #TAB# #TAB# _run_with_memory_scaling(params, tx_out_file, data) #LINE# #TAB# return out_file"
"Return the screen column width of one line of a text layout structure . This function ignores any existing shift applied to the line , represented by an ( amount , None ) tuple at the start of the line  <code> def line_width(segs): ",#LINE# #TAB# sc = 0 #LINE# #TAB# seglist = segs #LINE# #TAB# if segs and len(segs[0]) == 2 and segs[0][1] == None: #LINE# #TAB# #TAB# seglist = segs[1:] #LINE# #TAB# for s in seglist: #LINE# #TAB# #TAB# sc += s[0] #LINE# #TAB# return sc
Call main ( ) and log all exceptions as errors <code> def main_log_error() ->None: ,#LINE# #TAB# try: #LINE# #TAB# #TAB# args = worker.handle_args(worker.parseargs('Generic uploader')) #LINE# #TAB# #TAB# actapi = worker.init_act(args) #LINE# #TAB# #TAB# main(actapi) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# error('Unhandled exception: {}'.format(traceback.format_exc())) #LINE# #TAB# #TAB# raise
A faster SELECT DISTINCT unnest(actors ) FROM meta_imdb using meta_imdb_actors <code> def get_actors(search=None): ,"#LINE# #TAB# q = ZdbQuery(MetaImdbActors, session=db.session) #LINE# #TAB# if search: #LINE# #TAB# #TAB# search = search.replace('*', '') #LINE# #TAB# #TAB# if not isinstance(search, str): #LINE# #TAB# #TAB# #TAB# raise SearchException('search must be str') #LINE# #TAB# #TAB# q = q.filter(MetaImdbActors.actor.like(ZdbLiteral('%s*' % search))) #LINE# #TAB# results = q.all() #LINE# #TAB# return results"
"Reads the user given by identifier from HDX and returns User object <code> def read_from_hdx(identifier, configuration=None): ","#LINE# #TAB# #TAB# user = User(configuration=configuration) #LINE# #TAB# #TAB# result = user._load_from_hdx('user', identifier) #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# return user #LINE# #TAB# #TAB# return None"
"Return Python estimator , estimatorParamMaps , and evaluator from a Java ValidatorParams  <code> def from_java_impl(cls, java_stage): ","#LINE# #TAB# estimator = JavaParams._from_java(java_stage.getEstimator()) #LINE# #TAB# evaluator = JavaParams._from_java(java_stage.getEvaluator()) #LINE# #TAB# epms = [estimator._transfer_param_map_from_java(epm) for epm in #LINE# #TAB# #TAB# java_stage.getEstimatorParamMaps()] #LINE# #TAB# return estimator, epms, evaluator"
"Set heads to doc in UD annotation style . If fail to set , return doc without doing anything  <code> def set_heads(doc, heads): ","#LINE# #TAB# if max(heads) > len(doc) or min(heads) < 0: #LINE# #TAB# #TAB# return doc #LINE# #TAB# for head, token in zip(heads, doc): #LINE# #TAB# #TAB# if head == 0: #LINE# #TAB# #TAB# #TAB# token.head = token #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# token.head = doc[head - 1] #LINE# #TAB# return doc"
Provide current time as YYYY - MM - DD_HH - MM - SS Parameters -------- None Returns -------- time_str : str current time string <code> def current_time(): ,#LINE# #TAB# time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') #LINE# #TAB# return time_str
"Convert a obj file into a shape file <code> def obj_to_shp(obj_file, shp_file): ",#LINE# #TAB# shapefile.Writer() #LINE# #TAB# return True
"Check if class is a generic type , for example ` Union ` or ` List[int ] ` : param type _ : type to check <code> def is_type_hint(type_): ","#LINE# #TAB# if not hasattr(type_, '__module__'): #LINE# #TAB# #TAB# return False #LINE# #TAB# if type_.__module__ != 'typing': #LINE# #TAB# #TAB# if not any(t.__module__ == 'typing' for t in type_.mro()): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# origin = getattr(type_, '__origin__', type_) #LINE# #TAB# return origin in origin_type_checkers"
split profiles by days <code> def split_profiles_by_days(profile): ,"#LINE# #TAB# profile_dict = {} #LINE# #TAB# for day in weekdays: #LINE# #TAB# #TAB# profile_dict[day] = {} #LINE# #TAB# #TAB# day_name = daynames[day] #LINE# #TAB# #TAB# for num in range(1, 14): #LINE# #TAB# #TAB# #TAB# profile_dict[day]['TEMPERATURE_%s_%d' % (day_name, num) #LINE# #TAB# #TAB# #TAB# #TAB# ] = device_profile['TEMPERATURE_%s_%d' % (day_name, num)] #LINE# #TAB# #TAB# #TAB# profile_dict[day]['ENDTIME_%s_%d' % (day_name, num) #LINE# #TAB# #TAB# #TAB# #TAB# ] = device_profile['ENDTIME_%s_%d' % (day_name, num)] #LINE# #TAB# return profile_dict"
Show all available entry points available for searching . An entry point defines a uri that provides unfiltered access to all elements of the entry point type  <code> def object_types(): ,#LINE# #TAB# #TAB# types = [element.rel #LINE# #TAB# #TAB# #TAB# #TAB# for element in entry_point()] #LINE# #TAB# #TAB# types.extend(list(CONTEXTS)) #LINE# #TAB# #TAB# return types
"Call dot command , and provide helpful error message if we can not find it  <code> def call_graphviz_dot(src, fmt): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# svg = dot(src, T=fmt) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno == 2: #LINE# #TAB# #TAB# #TAB# cli.error( #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# raise #LINE# #TAB# return svg"
"Convert this graph to a Node - Link JSON object  <code> def to_json(graph: BELGraph) -> Mapping[str, Any]: ","#LINE# #TAB# graph_json_dict = node_link_data(graph) #LINE# #TAB# graph_json_dict['graph'][GRAPH_ANNOTATION_LIST] = { #LINE# #TAB# #TAB# keyword: list(sorted(values)) #LINE# #TAB# #TAB# for keyword, values in graph_json_dict['graph'].get(GRAPH_ANNOTATION_LIST, {}).items() #LINE# #TAB# } #LINE# #TAB# graph_json_dict['graph'][GRAPH_UNCACHED_NAMESPACES] = list( #LINE# #TAB# #TAB# graph_json_dict['graph'].get(GRAPH_UNCACHED_NAMESPACES, []) #LINE# #TAB# ) #LINE# #TAB# return graph_json_dict"
"Slice substrings from each string element in a column  <code> def str_slice(x, start=0, stop=None): ","#LINE# #TAB# if stop is None: #LINE# #TAB# #TAB# sll = _to_string_sequence(x).slice_string_end(start) #LINE# #TAB# else: #LINE# #TAB# #TAB# sll = _to_string_sequence(x).slice_string(start, stop) #LINE# #TAB# return sll"
Generates kernel for Hilbert transform using FFT . Parameters ---------- n : int Number of equidistant grid points . Returns ------- array Kernel used when performing Hilbert transform using FFT  <code> def kernel_fredriksen(n): ,"#LINE# #TAB# aux = np.zeros(n + 1, dtype=doublenp) #LINE# #TAB# for i in range(1, n + 1): #LINE# #TAB# #TAB# aux[i] = i * log(i) #LINE# #TAB# m = 2 * n #LINE# #TAB# ker = np.zeros(m, dtype=doublenp) #LINE# #TAB# for i in range(1, n): #LINE# #TAB# #TAB# ker[i] = aux[i + 1] - 2 * aux[i] + aux[i - 1] #LINE# #TAB# #TAB# ker[m - i] = -ker[i] #LINE# #TAB# return fft(ker) / pi"
"Remove the DNS server from the network interface <code> def rm_dns(ip, interface='Local Area Connection'): ","#LINE# #TAB# cmd = ['netsh', 'interface', 'ip', 'delete', 'dns', interface, ip, 'validate=no'] #LINE# #TAB# return __salt__['cmd.retcode'](cmd, python_shell=False) == 0"
"Set time coverage global attributes Based on input content , define the time coverage attributes as recommended by CF  <code> def set_time_attributes(ds): ",#LINE# #TAB# ds = ds.copy(deep=True) #LINE# #TAB# time_vars = [v for v in ds.coords if ds[v].dtype == 'datetime64[ns]'] #LINE# #TAB# time = pd.Series() #LINE# #TAB# for v in time_vars: #LINE# #TAB# #TAB# time = time.append(ds[v].to_series()) #LINE# #TAB# ds.attrs['time_coverage_start'] = pd.to_datetime(time.min()).strftime( #LINE# #TAB# #TAB# '%Y-%m-%dT%H:%M:%SZ') #LINE# #TAB# ds.attrs['time_coverage_end'] = pd.to_datetime(time.max()).strftime( #LINE# #TAB# #TAB# '%Y-%m-%dT%H:%M:%SZ') #LINE# #TAB# return ds
"update constant classes , so the keys of CONST_CLS can be reused <code> def update_const_classes(): ","#LINE# #TAB# klasses = bool, int, float, complex, str, bytes #LINE# #TAB# for kls in klasses: #LINE# #TAB# #TAB# CONST_CLS[kls] = Const"
"Generate n random regions of size in the provided base spread  <code> def random_regions(base, n, size): ","#LINE# #TAB# spread = size // 2 #LINE# #TAB# base_info = collections.defaultdict(list) #LINE# #TAB# for space, start, end in base: #LINE# #TAB# #TAB# base_info[space].append(start + spread) #LINE# #TAB# #TAB# base_info[space].append(end - spread) #LINE# #TAB# regions = [] #LINE# #TAB# for _ in range(n): #LINE# #TAB# #TAB# space = random.choice(base_info.keys()) #LINE# #TAB# #TAB# pos = random.randint(min(base_info[space]), max(base_info[space])) #LINE# #TAB# #TAB# regions.append([space, pos-spread, pos+spread]) #LINE# #TAB# return regions"
"Returns the size of the batch dimension . Assumes a well - formed batch returns 0 otherwise  <code> def get_batch_size(batch: Union[Dict, torch.Tensor]) -> int: ","#LINE# #TAB# if isinstance(batch, torch.Tensor): #LINE# #TAB# #TAB# return batch.size(0) #LINE# #TAB# elif isinstance(batch, Dict): #LINE# #TAB# #TAB# return get_batch_size(next(iter(batch.values()))) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0"
"Loading a dictlet finder extension . Returns : ExtensionManager : the extension manager holding the extensions <code> def create_adapter(adapter_name, context): ","#LINE# #TAB# log2 = logging.getLogger('stevedore') #LINE# #TAB# out_hdlr = logging.StreamHandler(sys.stdout) #LINE# #TAB# out_hdlr.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# 'freckles connector plugin error -> %(message)s')) #LINE# #TAB# out_hdlr.setLevel(logging.INFO) #LINE# #TAB# log2.addHandler(out_hdlr) #LINE# #TAB# log2.setLevel(logging.INFO) #LINE# #TAB# log.debug('Loading freckles adapter...') #LINE# #TAB# mgr = driver.DriverManager(namespace='freckles.adapters', name= #LINE# #TAB# #TAB# adapter_name, invoke_on_load=True, invoke_args=(adapter_name, context)) #LINE# #TAB# return mgr.driver"
"Splitting info into variables and retrievals . Currently not used , but could be implemented for production rules  <code> def splitting_submodules(string): ","#LINE# #TAB# variables = re.findall('(?<=' + ACTRVARIABLER + ').*?(?=$)', string) #LINE# #TAB# retrievals = re.findall('(?<=' + ACTRRETRIEVER + ').*?(?=$)', string) #LINE# #TAB# return {'variables': set(variables), 'retrievals': set(retrievals)}"
Get the number of tokens in a string sperated by space  <code> def get_string_size(tensor): ,"#LINE# #TAB# split = tf.string_split(tf.reshape(tensor, [-1])) #LINE# #TAB# return tf.shape(split)[1]"
"test if object is a list only containing dict like items <code> def is_list_of_dict_like(obj, attr=('keys', 'items')): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# if len(obj) == 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# return all([is_dict_like(i, attr) for i in obj]) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False"
Return a normalized arXiv identifier from ` ` obj ` `  <code> def normalize_arxiv(obj): ,"#LINE# #TAB# obj = obj.split()[0] #LINE# #TAB# matched_arxiv_pre = RE_ARXIV_PRE_2007_CLASS.match(obj) #LINE# #TAB# if matched_arxiv_pre: #LINE# #TAB# #TAB# return '/'.join(matched_arxiv_pre.group('extraidentifier', #LINE# #TAB# #TAB# #TAB# 'identifier')).lower() #LINE# #TAB# matched_arxiv_post = RE_ARXIV_POST_2007_CLASS.match(obj) #LINE# #TAB# if matched_arxiv_post: #LINE# #TAB# #TAB# return matched_arxiv_post.group('identifier') #LINE# #TAB# return None"
Parses an azimuth measurement in azimuth or quadrant format  <code> def parse_azimuth(azimuth): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# azimuth = float(azimuth) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# if not azimuth[0].isalpha(): #LINE# #TAB# #TAB# #TAB# raise ValueError('Ambiguous azimuth: {}'.format(azimuth)) #LINE# #TAB# #TAB# azimuth = parse_quadrant_measurement(azimuth) #LINE# #TAB# return azimuth
"Handles replacement strings in the file or directory name <code> def replace_name(path, mapping): ","#LINE# #TAB# f_split = list(os.path.split(path)) #LINE# #TAB# name = f_split[1] #LINE# #TAB# if '${' in name: #LINE# #TAB# #TAB# new_name = Template(name).substitute(mapping) #LINE# #TAB# #TAB# new_path = os.path.join(f_split[0], new_name) #LINE# #TAB# #TAB# os.rename(path, new_path) #LINE# #TAB# else: #LINE# #TAB# #TAB# new_path = path #LINE# #TAB# return new_path"
"Constructs a ResNet-101 model . Args : pretrained ( bool ) : If True , returns a model pre - trained on ImageNet <code> def se_resnet101(num_classes): ","#LINE# #TAB# model = ResNet(PSEBottleneck, [3, 4, 23, 3], num_classes=num_classes) #LINE# #TAB# return model"
"Get the feature template for this media <code> def feature_template(media, feature_type): ","#LINE# #TAB# template_name = hook_handle(('feature_%s_template' % feature_type, #LINE# #TAB# #TAB# media.media_type)) #LINE# #TAB# if template_name is None: #LINE# #TAB# #TAB# return '/archivalook/feature_displays/default_%s.html' % feature_type #LINE# #TAB# return template_name"
get extname according filetype : param filetype : filetype ( in config.get_key('valid_filetype ' ) ) : return : file 's extend name <code> def get_file_ext(filetype): ,"#LINE# #TAB# return {'image/jpeg': 'jpg', 'image/png': 'png', 'image/webp': 'webp', #LINE# #TAB# #TAB# 'image/gif': 'gif'}[filetype]"
"Given a path , gets a config parser <code> def get_config_parser(path): ",#LINE# #TAB# ret = configparser.ConfigParser() #LINE# #TAB# ret.read(os.path.expanduser(path)) #LINE# #TAB# return ret
UK standard page is 1000 words per page  <code> def uk_1000_words_standard_page(page_counter): ,#LINE# #TAB# pages = round(page_counter.word_count() / 1000) #LINE# #TAB# if pages == 0: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return pages
Return the merge flattened type of the source type  <code> def merge_flatten_type(src): ,"#LINE# #TAB# if isinstance(src, MutableSequence): #LINE# #TAB# #TAB# return [merge_flatten_type(t) for t in src] #LINE# #TAB# if isinstance(src, MutableMapping) and src.get('type') == 'array': #LINE# #TAB# #TAB# return src #LINE# #TAB# return {'items': src, 'type': 'array'}"
"Return True if object is a sequence of numpy arrays , e.g. a list of images as 2D arrays . : param obj : list of arrays : return : boolean <code> def is_list_of_arrays(obj): ","#LINE# #TAB# if is_array(obj): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not hasattr(obj, '__len__'): #LINE# #TAB# #TAB# return False #LINE# #TAB# for arr in obj: #LINE# #TAB# #TAB# if not is_array(arr): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Get the root URL for the IBM Streams REST API  <code> def get_rest_api(): ,"#LINE# #TAB# assert _has_local_install #LINE# #TAB# url=[] #LINE# #TAB# ok = _run_st(['geturl', '--api'], lines=url) #LINE# #TAB# if not ok: #LINE# #TAB# #TAB# raise ChildProcessError('streamtool geturl') #LINE# #TAB# return url[0]"
"Return True if value is a floating point number , False otherwise  <code> def is_float(value): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# float(value) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False
"Returns True if SVN is installed , else False <code> def svn_exists(): ",#LINE# #TAB# if cfg['CFG_PATH_SVN']: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
Read a configuration file Parameters ---------- config_file_path : str the path to the configuration file we want to load Returns ------- config_parser : SafeConfigParser the configuration object .. note : : Raises IOError if the procedure fails  <code> def read_configuration_file(config_file_path): ,#LINE# #TAB# config_parser = SafeConfigParser() #LINE# #TAB# try: #LINE# #TAB# #TAB# config_parser.read(config_file_path) #LINE# #TAB# except: #LINE# #TAB# #TAB# strmsg = 'Cannot open {0} \n'.format(config_file_path) #LINE# #TAB# #TAB# raise IOError(strmsg) #LINE# #TAB# return config_parser
"Take the first ` n ` items of a collection  <code> def take_n(n, iterable): ","#LINE# #TAB# for i, item in enumerate(iterable): #LINE# #TAB# #TAB# if i >= n: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# yield item"
Set the model timestep  <code> def set_time_step(): ,"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('time_step', dtype='float64', direction=function. #LINE# #TAB# #TAB# IN, description='The current model timestep') #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# function.result_doc = """""" #LINE# #TAB# #TAB# 0 - OK #LINE# #TAB# #TAB# #TAB# Current value of the time step was retrieved #LINE# #TAB# #TAB# -1 - ERROR #LINE# #TAB# #TAB# #TAB# The code does not have support for querying the time #LINE# #TAB# #TAB# """""" #LINE# #TAB# return function"
"Set the hostname of the Palo Alto proxy minion . A commit will be required before this is processed  <code> def set_hostname(hostname=None, deploy=False): ","#LINE# #TAB# if not hostname: #LINE# #TAB# #TAB# raise CommandExecutionError(""Hostname option must not be none."") #LINE# #TAB# ret = {} #LINE# #TAB# query = {'type': 'config', #LINE# #TAB# #TAB# #TAB# 'action': 'set', #LINE# #TAB# #TAB# #TAB# 'xpath': '/config/devices/entry[@name=\'localhost.localdomain\']/deviceconfig/system', #LINE# #TAB# #TAB# #TAB# 'element': '<hostname>{0}</hostname>'.format(hostname)} #LINE# #TAB# ret.update(__proxy__['panos.call'](query)) #LINE# #TAB# if deploy is True: #LINE# #TAB# #TAB# ret.update(commit()) #LINE# #TAB# return ret"
"Set is we are a fork master or not : param b : True if we are master process , False if we are a child process . : type b : bool : return Nothing <code> def set_master_process(cls, b): ","#LINE# #TAB# logger.debug('Switching _masterProcess to %s', b) #LINE# #TAB# cls._master_process = b"
"Set an HTTP Cache Control header on a request  <code> def set_http_caching(request, gateway='crab', region='permanent'): ","#LINE# #TAB# crabpy_exp = request.registry.settings.get('crabpy.%s.cache_config.%s.expiration_time' % (gateway, region), None) #LINE# #TAB# if crabpy_exp is None: #LINE# #TAB# #TAB# return request #LINE# #TAB# ctime = int(int(crabpy_exp) * 1.05) #LINE# #TAB# request.response.cache_expires(ctime, public=True) #LINE# #TAB# return request"
Get all possibles cache keys for given instance <code> def get_all_cache_keys(instance): ,"#LINE# #TAB# keys = [] #LINE# #TAB# serializers = cache_registry.get(instance.__class__) #LINE# #TAB# for serializer in serializers: #LINE# #TAB# #TAB# keys.append(get_cache_key(instance, serializer, 'http')) #LINE# #TAB# #TAB# keys.append(get_cache_key(instance, serializer, 'https')) #LINE# #TAB# return keys"
"Returns ` ` input_vars ` ` as a flat dictionary where keys are tuples in the form ` ` ( process_name , var_name ) ` ` . Raises an error if the given format appears to be invalid  <code> def flatten_inputs(input_vars): ","#LINE# #TAB# flatten_vars = {} #LINE# #TAB# for key, val in input_vars.items(): #LINE# #TAB# #TAB# if isinstance(key, str) and isinstance(val, dict): #LINE# #TAB# #TAB# #TAB# for var_name, var_value in val.items(): #LINE# #TAB# #TAB# #TAB# #TAB# flatten_vars[key, var_name] = var_value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# flatten_vars[as_variable_key(key)] = val #LINE# #TAB# return flatten_vars"
Load background DataFrame Args : df_path : Path to DataFrame Returns : Background DataFrame <code> def load_df(df_path: str) ->pd.DataFrame: ,"#LINE# #TAB# if df_path.endswith('.csv'): #LINE# #TAB# #TAB# df = pd.read_csv(df_path, index_col=0) #LINE# #TAB# elif df_path.endswith('.tsv'): #LINE# #TAB# #TAB# df = pd.read_csv(df_path, sep='\t', index_col=0) #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# df = pd.read_hdf(df_path) #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# #TAB# raise RuntimeError(f'Failed to open DataFrame: {df_path}') #LINE# #TAB# return df"
"Look up the "" base "" unit for this unit and the factor for converting to it . Returns a 2-tuple of ` factor , base_unit `  <code> def get_conversion_factor(unit): ","#LINE# #TAB# if unit in BASE_UNIT_CONVERSIONS: #LINE# #TAB# #TAB# return BASE_UNIT_CONVERSIONS[unit] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1, unit"
Convert from a name like ethfinex . eth to a label like ethfinex If name is already a label this should be a noop except for converting to a string and validating the name syntax  <code> def dot_eth_label(name): ,"#LINE# #TAB# label = name_to_label(name, registrar='eth') #LINE# #TAB# if len(label) < MIN_ETH_LABEL_LENGTH: #LINE# #TAB# #TAB# raise InvalidLabel('name %r is too short' % label) #LINE# #TAB# else: #LINE# #TAB# #TAB# return label"
Read SVM model in PICKLE format : param svm_file_name : name of the file to read from : return : a tupple of weight and biais <code> def read_svm(svm_file_name): ,"#LINE# #TAB# with gzip.open(svm_file_name, 'rb') as f: #LINE# #TAB# #TAB# w, b = pickle.load(f) #LINE# #TAB# return numpy.squeeze(w), b"
"Returns default value of the attribute in the given node : param node : str : param attribute_name : str : return : object <code> def attribute_default_value(node, attribute_name): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return maya.cmds.attributeQuery(attribute_name, node=node, #LINE# #TAB# #TAB# #TAB# listDefault=True) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return maya.cmds.addAttr('{}.{}'.format(node, attribute_name), #LINE# #TAB# #TAB# #TAB# #TAB# query=True, dv=True) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# return None"
"Create categories from a breadcrumb string <code> def create_from_full_slug(breadcrumb_str, separator='/'): ","#LINE# #TAB# category_names = [x.strip() for x in breadcrumb_str.split(separator)] #LINE# #TAB# categories = create_from_sequence(category_names, True) #LINE# #TAB# return categories[-1]"
"Returns member by name from parent class if it exists  <code> def FindParentMember(cls, typecheck_parent, name): ","#LINE# #TAB# if typecheck_parent and hasattr(typecheck_parent, name): #LINE# #TAB# return getattr(typecheck_parent, name) #LINE# #TAB# return None"
Return the application data directory . If the directory path does not yet exists then create it  <code> def data_dir(): ,"#LINE# #TAB# init() #LINE# #TAB# datadir = QStandardPaths.writableLocation(QStandardPaths.DataLocation) #LINE# #TAB# version = QCoreApplication.applicationVersion() #LINE# #TAB# datadir = os.path.join(datadir, version) #LINE# #TAB# if not os.path.isdir(datadir): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.makedirs(datadir, exist_ok=True) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return datadir"
"Bisect ` func(i ) ` , returning an index such that preceding values are less than ` x ` . If ` x ` is present , the returned index is its first occurrence . EOF is assumed if ` func ` returns None  <code> def bisect_func_left(x, lo, hi, func): ",#LINE# #TAB# while lo < hi: #LINE# #TAB# #TAB# mid = (lo + hi) // 2 #LINE# #TAB# #TAB# k = func(mid) #LINE# #TAB# #TAB# if k is not None and k < x: #LINE# #TAB# #TAB# #TAB# lo = mid + 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# hi = mid #LINE# #TAB# return lo
"removes all chars in char_list from str_ <code> def remove_chars(str_, char_list): ","#LINE# #TAB# outstr = str_[:] #LINE# #TAB# for char in char_list: #LINE# #TAB# #TAB# outstr = outstr.replace(char, '') #LINE# #TAB# return outstr"
"Extracts call option table for input ticker and expiration date @param : ticker @param : date <code> def get_calls(ticker, date=None): ","#LINE# #TAB# options_chain = get_options_chain(ticker, date) #LINE# #TAB# return options_chain['calls']"
"Compute the line numbers for all bytecode instructions  <code> def compute_lineno(cls, table, code): ","#LINE# #TAB# for offset, lineno in dis.findlinestarts(code): #LINE# #TAB# #TAB# adj_offset = offset + _FIXED_OFFSET #LINE# #TAB# #TAB# if adj_offset in table: #LINE# #TAB# #TAB# #TAB# table[adj_offset].lineno = lineno #LINE# #TAB# known = table[_FIXED_OFFSET].lineno #LINE# #TAB# for inst in table.values(): #LINE# #TAB# #TAB# if inst.lineno >= 0: #LINE# #TAB# #TAB# #TAB# known = inst.lineno #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# inst.lineno = known #LINE# #TAB# return table"
"Return position of point regarding a segment : param : : point : point that we position : segment : segment that : return : - ' LEFT ' - ' RIGHT ' - ' ON ' - ' NEITHER ' <code> def point_position_segment(point, segment, precision=None): ","#LINE# #TAB# segment_b = segment[1], point #LINE# #TAB# orientation = ccw_or_cw_segments(segment, segment_b) #LINE# #TAB# if orientation in {'CW', 'CCW'}: #LINE# #TAB# #TAB# if orientation == 'CCW': #LINE# #TAB# #TAB# #TAB# return 'LEFT' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 'RIGHT' #LINE# #TAB# else: #LINE# #TAB# #TAB# seg_bbox = segment_to_bbox(segment) #LINE# #TAB# #TAB# if point_intersects_bbox(point, seg_bbox): #LINE# #TAB# #TAB# #TAB# return 'ON' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 'NEITHER'"
Tokenizes a string suppressing significant whitespace  <code> def tokenize_wrapper(input): ,"#LINE# #TAB# skip = token.NEWLINE, token.INDENT, token.DEDENT #LINE# #TAB# tokens = tokenize.generate_tokens(driver.generate_lines(input).next) #LINE# #TAB# for quintuple in tokens: #LINE# #TAB# #TAB# type, value, start, end, line_text = quintuple #LINE# #TAB# #TAB# if type not in skip: #LINE# #TAB# #TAB# #TAB# yield quintuple"
Strip the Ethernet frame from a packet  <code> def strip_ethernet(packet): ,#LINE# #TAB# if not type(packet) == Ethernet: #LINE# #TAB# #TAB# packet = Ethernet(packet) #LINE# #TAB# payload = packet.payload #LINE# #TAB# if type(payload) == str: #LINE# #TAB# #TAB# payload = binascii.unhexlify(payload) #LINE# #TAB# return payload
Check if build on pull request build  <code> def is_pull_request(): ,#LINE# #TAB# if not os.environ.get('CI_PULL_REQUEST'): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Get the oldest object for a specific class name <code> def get_oldest(class_name): ,"#LINE# #TAB# for cls, wdict in six.iteritems(live_refs): #LINE# #TAB# #TAB# if cls.__name__ == class_name: #LINE# #TAB# #TAB# #TAB# if not wdict: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# #TAB# return min(six.iteritems(wdict), key=itemgetter(1))[0]"
Attempt to convert pandas . NaT <code> def encode_as_pandas(obj): ,"#LINE# #TAB# pandas = get_module('pandas', should_load=False) #LINE# #TAB# if not pandas: #LINE# #TAB# #TAB# raise NotEncodable #LINE# #TAB# if obj is pandas.NaT: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# raise NotEncodable"
Convert a series to categorical values  <code> def to_categorical(series): ,#LINE# #TAB# series = series.astype('category') #LINE# #TAB# series = series.apply(str) #LINE# #TAB# return series
Set up relative paths for subdirectories  <code> def set_paths(experiment_name): ,"#LINE# #TAB# experiment = convert_to_filename(experiment_name) #LINE# #TAB# samples_yaml = 'results/{}/samples.yml'.format(experiment) #LINE# #TAB# summary_log = 'results/{}/log.txt'.format(experiment) #LINE# #TAB# path = {'experiment': experiment, 'samples_yaml': samples_yaml, #LINE# #TAB# #TAB# 'summary_log': summary_log} #LINE# #TAB# return path"
Try to send args to an existing application . @return false if it ca n't cannect to one  <code> def send_server(args): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# #TAB# s.connect((HOST, PORT)) #LINE# #TAB# #TAB# package = {'args': args} #LINE# #TAB# #TAB# s.sendall(pickle.dumps(package)) #LINE# #TAB# #TAB# s.close() #LINE# #TAB# #TAB# return True #LINE# #TAB# except socket.error: #LINE# #TAB# #TAB# return False"
Convert n digit binary result from the QVM to a value on a die  <code> def process_results(results): ,#LINE# #TAB# raw_results = results[0] #LINE# #TAB# processing_result = 0 #LINE# #TAB# for each_qubit_measurement in raw_results: #LINE# #TAB# #TAB# processing_result = 2*processing_result + each_qubit_measurement #LINE# #TAB# die_value = processing_result + 1 #LINE# #TAB# return die_value
"Reconcile a unit based on its dimensionality  <code> def get_unit_from_dimensions(dimensions, text): ","#LINE# #TAB# key = l.get_key_from_dimensions(dimensions) #LINE# #TAB# try: #LINE# #TAB# #TAB# unit = l.DERIVED_UNI[key] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# logging.debug(u'\tCould not find unit for: %s', key) #LINE# #TAB# #TAB# unit = c.Unit(name=build_unit_name(dimensions), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# dimensions=dimensions, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# entity=get_entity_from_dimensions(dimensions, text)) #LINE# #TAB# return unit"
"draw anti - aliased line between two endpoints  <code> def clip_and_draw_aaline(surf, rect, color, line, blend): ","#LINE# #TAB# if not clip_line(line, rect.x - 1, rect.y - 1, rect.x + rect.w, rect.y + #LINE# #TAB# #TAB# rect.h, use_float=True): #LINE# #TAB# #TAB# return #LINE# #TAB# _draw_aaline(surf, color, line[0], line[1], line[2], line[3], blend) #LINE# #TAB# return"
"Given a number of points to use and the bounds , return a triplet of integers for a uniform mesh with approximately that many points  <code> def get_nx_ny_nz(num_points, bounds): ","#LINE# #TAB# bounds = np.asarray(bounds, dtype=float) #LINE# #TAB# length = bounds[1::2] - bounds[::2] #LINE# #TAB# total_length = length.sum() #LINE# #TAB# rel_length = length / total_length #LINE# #TAB# non_zero = rel_length > 0.001 #LINE# #TAB# dim = int(non_zero.sum()) #LINE# #TAB# volume = np.prod(length[non_zero]) #LINE# #TAB# delta = pow(volume / num_points, 1.0 / dim) #LINE# #TAB# dimensions = np.ones(3, dtype=int) #LINE# #TAB# for i in range(3): #LINE# #TAB# #TAB# if rel_length[i] > 0.0001: #LINE# #TAB# #TAB# #TAB# dimensions[i] = int(round(length[i] / delta)) #LINE# #TAB# return dimensions"
"Solves X*a = b for a where X is a banded matrix with 1 or zero , and args along the diagonal band <code> def set_garch_arch_c(panel, args): ","#LINE# #TAB# n = panel.max_T #LINE# #TAB# rho = np.insert(-args['rho'], 0, 1) #LINE# #TAB# psi = np.insert(args['psi'], 0, 0) #LINE# #TAB# r = np.arange(n) #LINE# #TAB# AMA_1, AMA_1AR, GAR_1, GAR_1MA = np.diag(np.ones(n)), np.zeros((n, n) #LINE# #TAB# #TAB# ), np.diag(np.ones(n)), np.zeros((n, n)) #LINE# #TAB# c.bandinverse(args['lambda'], rho, -args['gamma'], psi, n, AMA_1, #LINE# #TAB# #TAB# AMA_1AR, GAR_1, GAR_1MA) #LINE# #TAB# return AMA_1, AMA_1AR, GAR_1, GAR_1MA"
"Gets the Weichert adjustment factor for each the magnitude bins <code> def get_weichert_factor(beta, cmag, cyear, end_year): ","#LINE# #TAB# if len(cmag) > 1: #LINE# #TAB# #TAB# dmag = (cmag[1:] + cmag[:-1]) / 2. #LINE# #TAB# #TAB# cval = np.hstack([dmag, cmag[-1] + (dmag[-1] - cmag[-2])]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1.0 / (end_year - cyear[0] + 1), None #LINE# #TAB# t_f = sum(np.exp(-beta * cval)) / sum((end_year - cyear + 1) * #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# np.exp(-beta * cval)) #LINE# #TAB# return t_f, cval"
"Check whether the active OpenGL context suffices our requirements  <code> def check_opengl_context(context, version): ",#LINE# #TAB# return context and context.isValid() and context.format().version( #LINE# #TAB# #TAB# ) >= version and bool(GL.glCreateShader) and gl_version() >= version
"Given a string or bytes input return a string  <code> def decode_bytes(byt, enc='utf-8'): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# strg = byt.decode(enc) #LINE# #TAB# except UnicodeDecodeError as err: #LINE# #TAB# #TAB# strg = ""Unable to decode message:\n{}\n{}"".format(str(byt), err) #LINE# #TAB# except (AttributeError, UnicodeEncodeError): #LINE# #TAB# #TAB# return byt #LINE# #TAB# return strg"
"Get list of CAs from configuration : rtype : list : Returns list of CAs loaded from configuration file <code> def get_ca_configuration(cls, ca_name): ",#LINE# #TAB# for cert_authority in cls.cas: #LINE# #TAB# #TAB# if ca_name == cert_authority['name']: #LINE# #TAB# #TAB# #TAB# return cert_authority #LINE# #TAB# return []
Format the URL pattern for display in warning messages  <code> def describe_pattern(pattern): ,"#LINE# #TAB# description = ""'{}'"".format(pattern.regex.pattern) #LINE# #TAB# if getattr(pattern, 'name', False): #LINE# #TAB# #TAB# description += "" [name='{}']"".format(pattern.name) #LINE# #TAB# return description"
"Clean the trace . For now , this just means replacing values of -999.0 with 0.0 <code> def clean_trace(trace): ",#LINE# #TAB# bad = np.where(trace == -999.0) #LINE# #TAB# trace[bad] = 0.0 #LINE# #TAB# return
"Internal function to be used with apply to multiply each column in a dataframe by every other column a b c - > a*a , a*b , b*b , b*c , c*c <code> def mul_cols(df, cols): ","#LINE# #TAB# _df = type(df)() #LINE# #TAB# for i, j in it.combinations_with_replacement(cols, 2): #LINE# #TAB# #TAB# col = '%s%s' % (i, j) #LINE# #TAB# #TAB# _df[col] = df[i] * df[j] #LINE# #TAB# return _df"
"Square function . Returns : np.ndarray : array with square values on source positions  <code> def square_source(source, i): ","#LINE# #TAB# answer = sineSource(source, i) #LINE# #TAB# answer[answer > 0] = 1.0 #LINE# #TAB# answer[answer < 0] = -1.0 #LINE# #TAB# return answer"
"Return request 's ' Authorization : ' header , as a bytestring . Hide some test client ickyness where the header can be unicode  <code> def get_authorization_header(request): ","#LINE# #TAB# auth = request.META.get('HTTP_TOKEN', b'') #LINE# #TAB# if isinstance(auth, type('')): #LINE# #TAB# #TAB# auth = auth.encode(HTTP_HEADER_ENCODING) #LINE# #TAB# return auth"
"Input : dict1 - first check in this dict ( and remove if there ) key - key in dict1 default_value - default value if not found dict2 - then check from here Output : value <code> def get_from_dicts(dict1, key, default_value, dict2, extra=''): ","#LINE# #TAB# value = default_value #LINE# #TAB# if key not in dict1: #LINE# #TAB# #TAB# if dict2 != None: #LINE# #TAB# #TAB# #TAB# value = dict2.get(extra + key, default_value) #LINE# #TAB# else: #LINE# #TAB# #TAB# value = dict1[key] #LINE# #TAB# #TAB# del dict1[key] #LINE# #TAB# #TAB# if dict2 != None: #LINE# #TAB# #TAB# #TAB# dict2[extra + key] = value #LINE# #TAB# return value"
Replace any object value by ' internal data ' string to store in Mongo  <code> def sanitize_workflow_exec(obj): ,"#LINE# #TAB# types = [dict, list, tuple, str, int, float, bool, type(None), datetime] #LINE# #TAB# if type(obj) not in types: #LINE# #TAB# #TAB# obj = 'Internal server data: {}'.format(type(obj)) #LINE# #TAB# elif isinstance(obj, dict): #LINE# #TAB# #TAB# for key, value in obj.items(): #LINE# #TAB# #TAB# #TAB# obj[key] = sanitize_workflow_exec(value) #LINE# #TAB# elif isinstance(obj, list): #LINE# #TAB# #TAB# for item in obj: #LINE# #TAB# #TAB# #TAB# item = sanitize_workflow_exec(item) #LINE# #TAB# return obj"
"Split note 's text according to regex , and return fields <code> def dict_from_text(cls, text): ","#LINE# #TAB# note_tokens = re.split(cls.SPLITTER_PATTERN, text) #LINE# #TAB# assert len(note_tokens) > 8 #LINE# #TAB# note_dict = {} #LINE# #TAB# for item in cls.CORRESP_TABLE: #LINE# #TAB# #TAB# if not item[1]: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# note_dict[item[1]] = note_tokens[item[0]] #LINE# #TAB# return note_dict"
"Each value in the sequence must be an allowed values . Otherwise , False  <code> def values_in_acceptable_entries(sequence, allowed_values) ->bool: ",#LINE# #TAB# if len(sequence) == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# for item in sequence: #LINE# #TAB# #TAB# if item not in allowed_values: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
build a dictionary with the data necessary to construct the given primitive <code> def serialize_primitive(primitive): ,"#LINE# #TAB# args_dict = {name: val for name, val in primitive.get_arguments()} #LINE# #TAB# cls = type(primitive) #LINE# #TAB# return {'type': cls.__name__, 'module': cls.__module__, 'arguments': #LINE# #TAB# #TAB# args_dict}"
"Get the time delays between the detectors in the network compared to a reference detector <code> def get_network_time_delays(net, refdet, ra, dec, t_gps): ","#LINE# #TAB# dt_dets = {} #LINE# #TAB# for det in net: #LINE# #TAB# #TAB# dt_dets[det] = detectors.time_delay_between_obs(refdet, det, ra, #LINE# #TAB# #TAB# #TAB# dec, t_gps) #LINE# #TAB# return dt_dets"
Compute Julian day from Gregorian day month and year  <code> def gdate_to_jdn(date): ,#LINE# #TAB# not_jan_or_feb = (14 - date.month) // 12 #LINE# #TAB# year_since_4800bc = date.year + 4800 - not_jan_or_feb #LINE# #TAB# month_since_4800bc = date.month + 12 * not_jan_or_feb - 3 #LINE# #TAB# jdn = date.day + (153 * month_since_4800bc + 2) // 5 \ #LINE# #TAB# #TAB# + 365 * year_since_4800bc \ #LINE# #TAB# #TAB# + (year_since_4800bc // 4 - year_since_4800bc // 100 + #LINE# #TAB# #TAB# year_since_4800bc // 400) - 32045 #LINE# #TAB# return jdn
"Decode Base58 encoded bytes or string as integer  <code> def decode_to_int(v: Union[str, bytes]) ->int: ",#LINE# #TAB# v = _str_to_bytes(v) #LINE# #TAB# i = 0 #LINE# #TAB# for char in v: #LINE# #TAB# #TAB# i *= __base #LINE# #TAB# #TAB# i += __digits.index(char) #LINE# #TAB# return i
"Reads a tab separated value file  <code> def read_tsv(cls, input_file, quotechar=None): ","#LINE# #TAB# with tf.io.gfile.GFile(input_file, 'r') as f: #LINE# #TAB# #TAB# reader = csv.reader(f, delimiter='\t', quotechar=quotechar) #LINE# #TAB# #TAB# lines = [] #LINE# #TAB# #TAB# for line in reader: #LINE# #TAB# #TAB# #TAB# lines.append(line) #LINE# #TAB# #TAB# return lines"
Wrap regex compilation in a TagError if it fails : param pattern : The string pattern to compile : return : The regex Pattern <code> def compile_regex(pattern: str) ->Pattern: ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# regex = compile(pattern) #LINE# #TAB# except re_error as e: #LINE# #TAB# #TAB# raise TagError(""Bad regex: '{}'"".format(pattern), TagError. #LINE# #TAB# #TAB# #TAB# EXIT_BAD_REGEX) from e #LINE# #TAB# return regex"
Gets total number of distributed workers or returns one if distributed is not initialized  <code> def get_world_size(): ,#LINE# #TAB# if torch.distributed.is_available() and torch.distributed.is_initialized(): #LINE# #TAB# #TAB# world_size = torch.distributed.get_world_size() #LINE# #TAB# else: #LINE# #TAB# #TAB# world_size = 1 #LINE# #TAB# return world_size
Pad the left side of a bitarray with 0s to align its length with byte boundaries  <code> def build_byte_align_buff(bits): ,#LINE# #TAB# bitmod = len(bits)%8 #LINE# #TAB# if bitmod == 0: #LINE# #TAB# #TAB# rdiff = bitarray() #LINE# #TAB# else: #LINE# #TAB# #TAB# rdiff = bitarray(8-bitmod) #LINE# #TAB# #TAB# rdiff.setall(False) #LINE# #TAB# return rdiff+bits
Called if Url not found  <code> def not_found(start_response): ,"#LINE# #TAB# start_response('404 Not Found', [('Content-Type', 'text/plain')]) #LINE# #TAB# return [b'Error 404: Not Found']"
Return which platforms are available <code> def avail_platforms(): ,#LINE# #TAB# ret = {} #LINE# #TAB# for platform in CMD_MAP: #LINE# #TAB# #TAB# ret[platform] = True #LINE# #TAB# #TAB# for cmd in CMD_MAP[platform]: #LINE# #TAB# #TAB# #TAB# if not salt.utils.path.which(cmd): #LINE# #TAB# #TAB# #TAB# #TAB# ret[platform] = False #LINE# #TAB# return ret
is ` filename ` is a NeXus HDF5 file ? <code> def is_ne_xus_file(filename): ,"#LINE# #TAB# if not os.path.exists(filename): #LINE# #TAB# #TAB# return None #LINE# #TAB# f = h5py.File(filename, 'r') #LINE# #TAB# if isHdf5FileObject(f): #LINE# #TAB# #TAB# for item in f: #LINE# #TAB# #TAB# #TAB# if isNeXusGroup(f[item], 'NXentry'): #LINE# #TAB# #TAB# #TAB# #TAB# f.close() #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# f.close() #LINE# #TAB# return False"
Find duplicate keys in a layer of ordered pairs . Intended as the object_pairs_hook callable for json.load or loads . : param data : ordered pairs : return : Dictionary with no duplicate keys : raises ValueError if duplicate keys are found <code> def find_duplicate_keys(data): ,"#LINE# #TAB# out_dict = {} #LINE# #TAB# for key, value in data: #LINE# #TAB# #TAB# if key in out_dict: #LINE# #TAB# #TAB# #TAB# raise ValueError('Duplicate key: {}'.format(key)) #LINE# #TAB# #TAB# out_dict[key] = value #LINE# #TAB# return out_dict"
"Remove mean from numpy array along axis <code> def remove_mean(x, axis): ",#LINE# #TAB# idx = [slice(None)] * x.ndim #LINE# #TAB# idx[axis] = numpy.newaxis #LINE# #TAB# return x - x.mean(axis=axis)[idx]
"Computes the precision score of one image . Parameters ---------- label : string The true label of the image predictions : list A list of predicted elements ( order does matter , k predictions allowed per image ) k : int MAP@k Returns ------- score : double <code> def map_per_image(label, predictions, k: int=5): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# return 1 / (predictions[:k].index(label) + 1) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 0.0
""" c - a : lead(II ) perchlorate [ PM73 ]  <code> def bc_pbjj_clo4_pm73(T, P): ","#LINE# #TAB# b0 = 0.4443 * 3 / 4 #LINE# #TAB# b1 = 2.296 * 3 / 4 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.01667 * 3 / 2 ** (5 / 2) #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Pbjj'] * i2c['ClO4']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"Returns ` ` datetime.datetime ` ` from natural language time deltas > > > parse_human_datetime(""now "" ) < = datetime.now ( ) True <code> def parse_human_timedelta(s): ","#LINE# #TAB# cal = parsedatetime.Calendar() #LINE# #TAB# dttm = dttm_from_timtuple(datetime.now().timetuple()) #LINE# #TAB# d = cal.parse(s, dttm)[0] #LINE# #TAB# d = datetime(d.tm_year, d.tm_mon, d.tm_mday, d.tm_hour, d.tm_min, d.tm_sec) #LINE# #TAB# return d - dttm"
Transforms tag attributes . kwargs - name : String - value : String <code> def transform_attribute(attribute_list): ,"#LINE# #TAB# updated_attribute_list = [] #LINE# #TAB# for attribute_parameters in attribute_list: #LINE# #TAB# #TAB# updated_attribute_list.append({'Name': attribute_parameters.get( #LINE# #TAB# #TAB# #TAB# 'name'), 'Value': attribute_parameters.get('value')}) #LINE# #TAB# return updated_attribute_list"
"Determines if an expression is a valid function "" call "" <code> def is_parans_exp(istr): ",#LINE# #TAB# fxn = istr.split('(')[0] #LINE# #TAB# if not fxn.isalnum() and fxn != '(' or istr[-1] != ')': #LINE# #TAB# #TAB# return False #LINE# #TAB# plevel = 1 #LINE# #TAB# for c in '('.join(istr[:-1].split('(')[1:]): #LINE# #TAB# #TAB# if c == '(': #LINE# #TAB# #TAB# #TAB# plevel += 1 #LINE# #TAB# #TAB# elif c == ')': #LINE# #TAB# #TAB# #TAB# plevel -= 1 #LINE# #TAB# #TAB# if plevel == 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"Makes a HTTP call authenticated with the OAuth token  <code> def make_authenticated_call(method, url, auth_token, data): ","#LINE# #TAB# import requests #LINE# #TAB# import requests.exceptions #LINE# #TAB# try: #LINE# #TAB# #TAB# r = requests.request(method, blender_id_endpoint(url), data=data, #LINE# #TAB# #TAB# #TAB# headers={'Authorization': 'Bearer %s' % auth_token}, verify=True) #LINE# #TAB# except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError #LINE# #TAB# #TAB# ) as e: #LINE# #TAB# #TAB# raise BlenderIdCommError(str(e)) #LINE# #TAB# return r"
"implement full DCM using PX4 native SD log data <code> def px4_update(IMU, ATT): ","#LINE# #TAB# global px4_state #LINE# #TAB# if px4_state is None: #LINE# #TAB# #TAB# px4_state = PX4_State(degrees(ATT.Roll), degrees(ATT.Pitch), degrees(ATT.Yaw), IMU._timestamp) #LINE# #TAB# gyro = Vector3(IMU.GyroX, IMU.GyroY, IMU.GyroZ) #LINE# #TAB# accel = Vector3(IMU.AccX, IMU.AccY, IMU.AccZ) #LINE# #TAB# px4_state.update(gyro, accel, IMU._timestamp) #LINE# #TAB# return px4_state"
Return random node values for building binary trees  <code> def generate_random_node_values(height): ,#LINE# #TAB# max_node_count = 2 ** (height + 1) - 1 #LINE# #TAB# node_values = list(range(max_node_count)) #LINE# #TAB# random.shuffle(node_values) #LINE# #TAB# return node_values
Parse a postfix map file and return values  <code> def parse_map_file(path): ,"#LINE# #TAB# content = {} #LINE# #TAB# with open(path) as fp: #LINE# #TAB# #TAB# for line in fp: #LINE# #TAB# #TAB# #TAB# if not line or line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# name, value = line.split('=', 1) #LINE# #TAB# #TAB# #TAB# content[name.strip()] = value.strip() #LINE# #TAB# return content"
Evals the env.py script  <code> def eval_env() ->dict: ,"#LINE# #TAB# p = migrations_dir / 'env.py' #LINE# #TAB# d = {} #LINE# #TAB# with p.open(mode='r') as f: #LINE# #TAB# #TAB# content = f.read() #LINE# #TAB# exec(content, d, d) #LINE# #TAB# return d"
"Split the output of git log into separate entries per commit . Parameters ---------- whole_log : str A string containing the entire git log . Returns ------- list(str ) A list of log entries , with each commit as its own string  <code> def split_commits(whole_log): ","#LINE# #TAB# lines = whole_log.splitlines() #LINE# #TAB# commit_line_idxs = [i for i, line in enumerate(lines) if re.match( #LINE# #TAB# #TAB# '^commit \\w{40}$', line)] #LINE# #TAB# commit_lines = np.array_split(lines, commit_line_idxs) #LINE# #TAB# return ['\n'.join(arr) for arr in commit_lines[1:]]"
helper function for cycling over color list if the number of items is higher than the legnth of the color list <code> def get_color(index): ,#LINE# #TAB# corrected_index = index % len(COLOR_LIST) #LINE# #TAB# return COLOR_LIST[corrected_index]
Turns the field generating functions on QuestionField into a series of options Formatted to be consumed by Question.type.choices <code> def get_field_options(): ,"#LINE# #TAB# inspected_funcs = inspect.getmembers(QuestionField, predicate=inspect. #LINE# #TAB# #TAB# ismethod) #LINE# #TAB# field_names = [(item[0], item[0]) for item in inspected_funcs] #LINE# #TAB# return field_names"
Gets the unique count of categorical features in a data set . Parameters ----------- data : DataFrame or named Series Returns ------- DataFrame or Series Unique value counts of the features in a dataset  <code> def get_unique_counts(data=None): ,"#LINE# #TAB# if data is None: #LINE# #TAB# #TAB# raise ValueError(""data: Expecting a DataFrame or Series, got 'None'"") #LINE# #TAB# features = get_cat_feats(data) #LINE# #TAB# temp_len = [] #LINE# #TAB# for feature in features: #LINE# #TAB# #TAB# temp_len.append(len(data[feature].unique())) #LINE# #TAB# df = list(zip(features, temp_len)) #LINE# #TAB# df = pd.DataFrame(df, columns=['Feature', 'Unique Count']) #LINE# #TAB# df = df.style.bar(subset=['Unique Count'], align='mid') #LINE# #TAB# return df"
Load a YAML configuration file  <code> def load_yaml_config(filename): ,"#LINE# #TAB# with open(filename, 'rt', encoding='utf-8') as file: #LINE# #TAB# #TAB# config_dict = yaml.safe_load(file) #LINE# #TAB# return config_dict"
cpc subset partition list Return the list of partitions which is managed by one compute service ( cpc subset ) : param cpc : cpc : return : list of partitions managed by compute service <code> def cpcsubset_partition_list(cpc): ,#LINE# #TAB# cpc_partition_list = cpc.partitions.list() #LINE# #TAB# openstack_partition_list = [] #LINE# #TAB# for partition in cpc_partition_list: #LINE# #TAB# #TAB# if is_valid_partition_name(partition.get_property('name')): #LINE# #TAB# #TAB# #TAB# openstack_partition_list.append(partition) #LINE# #TAB# return openstack_partition_list
"Transform metadata model mappings Parameters ---------- queryables : dict typename : dict <code> def transform_mappings(queryables, typename): ","#LINE# #TAB# for item in queryables: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# matching_typename = [key for key, value in typename.items() if #LINE# #TAB# #TAB# #TAB# #TAB# value == item][0] #LINE# #TAB# #TAB# #TAB# queryable_value = queryables[matching_typename] #LINE# #TAB# #TAB# #TAB# queryables[item] = {'xpath': queryable_value['xpath'], 'dbcol': #LINE# #TAB# #TAB# #TAB# #TAB# queryable_value['dbcol']} #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass"
Add the local timezone to value to make it aware  <code> def local_timezone(value): ,"#LINE# #TAB# if hasattr(value, ""tzinfo"") and value.tzinfo is None: #LINE# #TAB# #TAB# return value.replace(tzinfo=dateutil.tz.tzlocal()) #LINE# #TAB# return value"
True if d is a string and it s an existing directory  <code> def dir_param_valid(d): ,"#LINE# #TAB# r = True #LINE# #TAB# if not isinstance(d, str) : #LINE# #TAB# #TAB# r = False #LINE# #TAB# #TAB# raise TypeError #LINE# #TAB# if not os.path.isdir(d): #LINE# #TAB# #TAB# r = False #LINE# #TAB# #TAB# raise ValueError #LINE# #TAB# return r"
Compute the initial path label of a node recursively  <code> def get_initial_path_helper(node): ,#LINE# #TAB# if node.parent == None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# else: #LINE# #TAB# #TAB# return node.parent.get_path() + node.edge_label[0]
"Split a string into items and trim any excess spaces <code> def split_and_strip_without(string, exclude, separator_regexp=None): ","#LINE# #TAB# result = split_and_strip(string, separator_regexp) #LINE# #TAB# if not exclude: #LINE# #TAB# #TAB# return result #LINE# #TAB# return [x for x in result if x not in exclude]"
"The ratio is smaller the better <code> def ratio_scores(parameters_value, clusteringmodel_gmm_good, clusteringmodel_gmm_bad): ","#LINE# #TAB# ratio = clusteringmodel_gmm_good.score([parameters_value]) / clusteringmodel_gmm_bad.score([parameters_value]) #LINE# #TAB# sigma = 0 #LINE# #TAB# return ratio, sigma"
"Average rotate a given grid <code> def rotate_grid(grid, symmetry): ","#LINE# #TAB# rotation = 360 / symmetry #LINE# #TAB# out = np.mean([rotate(grid, rotation * i, reshape=False, mode='wrap') for #LINE# #TAB# #TAB# i in range(symmetry)], axis=0) #LINE# #TAB# return out"
Get a NumPy array view of a VNL vector  <code> def get_array_view_from_vnl_vector(vnl_vector): ,"#LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# itksize = vnl_vector.size() #LINE# #TAB# shape = [itksize] #LINE# #TAB# pixelType = 'D' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlD._get_array_view_from_vnl_vector(vnl_vector) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_vector) #LINE# #TAB# return itk_view"
"Convert a Unicode Universal Character Number ( e.g. "" U+4E00 "" or "" 4E00 "" ) to Python unicode ( u'\u4e00 ' ) <code> def ucn_to_unicode(ucn): ","#LINE# #TAB# if isinstance(ucn, string_types): #LINE# #TAB# #TAB# ucn = ucn.strip('U+') #LINE# #TAB# #TAB# if len(ucn) > int(4): #LINE# #TAB# #TAB# #TAB# char = b'\\U' + format(int(ucn, 16), '08x').encode('latin1') #LINE# #TAB# #TAB# #TAB# char = char.decode('unicode_escape') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# char = unichr(int(ucn, 16)) #LINE# #TAB# else: #LINE# #TAB# #TAB# char = unichr(ucn) #LINE# #TAB# assert isinstance(char, text_type) #LINE# #TAB# return char"
"Renders a templated yaml document from file path  <code> def render_from_path(path, context=None, globals=None): ","#LINE# #TAB# abs_source = os.path.abspath(os.path.expanduser(path)) #LINE# #TAB# yaml_resolver = resolver.TYamlResolver.new_from_path(abs_source) #LINE# #TAB# return yaml_resolver.resolve(Context(context), globals)._data"
"must be an iterable ( list , array , tuple ) <code> def is_iterable(value): ","#LINE# #TAB# return isinstance(value, np.ndarray) or isinstance(value, list #LINE# #TAB# #TAB# ) or isinstance(value, tuple), value"
"Read all remaining strings from standard input , convert each to a bool , and return those bools in an array . Raise a ValueError if any of the strings can not be converted to a bool  <code> def read_all_bools(): ",#LINE# #TAB# strings = readAllStrings() #LINE# #TAB# bools = [] #LINE# #TAB# for s in strings: #LINE# #TAB# #TAB# b = bool(s) #LINE# #TAB# #TAB# bools.append(b) #LINE# #TAB# return bools
Infer preferred figure format from output format  <code> def get_figformat(format): ,#LINE# #TAB# if format == 'pdf': #LINE# #TAB# #TAB# figformat = 'pdf' #LINE# #TAB# elif 'html' in format: #LINE# #TAB# #TAB# figformat = 'svg' #LINE# #TAB# else: #LINE# #TAB# #TAB# figformat = 'png' #LINE# #TAB# return figformat
"Create a SHAR archive  <code> def create_shar(archive, compression, cmd, verbosity, interactive, filenames): ","#LINE# #TAB# cmdlist = [util.shell_quote(cmd)] #LINE# #TAB# cmdlist.extend([util.shell_quote(x) for x in filenames]) #LINE# #TAB# cmdlist.extend(['>', util.shell_quote(archive)]) #LINE# #TAB# return cmdlist, {'shell': True}"
"As an element is valid and contains text , extract it and return <code> def extract_text_from_element(elem): ","#LINE# #TAB# if elem and hasattr(elem, 'text') and elem.text: #LINE# #TAB# #TAB# text = elem.text #LINE# #TAB# else: #LINE# #TAB# #TAB# text = None #LINE# #TAB# return text"
"Converts an ISO week date tuple into a datetime object  <code> def iso_to_gregorian(iso_year, iso_week, iso_day): ","#LINE# #TAB# if not 1 <= iso_week <= 53: #LINE# #TAB# #TAB# raise ValueError('ISO Calendar week value must be between 1-53.') #LINE# #TAB# if not 1 <= iso_day <= 7: #LINE# #TAB# #TAB# raise ValueError('ISO Calendar day value must be between 1-7') #LINE# #TAB# fourth_jan = datetime.date(iso_year, 1, 4) #LINE# #TAB# delta = datetime.timedelta(fourth_jan.isoweekday() - 1) #LINE# #TAB# year_start = fourth_jan - delta #LINE# #TAB# gregorian = year_start + datetime.timedelta(days=iso_day - 1, weeks= #LINE# #TAB# #TAB# iso_week - 1) #LINE# #TAB# return gregorian"
"Calls the analytics_data_excel module to create the Workbook <code> def create_excel_workbook(data, result_info_key, identifier_keys): ","#LINE# #TAB# workbook = analytics_data_excel.get_excel_workbook(data, result_info_key, identifier_keys) #LINE# #TAB# adjust_column_width_workbook(workbook) #LINE# #TAB# return workbook"
Get the current stderr log scheme  <code> def stderr_log_scheme(): ,#LINE# #TAB# if LogOptions._STDOUT_LOG_SCHEME is None: #LINE# #TAB# #TAB# LogOptions.set_stderr_log_level(app.get_options(). #LINE# #TAB# #TAB# #TAB# twitter_common_log_stderr_log_level) #LINE# #TAB# return LogOptions._STDOUT_LOG_SCHEME
"Check if flatboxed RGB pixels could be converted to greyscale <code> def try_greyscale(pixels, alpha=False, dirty_alpha=True): ","#LINE# #TAB# planes = 3 + bool(alpha) #LINE# #TAB# res = list() #LINE# #TAB# apix = list() #LINE# #TAB# for row in pixels: #LINE# #TAB# #TAB# green = row[1::planes] #LINE# #TAB# #TAB# if alpha: #LINE# #TAB# #TAB# #TAB# apix.append(row[4:planes]) #LINE# #TAB# #TAB# if (green != row[0::planes] or green != row[2::planes]): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# res.append(green) #LINE# #TAB# if alpha: #LINE# #TAB# #TAB# return MergedPlanes(res, 1, apix, 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return res"
"Wait for the result of a future in a way that can be interrupted by a KeyboardInterrupt . This is not necessary in Python 3 , but is needed for Python 2 . : param future : a future to get result of : type future : Future <code> def interruptible_get_result(future): ",#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return future.result(timeout=1.0) #LINE# #TAB# #TAB# except futures.TimeoutError: #LINE# #TAB# #TAB# #TAB# pass
"Given jd as a reconstituted json.loads ( ) of the result from .to_json ( ) , return the corresponding object  <code> def from_json_dict(cls, jd, stack_is_interesting=None): ","#LINE# #TAB# stack_is_interesting = (stack_is_interesting if stack_is_interesting is not #LINE# #TAB# #TAB# None else True) #LINE# #TAB# txt = jd.pop('txt', '') #LINE# #TAB# ret = cls(txt) #LINE# #TAB# if stack_is_interesting is not None: #LINE# #TAB# #TAB# setattr(ret, 'stack_is_interesting', stack_is_interesting) #LINE# #TAB# return ret"
Configure the default logger for Flowy  <code> def setup_default_logger(): ,#LINE# #TAB# handler = logging.StreamHandler() #LINE# #TAB# handler.setFormatter(logging.Formatter( #LINE# #TAB# #TAB# '%(asctime)s %(levelname)s\t%(name)s: %(message)s')) #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# logger.setLevel('INFO') #LINE# #TAB# logger.propagate = False
"Convert text to bytes of the specified encoding  <code> def safe_bytes(value, encoding='utf-8'): ","#LINE# #TAB# if isinstance(value, six.text_type): #LINE# #TAB# #TAB# value = value.encode(encoding) #LINE# #TAB# return value"
"Given a status we give the coloration for the simple mode . : param str status : An official status output  <code> def get_simple_coloration(cls, status): ","#LINE# #TAB# if status in [PyFunceble.STATUS.official.up, PyFunceble.STATUS.official #LINE# #TAB# #TAB# .valid, PyFunceble.STATUS.official.sane]: #LINE# #TAB# #TAB# return Fore.GREEN + Style.BRIGHT #LINE# #TAB# if status in [PyFunceble.STATUS.official.down, PyFunceble.STATUS. #LINE# #TAB# #TAB# official.malicious]: #LINE# #TAB# #TAB# return Fore.RED + Style.BRIGHT #LINE# #TAB# return Fore.CYAN + Style.BRIGHT"
Get the select options for the application selector : return : <code> def get_application_choices(): ,"#LINE# #TAB# result = [] #LINE# #TAB# keys = set() #LINE# #TAB# for ct in ContentType.objects.order_by('app_label', 'model'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if issubclass(ct.model_class(), TranslatableModel #LINE# #TAB# #TAB# #TAB# #TAB# ) and ct.app_label not in keys: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(('{}'.format(ct.app_label), '{}'.format(ct. #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# app_label.capitalize()))) #LINE# #TAB# #TAB# #TAB# #TAB# keys.add(ct.app_label) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return result"
"Get the definition name of the given $ ref value(Swagger value ) . Args : ref : ref value ( ex : "" # /definitions / CustomDefinition "" ) Returns : The definition name corresponding to the ref  <code> def get_definition_name_from_ref(ref): ","#LINE# #TAB# p = re.compile('#\\/definitions\\/(.*)') #LINE# #TAB# definition_name = re.sub(p, '\\1', ref) #LINE# #TAB# return definition_name"
"Gets the bounding box for an image in display ( pixel ) coordinates <code> def get_im_window_extent(im_obj, ax): ","#LINE# #TAB# im_ext = im_obj.get_extent() #LINE# #TAB# im_pts = _np.array([[im_ext[0], im_ext[2]], [im_ext[1], im_ext[3]]]) #LINE# #TAB# bbox = _mpl.transforms.Bbox(im_pts) #LINE# #TAB# ax.parent_fig.mpl_fig.canvas.draw() #LINE# #TAB# bbox = bbox.transformed(ax.mpl_ax.transData) #LINE# #TAB# return bbox"
Returns the signature of installed Python dependencies <code> def bundle_signature(): ,"#LINE# #TAB# pkgvers = sorted((package.project_name, package.version) for package in #LINE# #TAB# #TAB# get_installed_distributions()) #LINE# #TAB# pkgvers = '|'.join('{}-{}'.format(name, version) for name, version in #LINE# #TAB# #TAB# pkgvers) #LINE# #TAB# sig = sha1(pkgvers.encode()).hexdigest() #LINE# #TAB# return {'bundle_signature': sig}"
"Helper function : generate a random number as a lower - endian digits list  <code> def random_number_lower_endian(length, base): ",#LINE# #TAB# if length == 1: #LINE# #TAB# #TAB# return [onp.random.randint(base)] #LINE# #TAB# prefix = [onp.random.randint(base) for _ in range(length - 1)] #LINE# #TAB# return prefix + [onp.random.randint(base - 1) + 1]
Return the set of all variables that appear free in the given effect  <code> def collect_effect_free_variables(eff: fs.BaseEffect): ,"#LINE# #TAB# free = set() #LINE# #TAB# _collect_effect_free_variables(eff, free) #LINE# #TAB# return free"
"sha256 may be None , in which case it is assumed no special handling through black list is needed  <code> def convert_if_needed(data, sha256, strict): ","#LINE# #TAB# decoded_data = EGG_PKG_INFO_BLACK_LIST.get(sha256) #LINE# #TAB# if decoded_data is None: #LINE# #TAB# #TAB# if strict: #LINE# #TAB# #TAB# #TAB# return data.decode(PKG_INFO_ENCODING) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return data.decode(PKG_INFO_ENCODING, 'replace') #LINE# #TAB# else: #LINE# #TAB# #TAB# return decoded_data"
"Decode raw_data from base64 / ungzip <code> def decode_data(raw_data, is_base64, is_gzip): ","#LINE# #TAB# if not raw_data: #LINE# #TAB# #TAB# return #LINE# #TAB# if is_base64: #LINE# #TAB# #TAB# raw_data = base64.b64decode(raw_data) #LINE# #TAB# if is_gzip: #LINE# #TAB# #TAB# with gzip.GzipFile(fileobj=io.BytesIO(raw_data), mode='rb') as dt: #LINE# #TAB# #TAB# #TAB# raw_data = dt.read() #LINE# #TAB# return raw_data"
Convert enbedded bytes to strings if possible . This is necessary because Python 3 makes a distinction between these types  <code> def decode_embedded_strs(src): ,"#LINE# #TAB# if not six.PY3: #LINE# #TAB# #TAB# return src #LINE# #TAB# if isinstance(src, dict): #LINE# #TAB# #TAB# return _decode_embedded_dict(src) #LINE# #TAB# elif isinstance(src, list): #LINE# #TAB# #TAB# return _decode_embedded_list(src) #LINE# #TAB# elif isinstance(src, bytes): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return src.decode() #LINE# #TAB# #TAB# except UnicodeError: #LINE# #TAB# #TAB# #TAB# return src #LINE# #TAB# else: #LINE# #TAB# #TAB# return src"
"Create patch include paths , i.e. a "" negation "" of the exclude paths  <code> def patch_path_filter(file_status, exclude_regex=None): ","#LINE# #TAB# if exclude_regex: #LINE# #TAB# #TAB# include_paths = [] #LINE# #TAB# #TAB# for file_list in file_status.values(): #LINE# #TAB# #TAB# #TAB# for fname in file_list: #LINE# #TAB# #TAB# #TAB# #TAB# if not re.match(exclude_regex, fname): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# include_paths.append(fname) #LINE# #TAB# else: #LINE# #TAB# #TAB# include_paths = ['.'] #LINE# #TAB# return include_paths"
Given a string representing a date return a datetime object <code> def datestring_to_date(datestring): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return parse_date(str(datestring), fuzzy=True, default=datetime. #LINE# #TAB# #TAB# #TAB# datetime(9999, 1, 1)) #LINE# #TAB# except Exception as exc: #LINE# #TAB# #TAB# print(' * could not parse datestring {}'.format(datestring)) #LINE# #TAB# #TAB# return datestring"
"Take the given tree , generate a conversion table and apply the transformation in - place in the tree . Also return the tree  <code> def convert_dynamic_require(tree): ","#LINE# #TAB# nodemap = {node: create_calmjs_require(node) for node in #LINE# #TAB# #TAB# extract_dynamic_require(tree)} #LINE# #TAB# replacer.replace(tree, nodemap) #LINE# #TAB# return tree"
"If current method is PUT or POST , return concatenated ` request.form ` with ` request.files ` or ` None ` otherwise  <code> def get_form_data(): ",#LINE# #TAB# if is_form_submitted(): #LINE# #TAB# #TAB# formdata = request.form #LINE# #TAB# #TAB# if request.files: #LINE# #TAB# #TAB# #TAB# formdata = formdata.copy() #LINE# #TAB# #TAB# #TAB# formdata.update(request.files) #LINE# #TAB# #TAB# return formdata #LINE# #TAB# return None
Sets up and starts / restarts the web service  <code> def setup_server(config): ,"#LINE# #TAB# web_server = WebServer(bind=config['bind'], port=config['port'], #LINE# #TAB# #TAB# ssl_certificate=config['ssl_certificate'], ssl_private_key=config[ #LINE# #TAB# #TAB# 'ssl_private_key'], base_url=config['base_url']) #LINE# #TAB# _default_app.secret_key = get_secret() #LINE# #TAB# user = get_user() #LINE# #TAB# if not user or not user.password: #LINE# #TAB# #TAB# logger.warning( #LINE# #TAB# #TAB# #TAB# 'No password set for web server, create one by using `flexget web passwd <password>`' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# if _app_register: #LINE# #TAB# #TAB# web_server.start() #LINE# #TAB# return web_server"
"Checks if the given value is a list comprised only of numbers  <code> def check_number_list(cls, label, value): ","#LINE# #TAB# if value is None or value == []: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not isinstance(value, list): #LINE# #TAB# #TAB# raise ValueError('{0} must be a list of numbers.'.format(label)) #LINE# #TAB# non_number = [k for k in value if not isinstance(k, numbers.Number)] #LINE# #TAB# if non_number: #LINE# #TAB# #TAB# raise ValueError('{0} must not contain non-number values.'.format( #LINE# #TAB# #TAB# #TAB# label)) #LINE# #TAB# return value"
"Return the offset in seconds between UTC and the given interferometer <code> def get_timezone_offset(ifo, dt=None): ",#LINE# #TAB# import pytz #LINE# #TAB# dt = dt or datetime.datetime.now() #LINE# #TAB# offset = pytz.timezone(get_timezone(ifo)).utcoffset(dt) #LINE# #TAB# return offset.days * 86400 + offset.seconds + offset.microseconds * 1e-6
"Safely encodes a string from unicode if it is n't already  <code> def from_unicode(obj, encoding=default_encoding, errors='strict'): ","#LINE# #TAB# if isinstance(obj, unicode): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# obj = obj.encode(encoding, errors=errors) #LINE# #TAB# #TAB# except UnicodeEncodeError: #LINE# #TAB# #TAB# #TAB# raise EncodingException(obj, encoding, True) #LINE# #TAB# return obj"
Check whether array is valid De : code cube  <code> def assert_isdcube(cube): ,"#LINE# #TAB# message = 'Invalid De:code cube' #LINE# #TAB# assert isinstance(cube, xr.DataArray), message #LINE# #TAB# assert set(cube.dims) == DCUBE_DIMS, message #LINE# #TAB# assert set(cube.coords) >= DCUBE_COORDS, message"
Delete specific log level by endpoint Returns a endpoint : param endpoint : endpoint that needs to be fetched : type endpoint : str : rtype : dict <code> def rm_log(endpoint): ,"#LINE# #TAB# if endpoint in SpecificLevelLog.keys(): #LINE# #TAB# #TAB# del SpecificLevelLog[endpoint] #LINE# #TAB# return {'endpoint': endpoint, 'level': _levelToName[SpecificLevelLog[ #LINE# #TAB# #TAB# endpoint]]}"
Convert list of parameters to string <code> def make_params_string(params): ,"#LINE# #TAB# p_str = '&'.join(['{}={}'.format(f, params[f]) for f in params.keys()]) #LINE# #TAB# return p_str"
"Builds a logger for you with a SlackHandler and sensible defaults  <code> def build_logger(hook_url, level=logging.DEBUG, fmt=DEFAULT_FORMAT): ",#LINE# #TAB# logger = logging.getLogger(__name__) #LINE# #TAB# handler = SlackHandler(hook_url) #LINE# #TAB# handler.setFormatter(logging.Formatter(fmt)) #LINE# #TAB# logger.addHandler(handler) #LINE# #TAB# logger.setLevel(level) #LINE# #TAB# return logger
"Returns ` int(x ) ` if ` x ` is a valid ` int ` or ` None ` otherwise . ` x ` is valid if ` 1 < = x < = limit `  <code> def int_or_none(x, limit): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# value = int(x) #LINE# #TAB# #TAB# if 1 <= value <= limit: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None
This will create the ` welcome forum ` with a welcome topic . Returns True if it 's created successfully  <code> def create_welcome_forum(): ,"#LINE# #TAB# if User.query.count() < 1: #LINE# #TAB# #TAB# return False #LINE# #TAB# user = User.query.filter_by(id=1).first() #LINE# #TAB# category = Category(title='My Category', position=1) #LINE# #TAB# category.save() #LINE# #TAB# forum = Forum(title='Welcome', description='Your first forum', #LINE# #TAB# #TAB# category_id=category.id) #LINE# #TAB# forum.save() #LINE# #TAB# topic = Topic(title='Welcome!') #LINE# #TAB# post = Post(content='Have fun with your new FlaskBB Forum!') #LINE# #TAB# topic.save(user=user, forum=forum, post=post) #LINE# #TAB# return True"
"Get places with missing tokens Parameters ---------- t Transition to enable marking Current marking <code> def get_places_with_missing_tokens(t, marking): ",#LINE# #TAB# places_with_missing = set() #LINE# #TAB# for a in t.in_arcs: #LINE# #TAB# #TAB# if marking[a.source] < a.weight: #LINE# #TAB# #TAB# #TAB# places_with_missing.add(a.source) #LINE# #TAB# return places_with_missing
"Unpack the type parameter from ContainerType[T ]  <code> def match_type_container(var, container_type_name): ",#LINE# #TAB# if var is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# data = var.data[0] #LINE# #TAB# if data.isinstance_Instance(): #LINE# #TAB# #TAB# cls = data.cls #LINE# #TAB# elif data.isinstance_Class(): #LINE# #TAB# #TAB# cls = data #LINE# #TAB# else: #LINE# #TAB# #TAB# return None #LINE# #TAB# if not (cls.isinstance_ParameterizedClass() and cls.full_name == #LINE# #TAB# #TAB# container_type_name): #LINE# #TAB# #TAB# return None #LINE# #TAB# param = cls.get_formal_type_parameter(T) #LINE# #TAB# return param
"Indicate whether the specified email address is compliant with RFC 2822 . : param address : an email address . : return : ` True ` if the specified email address is compliant with RFC 2822 ; ` False ` otherwise  <code> def is_address_valid(cls, address): ",#LINE# #TAB# if cls.REGEX_EMAIL_ADDRESS is None: #LINE# #TAB# #TAB# cls.REGEX_EMAIL_ADDRESS = re.compile(regex.REGEX_PATTERN_EMAIL_ADDRESS) #LINE# #TAB# return cls.REGEX_EMAIL_ADDRESS.match(address) is not None
"Check required fields are present : param design_dict : : param throw_exceptions : : return : <code> def validate_dict(design_dict, throw_exceptions=True): ","#LINE# #TAB# required_fields = ['name', 'description', 'metabolites', 'reactions', #LINE# #TAB# #TAB# 'genes'] #LINE# #TAB# for field in required_fields: #LINE# #TAB# #TAB# if field not in design_dict: #LINE# #TAB# #TAB# #TAB# if not throw_exceptions: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# #TAB# raise DesignError('Required field {} missing from design'. #LINE# #TAB# #TAB# #TAB# #TAB# format(field)) #LINE# #TAB# return True"
"Used for calculating mean density with a slide window <code> def slide_window_average(ratio, start, window, step): ","#LINE# #TAB# winLen = len(ratio) #LINE# #TAB# tmp_data = np.zeros(winLen) #LINE# #TAB# tmp_data[0:int(start)] += ratio[0:int(start)] #LINE# #TAB# tmp_data[-int(start):] += ratio[-int(start):] #LINE# #TAB# for j in np.arange(start, winLen - start, step): #LINE# #TAB# #TAB# tmp_data[j] += np.mean(ratio[j - int((window - 1) / 2):j + int(( #LINE# #TAB# #TAB# #TAB# window - 1) / 2)]) #LINE# #TAB# return tmp_data"
Build a default config from an iterable of options  <code> def build_default_config(options): ,"#LINE# #TAB# config = {} #LINE# #TAB# for option in options: #LINE# #TAB# #TAB# subconfig = config #LINE# #TAB# #TAB# for segment in option.path: #LINE# #TAB# #TAB# #TAB# subconfig = subconfig.setdefault(segment, {}) #LINE# #TAB# #TAB# subconfig[option.name] = option.default #LINE# #TAB# return config"
"Converts a packed float RGB format to an RGB list Args : float_rgb : RGB value packed as a float Returns : color ( list ) : 3-element list of integers [ 0 - 255,0 - 255,0 - 255 ] <code> def float_to_rgb(float_rgb): ","#LINE# #TAB# s = struct.pack('>f', float_rgb) #LINE# #TAB# i = struct.unpack('>l', s)[0] #LINE# #TAB# pack = ctypes.c_uint32(i).value #LINE# #TAB# r = (pack & 16711680) >> 16 #LINE# #TAB# g = (pack & 65280) >> 8 #LINE# #TAB# b = pack & 255 #LINE# #TAB# color = [r, g, b] #LINE# #TAB# return color"
"If a path is longer than max_width it substitute it with the first and last element joined by ... . For example this . is . a . long . path . which . we . want . to . shorten becomes this ... shorten <code> def long_path_formatter(line, max_width=pd.get_option('max_colwidth')): ","#LINE# #TAB# if len(line) > max_width: #LINE# #TAB# #TAB# tokens = line.split(""."") #LINE# #TAB# #TAB# trial1 = ""%s...%s"" % (tokens[0], tokens[-1]) #LINE# #TAB# #TAB# if len(trial1) > max_width: #LINE# #TAB# #TAB# #TAB# return ""...%s"" %(tokens[-1][-1:-(max_width-3)]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return trial1 #LINE# #TAB# else: #LINE# #TAB# #TAB# return line"
"Convert a dataset with float multiclass probabilities to a dataset with indicator probabilities by duplicating X rows and sampling true labels  <code> def expand_dataset(X, y_proba, factor=10, random_state=None, extra_arrays=None): ","#LINE# #TAB# rng = check_random_state(random_state) #LINE# #TAB# extra_arrays = extra_arrays or [] #LINE# #TAB# n_classes = y_proba.shape[1] #LINE# #TAB# classes = np.arange(n_classes, dtype=int) #LINE# #TAB# for el in zip(X, y_proba, *extra_arrays): #LINE# #TAB# #TAB# x, probs = el[0:2] #LINE# #TAB# #TAB# rest = el[2:] #LINE# #TAB# #TAB# for label in rng.choice(classes, size=factor, p=probs): #LINE# #TAB# #TAB# #TAB# yield (x, label) + rest"
"Upload public key to cloud provider . Similar to EC2 import_keypair  <code> def import_keypair(kwargs=None, call=None): ","#LINE# #TAB# with salt.utils.files.fopen(kwargs['file'], 'r') as public_key_filename: #LINE# #TAB# #TAB# public_key_content = salt.utils.stringutils.to_unicode(public_key_filename.read()) #LINE# #TAB# digitalocean_kwargs = { #LINE# #TAB# #TAB# 'name': kwargs['keyname'], #LINE# #TAB# #TAB# 'public_key': public_key_content #LINE# #TAB# } #LINE# #TAB# created_result = create_key(digitalocean_kwargs, call=call) #LINE# #TAB# return created_result"
"Reindex a pandas DataFrame so that it becomes square , meaning that the row and column indices contain the same values , in the same order . The row and column index are extended to achieve this  <code> def make_df_square(table): ","#LINE# #TAB# if not isinstance(table, pd.DataFrame): #LINE# #TAB# #TAB# return table #LINE# #TAB# if not table.index.equals(table.columns): #LINE# #TAB# #TAB# ix = list(set(table.index) | set(table.columns)) #LINE# #TAB# #TAB# ix.sort() #LINE# #TAB# #TAB# table = table.reindex(index=ix, columns=ix, fill_value=0) #LINE# #TAB# table = table.reindex(table.columns) #LINE# #TAB# return table"
Computes the approximate length of the parametric curve  <code> def length_curve(obj): ,"#LINE# #TAB# if not isinstance(obj, abstract.Curve): #LINE# #TAB# #TAB# raise GeomdlException(""Input shape must be an instance of abstract.Curve class"") #LINE# #TAB# length = 0.0 #LINE# #TAB# evalpts = obj.evalpts #LINE# #TAB# num_evalpts = len(obj.evalpts) #LINE# #TAB# for idx in range(num_evalpts - 1): #LINE# #TAB# #TAB# length += linalg.point_distance(evalpts[idx], evalpts[idx + 1]) #LINE# #TAB# return length"
SNAGGED FROM traceback . py Altered to return Data <code> def extract_stack(start=0): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# raise ZeroDivisionError #LINE# #TAB# except ZeroDivisionError: #LINE# #TAB# #TAB# trace = sys.exc_info()[2] #LINE# #TAB# #TAB# f = trace.tb_frame.f_back #LINE# #TAB# for i in range(start): #LINE# #TAB# #TAB# f = f.f_back #LINE# #TAB# stack = [] #LINE# #TAB# while f is not None: #LINE# #TAB# #TAB# stack.append({ #LINE# #TAB# #TAB# #TAB# ""line"": f.f_lineno, #LINE# #TAB# #TAB# #TAB# ""file"": f.f_code.co_filename, #LINE# #TAB# #TAB# #TAB# ""method"": f.f_code.co_name #LINE# #TAB# #TAB# }) #LINE# #TAB# #TAB# f = f.f_back #LINE# #TAB# return stack"
Do our best to extract host and database specification from DSN  <code> def split_dsn(dsn): ,"#LINE# #TAB# if dsn[:2] == '\\\\': #LINE# #TAB# #TAB# i = dsn.find('\\', 2) #LINE# #TAB# #TAB# host = dsn[2:i] #LINE# #TAB# #TAB# db = dsn[i:] #LINE# #TAB# else: #LINE# #TAB# #TAB# colon = dsn.find(':') #LINE# #TAB# #TAB# if colon in (-1, 1): #LINE# #TAB# #TAB# #TAB# host = None #LINE# #TAB# #TAB# #TAB# db = dsn #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# host, db = dsn.split(':', 1) #LINE# #TAB# return host, db"
Look at all supported languages . Assume each language is a subdirectory of the parser . Special omission of python cache and test directories <code> def get_available_languages(): ,"#LINE# #TAB# languageDir = os.path.dirname(os.path.realpath(__file__)) + '/' #LINE# #TAB# languages = set() #LINE# #TAB# for file in os.listdir(languageDir): #LINE# #TAB# #TAB# fullpath = os.path.join(languageDir, file) #LINE# #TAB# #TAB# if file != '__pycache__' and file != 'test' and os.path.isdir(fullpath #LINE# #TAB# #TAB# #TAB# ): #LINE# #TAB# #TAB# #TAB# languages.add(file) #LINE# #TAB# for entry_point in pkg_resources.iter_entry_points('msgtools.parser.plugin' #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# languages.add(entry_point.name) #LINE# #TAB# languages = list(languages) #LINE# #TAB# languages.sort() #LINE# #TAB# return languages"
Read the .last_check file  <code> def read_last_check(file_path): ,"#LINE# #TAB# _UpdateHandler._check_path(file_path=file_path) #LINE# #TAB# with io.open(file_path, mode='r') as in_file: #LINE# #TAB# #TAB# first_line = in_file.readline() #LINE# #TAB# try: #LINE# #TAB# #TAB# last_check = datetime.strptime(first_line, UTC_FMT) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return last_check"
Create default CDN attributes  <code> def azion_cdn(cdn_name): ,"#LINE# #TAB# return {'name': cdn_name, 'origin_address': 'domain.{}'.format(cdn_name #LINE# #TAB# #TAB# ), 'cname_access_only': True, 'cname': ['www1.{}'.format(cdn_name)], #LINE# #TAB# #TAB# 'delivery_protocol': 'http', 'cdn_cache_settings': 'override', #LINE# #TAB# #TAB# 'cdn_cache_settings_minimum_ttl': 2592000, 'origin_protocol_policy': #LINE# #TAB# #TAB# 'preserve'}"
Phase progeny genotypes from a trio or cross using Mendelian transmission  <code> def phase_progeny_by_transmission(g): ,"#LINE# #TAB# g = GenotypeArray(g, dtype='i1', copy=True) #LINE# #TAB# check_ploidy(g.ploidy, 2) #LINE# #TAB# check_min_samples(g.n_samples, 3) #LINE# #TAB# is_phased = _opt_phase_progeny_by_transmission(g.values) #LINE# #TAB# g.is_phased = np.asarray(is_phased).view(bool) #LINE# #TAB# return g"
"Call functions to free each member <code> def free_pyc(members, disable=False): ","#LINE# #TAB# members = members_formated(members) #LINE# #TAB# if not members: #LINE# #TAB# #TAB# return '' #LINE# #TAB# result = '' #LINE# #TAB# for m in members: #LINE# #TAB# #TAB# fname = detect_py_to_c(m) + '_free' #LINE# #TAB# #TAB# result += '%s(&%s, %s);\n' % (fname, m['name'], 1 if disable else 0) #LINE# #TAB# return result"
"Does t have wider arguments than s ? <code> def is_more_general_arg_prefix(t: FunctionLike, s: FunctionLike) ->bool: ","#LINE# #TAB# if isinstance(t, CallableType): #LINE# #TAB# #TAB# if isinstance(s, CallableType): #LINE# #TAB# #TAB# #TAB# return is_callable_compatible(t, s, is_compat=is_proper_subtype, #LINE# #TAB# #TAB# #TAB# #TAB# ignore_return=True) #LINE# #TAB# elif isinstance(t, FunctionLike): #LINE# #TAB# #TAB# if isinstance(s, FunctionLike): #LINE# #TAB# #TAB# #TAB# if len(t.items()) == len(s.items()): #LINE# #TAB# #TAB# #TAB# #TAB# return all(is_same_arg_prefix(items, itemt) for items, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# itemt in zip(t.items(), s.items())) #LINE# #TAB# return False"
Encode credential(s ) from a json file . ` Args : ` credential_file : str The path to the json file with the credential to be encoded . ` Returns : ` str The encoded credential  <code> def encode_from_json_file(credential_file): ,"#LINE# #TAB# with open(credential_file, 'r') as f: #LINE# #TAB# #TAB# data = json.load(f) #LINE# #TAB# json_str = json.dumps(data) #LINE# #TAB# encoded_str = PREFIX + b64encode(bytes(json_str, 'utf-8')).decode('utf-8') #LINE# #TAB# return encoded_str"
"Parses a URL to determine a file name . Parameters ---------- url : str URL to parse  <code> def filename_from_url(url, path=None): ","#LINE# #TAB# r = requests.get(url, stream=True) #LINE# #TAB# if 'Content-Disposition' in r.headers: #LINE# #TAB# #TAB# filename = re.findall('filename=([^;]+)', r.headers[ #LINE# #TAB# #TAB# #TAB# 'Content-Disposition'])[0].strip('""""') #LINE# #TAB# else: #LINE# #TAB# #TAB# filename = os.path.basename(urllib.parse.urlparse(url).path) #LINE# #TAB# return filename"
Cancels and closes a previously created One Line Progress Meter window : param key : Key used when meter was created : type key : ( Any ) <code> def one_line_progress_meter_cancel(key): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# meter = QuickMeter.active_meters[key] #LINE# #TAB# #TAB# meter.window.Close() #LINE# #TAB# #TAB# del QuickMeter.active_meters[key] #LINE# #TAB# #TAB# QuickMeter.exit_reasons[key] = METER_REASON_CANCELLED #LINE# #TAB# except: #LINE# #TAB# #TAB# return
Process the subscription data according to out model <code> def process_subscription_data(post_data): ,"#LINE# #TAB# subscription_data = post_data.pop('subscription', {}) #LINE# #TAB# keys = subscription_data.pop('keys', {}) #LINE# #TAB# subscription_data.update(keys) #LINE# #TAB# subscription_data['browser'] = post_data.pop('browser') #LINE# #TAB# return subscription_data"
"Toggles ` ` DEBUG ` ` value , useful for testing <code> def debug_stahp(): ",#LINE# #TAB# global DEBUG #LINE# #TAB# DEBUG = not DEBUG
"Generate a blank HTML map file : param lon_avg : center longitude : param lat_avg : center latitude : return : file name <code> def generate_blank_map_html(lon_avg, lat_avg): ","#LINE# #TAB# my_map = folium.Map(location=[lon_avg, lat_avg], zoom_start=5) #LINE# #TAB# gc_path = get_create_gridcal_folder() #LINE# #TAB# path = os.path.join(gc_path, 'map.html') #LINE# #TAB# my_map.save(path) #LINE# #TAB# return path"
"Helper function to escape uncomfortable characters  <code> def escape_chars(text, chars): ","#LINE# #TAB# text = str(text) #LINE# #TAB# chars = list(set(chars)) #LINE# #TAB# if '\\' in chars: #LINE# #TAB# #TAB# chars.remove('\\') #LINE# #TAB# #TAB# chars.insert(0, '\\') #LINE# #TAB# for ch in chars: #LINE# #TAB# #TAB# text = text.replace(ch, '\\' + ch) #LINE# #TAB# return text"
"Write each decision tree in an ensemble to a file  <code> def output_tree_ensemble(tree_ensemble_obj, output_filename, attribute_names=None): ","#LINE# #TAB# for t, tree in enumerate(tree_ensemble_obj.estimators_): #LINE# #TAB# #TAB# print(""Writing Tree {0:d}"".format(t)) #LINE# #TAB# #TAB# out_file = open(output_filename + "".{0:d}.tree"", ""w"") #LINE# #TAB# #TAB# tree_str = print_tree_recursive(tree.tree_, 0, attribute_names) #LINE# #TAB# #TAB# out_file.write(tree_str) #LINE# #TAB# #TAB# out_file.close() #LINE# #TAB# return"
Calculate inclination predicted from latitude using the dipole equation Parameter ---------- lat : latitude in degrees Returns ------- inc : inclination calculated using the dipole equation <code> def inc_from_lat(lat): ,"#LINE# #TAB# rad = old_div(np.pi, 180.0) #LINE# #TAB# inc = old_div(np.arctan(2 * np.tan(lat * rad)), rad) #LINE# #TAB# return inc"
return list of products of a reaction <code> def get_listofproducts(reaction) ->list: ,#LINE# #TAB# listOfProducts = [] #LINE# #TAB# for e in reaction: #LINE# #TAB# #TAB# tag = get_sbml_tag(e) #LINE# #TAB# #TAB# if tag == 'listOfProducts': #LINE# #TAB# #TAB# #TAB# listOfProducts = e #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return listOfProducts
"Convenience utility for managing passphrase input from stdin <code> def parse_passphrase_cmdline_arg(cls, passphrase_in): ",#LINE# #TAB# if passphrase_in == '-': #LINE# #TAB# #TAB# return sys.stdin.read().strip() #LINE# #TAB# else: #LINE# #TAB# #TAB# return passphrase_in
Coerce a retention period to a Python value  <code> def coerce_retention_period(value): ,"#LINE# #TAB# if not isinstance(value, numbers.Number): #LINE# #TAB# #TAB# if not isinstance(value, string_types): #LINE# #TAB# #TAB# #TAB# msg = ""Expected string, got %s instead!"" #LINE# #TAB# #TAB# #TAB# raise ValueError(msg % type(value)) #LINE# #TAB# #TAB# value = value.strip() #LINE# #TAB# #TAB# if value.lower() == 'always': #LINE# #TAB# #TAB# #TAB# value = 'always' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# value = simple_eval(value) #LINE# #TAB# #TAB# #TAB# if not isinstance(value, numbers.Number): #LINE# #TAB# #TAB# #TAB# #TAB# msg = ""Expected numeric result, got %s instead!"" #LINE# #TAB# #TAB# #TAB# #TAB# raise ValueError(msg % type(value)) #LINE# #TAB# return value"
"Check whether matplotlib is using an inline backend , e.g. for notebooks <code> def use_inline_backend(): ",#LINE# #TAB# if 'matplotlib.pyplot' in sys.modules: #LINE# #TAB# #TAB# backend = mpl.get_backend() #LINE# #TAB# #TAB# return backend.endswith('inline') or backend == 'nbAgg'
"Return the data for the alien status <code> def reftrack_alien_data(rt, role): ","#LINE# #TAB# alien = rt.alien() #LINE# #TAB# if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole: #LINE# #TAB# #TAB# if alien: #LINE# #TAB# #TAB# #TAB# return ""Yes"" #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return ""No"""
"Validate extra context  <code> def validate_extra_context(ctx, param, value): ","#LINE# #TAB# for s in value: #LINE# #TAB# #TAB# if '=' not in s: #LINE# #TAB# #TAB# #TAB# raise click.BadParameter( #LINE# #TAB# #TAB# #TAB# #TAB# ""EXTRA_CONTEXT should contain items of the form key=value; '{}' doesn't match that form"" #LINE# #TAB# #TAB# #TAB# #TAB# .format(s)) #LINE# #TAB# return collections.OrderedDict(s.split('=', 1) for s in value) or None"
Assert that the class is correctly configured <code> def class_check(cls): ,"#LINE# #TAB# assert issubclass(cls.MODEL_CLASS, Base #LINE# #TAB# #TAB# ), 'Must set the MODEL_CLASS in the derived class to a SQLA model'"
Returns the command used to execute the ( GNU ) ` ` time ` ` tool ( not the built in shell tool )  <code> def time_file(_tmp=[]) ->str: ,#LINE# #TAB# if len(_tmp) == 0: #LINE# #TAB# #TAB# if on_apple_os(): #LINE# #TAB# #TAB# #TAB# if does_program_exist('gtime'): #LINE# #TAB# #TAB# #TAB# #TAB# _tmp.append(shutil.which('gtime')) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return 'false && ' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _tmp.append('/usr/bin/time') #LINE# #TAB# assert _tmp[0] is not None #LINE# #TAB# return _tmp[0]
"This behaviour allows a composite with oneshot ancestors to run multiple times , resetting the oneshot variables after each execution <code> def repeatable_behavior(behaviour, name=None): ","#LINE# #TAB# if not name: #LINE# #TAB# #TAB# name = behaviour.name #LINE# #TAB# clear_descendant_variables = ClearBlackboardVariablesStartingWith(name= #LINE# #TAB# #TAB# 'Clear Descendant Variables of {}'.format(name), #LINE# #TAB# #TAB# variable_name_beginning=get_py_tree_path(behaviour) + '>') #LINE# #TAB# if isinstance(behaviour, py_trees.composites.Sequence): #LINE# #TAB# #TAB# behaviour.add_child(clear_descendant_variables) #LINE# #TAB# #TAB# sequence = behaviour #LINE# #TAB# else: #LINE# #TAB# #TAB# sequence = py_trees.composites.Sequence(name='RepeatableBehaviour') #LINE# #TAB# #TAB# sequence.add_children([behaviour, clear_descendant_variables]) #LINE# #TAB# return sequence"
Convert the names of the supported built - in operations to lowercase for comparison and invoke generation purpose  <code> def get_lowercase_builtin_map(builtin_map_capitalised_dict): ,#LINE# #TAB# builtin_map_dict = {} #LINE# #TAB# for fortran_name in builtin_map_capitalised_dict: #LINE# #TAB# #TAB# python_name = builtin_map_capitalised_dict[fortran_name] #LINE# #TAB# #TAB# builtin_map_dict[fortran_name.lower()] = python_name #LINE# #TAB# return builtin_map_dict
"Returns a sequence of trees.nodeid and trees.slug to be used to update the trees slug table value  <code> def generate_update_values(nodeid, title): ","#LINE# #TAB# logger.info('processing... {} - {}'.format(nodeid, title)) #LINE# #TAB# try: #LINE# #TAB# #TAB# slug = generate_slug(*title) #LINE# #TAB# except: #LINE# #TAB# #TAB# logger.exception(""failed to create slug for '{}'"".format(title)) #LINE# #TAB# #TAB# raise #LINE# #TAB# logger.info('... using {}'.format(slug)) #LINE# #TAB# return [str(nodeid), slug]"
"Get the ID and class of a window Returns ------- Tuple[str , str ] ( window i d , window class ) <code> def get_wm_class(window): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# reply = _get_wm_class(window).reply() #LINE# #TAB# except struct.error: #LINE# #TAB# #TAB# raise WMError('Invalid window: %s', window) #LINE# #TAB# try: #LINE# #TAB# #TAB# return reply[0], reply[1] #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return None, None"
"Removes all names after a certain position . If position is None , just returns the names list  <code> def filter_after_position(names, position): ","#LINE# #TAB# if position is None: #LINE# #TAB# #TAB# return names #LINE# #TAB# names_new = [] #LINE# #TAB# for n in names: #LINE# #TAB# #TAB# if n.start_pos[0] is not None and n.start_pos < position or isinstance( #LINE# #TAB# #TAB# #TAB# n.get_definition(), (tree.CompFor, tree.Lambda)): #LINE# #TAB# #TAB# #TAB# names_new.append(n) #LINE# #TAB# return names_new"
"Create a register object from a CSV file : param csvfile : CSV file path or stream : type csvfile : os . PathLike or file object <code> def from_csv(cls, csvfile): ","#LINE# #TAB# if isinstance(csvfile, (str, PathLike)): #LINE# #TAB# #TAB# with open(csvfile, newline='', encoding='utf-8') as f: #LINE# #TAB# #TAB# #TAB# data = dict(cls.read_csv(f)) #LINE# #TAB# else: #LINE# #TAB# #TAB# data = dict(cls.read_csv(csvfile)) #LINE# #TAB# register = cls.__new__(cls) #LINE# #TAB# register.data = data #LINE# #TAB# return register"
"Fix the discrepancy coming from old Raspa calculations , having a typo in the conversion label  <code> def get_molec_uc_to_mg_g(isot_dict): ",#LINE# #TAB# if 'conversion_factor_molec_uc_to_gr_gr' in isot_dict.get_dict(): #LINE# #TAB# #TAB# molec_uc_to_mg_g = isot_dict['conversion_factor_molec_uc_to_gr_gr'] #LINE# #TAB# elif 'conversion_factor_molec_uc_to_mg_g' in isot_dict.get_dict(): #LINE# #TAB# #TAB# molec_uc_to_mg_g = isot_dict['conversion_factor_molec_uc_to_mg_g'] #LINE# #TAB# return molec_uc_to_mg_g
Compare ground - state SCF cycles  <code> def abicomp_gs_scf(options): ,"#LINE# #TAB# paths = options.paths #LINE# #TAB# f0 = abilab.AbinitOutputFile(paths[0]) #LINE# #TAB# figures = f0.compare_gs_scf_cycles(paths[1:]) #LINE# #TAB# if not figures: #LINE# #TAB# #TAB# cprint('Cannot find GS-SCF sections in output files.', 'yellow') #LINE# #TAB# return 0"
return True if the Graph as cycle <code> def has_cycle(G): ,"#LINE# #TAB# if not isinstance(G, nx.DiGraph): #LINE# #TAB# #TAB# raise TypeError(""I'm expecting a DiGraph"") #LINE# #TAB# try: #LINE# #TAB# #TAB# nx.find_cycle(G) #LINE# #TAB# except nx.NetworkXNoCycle: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Remove a handler from a loop , ignoring EBADF or KeyError  <code> def try_remove_handler(loop, fd): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# loop.remove_handler(fd) #LINE# #TAB# except (KeyError, Error.EBADF): #LINE# #TAB# #TAB# pass"
"returns julian day = day since Jan 1 of year <code> def julian_day(year, month, day): ","#LINE# #TAB# ""returns julian day=day since Jan 1 of year"" #LINE# #TAB# hr = 12 #LINE# #TAB# t = time.mktime((year, month, day, hr, 0, 0.0, 0, 0, -1)) #LINE# #TAB# julDay = time.localtime(t)[7] #LINE# #TAB# return julDay"
"Get / cache a language pack Returns the langugage pack from cache if it exists , caches otherwise > > > get_language_pack('fr')['Dashboards ' ] "" Tableaux de bords "" <code> def get_language_pack(locale): ",#LINE# #TAB# pack = ALL_LANGUAGE_PACKS.get(locale) #LINE# #TAB# if not pack: #LINE# #TAB# #TAB# filename = DIR + '/{}/LC_MESSAGES/messages.json'.format(locale) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with open(filename) as f: #LINE# #TAB# #TAB# #TAB# #TAB# pack = json.load(f) #LINE# #TAB# #TAB# #TAB# #TAB# ALL_LANGUAGE_PACKS[locale] = pack #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return pack
Get the default global configuration option Returns ------- str Path to global config file <code> def get_default_config_filepath() ->str: ,"#LINE# #TAB# path = os.path.split(__file__)[0] #LINE# #TAB# globalConfigFile = os.path.join(path, '..', 'resisticsConfig.ini') #LINE# #TAB# if not checkFilepath(globalConfigFile): #LINE# #TAB# #TAB# errorPrint('getDefaultConfig', #LINE# #TAB# #TAB# #TAB# 'Default configuration file could not be found', quitRun=True) #LINE# #TAB# return globalConfigFile"
"Constructs a sensible plot title from the ` ` model ` `  <code> def get_plot_title(main, model, prefix=''): ",#LINE# #TAB# if main is not None: #LINE# #TAB# #TAB# main = prefix #LINE# #TAB# #TAB# title = model.get_title() #LINE# #TAB# #TAB# if title: #LINE# #TAB# #TAB# #TAB# main += f': {title}' #LINE# #TAB# return main
"Find all user sets  <code> def get_user_sets(client_id, user_id): ","#LINE# #TAB# data = api_call('get', 'users/{}/sets'.format(user_id), client_id=client_id) #LINE# #TAB# return [WordSet.from_dict(wordset) for wordset in data]"
"Legacy filter for determining if a given dict is present  <code> def is_sub_dict(sub_dict, dictionary): ","#LINE# #TAB# for key in sub_dict.keys(): #LINE# #TAB# #TAB# if key not in dictionary: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if (type(sub_dict[key]) is not dict) and (sub_dict[key] != dictionary[key]): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if (type(sub_dict[key]) is dict) and (not is_sub_dict(sub_dict[key], dictionary[key])): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Attempts to parse a date formatted in ISO 8601 format <code> def datetime_is_iso(date_str): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if len(date_str) > 10: #LINE# #TAB# #TAB# #TAB# dt = isodate.parse_datetime(date_str) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dt = isodate.parse_date(date_str) #LINE# #TAB# #TAB# return True, [] #LINE# #TAB# except: #LINE# #TAB# #TAB# return False, ['Datetime provided is not in a valid ISO 8601 format']"
"Update allele frequency . Parameters ---------- ` ` ind_base_prob ` ` : 2d - array like , n x 4 matrix  <code> def m_step(ind_base_prob): ",#LINE# #TAB# sample_size = ind_base_prob.shape[0] #LINE# #TAB# expect_allele_prob = ind_base_prob.sum(axis=0) / sample_size #LINE# #TAB# return expect_allele_prob
Converts an arbitary string to one that could be used as a filename  <code> def clean_str(string): ,#LINE# #TAB# cleaned = ''.join(c if c in CLEAN_CHARS else '.' for c in string) #LINE# #TAB# cleaned = cleaned.lower() #LINE# #TAB# if cleaned.startswith('.'): #LINE# #TAB# #TAB# cleaned = '_' + cleaned #LINE# #TAB# if len(cleaned) > MAX_CLEAN_STR_LEN: #LINE# #TAB# #TAB# head_len = MAX_CLEAN_STR_LEN // 2 - 1 #LINE# #TAB# #TAB# tail_len = MAX_CLEAN_STR_LEN - (head_len + 3) #LINE# #TAB# #TAB# cleaned = cleaned[:head_len] + '...' + cleaned[-tail_len:] #LINE# #TAB# return cleaned
"Returns a ( hostname , port ) from the environ variable if set <code> def get_mpd_default(): ","#LINE# #TAB# if not mpd: #LINE# #TAB# #TAB# raise Exception('Could not find the module python-mpd') #LINE# #TAB# address = ['localhost', 6600] #LINE# #TAB# if 'MPD_HOST' in os.environ.keys(): #LINE# #TAB# #TAB# address[0] = os.environ['MPD_HOST'] #LINE# #TAB# if 'MPD_PORT' in os.environ.keys(): #LINE# #TAB# #TAB# address[1] = os.environ['MPD_PORT'] #LINE# #TAB# return address"
Reads a file and returns it as json object . Expands home <code> def read_json(path): ,#LINE# #TAB# if path: #LINE# #TAB# #TAB# data = read_file(path) #LINE# #TAB# #TAB# if data: #LINE# #TAB# #TAB# #TAB# return json.loads(data) #LINE# #TAB# return None
"Check if a string 's Chinese characters are Traditional . This is equivalent to : > > > identify('foo ' ) in ( TRADITIONAL , BOTH ) <code> def is_traditional(s): ",#LINE# #TAB# chinese = _get_hanzi(s) #LINE# #TAB# if not chinese: #LINE# #TAB# #TAB# return False #LINE# #TAB# elif chinese.issubset(_SHARED_CHARACTERS): #LINE# #TAB# #TAB# return True #LINE# #TAB# elif chinese.issubset(_TRADITIONAL_CHARACTERS): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Return the list of built - in probes  <code> def list_probes(): ,"#LINE# #TAB# curdir = op.realpath(op.dirname(__file__)) #LINE# #TAB# return [op.splitext(fn)[0] for fn in os.listdir(op.join(curdir, 'probes')) #LINE# #TAB# #TAB# #TAB# if fn.endswith('.prb')]"
Convert string value to native python values . : param str value : value to interprete . : returns : the value coerced to python type <code> def native_value(value): ,"#LINE# #TAB# if isinstance(value, str): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# parsed_value = json.loads(value) #LINE# #TAB# #TAB# #TAB# if parsed_value != math.inf: #LINE# #TAB# #TAB# #TAB# #TAB# value = parsed_value #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return value #LINE# #TAB# return value"
Get current user  <code> def get_current_user(): ,#LINE# #TAB# if not 'user_id' in session: #LINE# #TAB# #TAB# return None #LINE# #TAB# user = User.query.filter(User.id == session['user_id']).first() #LINE# #TAB# if not user: #LINE# #TAB# #TAB# signout_user() #LINE# #TAB# #TAB# return None #LINE# #TAB# return user
A bar chart showing value axis region starting * below * zero  <code> def sample_v4b(): ,"#LINE# #TAB# drawing = Drawing(400, 200) #LINE# #TAB# data = [(13, 20)] #LINE# #TAB# bc = VerticalBarChart() #LINE# #TAB# bc.x = 50 #LINE# #TAB# bc.y = 50 #LINE# #TAB# bc.height = 125 #LINE# #TAB# bc.width = 300 #LINE# #TAB# bc.data = data #LINE# #TAB# bc.strokeColor = colors.black #LINE# #TAB# bc.valueAxis.valueMin = -10 #LINE# #TAB# bc.valueAxis.valueMax = 60 #LINE# #TAB# bc.valueAxis.valueStep = 15 #LINE# #TAB# bc.categoryAxis.labels.boxAnchor = 'n' #LINE# #TAB# bc.categoryAxis.labels.dy = -5 #LINE# #TAB# bc.categoryAxis.categoryNames = ['Ying', 'Yang'] #LINE# #TAB# drawing.add(bc) #LINE# #TAB# return drawing"
"Vectorized version of m2f midinotes : an array of midinotes out : if given , put the result here <code> def m2f_np(midinotes: np.ndarray, out: np.ndarray=None) ->np.ndarray: ","#LINE# #TAB# midinotes = np.asarray(midinotes, dtype=float) #LINE# #TAB# out = np.subtract(midinotes, 69, out=out) #LINE# #TAB# out /= 12.0 #LINE# #TAB# out = np.power(2.0, out, out) #LINE# #TAB# out *= A4 #LINE# #TAB# return out"
Get raw mediainfo output for given filename  <code> def get_raw_xml(filename): ,"#LINE# #TAB# p = Popen(['mediainfo', '--output=XML', filename], stdout=PIPE) #LINE# #TAB# out, err = p.communicate() #LINE# #TAB# if err: #LINE# #TAB# #TAB# raise MediaInfoError(err) #LINE# #TAB# else: #LINE# #TAB# #TAB# return out"
"Standard pre - processing callback . Save a pointer to the processed objects  <code> def before_processing(eng, objects): ",#LINE# #TAB# eng.signal.workflow_started(eng) #LINE# #TAB# eng.state.current_object_processed = False #LINE# #TAB# eng.objects = objects
Check if name comes from a block on the main path : param name : block_id : rtype : bool <code> def is_main_name(name): ,#LINE# #TAB# if 'alt' not in name: #LINE# #TAB# #TAB# return True #LINE# #TAB# if name.count('chr') > 1: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Valid a cellphone number . : param phone_number : Cellphone number . : type phone_number : str : returns : true if it 's a valid cellphone number . : rtype : bool <code> def valid_number(phone_number): ,"#LINE# #TAB# phone_number = phone_number.replace(' ', '') #LINE# #TAB# if len(phone_number) != 10: #LINE# #TAB# #TAB# return False #LINE# #TAB# for i in range(10): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# int(phone_number[i]) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"loading data from scratch or from cache <code> def load_data(cachestr, tags, fileending): ","#LINE# #TAB# filename = _get_cachefile(cachestr, tags, fileending) #LINE# #TAB# if filename is not None and os.path.exists(filename): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
Import all polygons from a .poly file . Returns a list of the imported polygon filters <code> def import_all(path): ,"#LINE# #TAB# plist = [] #LINE# #TAB# fid = 0 #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p = PolygonFilter(filename=path, fileid=fid) #LINE# #TAB# #TAB# #TAB# plist.append(p) #LINE# #TAB# #TAB# #TAB# fid += 1 #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return plist"
Shamelessly taken from DateField.to_python : param value : : return : <code> def parse_date(value): ,#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return value #LINE# #TAB# try: #LINE# #TAB# #TAB# parsed = parse_date(value) #LINE# #TAB# #TAB# if parsed is not None: #LINE# #TAB# #TAB# #TAB# return parsed #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValidationError #LINE# #TAB# raise ValidationError
_ declared_class_registry might be safer .. <code> def get_metadata(cls): ,"#LINE# #TAB# for c in cls.__mro__: #LINE# #TAB# #TAB# print('mro', c) #LINE# #TAB# #TAB# if hasattr(c, 'metadata'): #LINE# #TAB# #TAB# #TAB# return c.metadata"
"Set the x - scale of the canvas such that the minimum x value is min and the maximum x value is max  <code> def set_xscale(min=_DEFAULT_XMIN, max=_DEFAULT_XMAX): ",#LINE# #TAB# global _xmin #LINE# #TAB# global _xmax #LINE# #TAB# min = float(min) #LINE# #TAB# max = float(max) #LINE# #TAB# if min >= max: #LINE# #TAB# #TAB# raise Exception('min must be less than max') #LINE# #TAB# size = max - min #LINE# #TAB# _xmin = min - _BORDER * size #LINE# #TAB# _xmax = max + _BORDER * size
Build a hass frontend ready string out of the sourceAttribution  <code> def build_hass_attribution(source_attribution: Dict) ->Optional[str]: ,"#LINE# #TAB# suppliers = source_attribution.get('supplier') #LINE# #TAB# if suppliers is not None: #LINE# #TAB# #TAB# supplier_titles = [] #LINE# #TAB# #TAB# for supplier in suppliers: #LINE# #TAB# #TAB# #TAB# title = supplier.get('title') #LINE# #TAB# #TAB# #TAB# if title is not None: #LINE# #TAB# #TAB# #TAB# #TAB# supplier_titles.append(title) #LINE# #TAB# #TAB# joined_supplier_titles = ','.join(supplier_titles) #LINE# #TAB# #TAB# attribution = ( #LINE# #TAB# #TAB# #TAB# f'With the support of {joined_supplier_titles}. All information is provided without warranty of any kind.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return attribution"
"Returns the segment name based on recorder configuration and input host name . This is a helper generally used in web framework middleware where a host name is available from incoming request s headers  <code> def calculate_segment_name(host_name, recorder): ",#LINE# #TAB# if recorder.dynamic_naming: #LINE# #TAB# #TAB# return recorder.dynamic_naming.get_name(host_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# return recorder.service
"returns the std deviation , the mean , and the median of an array : param values : array to calculate : return : std deviation , mean , median <code> def calc_utils(values: numpy.array) ->Tuple[float, float, float]: ","#LINE# #TAB# mean = numpy.mean(values, dtype=float) #LINE# #TAB# stddev = numpy.std(values, dtype=float) #LINE# #TAB# median = numpy.median(values) #LINE# #TAB# return stddev, mean, median"
Returns server operator name or None  <code> def sc_opname(operator): ,#LINE# #TAB# ret = sc_spindex_opname(operator) #LINE# #TAB# return ret[1]
"Make a simple atmospheric correction formula  <code> def simple_atmo_opstring(haze, contrast, bias): ","#LINE# #TAB# gamma_b = 1 - haze #LINE# #TAB# gamma_g = 1 - (haze / 3.0) #LINE# #TAB# ops = ( #LINE# #TAB# #TAB# ""gamma g {gamma_g}, "" ""gamma b {gamma_b}, "" ""sigmoidal rgb {contrast} {bias}"" #LINE# #TAB# ).format(gamma_g=gamma_g, gamma_b=gamma_b, contrast=contrast, bias=bias) #LINE# #TAB# return ops"
"Read variables from the environment : param env_vars : a dict with key a name , value a environment variable name : param strict : boolean , if True enforces that all specified environment variables are found <code> def read_environment(env_vars, strict=False): ","#LINE# #TAB# result = dict([(k, os.environ.get(v)) for k, v in env_vars.items() if v in #LINE# #TAB# #TAB# os.environ]) #LINE# #TAB# if not len(env_vars) == len(result): #LINE# #TAB# #TAB# missing = ','.join([('%s / %s' % (k, v)) for k, v in env_vars.items #LINE# #TAB# #TAB# #TAB# () if k not in result]) #LINE# #TAB# #TAB# msg = 'Following name/variable not found in environment: %s' % missing #LINE# #TAB# #TAB# if strict: #LINE# #TAB# #TAB# #TAB# raise EasyBuildError(msg) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# _log.debug(msg) #LINE# #TAB# return result"
Create a saver for the variables we want to checkpoint . Args : exclude : List of regexes to match variable names to exclude . Returns : Saver object  <code> def define_saver(exclude=None): ,"#LINE# #TAB# variables = [] #LINE# #TAB# exclude = exclude or [] #LINE# #TAB# exclude = [re.compile(regex) for regex in exclude] #LINE# #TAB# for variable in tf.global_variables(): #LINE# #TAB# #TAB# if any(regex.match(variable.name) for regex in exclude): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# variables.append(variable) #LINE# #TAB# saver = tf.train.Saver(variables, keep_checkpoint_every_n_hours=5) #LINE# #TAB# return saver"
"Open the file "" multinest.txt "" and extract the weight values of every accepted live point as a list  <code> def weights_from_file_weighted_samples(file_weighted_samples) ->[float]: ",#LINE# #TAB# weighted_samples = open(file_weighted_samples) #LINE# #TAB# total_samples = 0 #LINE# #TAB# for line in weighted_samples: #LINE# #TAB# #TAB# total_samples += 1 #LINE# #TAB# weighted_samples.seek(0) #LINE# #TAB# log_likelihoods = [] #LINE# #TAB# for line in range(total_samples): #LINE# #TAB# #TAB# weighted_samples.read(4) #LINE# #TAB# #TAB# log_likelihoods.append(float(weighted_samples.read(24))) #LINE# #TAB# #TAB# weighted_samples.readline() #LINE# #TAB# weighted_samples.close() #LINE# #TAB# return log_likelihoods
"Concatenate the index and the value of the series  <code> def label_both(label_info, multi_line=True, sep=': '): ","#LINE# #TAB# label_info = label_info.astype(str) #LINE# #TAB# for var in label_info.index: #LINE# #TAB# #TAB# label_info[var] = '{0}{1}{2}'.format(var, sep, label_info[var]) #LINE# #TAB# if not multi_line: #LINE# #TAB# #TAB# label_info = collapse_label_lines(label_info) #LINE# #TAB# return label_info"
"Returns True if the two strings are equal , False otherwise . The time taken is independent of the number of characters that match . Theoretically , this is useful in avoiding timing attacks related to simple string equality checks  <code> def constant_time_compare(val1, val2): ","#LINE# #TAB# if len(val1) != len(val2): #LINE# #TAB# #TAB# return False #LINE# #TAB# result = 0 #LINE# #TAB# for x, y in zip(val1, val2): #LINE# #TAB# #TAB# result |= ord(x) ^ ord(y) #LINE# #TAB# return result == 0"
Attempt to split a URL to a filename on disk <code> def split_url_to_filename(s): ,"#LINE# #TAB# url_info = urllib.parse.urlsplit(s) #LINE# #TAB# l = [sanitize_str(url_info.netloc)] #LINE# #TAB# for part in url_info.path.lstrip('/').split('/'): #LINE# #TAB# #TAB# part = sanitize_str(part) #LINE# #TAB# #TAB# l.append(part) #LINE# #TAB# #TAB# if not part: #LINE# #TAB# #TAB# #TAB# l[-1] = append_index_filename(part) #LINE# #TAB# if url_info.query: #LINE# #TAB# #TAB# l[-1] += '_' + sanitize_str(url_info.query) #LINE# #TAB# if frozenset([os.curdir, os.pardir, '.', '..']) & frozenset(l): #LINE# #TAB# #TAB# raise ValueError('Path contains directory traversal filenames') #LINE# #TAB# return l"
"Please have a look at the function description / documentation in the V - REP user manual <code> def simx_get_distance_handle(clientID, distanceObjectName, operationMode): ","#LINE# #TAB# handle = ct.c_int() #LINE# #TAB# if sys.version_info[0] == 3 and type(distanceObjectName) is str: #LINE# #TAB# #TAB# distanceObjectName = distanceObjectName.encode('utf-8') #LINE# #TAB# return c_GetDistanceHandle(clientID, distanceObjectName, ct.byref( #LINE# #TAB# #TAB# handle), operationMode), handle.value"
"Get env to be used by the script process . This env must at the very least contain the proxy url , and a PATH allowing bash scripts to use ` ctx ` , which is expected to live next to the current executable  <code> def get_process_environment(process, proxy): ","#LINE# #TAB# env = os.environ.copy() #LINE# #TAB# env.setdefault('TMPDIR', get_exec_tempdir()) #LINE# #TAB# process_env = process.get('env', {}) #LINE# #TAB# env.update(process_env) #LINE# #TAB# env[CTX_SOCKET_URL] = proxy.socket_url #LINE# #TAB# env_path = env.get('PATH') #LINE# #TAB# bin_dir = os.path.dirname(sys.executable) #LINE# #TAB# if env_path: #LINE# #TAB# #TAB# if bin_dir not in env_path.split(os.pathsep): #LINE# #TAB# #TAB# #TAB# env['PATH'] = os.pathsep.join([env_path, bin_dir]) #LINE# #TAB# else: #LINE# #TAB# #TAB# env['PATH'] = bin_dir #LINE# #TAB# return env"
"Traverse the directory tree identified by start until a directory already in checked is encountered or the path of mapreduce . yaml is found  <code> def find_mapreduce_yaml(start, checked): ","#LINE# dir = start #LINE# while dir not in checked: #LINE# #TAB# checked.add(dir) #LINE# #TAB# for mr_yaml_name in MR_YAML_NAMES: #LINE# #TAB# yaml_path = os.path.join(dir, mr_yaml_name) #LINE# #TAB# if os.path.exists(yaml_path): #LINE# #TAB# #TAB# return yaml_path #LINE# #TAB# dir = os.path.dirname(dir) #LINE# return None"
"Check that the package_name is not registered on Dapi  <code> def check_name_not_on_dapi(cls, dap): ","#LINE# #TAB# #TAB# problems = list() #LINE# #TAB# #TAB# if dap.meta['package_name']: #LINE# #TAB# #TAB# #TAB# from . import dapicli #LINE# #TAB# #TAB# #TAB# d = dapicli.metadap(dap.meta['package_name']) #LINE# #TAB# #TAB# #TAB# if d: #LINE# #TAB# #TAB# #TAB# #TAB# problems.append(DapProblem('This dap name is already registered on Dapi', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# level=logging.WARNING)) #LINE# #TAB# #TAB# return problems"
"Create and return an anonymous gist with a single file and specified contents <code> def make_gist(contents, description='', filename='data.geojson'): ","#LINE# #TAB# ghapi = github3.GitHub() #LINE# #TAB# files = {filename: {'content': contents}} #LINE# #TAB# gist = ghapi.create_gist(description, files) #LINE# #TAB# return gist"
Validate ScalingConfiguration capacity for serverless DBCluster <code> def validate_capacity(capacity): ,"#LINE# #TAB# if capacity not in VALID_SCALING_CONFIGURATION_CAPACITIES: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""ScalingConfiguration capacity must be one of: {}"".format( #LINE# #TAB# #TAB# #TAB# #TAB# "", "".join(map( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# str, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# VALID_SCALING_CONFIGURATION_CAPACITIES #LINE# #TAB# #TAB# #TAB# #TAB# )) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# ) #LINE# #TAB# return capacity"
"Download image from url to the destination : param url : url to the image : param destination : where the image will be stored : return : None <code> def download_image(url, destination=''): ","#LINE# #TAB# f_name = url.split('/')[-1] #LINE# #TAB# f_dest = os.path.join(destination, f_name) #LINE# #TAB# f_dest += '.jpg' if '.' not in f_name else '' #LINE# #TAB# if os.path.isfile(f_dest): #LINE# #TAB# #TAB# return f_dest #LINE# #TAB# page = requests.get(url) #LINE# #TAB# with open(f_dest, 'wb') as f: #LINE# #TAB# #TAB# f.write(page.content) #LINE# #TAB# return f_dest"
split data using | separator and remove first item ( because the first item is empty ) <code> def get_parts(data): ,#LINE# #TAB# parts = data.split('|')[1:] #LINE# #TAB# return parts
Returns a valid new index for the given list widget ` list_wid ` <code> def make_valid_index(list_wid): ,"#LINE# #TAB# lowest_allowed = 0 #LINE# #TAB# if str(list_wid.objectName()) == 'listActions': #LINE# #TAB# #TAB# lowest_allowed = 1 #LINE# #TAB# for i in range(lowest_allowed, 31): #LINE# #TAB# #TAB# valid = True #LINE# #TAB# #TAB# for row in range(0, list_wid.count()): #LINE# #TAB# #TAB# #TAB# if int(str(list_wid.item(row).text()).split()[0]) == i: #LINE# #TAB# #TAB# #TAB# #TAB# valid = False #LINE# #TAB# #TAB# if valid is True: #LINE# #TAB# #TAB# #TAB# return i"
"Returns a sanitized filename from the one given  <code> def get_safe_filename(s, default_filename='unnamed_file'): ","#LINE# #TAB# if not s: #LINE# #TAB# #TAB# return default_filename #LINE# #TAB# if len(s) > 255: #LINE# #TAB# #TAB# s, ext = os.path.splitext(s) #LINE# #TAB# #TAB# ext = ext[:255] #LINE# #TAB# #TAB# s = s[:255 - len(ext)] + ext #LINE# #TAB# s = s.replace(' ', '_') #LINE# #TAB# s = get_valid_filename(s).strip('.') #LINE# #TAB# if len(s) == 0: #LINE# #TAB# #TAB# return default_filename #LINE# #TAB# return s"
"Open and return the repository containing the specified file  <code> def open_repo(args, path_key='<path>'): ","#LINE# #TAB# path = pathlib.Path(args[path_key]) if args[path_key] else None #LINE# #TAB# try: #LINE# #TAB# #TAB# repo = open_repository(path) #LINE# #TAB# except ValueError as exc: #LINE# #TAB# #TAB# raise ExitError(ExitCode.DATA_ERR, str(exc)) #LINE# #TAB# return repo"
"node_type = "" metabolites "" | "" reactions "" <code> def degree_dist(model, node_type='metabolites', bipartite=True): ","#LINE# #TAB# rv = {} #LINE# #TAB# if 'met' in node_type: #LINE# #TAB# #TAB# names = model.Metabolites() #LINE# #TAB# elif 'reac' in node_type: #LINE# #TAB# #TAB# names = model.Reactions() #LINE# #TAB# for n in names: #LINE# #TAB# #TAB# deg = model.Degree(n, bipartite=bipartite) #LINE# #TAB# #TAB# if deg in rv.keys(): #LINE# #TAB# #TAB# #TAB# rv[deg] = rv[deg] + 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# rv[deg] = 1 #LINE# #TAB# return rv"
Get time span in milliseconds of files in a queue  <code> def queue_duration(queue): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# oldkey, _ = queue[0] #LINE# #TAB# #TAB# newkey, _ = queue[-1] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# return newkey - oldkey"
"Divide the received image in multiple tiles <code> def slice_image(image, divWidth, divHeight): ","#LINE# #TAB# w, h = image.size #LINE# #TAB# tiles = [] #LINE# #TAB# for y in range(0, h - 1, h / divHeight): #LINE# #TAB# #TAB# my = min(y + h / divHeight, h) #LINE# #TAB# #TAB# for x in range(0, w - 1, w / divWidth): #LINE# #TAB# #TAB# #TAB# mx = min(x + w / divWidth, w) #LINE# #TAB# #TAB# #TAB# tiles.append(image.crop((x, y, mx, my))) #LINE# #TAB# return tiles"
"Locate file in folder <code> def find_path(folder, file_name): ","#LINE# #TAB# for root_folder, folder_names, file_names in walk(folder): #LINE# #TAB# #TAB# if file_name in file_names: #LINE# #TAB# #TAB# #TAB# file_path = join(root_folder, file_name) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# raise IOError('cannot find {0} in {1}'.format(file_name, folder)) #LINE# #TAB# return file_path"
Unpack the whole buffer including header pack  <code> def unpack_message(buffer): ,"#LINE# #TAB# hdr_size = Header().get_size() #LINE# #TAB# hdr_buff, msg_buff = buffer[:hdr_size], buffer[hdr_size:] #LINE# #TAB# header = Header() #LINE# #TAB# header.unpack(hdr_buff) #LINE# #TAB# message = new_message_from_header(header) #LINE# #TAB# message.unpack(msg_buff) #LINE# #TAB# return message"
"Execute the write_state tool  <code> def execute_tool(tool, path, info, args): ","#LINE# #TAB# log.info('Writing state in %s', path) #LINE# #TAB# if 'changed' in info: #LINE# #TAB# #TAB# with open('menhir-state.yaml', 'w+') as f: #LINE# #TAB# #TAB# #TAB# yaml = yaml_dumper() #LINE# #TAB# #TAB# #TAB# yaml.dump({'changed': info['changed']}, f) #LINE# #TAB# #TAB# #TAB# return OK #LINE# #TAB# return NOTHING_TO_DO"
"Detect an handler and return its wanted signal name  <code> def is_handler(cls, name, value): ","#LINE# #TAB# signal_name = False #LINE# #TAB# if callable(value) and hasattr(value, SPEC_CONTAINER_MEMBER_NAME): #LINE# #TAB# #TAB# spec = getattr(value, SPEC_CONTAINER_MEMBER_NAME) #LINE# #TAB# #TAB# if spec['kind'] == 'handler': #LINE# #TAB# #TAB# #TAB# signal_name = spec['name'] #LINE# #TAB# return signal_name"
"Helper function to obtain the weight of the edge between two nodes , with exception handling , returns 0 if no edge is found <code> def get_edgeval(G, n1, n2, weightParameter): ","#LINE# #TAB# if weightParameter: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# valDict = G.edges[n1, n2] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# return 0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return valDict[weightParameter] #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1"
"Set opacity of window . If opacity is None , request is ignored  <code> def set_opacity(window, opacity, checked=True): ","#LINE# #TAB# if opacity: #LINE# #TAB# #TAB# cookie = set_wm_window_opacity_checked(window, opacity) #LINE# #TAB# #TAB# if checked: #LINE# #TAB# #TAB# #TAB# return cookie.check() #LINE# #TAB# #TAB# return cookie"
Convert an argparse namespace object to a dictionary  <code> def namespace_to_dict(o): ,"#LINE# #TAB# d = {} #LINE# #TAB# for k, v in o.__dict__.items(): #LINE# #TAB# #TAB# if not callable(v): #LINE# #TAB# #TAB# #TAB# d[k] = v #LINE# #TAB# return d"
Returns all models that are referenced by a foreign key of the given class  <code> def parent_classes(cls): ,"#LINE# #TAB# parents = [] #LINE# #TAB# for field in cls._meta.get_fields(): #LINE# #TAB# #TAB# if isinstance(field, (models.ForeignKey, models.ManyToManyField)): #LINE# #TAB# #TAB# #TAB# parents.append(field.rel.to) #LINE# #TAB# return parents"
"Add human readable key names to dictionary while leaving any existing key names  <code> def map_keys(pvs, keys): ","#LINE# #TAB# rs = [] #LINE# #TAB# for pv in pvs: #LINE# #TAB# #TAB# r = dict((v, None) for k, v in keys.items()) #LINE# #TAB# #TAB# for k, v in pv.items(): #LINE# #TAB# #TAB# #TAB# if k in keys: #LINE# #TAB# #TAB# #TAB# #TAB# r[keys[k]] = v #LINE# #TAB# #TAB# #TAB# r[k] = v #LINE# #TAB# #TAB# rs.append(r) #LINE# #TAB# return rs"
Returns value as in integer <code> def to_digit(value): ,#LINE# #TAB# if value.isdigit(): #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# match = RE_NUM.match(value) #LINE# #TAB# return int(match.groups()[0]) if match else 0
"Get the environment variable or return exception  <code> def get_env_variable(var_name, default=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return os.environ[var_name] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# if default is None: #LINE# #TAB# #TAB# #TAB# error_msg = ""Set the %s environment variable"" % var_name #LINE# #TAB# #TAB# #TAB# raise ImproperlyConfigured(error_msg) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return default"
"Used internally by image_property_create and image_property_update  <code> def image_property_update(context, prop_ref, values, session=None): ","#LINE# #TAB# _drop_protected_attrs(models.ImageProperty, values) #LINE# #TAB# values['deleted'] = False #LINE# #TAB# prop_ref.update(values) #LINE# #TAB# prop_ref.save(session=session) #LINE# #TAB# return prop_ref"
Return the number of fingers needed to play the given fingering  <code> def fingers_needed(fingering): ,#LINE# #TAB# split = False #LINE# #TAB# indexfinger = False #LINE# #TAB# minimum = min(finger for finger in fingering if finger) #LINE# #TAB# result = 0 #LINE# #TAB# for finger in reversed(fingering): #LINE# #TAB# #TAB# if finger == 0: #LINE# #TAB# #TAB# #TAB# split = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if not split and finger == minimum: #LINE# #TAB# #TAB# #TAB# #TAB# if not indexfinger: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# indexfinger = True #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# result += 1 #LINE# #TAB# return result
Creates a Rhino Transform instance from a : class:`Transformation ` . Args : transformation ( : class:`Transformation ` ) : the transformation . Returns : ( : class:`Rhino . Geometry . Transform ` ) <code> def xform_from_transformation(transformation): ,"#LINE# #TAB# transform = Rhino.Geometry.Transform(1.0) #LINE# #TAB# for i in range(0, 4): #LINE# #TAB# #TAB# for j in range(0, 4): #LINE# #TAB# #TAB# #TAB# transform[i, j] = transformation[i, j] #LINE# #TAB# return transform"
Cautious assessment of the response body for no content  <code> def no_content_response(response): ,"#LINE# #TAB# if not hasattr(response, '_container'): #LINE# #TAB# #TAB# return True #LINE# #TAB# if response._container is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# if isinstance(response._container, (list, tuple)): #LINE# #TAB# #TAB# if len(response._container) == 1 and not response._container[0]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Our best approximation of how to map facilities to zones <code> def from_zone(cls, zone): ",#LINE# #TAB# facilities = set(Facility.objects.filter(zone_fallback=zone)) #LINE# #TAB# for device_zone in DeviceZone.objects.filter(zone=zone): #LINE# #TAB# #TAB# device = device_zone.device #LINE# #TAB# #TAB# facilities = facilities.union(set(Facility.objects.filter(signed_by #LINE# #TAB# #TAB# #TAB# =device))) #LINE# #TAB# return facilities
"Reduce the array along a given axis by sum square value <code> def reduce_sum_square(attrs, inputs, proto_obj): ","#LINE# #TAB# square_op = symbol.square(inputs[0]) #LINE# #TAB# sum_op = symbol.sum(square_op, axis=attrs.get('axes'), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# keepdims=attrs.get('keepdims')) #LINE# #TAB# return sum_op, attrs, inputs"
Parser for list buckets response  <code> def parse_list_buckets(data): ,"#LINE# #TAB# root = S3Element.fromstring('ListBucketsResult', data) #LINE# #TAB# return [ #LINE# #TAB# #TAB# Bucket(bucket.get_child_text('Name'), #LINE# #TAB# #TAB# #TAB# bucket.get_localized_time_elem('CreationDate')) #LINE# #TAB# #TAB# for buckets in root.findall('Buckets') #LINE# #TAB# #TAB# for bucket in buckets.findall('Bucket') #LINE# #TAB# ]"
checks if grpcio - tools protoc is installed <code> def has_grpcio_protoc(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# import grpc_tools.protoc #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"takes list of ( frame_size , frame_frames ) tuples and converts it to a list of ( cumulative_size , frame_frames ) tuples <code> def sizes_to_offsets(sizes): ","#LINE# #TAB# current_position = 0 #LINE# #TAB# offsets = [] #LINE# #TAB# for frame_size, frame_frames in sizes: #LINE# #TAB# #TAB# offsets.append((current_position, frame_frames)) #LINE# #TAB# #TAB# current_position += frame_size #LINE# #TAB# return offsets"
"Get rid of ( A ) or ( C ) when a player has it attached to their name : param player : list of player info - > [ number , position , name ] : return : fixed list <code> def fix_name(player): ",#LINE# #TAB# if player[2].find('(A)') != -1: #LINE# #TAB# #TAB# player[2] = player[2][:player[2].find('(A)')].strip() #LINE# #TAB# elif player[2].find('(C)') != -1: #LINE# #TAB# #TAB# player[2] = player[2][:player[2].find('(C)')].strip() #LINE# #TAB# return player
Return the TRUSTEE indy - sdk role for an anchor acting in an AnchorSmith capacity . : return : TRUSTEE role <code> def least_role() ->Role: ,"#LINE# #TAB# LOGGER.debug('AnchorSmith.least_role >>>') #LINE# #TAB# rv = Role.TRUSTEE #LINE# #TAB# LOGGER.debug('AnchorSmith.least_role <<< %s', rv) #LINE# #TAB# return rv"
Converts the returned data from the url and converts the JSON to a list : param url : String : return : List <code> def get_result(url): ,#LINE# #TAB# result = requests.get(url) #LINE# #TAB# result = json.loads(result.text) #LINE# #TAB# return result
"Execute ` command ` and return the response  <code> def telnet_request(cls, telnet, command, expected_prefix): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# telnet.write(command.encode('ASCII') + b'\r') #LINE# #TAB# except telnetlib.socket.timeout: #LINE# #TAB# #TAB# _LOGGER.debug('Pioneer command %s timed out', command) #LINE# #TAB# #TAB# return None #LINE# #TAB# for _ in range(3): #LINE# #TAB# #TAB# result = telnet.read_until(b'\r\n', timeout=0.2).decode('ASCII').strip( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# if result.startswith(expected_prefix): #LINE# #TAB# #TAB# #TAB# return result #LINE# #TAB# return None"
"Used to over - sampling unbalanced data <code> def re_sampling(sel, feature, label): ","#LINE# #TAB# from imblearn.over_sampling import RandomOverSampler #LINE# #TAB# ros = RandomOverSampler(random_state=0) #LINE# #TAB# feature_resampled, label_resampled = ros.fit_resample(feature, label) #LINE# #TAB# from collections import Counter #LINE# #TAB# print(sorted(Counter(label_resampled).items())) #LINE# #TAB# return feature_resampled, label_resampled"
Return a list of all help texts of all configured validators  <code> def password_validators_help_texts(password_validators=None): ,#LINE# #TAB# help_texts = [] #LINE# #TAB# if password_validators is None: #LINE# #TAB# #TAB# password_validators = get_default_password_validators() #LINE# #TAB# for validator in password_validators: #LINE# #TAB# #TAB# help_texts.append(validator.get_help_text()) #LINE# #TAB# return help_texts
Get UPF version  <code> def get_upf_version(upf): ,#LINE# #TAB# line = upf.split('\n')[0] #LINE# #TAB# if '<PP_INFO>' in line: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# if 'UPF version' in line: #LINE# #TAB# #TAB# return 2 #LINE# #TAB# return 0
"Retrieves the messages from a public , group , or direct channel  <code> def channels_messages(context, response): ",#LINE# #TAB# jdata = validate_response(response) #LINE# #TAB# messages = jdata.get(PLURAL_OBJECT_RESPONSE_MAP[MESSAGE]) #LINE# #TAB# for message in messages: #LINE# #TAB# #TAB# message['channel_type'] = context.urlargs['channel_type'] #LINE# #TAB# #TAB# message['unreadNotLoaded'] = jdata.get('unreadNotLoaded') #LINE# #TAB# return messages
"Fetch all sids using game type <code> def fetch_sid(game_no, database='kidaura_v1'): ",#LINE# #TAB# coll = get_conn()[f'game_{game_no}_raw'] #LINE# #TAB# data = list(coll.distinct('S_Id')) #LINE# #TAB# return data
Prints the the policy and corresponding rejected resources from projects <code> def print_project_policies_rejection(policy_dict): ,"#LINE# #TAB# output = '' #LINE# #TAB# for policy in policy_dict: #LINE# #TAB# #TAB# output += 'Rejected by Policy ' + '""' + policy + '"":\n' #LINE# #TAB# #TAB# for node in policy_dict[policy]: #LINE# #TAB# #TAB# #TAB# output += '\t* ' + node.resource.displayName #LINE# #TAB# #TAB# #TAB# licenses = node.resource.licenses #LINE# #TAB# #TAB# #TAB# if licenses is not None: #LINE# #TAB# #TAB# #TAB# #TAB# license_output = ' (' #LINE# #TAB# #TAB# #TAB# #TAB# for lice in licenses: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# license_output += lice + ', ' #LINE# #TAB# #TAB# #TAB# #TAB# output += license_output[:-2] + ') \n' #LINE# #TAB# return output"
Return the path to the file containing the redundant files and directories to be removed prior to log file creation . : return : file path ( string )  <code> def get_redundant_path(): ,"#LINE# #TAB# filename = config.Pilot.redundant #LINE# #TAB# if filename.startswith('/cvmfs') and os.environ.get('ATLAS_SW_BASE', False #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# filename = filename.replace('/cvmfs', os.environ.get('ATLAS_SW_BASE')) #LINE# #TAB# return filename"
"Joint distribution of values from pmf1 and pmf2  <code> def make_joint(pmf1, pmf2): ","#LINE# #TAB# joint = Joint() #LINE# #TAB# for v1, p1 in pmf1.Items(): #LINE# #TAB# #TAB# for v2, p2 in pmf2.Items(): #LINE# #TAB# #TAB# #TAB# joint.Set((v1, v2), p1 * p2) #LINE# #TAB# return joint"
Return the manufacturer name for a given manufacturer IDENT  <code> def mfg_name(ident): ,#LINE# #TAB# if ident in _MFG_ID: #LINE# #TAB# #TAB# return _MFG_ID[ident][0] #LINE# #TAB# return None
Return a dict where Quantities ( usually from a JSON file ) have been converted from unit / value Parameters ---------- idict : dict Input dict Returns ------- obj : dict or Quantity <code> def convert_quantity_in_dict(idict): ,"#LINE# #TAB# if 'unit' in idict.keys(): #LINE# #TAB# #TAB# obj = Quantity(idict['value'], unit=idict['unit']) #LINE# #TAB# #TAB# return obj #LINE# #TAB# else: #LINE# #TAB# #TAB# for key in idict.keys(): #LINE# #TAB# #TAB# #TAB# if isinstance(idict[key], dict): #LINE# #TAB# #TAB# #TAB# #TAB# idict[key] = convert_quantity_in_dict(idict[key]) #LINE# #TAB# #TAB# return idict"
Return the active default backend <code> def get_active_backend(): ,"#LINE# #TAB# active_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None) #LINE# #TAB# if active_backend_and_jobs is not None: #LINE# #TAB# #TAB# return active_backend_and_jobs #LINE# #TAB# active_backend = BACKENDS[DEFAULT_BACKEND]() #LINE# #TAB# return active_backend, DEFAULT_N_JOBS"
Prepend the Plone site URL to the prefix if it starts with / <code> def expand_absolute_prefix(prefix): ,#LINE# #TAB# if not prefix or not prefix.startswith('/'): #LINE# #TAB# #TAB# return prefix #LINE# #TAB# portal = getPortal() #LINE# #TAB# if portal is None: #LINE# #TAB# #TAB# return '' #LINE# #TAB# path = portal.absolute_url_path() #LINE# #TAB# if path and path.endswith('/'): #LINE# #TAB# #TAB# path = path[:-1] #LINE# #TAB# return path + prefix
Returns the list of UUIDs locked by this thread  <code> def get_locks(): ,"#LINE# #TAB# locks = getattr(_local, 'entry_transaction', None) #LINE# #TAB# if locks is None: #LINE# #TAB# #TAB# locks = [] #LINE# #TAB# #TAB# _set_locks(locks) #LINE# #TAB# return locks"
"Return the index of faces in the mesh which break the watertight status of the mesh  <code> def broken_faces(mesh, color=None): ","#LINE# #TAB# adjacency = nx.from_edgelist(mesh.face_adjacency) #LINE# #TAB# broken = [k for k, v in dict(adjacency.degree()).items() #LINE# #TAB# #TAB# #TAB# if v != 3] #LINE# #TAB# broken = np.array(broken) #LINE# #TAB# if color is not None: #LINE# #TAB# #TAB# color = np.array(color) #LINE# #TAB# #TAB# if not (color.shape == (4,) or color.shape == (3,)): #LINE# #TAB# #TAB# #TAB# color = [255, 0, 0, 255] #LINE# #TAB# #TAB# mesh.visual.face_colors[broken] = color #LINE# #TAB# return broken"
Return a list of all public files  <code> def list_public_files(): ,"#LINE# #TAB# public_file_filename = join(dirname(__file__), 'metasub_public_files.txt') #LINE# #TAB# out = [] #LINE# #TAB# with open(public_file_filename, 'r') as f: #LINE# #TAB# #TAB# for line in f: #LINE# #TAB# #TAB# #TAB# out.append(line.strip()) #LINE# #TAB# return out"
Retrieve the last biz day from today or from d if the arg is given . Holidays are ignored : params d : A datetime object . : return : A datetime object of the last biz day  <code> def get_last_work_day(d=None): ,#LINE# #TAB# now = dt.datetime.today() if not d else d #LINE# #TAB# deltDays = 0 #LINE# #TAB# if now.weekday() > 4: #LINE# #TAB# #TAB# deltDays = now.weekday() - 4 #LINE# #TAB# bizday = now - dt.timedelta(deltDays) #LINE# #TAB# return bizday
"Compare code , removing swarm metadata if necessary : param code_1 : : param code_2 : : return : True if same code , False otherwise <code> def compare_byte_code(code_1: bytes, code_2: bytes) ->bool: ","#LINE# #TAB# if code_1 == code_2: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# codes = [] #LINE# #TAB# #TAB# for code in (code_1, code_2): #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# codes.append(remove_swarm_metadata(code)) #LINE# #TAB# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# #TAB# codes.append(code) #LINE# #TAB# #TAB# return codes[0] == codes[1]"
"Initialise a 3x3 tensor with the plate depolarisation matrix , returns a tensor <code> def initialise_plate_depolarisation_matrix(normal): ","#LINE# #TAB# normal = normal / np.linalg.norm(normal) #LINE# #TAB# tensor = np.outer(normal, normal) #LINE# #TAB# tensor = tensor / np.trace(tensor) #LINE# #TAB# return tensor"
Builds the implication graph from the formula <code> def build_graph(formula): ,"#LINE# #TAB# graph = {} #LINE# #TAB# for clause in formula: #LINE# #TAB# #TAB# for (lit, _) in clause: #LINE# #TAB# #TAB# #TAB# for neg in [False, True]: #LINE# #TAB# #TAB# #TAB# #TAB# graph[(lit, neg)] = [] #LINE# #TAB# for ((a_lit, a_neg), (b_lit, b_neg)) in formula: #LINE# #TAB# #TAB# add_edge(graph, (a_lit, a_neg), (b_lit, not b_neg)) #LINE# #TAB# #TAB# add_edge(graph, (b_lit, b_neg), (a_lit, not a_neg)) #LINE# #TAB# return graph"
"Read model output variables ( e.g. Plug - in Gait ) <code> def get_model_data(vicon, model): ","#LINE# #TAB# modeldata = dict() #LINE# #TAB# var_dims = 3, vicon.GetFrameCount() #LINE# #TAB# subj = get_subjectnames() #LINE# #TAB# for var in model.read_vars: #LINE# #TAB# #TAB# nums, bools = vicon.GetModelOutput(subj, var) #LINE# #TAB# #TAB# if nums: #LINE# #TAB# #TAB# #TAB# data = np.squeeze(np.array(nums)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# logger.info('cannot read variable %s, returning nans' % var) #LINE# #TAB# #TAB# #TAB# data = np.empty(var_dims) #LINE# #TAB# #TAB# #TAB# data[:] = np.nan #LINE# #TAB# #TAB# modeldata[var] = data #LINE# #TAB# return modeldata"
Verify that external dependencies are installed . Prints a message to stderr and returns False if any dependencies are missing . : returns : bool <code> def verify_dependencies(): ,"#LINE# #TAB# exiftool = get_exiftool() #LINE# #TAB# if exiftool is None: #LINE# #TAB# #TAB# print(EXIFTOOL_ERROR, file=sys.stderr) #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Read expressions from file and return only expressions of genes in gene_set  <code> def get_expression(fname, sep='\t', gene_set=[]): ","#LINE# #TAB# df = pd.read_csv(filepath_or_buffer=fname, sep=sep, header=0, index_col #LINE# #TAB# #TAB# =0, compression='gzip', dtype={(0): str, (1): float}, #LINE# #TAB# #TAB# keep_default_na=False) #LINE# #TAB# df.index = df.index.map(str) #LINE# #TAB# if not gene_set: #LINE# #TAB# #TAB# return df #LINE# #TAB# intersection = [gene for gene in gene_set if gene in df.index] #LINE# #TAB# return df.loc[intersection]"
"Allow file object to already be opened in any of the valid modes and and leave the file in the same state ( opened or closed ) as when the function was called  <code> def get_file_mode(filename, default='readonly'): ","#LINE# #TAB# mode = default #LINE# #TAB# closed = fileobj_closed(filename) #LINE# #TAB# fmode = fileobj_mode(filename) #LINE# #TAB# if fmode is not None: #LINE# #TAB# #TAB# mode = FILE_MODES.get(fmode) #LINE# #TAB# #TAB# if mode is None: #LINE# #TAB# #TAB# #TAB# raise IOError( #LINE# #TAB# #TAB# #TAB# #TAB# 'File mode of the input file object (%r) cannot be used to read/write FITS files.' #LINE# #TAB# #TAB# #TAB# #TAB# % fmode) #LINE# #TAB# return mode, closed"
Returns a random string of length string_length <code> def random_str_uuid(string_length): ,"#LINE# #TAB# if not isinstance(string_length, int) or not 1 <= string_length <= 32: #LINE# #TAB# #TAB# msg = ""string_length must be type int where 1 <= string_length <= 32"" #LINE# #TAB# #TAB# raise ValueError(msg) #LINE# #TAB# random = str(uuid.uuid4()).upper().replace('-', '') #LINE# #TAB# return random[0:string_length]"
Operations that should be performed to modify class attributes prior to registration  <code> def before_registration(cls): ,#LINE# #TAB# CommBase.CommBase.before_registration(cls) #LINE# #TAB# if cls._filetype != 'binary': #LINE# #TAB# #TAB# assert 'serializer' not in cls._schema_properties #LINE# #TAB# #TAB# cls._schema_properties.update(cls._default_serializer_class. #LINE# #TAB# #TAB# #TAB# _schema_properties) #LINE# #TAB# #TAB# del cls._schema_properties['seritype']
"Samples a 3D cube of white noise of desired size <code> def white_noise(nc, batch_size=1, seed=None, type='complex', name=None): ","#LINE# #TAB# with tf.name_scope(name, 'WhiteNoise'): #LINE# #TAB# #TAB# assert batch_size >= 1 #LINE# #TAB# #TAB# white = tf.random_normal(shape=(batch_size, nc, nc, nc), mean=0, #LINE# #TAB# #TAB# #TAB# stddev=nc ** 1.5, seed=seed) #LINE# #TAB# #TAB# if type == 'real': #LINE# #TAB# #TAB# #TAB# return white #LINE# #TAB# #TAB# elif type == 'complex': #LINE# #TAB# #TAB# #TAB# whitec = r2c3d(white, norm=nc ** 3) #LINE# #TAB# #TAB# #TAB# return whitec"
"Delete a physical dimension from the list of dimensions . Please note that deleting works only for dimensions listed in the custom file  <code> def delete_dimension(ctx, dimension): ","#LINE# #TAB# result = units.delete_dimension(dimension, **ctx.in_header.__dict__) #LINE# #TAB# return result"
Install application - specific GRIB image handler . : param handler : Handler object  <code> def register_handler(handler): ,#LINE# #TAB# global _handler #LINE# #TAB# _handler = handler
"List the npm modules that get installed in a docker image for the react server <code> def list_npm_modules(collector, no_print=False): ","#LINE# #TAB# default = ReactServer().default_npm_deps() #LINE# #TAB# for _, module in sorted(collector.configuration[""__active_modules__""].items()): #LINE# #TAB# #TAB# default.update(module.npm_deps()) #LINE# #TAB# if not no_print: #LINE# #TAB# #TAB# print(json.dumps(default, indent=4, sort_keys=True)) #LINE# #TAB# return default"
"Returns True if the file should be skipped based on the passed in settings  <code> def should_skip(filename, config, path='/'): ","#LINE# #TAB# for skip_path in config['skip']: #LINE# #TAB# #TAB# if posixpath.abspath(posixpath.join(path, filename)) == posixpath.abspath(skip_path.replace('\\', '/')): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# position = os.path.split(filename) #LINE# #TAB# while position[1]: #LINE# #TAB# #TAB# if position[1] in config['skip']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# position = os.path.split(position[0]) #LINE# #TAB# for glob in config['skip_glob']: #LINE# #TAB# #TAB# if fnmatch.fnmatch(filename, glob): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Get latest builds for multiple Brew components : param tag_component_tuples : List of ( tag , component_name ) tuples : param session : instance of Brew session : return : a list Koji / Brew build objects <code> def get_latest_builds(tag_component_tuples: List[Tuple[str, str]], session: ","#LINE# #TAB# koji.ClientSession) ->List[Optional[List[Dict]]]: #LINE# #TAB# tasks = [] #LINE# #TAB# with session.multicall(strict=True) as m: #LINE# #TAB# #TAB# for tag, component_name in tag_component_tuples: #LINE# #TAB# #TAB# #TAB# if not (tag and component_name): #LINE# #TAB# #TAB# #TAB# #TAB# tasks.append(None) #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# tasks.append(m.getLatestBuilds(tag, package=component_name)) #LINE# #TAB# return [(task.result if task else None) for task in tasks]"
"Try to identify an IPv4 address in the given content . * Unhandled * IPv6 , which geocoder sometimes returns . Parameters ---------- content : str Returns ------- ip : str or None <code> def identify_ip(content): ","#LINE# #TAB# if not content: #LINE# #TAB# #TAB# matches = [] #LINE# #TAB# elif isinstance(content, bytes): #LINE# #TAB# #TAB# matches = re.findall(b'\\d+\\.\\d+\\.\\d+\\.\\d+', content) #LINE# #TAB# else: #LINE# #TAB# #TAB# matches = re.findall('\\d+\\.\\d+\\.\\d+\\.\\d+', content) #LINE# #TAB# matches = list(set(matches)) #LINE# #TAB# ip = None #LINE# #TAB# if len(matches) == 1: #LINE# #TAB# #TAB# ip = matches[0] #LINE# #TAB# elif len(matches) > 1: #LINE# #TAB# #TAB# matches = sorted(matches) #LINE# #TAB# #TAB# sys.stderr.write('Ambiguous IP address results: %s' % matches) #LINE# #TAB# return ip"
"Given two lists of sensations create a new list where the sensory SDRs are union of the individual sensory SDRs . Keep the location SDRs from the object  <code> def create_superimposed_sensory_sdrs(sequenceSensations, objectSensations): ","#LINE# assert len(sequenceSensations) == len(objectSensations) #LINE# superimposedSensations = [] #LINE# for i, objectSensation in enumerate(objectSensations): #LINE# #TAB# newSensation = { #LINE# #TAB# 0: (objectSensation[0][0], #LINE# #TAB# #TAB# sequenceSensations[i][0][1].union(objectSensation[0][1])) #LINE# #TAB# } #LINE# #TAB# superimposedSensations.append(newSensation) #LINE# return superimposedSensations"
Get a Provider1Definition for the provider that handles the exporters machinery  <code> def get_exporters_def(): ,"#LINE# #TAB# exporters_def = Provider1Definition() #LINE# #TAB# exporters_def.name = 'com.canonical.plainbox:exporters' #LINE# #TAB# exporters_def.version = '1.0' #LINE# #TAB# exporters_def.description = N_('Exporters Provider') #LINE# #TAB# exporters_def.secure = False #LINE# #TAB# exporters_def.gettext_domain = 'plainbox-provider-exporters' #LINE# #TAB# exporters_def.location = os.path.join(get_plainbox_dir(), #LINE# #TAB# #TAB# 'impl/providers/exporters') #LINE# #TAB# return exporters_def"
Return a prototype Scanner instance for scanning LaTeX source files when built with pdflatex  <code> def pdfla_te_xscanner(): ,"#LINE# #TAB# ds = LaTeX(name='pdfla_te_xscanner', suffixes='$LATEXSUFFIXES', #LINE# #TAB# #TAB# graphics_extensions=LatexGraphics, recursive=0) #LINE# #TAB# return ds"
"Return a grayed version image preview from a LUT name . This images are cached into a global structure . : param Union[str , numpy.ndarray ] colormap : Description of the LUT : rtype : qt . QImage <code> def get_grayed_colormap_image(colormap): ","#LINE# #TAB# key = LegendIconWidget._getColormapKey(colormap) #LINE# #TAB# grayKey = key, 'gray' #LINE# #TAB# image = _colormapImage.get(grayKey, None) #LINE# #TAB# if image is None: #LINE# #TAB# #TAB# image = LegendIconWidget.getColormapImage(colormap) #LINE# #TAB# #TAB# image = image.convertToFormat(qt.QImage.Format_Grayscale8) #LINE# #TAB# #TAB# _colormapImage[grayKey] = image #LINE# #TAB# return image"
Return the table row from an input item <code> def row_from_item(input_tuple): ,"#LINE# #TAB# metabolite, stoichiometry = input_tuple #LINE# #TAB# id_item = LinkedItem(metabolite.id, metabolite) #LINE# #TAB# id_item.setFlags(Qt.ItemIsEnabled | Qt.ItemIsSelectable) #LINE# #TAB# id_item.setToolTip('Formula: {}\nCharge: {}'.format(metabolite.formula, #LINE# #TAB# #TAB# metabolite.charge)) #LINE# #TAB# value_item = LinkedItem(link=metabolite) #LINE# #TAB# value_item.setData(stoichiometry, 2) #LINE# #TAB# return [id_item, value_item]"
"Confirms a link , ie , flips its status to True  <code> def confirm_link(code): ","#LINE# #TAB# status = _get_status(code) #LINE# #TAB# if not status: #LINE# #TAB# #TAB# raise LinkExpiredException('Code not found') #LINE# #TAB# status['linked'] = True #LINE# #TAB# _set_status(code, status) #LINE# #TAB# return status"
"Convert an offset in a file to ( row , column ) coordinates <code> def offset_to_column_line(text, offset): ","#LINE# #TAB# if offset > len(text): #LINE# #TAB# #TAB# return False #LINE# #TAB# lines = text.split('\n') #LINE# #TAB# row = 0 #LINE# #TAB# column = offset #LINE# #TAB# for line_number in range(0, len(lines)): #LINE# #TAB# #TAB# line_length = len(lines[line_number]) #LINE# #TAB# #TAB# if column < line_length: #LINE# #TAB# #TAB# #TAB# row = line_number #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# column -= line_length + 1 #LINE# #TAB# return row + 1, column + 1"
"Create a symlink from link_filename to real_filename  <code> def safe_setup_link(link_filename, real_filename): ","#LINE# #TAB# real_filename = os.path.relpath(real_filename, os.path.dirname( #LINE# #TAB# #TAB# link_filename)) #LINE# #TAB# if os.path.exists(link_filename): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.unlink(link_filename) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# try: #LINE# #TAB# #TAB# os.symlink(real_filename, link_filename) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# pass"
find a list of run directories in the directory given : param directory : : return : list of DirectoryStatus objects <code> def find_runs(directory): ,"#LINE# #TAB# logging.info('looking for runs in {}'.format(directory)) #LINE# #TAB# runs = [] #LINE# #TAB# directory_list = Parser._find_directory_list(directory) #LINE# #TAB# for d in directory_list: #LINE# #TAB# #TAB# runs.append(progress.get_directory_status(d, Parser. #LINE# #TAB# #TAB# #TAB# get_required_file_list())) #LINE# #TAB# return runs"
Get the path to the Anaconda binary <code> def get_conda_bin(): ,"#LINE# #TAB# conda_bin = os.popen('echo $CONDA_EXE').read().strip() #LINE# #TAB# if conda_bin == '': #LINE# #TAB# #TAB# if os.popen('conda').read().strip() == '': #LINE# #TAB# #TAB# #TAB# raise CondaError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Anaconda binary not found. Please make sure the ""conda"" command is in your system PATH or the environment variable $CONDA_EXE points to the anaconda binary' #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# conda_bin = 'conda' #LINE# #TAB# logging.info(f'Anaconda binary: {conda_bin}') #LINE# #TAB# return conda_bin"
Process a way element entry into a list of dicts suitable for going into a Pandas DataFrame  <code> def process_way(e): ,"#LINE# #TAB# way = {'id': e['id']} #LINE# #TAB# if 'tags' in e: #LINE# #TAB# #TAB# if e['tags'] is not np.nan: #LINE# #TAB# #TAB# #TAB# for t, v in list(e['tags'].items()): #LINE# #TAB# #TAB# #TAB# #TAB# if t in config.settings.keep_osm_tags: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# way[t] = v #LINE# #TAB# waynodes = [] #LINE# #TAB# for n in e['nodes']: #LINE# #TAB# #TAB# waynodes.append({'way_id': e['id'], 'node_id': n}) #LINE# #TAB# return way, waynodes"
"Return the default language given the notebook metadata and a file extension <code> def default_language_from_metadata_and_ext(metadata, ext): ","#LINE# #TAB# default_from_ext = _SCRIPT_EXTENSIONS.get(ext, {}).get('language', 'python') #LINE# #TAB# language = (metadata.get('jupytext', {}).get('main_language') #LINE# #TAB# #TAB# #TAB# #TAB# or metadata.get('kernelspec', {}).get('language') #LINE# #TAB# #TAB# #TAB# #TAB# or default_from_ext) #LINE# #TAB# if language.startswith('C++'): #LINE# #TAB# #TAB# language = 'c++' #LINE# #TAB# return language"
Add a separator after default encodings in encoding combobox <code> def add_separator_after_default_encodings(combobox): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# separatorPosition = getPredefinedEncodings().index('[SEPARATOR]') #LINE# #TAB# #TAB# combobox.removeItem(separatorPosition) #LINE# #TAB# #TAB# combobox.insertSeparator(separatorPosition) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# return
"Return True if the target is valid , False otherwise <code> def test_target(name, value, ipv6=False): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# iptc_rule = Rule6() if ipv6 else Rule() #LINE# #TAB# #TAB# _iptc_settarget(iptc_rule, {name: value}) #LINE# #TAB# #TAB# return True #LINE# #TAB# except: #LINE# #TAB# #TAB# return False"
Ensure that some characters are escaped before writing to XML <code> def escape_specials(string): ,"#LINE# #TAB# string = six.text_type(string) #LINE# #TAB# string = string.replace('\\', '\\\\') #LINE# #TAB# for i in range(0, 32): #LINE# #TAB# #TAB# string = string.replace(six.unichr(i), '\\%02d' % i) #LINE# #TAB# return string"
"Returns a dictionary of a VMs info as returned by OpenStack : param nova : the Nova client : param server : the old server object : return : a dict of the info if VM exists else None <code> def get_server_info(nova, server): ","#LINE# #TAB# vm = __get_latest_server_os_object(nova, server) #LINE# #TAB# if vm: #LINE# #TAB# #TAB# return vm._info #LINE# #TAB# return None"
"Determine whether the provided address is within a network range  <code> def is_address_in_network(network, address): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# network = netaddr.IPNetwork(network) #LINE# #TAB# except (netaddr.core.AddrFormatError, ValueError): #LINE# #TAB# #TAB# raise ValueError(""Network (%s) is not in CIDR presentation format"" % #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# network) #LINE# #TAB# try: #LINE# #TAB# #TAB# address = netaddr.IPAddress(address) #LINE# #TAB# except (netaddr.core.AddrFormatError, ValueError): #LINE# #TAB# #TAB# raise ValueError(""Address (%s) is not in correct presentation format"" % #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# address) #LINE# #TAB# if address in network: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"Computes hard calls from probability matrix . Args : a1 ( str ) : the first allele a2 ( str ) : the second allele probs ( numpy.array ) : the probability matrix Returns : numpy.array : the hard calls computed from the probabilities <code> def hard_calls_from_probs(a1, a2, probs): ","#LINE# #TAB# possible_geno = np.array([' '.join([a1] * 2), ' '.join([a1, a2]), ' '. #LINE# #TAB# #TAB# join([a2] * 2)]) #LINE# #TAB# return possible_geno[np.argmax(probs, axis=1)]"
"Converts a negative index into a positive index . Args : index : The index to convert . Can be None . dim : The length of the dimension being indexed  <code> def wrap_neg_index(index, dim, neg_step=False): ",#LINE# #TAB# if index is not None and index < 0 and not (neg_step and index == -1): #LINE# #TAB# #TAB# index += dim #LINE# #TAB# return index
"This method should only be called during unit testing , e.g. to test auto - adding reactors  <code> def clear_global_state(): ",#LINE# #TAB# Plant._SubreactorFunctions = {} #LINE# #TAB# Plant._ReactorFunctions = []
Outputs root path of the module . Acounts for changes in path when app is frozen . Returns ------- str string with root module path See also -------- : func:`we_are_frozen ` <code> def module_path() ->Path: ,"#LINE# #TAB# if hasattr(sys, 'frozen'): #LINE# #TAB# #TAB# return Path(sys.executable).resolve().parent #LINE# #TAB# else: #LINE# #TAB# #TAB# return (Path(__file__) / '..').resolve().parent"
"I run these scripts on platforms that do n't support openmeta , or do n't have the binary , or a working binary , or .. <code> def is_openmeta_working(): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# Popen('openmeta', stdout=PIPE, stderr=PIPE).communicate() #LINE# #TAB# #TAB# return True #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return False"
Start the build to create Docker images . : param codebuild_project : CodeBuild project that builds docker container images  <code> def create_docker_images(codebuild_project): ,"#LINE# #TAB# build_id = trigger_codebuild(codebuild_project) #LINE# #TAB# logger.info('Docker images creation: STARTED') #LINE# #TAB# logger.info('Build id: %s', build_id) #LINE# #TAB# return build_id"
Setup the smoothing sigmas array for registration <code> def setup_smoothing_sigmas(scale: int=1): ,"#LINE# #TAB# smoothing_sigmas = [0] #LINE# #TAB# if scale > 1: #LINE# #TAB# #TAB# for idx in range(1, scale, 1): #LINE# #TAB# #TAB# #TAB# smoothing_sigmas.insert(0, 2 ** (idx - 1)) #LINE# #TAB# #TAB# print('No smoothing sigmas given, setting to {}'.format( #LINE# #TAB# #TAB# #TAB# smoothing_sigmas)) #LINE# #TAB# return smoothing_sigmas"
"Converts a line of CollectAllelicCounts into AMBER line  <code> def counts_to_amber(t_vals, n_vals): ","#LINE# #TAB# t_depth = int(t_vals[""REF_COUNT""]) + int(t_vals[""ALT_COUNT""]) #LINE# #TAB# n_depth = int(n_vals[""REF_COUNT""]) + int(n_vals[""ALT_COUNT""]) #LINE# #TAB# if n_depth > 0 and t_depth > 0: #LINE# #TAB# #TAB# t_baf = float(t_vals[""ALT_COUNT""]) / float(t_depth) #LINE# #TAB# #TAB# n_baf = float(n_vals[""ALT_COUNT""]) / float(n_depth) #LINE# #TAB# #TAB# return [t_vals[""CONTIG""], t_vals[""POSITION""], t_baf, _normalize_baf(t_baf), t_depth, #LINE# #TAB# #TAB# #TAB# #TAB# n_baf, _normalize_baf(n_baf), n_depth]"
Find the last backup given a directory with dated child directories  <code> def get_last(path): ,"#LINE# #TAB# if path.startswith('rsync://'): #LINE# #TAB# #TAB# rsync = 'rsync' + ' ' + path + '/ | ' #LINE# #TAB# #TAB# pipe = ""awk '{print $5}' | sed '/\\./d' | sort -nr | awk 'NR==2'"" #LINE# #TAB# #TAB# return subprocess.Popen(rsync + pipe, shell=True, stdout=subprocess #LINE# #TAB# #TAB# #TAB# .PIPE, universal_newlines=True).stdout.read().strip() #LINE# #TAB# else: #LINE# #TAB# #TAB# return sorted(os.listdir(path), reverse=True)[0]"
Return True if array of longitudes covers the whole sphere  <code> def is_longitude_global(lon_points): ,#LINE# #TAB# dx = np.diff(lon_points)[0] #LINE# #TAB# case_0_360 = lon_points[0] - dx <= 0 and lon_points[-1] + dx >= 360 #LINE# #TAB# case_pm180 = lon_points[0] - dx <= -180 and lon_points[-1] + dx >= 180 #LINE# #TAB# return case_0_360 or case_pm180
Strip trailing slash if exists . : param str value : The value to strip trailing slash from . : returns : Path without a trailing slash <code> def strip_trailing_slash(value): ,#LINE# #TAB# if value.endswith('/'): #LINE# #TAB# #TAB# return value[:-1] #LINE# #TAB# return value
Displays the data of a target  <code> def target_data(target): ,"#LINE# #TAB# extras = {k['name']: target.extra_fields.get(k['name'], '') for k in #LINE# #TAB# #TAB# settings.EXTRA_FIELDS if not k.get('hidden')} #LINE# #TAB# return {'target': target, 'extras': extras}"
"reads in a csv of data , excluding the header row : param csv_file : csv file of data , defaults to ' allofplos_metadata.csv ' : return : list of tuples of article metadata <code> def read_corpus_metadata_from_csv(csv_file='allofplos_metadata.csv'): ","#LINE# #TAB# with open(csv_file, 'r') as csv_file: #LINE# #TAB# #TAB# reader = csv.reader(csv_file) #LINE# #TAB# #TAB# next(reader, None) #LINE# #TAB# #TAB# corpus_metadata = [tuple(line) for line in reader] #LINE# #TAB# return corpus_metadata"
"Run the command on a host using the username  <code> def ssh_with_username(hostname, username, command): ","#LINE# #TAB# if hostname != '' and username != '' and command != '': #LINE# #TAB# #TAB# command = 'ssh {username}@{hostname} -- {command}'.format(username= #LINE# #TAB# #TAB# #TAB# username, hostname=hostname, command=command) #LINE# #TAB# #TAB# LOGGER.debug('command:%s', command) #LINE# #TAB# #TAB# result = subprocess.run(command, shell=True, capture_output=True) #LINE# #TAB# #TAB# stdout = result.stdout.decode('utf-8') #LINE# #TAB# #TAB# stderr = result.stderr.decode('utf-8') #LINE# #TAB# #TAB# return result.returncode, stdout, stderr"
Return function link to notebook documentation of ft . Private functions link to source code <code> def get_fn_link(ft)->str: ,"#LINE# #TAB# ""Return function link to notebook documentation of `ft`. Private functions link to source code"" #LINE# #TAB# ft = getattr(ft, '__func__', ft) #LINE# #TAB# anchor = strip_fastai(get_anchor(ft)) #LINE# #TAB# module_name = strip_fastai(get_module_name(ft)) #LINE# #TAB# base = '' if use_relative_links else FASTAI_DOCS #LINE# #TAB# return f'{base}/{module_name}.html#{anchor}'"
Used for weight generation by calculate_731_checksum <code> def weight_generator(): ,#LINE# #TAB# while True: #LINE# #TAB# #TAB# yield 7 #LINE# #TAB# #TAB# yield 3 #LINE# #TAB# #TAB# yield 1
Helper function to extract a token from the request header  <code> def get_token_from_header(request): ,#LINE# #TAB# token = None #LINE# #TAB# if 'Authorization' in request.headers: #LINE# #TAB# #TAB# split_header = request.headers.get('Authorization').split() #LINE# #TAB# #TAB# if len(split_header) == 2 and split_header[0] == 'Bearer': #LINE# #TAB# #TAB# #TAB# token = split_header[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# token = request.access_token #LINE# #TAB# return token
Called before a record is fetched through the view . : returns : ` ` accpac . Continue ` ` or ` ` accpac . Abort ` ` : rtype : int <code> def on_before_fetch(): ,#LINE# #TAB# rvspyTrace('on_before_fetch') #LINE# #TAB# return Continue
Return the variable name set in Bird configuration  <code> def get_variable_name_from_bird(bird_conf): ,"#LINE# #TAB# bird_variable_pattern = re.compile( #LINE# #TAB# ) #LINE# #TAB# with open(bird_conf, 'r') as content: #LINE# #TAB# #TAB# for line in content.readlines(): #LINE# #TAB# #TAB# #TAB# variable_match = bird_variable_pattern.search(line) #LINE# #TAB# #TAB# #TAB# if variable_match: #LINE# #TAB# #TAB# #TAB# #TAB# return variable_match.group('name') #LINE# #TAB# return None"
"Return a configuration object of MongoDBManager for given collection_name If collection_name in any configuration file then use the configured collection_name Else use the given collection_name instead <code> def mongo_config_for_collection(cls, collection_name: str=None): ","#LINE# #TAB# if collection_name is None: #LINE# #TAB# #TAB# collection_name = cls.COLLECTION_NAME #LINE# #TAB# if not getattr(cls, collection_name, False): #LINE# #TAB# #TAB# collection_name = collection_name #LINE# #TAB# else: #LINE# #TAB# #TAB# collection_name = getattr(cls, collection_name) #LINE# #TAB# return {'MONGODB_URI': cls.MONGODB_URI, 'DB_NAME': cls.DB_NAME, #LINE# #TAB# #TAB# 'COLLECTION_NAME': collection_name}"
tree.io seems to anticipate this function . It is copied from another module  <code> def get_widgets(request): ,#LINE# #TAB# widgets = {} #LINE# #TAB# widgets.update(WIDGETS) #LINE# #TAB# return widgets
Encode escape sequences . Args : s ( str ) : String that should be encoded . Returns : str : Result of encoding escape sequences  <code> def encode_escape(s): ,"#LINE# #TAB# if PY2: #LINE# #TAB# #TAB# out = match_stype(s, as_str(s).encode('string-escape')) #LINE# #TAB# else: #LINE# #TAB# #TAB# out = match_stype(s, as_unicode(s).encode('unicode-escape')) #LINE# #TAB# return out"
"If the annotations file is in the same format as the original data files , this method can be used to extract a dict of query ids and answers  <code> def get_answers_from_data(annotations: Dict[str, Any]) ->Dict[str, List[str]]: ","#LINE# #TAB# answers_dict: Dict[str, List[str]] = {} #LINE# #TAB# for article_info in annotations['data']: #LINE# #TAB# #TAB# for paragraph_info in article_info['paragraphs']: #LINE# #TAB# #TAB# #TAB# for qa_pair in paragraph_info['qas']: #LINE# #TAB# #TAB# #TAB# #TAB# query_id = qa_pair['id'] #LINE# #TAB# #TAB# #TAB# #TAB# candidate_answers = [answer['text'] for answer in qa_pair[ #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'answers']] #LINE# #TAB# #TAB# #TAB# #TAB# answers_dict[query_id] = candidate_answers #LINE# #TAB# return answers_dict"
Reverse the effect of enable_metadata_credential_caching ( ) <code> def disable_metadata_credential_caching(): ,#LINE# #TAB# global _populate_keys_from_metadata_server_orig #LINE# #TAB# with monkey_patch_lock: #LINE# #TAB# #TAB# if _populate_keys_from_metadata_server_orig is not None: #LINE# #TAB# #TAB# #TAB# from boto.provider import Provider #LINE# #TAB# #TAB# #TAB# Provider._populate_keys_from_metadata_server = ( #LINE# #TAB# #TAB# #TAB# #TAB# _populate_keys_from_metadata_server_orig) #LINE# #TAB# #TAB# #TAB# _populate_keys_from_metadata_server_orig = None
"Make request parameters right  <code> def prepare_request(uri, headers=None, data=None, method=None): ","#LINE# #TAB# if headers is None: #LINE# #TAB# #TAB# headers = {} #LINE# #TAB# if data and not method: #LINE# #TAB# #TAB# method = 'POST' #LINE# #TAB# elif not method: #LINE# #TAB# #TAB# method = 'GET' #LINE# #TAB# if method == 'GET' and data: #LINE# #TAB# #TAB# uri = add_params_to_uri(uri, data) #LINE# #TAB# #TAB# data = None #LINE# #TAB# return uri, headers, data, method"
return list of products of a reaction <code> def get_listofproducts(reaction): ,"#LINE# #TAB# listOfProducts = None #LINE# #TAB# for e in reaction: #LINE# #TAB# #TAB# if e.tag[0] == '{': #LINE# #TAB# #TAB# #TAB# uri, tag = e.tag[1:].split('}') #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# tag = e.tag #LINE# #TAB# #TAB# if tag == 'listOfProducts': #LINE# #TAB# #TAB# #TAB# listOfProducts = e #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return listOfProducts"
Returns a list of interface numbers from the output tshark -D. Used internally to capture on multiple interfaces  <code> def get_tshark_interfaces(tshark_path=None): ,"#LINE# #TAB# parameters = [get_process_path(tshark_path), '-D'] #LINE# #TAB# with open(os.devnull, 'w') as null: #LINE# #TAB# #TAB# tshark_interfaces = subprocess.check_output(parameters, stderr=null #LINE# #TAB# #TAB# #TAB# ).decode('utf-8') #LINE# #TAB# return [line.split('.')[0] for line in tshark_interfaces.splitlines()]"
"Find a filesystem path for module ` i d ` or ` None ` if not found  <code> def find_module_simple(id: str, manager: BuildManager) ->Optional[str]: ","#LINE# #TAB# x = find_module_with_reason(id, manager) #LINE# #TAB# if isinstance(x, ModuleNotFoundReason): #LINE# #TAB# #TAB# return None #LINE# #TAB# return x"
"Returns True if this Property is set  <code> def get_global_property(name='', value=0): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# prop = GlobalProperty.objects.get(name=name) #LINE# #TAB# #TAB# return bool(prop.value) #LINE# #TAB# except GlobalProperty.DoesNotExist: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False
Generate all maximum matchings of given bipartite graph  <code> def generate_maximum_matchings(bgraph): ,"#LINE# #TAB# initial = maximum_match(bgraph) #LINE# #TAB# initial = align_part(bgraph, initial) #LINE# #TAB# yield initial #LINE# #TAB# digraph = create_directed_bipartite(bgraph, initial) #LINE# #TAB# for matching in sub_generate_maximum_matching(digraph, initial): #LINE# #TAB# #TAB# matching = align_part(digraph, matching) #LINE# #TAB# #TAB# yield matching"
"Convert the delete opcode from a word based offset , to a character based offset  <code> def convert_delete(op, old_text_list): ","#LINE# #TAB# opcode, s, e = op #LINE# #TAB# prefix = reconstruct_text(old_text_list[0:s]) #LINE# #TAB# prefix_length = len(prefix) #LINE# #TAB# text = reconstruct_text(old_text_list[s:e]) #LINE# #TAB# text_length = len(text) #LINE# #TAB# char_offset_start = prefix_length #LINE# #TAB# char_offset_end = prefix_length + text_length #LINE# #TAB# return opcode, char_offset_start, char_offset_end"
"Return a nx . Graph object <code> def bipartite_graph(red, black, edges): ","#LINE# #TAB# graph = nx.Graph() #LINE# #TAB# graph.add_nodes_from(red, bipartite=0) #LINE# #TAB# graph.add_nodes_from(black, bipartite=1) #LINE# #TAB# graph.add_edges_from(edges) #LINE# #TAB# return graph"
"Compares two python dictionaries at the top level and report differences if any to stdout <code> def dict_diff_and_report(da, db): ","#LINE# differences = dictDiff(da, db) #LINE# if not differences: #LINE# #TAB# return differences #LINE# if differences['inAButNotInB']: #LINE# #TAB# print "">>> inAButNotInB: %s"" % differences['inAButNotInB'] #LINE# if differences['inBButNotInA']: #LINE# #TAB# print "">>> inBButNotInA: %s"" % differences['inBButNotInA'] #LINE# for key in differences['differentValues']: #LINE# #TAB# print "">>> da[%s] != db[%s]"" % (key, key) #LINE# #TAB# print ""da[%s] = %r"" % (key, da[key]) #LINE# #TAB# print ""db[%s] = %r"" % (key, db[key]) #LINE# return differences"
Takes a string representing a regular expression as input and tokenizes it  <code> def tokenize_regex(input): ,"#LINE# #TAB# tokens = [] #LINE# #TAB# while input: #LINE# #TAB# #TAB# m = p.match(input) #LINE# #TAB# #TAB# if m: #LINE# #TAB# #TAB# #TAB# token, input = input[:m.end()], input[m.end():] #LINE# #TAB# #TAB# #TAB# if not token.isspace(): #LINE# #TAB# #TAB# #TAB# #TAB# tokens.append(token) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise Exception('Could not tokenize input regex.') #LINE# #TAB# return tokens"
Sets a global variable for tracking the timer accross multiple files <code> def code_timer(reset=False): ,#LINE# #TAB# global CODE_TIMER #LINE# #TAB# if reset: #LINE# #TAB# #TAB# CODE_TIMER = CodeTimer() #LINE# #TAB# else: #LINE# #TAB# #TAB# if CODE_TIMER is None: #LINE# #TAB# #TAB# #TAB# return CodeTimer() #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return CODE_TIMER
Extracts all allowed SBtab table types from the definitions file . Returns : list List of supported SBtab table types  <code> def extract_supported_table_types(): ,#LINE# #TAB# sbtab_def = open_definitions_file() #LINE# #TAB# supported_types = [] #LINE# #TAB# for row in sbtab_def.value_rows: #LINE# #TAB# #TAB# t = row[sbtab_def.columns_dict['!IsPartOf']] #LINE# #TAB# #TAB# if t not in supported_types: #LINE# #TAB# #TAB# #TAB# supported_types.append(t) #LINE# #TAB# return supported_types
"Distance ( in meters ) between two points in WGS84 coord system  <code> def wgs84_distance(lat1, lon1, lat2, lon2): ","#LINE# #TAB# dLat = math.radians(lat2 - lat1) #LINE# #TAB# dLon = math.radians(lon2 - lon1) #LINE# #TAB# a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(math.radians(lat1) #LINE# #TAB# #TAB# ) * math.cos(math.radians(lat2)) * math.sin(dLon / 2) * math.sin( #LINE# #TAB# #TAB# dLon / 2) #LINE# #TAB# c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) #LINE# #TAB# d = EARTH_RADIUS * c #LINE# #TAB# return d"
"Return the sum1_ij and sum2_ij values given the input indices and data instances  <code> def compute_sum_values(i, j, data1, data2): ","#LINE# #TAB# sum1_ij = 1. #LINE# #TAB# for idx, d in zip([i,j], [data1, data2]): #LINE# #TAB# #TAB# if isinstance(d, field): sum1_ij *= d.wvalue[idx] #LINE# #TAB# #TAB# elif isinstance(d, points): sum1_ij *= d.weights[idx] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise NotImplementedError(""data type not recognized"") #LINE# #TAB# sum2_ij = data1.weights[i] * data2.weights[j] #LINE# #TAB# return sum1_ij, sum2_ij"
"Return the checksum if the type is the expected  <code> def get_checksum(checksum, expected='md5'): ",#LINE# #TAB# checksum = checksum.split(':') #LINE# #TAB# if checksum[0] != expected or len(checksum) != 2: #LINE# #TAB# #TAB# raise AttributeError('Checksum format is not correct.') #LINE# #TAB# else: #LINE# #TAB# #TAB# return checksum[1]
Initialize the parties in the MuSig session and set their index number  <code> def signers_init(n_signers): ,"#LINE# #TAB# signers = [] #LINE# #TAB# for i in range(n_signers): #LINE# #TAB# #TAB# signers.append({'index': i, 'present': 0}) #LINE# #TAB# return signers"
Finds out whether there are intervals to expand and creates the charset <code> def parse_charset(charset): ,"#LINE# #TAB# import re #LINE# #TAB# regex = '(\\w-\\w)' #LINE# #TAB# pat = re.compile(regex) #LINE# #TAB# found = pat.findall(charset) #LINE# #TAB# result = '' #LINE# #TAB# if found: #LINE# #TAB# #TAB# for element in found: #LINE# #TAB# #TAB# #TAB# for char in char_range(element[0], element[-1]): #LINE# #TAB# #TAB# #TAB# #TAB# result += char #LINE# #TAB# #TAB# return result #LINE# #TAB# return charset"
"Gets a variant from the parser while disabling logging  <code> def get_variant_silent(parser, variant): ",#LINE# #TAB# prev_log = config.LOG_NOT_FOUND #LINE# #TAB# config.LOG_NOT_FOUND = False #LINE# #TAB# results = parser.get_variant_genotypes(variant) #LINE# #TAB# config.LOG_NOT_FOUND = prev_log #LINE# #TAB# return results
"Load inference model from file <code> def bnn_load_model(device, path): ",#LINE# #TAB# result = SUCCESS #LINE# #TAB# b_path = path.encode('utf-8') #LINE# #TAB# result = _lib.xq_bnn_load_model(ctypes.c_char_p(b_path)) #LINE# #TAB# return result
Grab the checksum file for a given entry  <code> def grab_checksums_file(entry): ,#LINE# #TAB# http_url = convert_ftp_url(entry['ftp_path']) #LINE# #TAB# full_url = '{}/md5checksums.txt'.format(http_url) #LINE# #TAB# req = requests.get(full_url) #LINE# #TAB# return req.text
Convert * obj * to an : class:`xarray . DataArray ` with sparse . COO storage  <code> def as_sparse_xarray(obj): ,"#LINE# #TAB# import sparse #LINE# #TAB# from xarray.core.dtypes import maybe_promote #LINE# #TAB# if isinstance(obj, xr.DataArray) and isinstance(obj.data, numpy.ndarray): #LINE# #TAB# #TAB# return xr.DataArray(data=sparse.COO.from_numpy(obj.data, fill_value #LINE# #TAB# #TAB# #TAB# =maybe_promote(obj.data.dtype)[1]), coords=obj.coords, dims=obj #LINE# #TAB# #TAB# #TAB# .dims, name=obj.name, attrs=obj.attrs) #LINE# #TAB# elif isinstance(obj, pd.Series): #LINE# #TAB# #TAB# return xr.DataArray.from_series(obj, sparse=True) #LINE# #TAB# else: #LINE# #TAB# #TAB# print(type(obj), type(obj.data)) #LINE# #TAB# #TAB# return obj"
"Get a stats exporter and running transport thread  <code> def new_stats_exporter(options=None, interval=None): ","#LINE# #TAB# if options is None: #LINE# #TAB# #TAB# _, project_id = google.auth.default() #LINE# #TAB# #TAB# options = Options(project_id=project_id) #LINE# #TAB# if str(options.project_id).strip() == """": #LINE# #TAB# #TAB# raise ValueError(ERROR_BLANK_PROJECT_ID) #LINE# #TAB# ci = client_info.ClientInfo(client_library_version=get_user_agent_slug()) #LINE# #TAB# client = monitoring_v3.MetricServiceClient(client_info=ci) #LINE# #TAB# exporter = StackdriverStatsExporter(client=client, options=options) #LINE# #TAB# transport.get_exporter_thread(stats.stats, exporter, interval=interval) #LINE# #TAB# return exporter"
"Calculate instance login - username based on image - name  <code> def cmd_ssh_user(tar_aminame, inst_name): ","#LINE# #TAB# if tar_aminame == ""Unknown"": #LINE# #TAB# #TAB# tar_aminame = inst_name #LINE# #TAB# userlu = {""ubunt"": ""ubuntu"", ""debia"": ""admin"", ""fedor"": ""root"", #LINE# #TAB# #TAB# #TAB# ""cento"": ""centos"", ""openb"": ""root""} #LINE# #TAB# usertemp = ['name'] + [value for key, value in list(userlu.items()) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# if key in tar_aminame.lower()] #LINE# #TAB# usertemp = dict(zip(usertemp[::2], usertemp[1::2])) #LINE# #TAB# username = usertemp.get('name', 'ec2-user') #LINE# #TAB# debg.dprint(""loginuser Calculated: "", username) #LINE# #TAB# return username"
"Set the requested attribute key and value for matched xpath element  <code> def set_attribute(file, element, key, value): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# root = ET.parse(file) #LINE# #TAB# #TAB# element = root.find(element) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# log.error(""Unable to find element matching %s"", element) #LINE# #TAB# #TAB# return False #LINE# #TAB# element.set(key, str(value)) #LINE# #TAB# root.write(file) #LINE# #TAB# return True"
@summary : query status of order @param orderno : order no <code> def query_order(orderno): ,"#LINE# #TAB# orderItemObj = OrderItem.objects.get(orderno=orderno) #LINE# #TAB# if orderItemObj.paied: #LINE# #TAB# #TAB# return PayResult(orderItemObj.orderno) #LINE# #TAB# elif orderItemObj.lapsed: #LINE# #TAB# #TAB# return PayResult(orderItemObj.orderno, succ=False, lapsed=True) #LINE# #TAB# else: #LINE# #TAB# #TAB# payResult = _PAY_GATEWAY[orderItemObj.payway].query_order(orderItemObj #LINE# #TAB# #TAB# #TAB# .orderno) #LINE# #TAB# #TAB# _update_order_pay_result(payResult) #LINE# #TAB# #TAB# return payResult"
Get the actions that are within the privileged list : param privileges : : return : list of allowed actions <code> def get_privileged_actions(privileges): ,#LINE# #TAB# allowed_action = [] #LINE# #TAB# for action in actions: #LINE# #TAB# #TAB# add = len(action['privileges']) > 0 #LINE# #TAB# #TAB# for action_privilege in action['privileges']: #LINE# #TAB# #TAB# #TAB# if action_privilege not in privileges.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# add = False #LINE# #TAB# #TAB# #TAB# elif not privileges[action_privilege] == 1: #LINE# #TAB# #TAB# #TAB# #TAB# add = False #LINE# #TAB# #TAB# if add: #LINE# #TAB# #TAB# #TAB# allowed_action.append(action) #LINE# #TAB# return allowed_action
Merges the default adapters file in the trimmomatic adapters directory Returns ------- str Path with the merged adapters file  <code> def merge_default_adapters(): ,"#LINE# #TAB# default_adapters = [os.path.join(ADAPTERS_PATH, x) for x in os.listdir( #LINE# #TAB# #TAB# ADAPTERS_PATH)] #LINE# #TAB# filepath = os.path.join(os.getcwd(), 'default_adapters.fasta') #LINE# #TAB# with open(filepath, 'w') as fh, fileinput.input(default_adapters) as in_fh: #LINE# #TAB# #TAB# for line in in_fh: #LINE# #TAB# #TAB# #TAB# fh.write(line) #LINE# #TAB# return filepath"
allows other ' plugin ' code to be imported into a game . this works a little like load module <code> def import_plugin(plugin_file): ,"#LINE# #TAB# from . import api #LINE# #TAB# code, mod = load_module(plugin_file, api) #LINE# #TAB# exec(code, mod.__dict__) #LINE# #TAB# return mod"
"Hook function passed to json - deserializer as "" object_hook "" . EnumEncoder in runhistory / runhistory  <code> def enum_hook(obj): ","#LINE# #TAB# if '__enum__' in obj: #LINE# #TAB# #TAB# name, member = obj['__enum__'].split('.') #LINE# #TAB# #TAB# if name == 'StatusType': #LINE# #TAB# #TAB# #TAB# return getattr(globals()[name], member) #LINE# #TAB# return obj"
Get the dependencies  <code> def get_requirements(): ,"#LINE# #TAB# with open(""requirements/project.txt"") as f: #LINE# #TAB# #TAB# requirements = [] #LINE# #TAB# #TAB# for line in f.readlines(): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if line and not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# requirements.append(line) #LINE# #TAB# return requirements"
"Calculates binary metrics given the specificity  <code> def get_stats(Ytrue, Yscore, specificity): ","#LINE# #TAB# sensitivity, cutoffscore = get_sens_cuttoff(Ytrue, Yscore, specificity) #LINE# #TAB# stats = binary_metrics(Ytrue, Yscore, cut_off=cutoffscore) #LINE# #TAB# return stats"
"Given a layer object , check against cache to see if that layer i d exists if yes then load any relevant layer level information . The default operation is to not redo the cache . Add notices to the layer 's origins matching the origin_str <code> def load_from_cache(layer, redo=False): ",#LINE# #TAB# loaded = False #LINE# #TAB# if not redo: #LINE# #TAB# #TAB# if load_packages_from_cache(layer): #LINE# #TAB# #TAB# #TAB# loaded = True #LINE# #TAB# #TAB# if layer.fs_hash in cache.get_layers(): #LINE# #TAB# #TAB# #TAB# layer.files_analyzed = cache.cache[layer.fs_hash]['files_analyzed'] #LINE# #TAB# #TAB# #TAB# layer.os_guess = cache.cache[layer.fs_hash]['os_guess'] #LINE# #TAB# #TAB# #TAB# layer.pkg_format = cache.cache[layer.fs_hash]['pkg_format'] #LINE# #TAB# #TAB# load_files_from_cache(layer) #LINE# #TAB# return loaded
Setup config commands for conda  <code> def _setup_config_from_kwargs(kwargs): ,"#LINE# #TAB# #TAB# cmd_list = ['--json', '--force'] #LINE# #TAB# #TAB# if 'file' in kwargs: #LINE# #TAB# #TAB# #TAB# cmd_list.extend(['--file', kwargs['file']]) #LINE# #TAB# #TAB# if 'system' in kwargs: #LINE# #TAB# #TAB# #TAB# cmd_list.append('--system') #LINE# #TAB# #TAB# return cmd_list"
"Helper function to determine the difference of two values that can be np.uints . Works in python and numba mode . Circumvents numba bug # 1653 <code> def is_in_max_difference(value_1, value_2, max_difference): ","#LINE# #TAB# if value_1 <= value_2: #LINE# #TAB# #TAB# return np.nextafter(value_2, value_1) - np.nextafter(value_1, value_2 #LINE# #TAB# #TAB# #TAB# ) <= max_difference #LINE# #TAB# return np.nextafter(value_1, value_2) - np.nextafter(value_2, value_1 #LINE# #TAB# #TAB# ) <= max_difference"
"Posts a signed message to LTI consumer <code> def post_message(consumers, lti_key, url, body): ","#LINE# #TAB# content_type = 'application/xml' #LINE# #TAB# method = 'POST' #LINE# #TAB# (_, content) = _post_patched_request( #LINE# #TAB# #TAB# consumers, #LINE# #TAB# #TAB# lti_key, #LINE# #TAB# #TAB# body, #LINE# #TAB# #TAB# url, #LINE# #TAB# #TAB# method, #LINE# #TAB# #TAB# content_type, #LINE# #TAB# ) #LINE# #TAB# is_success = b""<imsx_codeMajor>success</imsx_codeMajor>"" in content #LINE# #TAB# log.debug(""is success %s"", is_success) #LINE# #TAB# return is_success"
"get_specie_db_path : param root_dir : : type root_dir : str : param specie : : type specie : str : rtype : str <code> def get_specie_db_path(root_dir: str, specie: str) ->str: ","#LINE# #TAB# path = os.path.join(root_dir, specie, f'{specie}.db') #LINE# #TAB# return path"
"parse an ISO8601 date string If it is None or not a valid ISO8601 timestamp , it will be returned unmodified . Otherwise , it will return a datetime object  <code> def parse_date(s): ",#LINE# #TAB# if s is None: #LINE# #TAB# #TAB# return s #LINE# #TAB# m = ISO8601_PAT.match(s) #LINE# #TAB# if m: #LINE# #TAB# #TAB# dt = dateutil_parse(s) #LINE# #TAB# #TAB# return _ensure_tzinfo(dt) #LINE# #TAB# return s
"For a range indicated from start to end , replace with replacement  <code> def replace_token_range(tokens, start, end, replacement): ",#LINE# #TAB# tokens = tokens[:start] + replacement + tokens[end:] #LINE# #TAB# return tokens
"Returns effective resource group , workspace and name for the given resource <code> def get_effective_resource_parameters(name_or_id, resource_group, workspace): ","#LINE# #TAB# if not name_or_id: #LINE# #TAB# #TAB# return None, None, None #LINE# #TAB# if is_valid_resource_id(name_or_id): #LINE# #TAB# #TAB# parts = parse_resource_id(name_or_id) #LINE# #TAB# #TAB# return parts['resource_group'], parts['name'], parts['resource_name'] #LINE# #TAB# return resource_group, workspace, name_or_id"
"Get the wallet 's balance . Returns a dict with ' available ' and ' total ' balances , indicating what can be spent right now , and what is the total including unconfirmed funds . : rtype : dict <code> def get_balance(): ","#LINE# #TAB# hwb = ses.query(wm.HWBalance).filter(wm.HWBalance.network == NETWORK #LINE# #TAB# #TAB# ).order_by(wm.HWBalance.time.desc()).first() #LINE# #TAB# return {'total': hwb.total, 'available': hwb.available}"
"Create an INstanceAttribute from a list of InstnaceFields <code> def concatenate_fields(fields, dim): ","#LINE# #TAB# 'Create an INstanceAttribute from a list of InstnaceFields' #LINE# #TAB# if len(fields) == 0: #LINE# #TAB# #TAB# raise ValueError('fields cannot be an empty list') #LINE# #TAB# if len(set((f.name, f.shape, f.dtype) for f in fields)) != 1: #LINE# #TAB# #TAB# raise ValueError('fields should have homogeneous name, shape and dtype') #LINE# #TAB# tpl = fields[0] #LINE# #TAB# attr = InstanceAttribute(tpl.name, shape=tpl.shape, dtype=tpl.dtype, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# dim=dim, alias=None) #LINE# #TAB# attr.value = np.array([f.value for f in fields], dtype=tpl.dtype) #LINE# #TAB# return attr"
"Detect a workflow path if it is not passed  <code> def validate_path(ctx, param, value): ","#LINE# #TAB# client = ctx.obj #LINE# #TAB# if value is None: #LINE# #TAB# #TAB# from renku.core.models.provenance.activities import ProcessRun #LINE# #TAB# #TAB# activity = client.process_commit() #LINE# #TAB# #TAB# if not isinstance(activity, ProcessRun): #LINE# #TAB# #TAB# #TAB# raise click.BadParameter('No tool was found.') #LINE# #TAB# #TAB# return activity.path #LINE# #TAB# return value"
"Compute the areaness of every face of a volmesh . Parameters ---------- volmesh : volmesh object Returns ------- dict A dictionary of area deviation for each face of the volmesh  <code> def volmesh_face_areaness(volmesh, target_areas): ",#LINE# #TAB# areaness_dict = {} #LINE# #TAB# for hfkey in target_areas: #LINE# #TAB# #TAB# area = volmesh.halfface_oriented_area(hfkey) #LINE# #TAB# #TAB# areaness_dict[hfkey] = abs(target_areas[hfkey] - area) #LINE# #TAB# return areaness_dict
"The bulk moment is x**2+y**2+z**2 <code> def is_bulk_moment(moment, dim): ","#LINE# #TAB# if type(moment) is not tuple: #LINE# #TAB# #TAB# moment = polynomial_to_exponent_representation(moment) #LINE# #TAB# quadratic = False #LINE# #TAB# found = [(0) for _ in range(dim)] #LINE# #TAB# for prefactor, monomial in moment: #LINE# #TAB# #TAB# if sum(monomial) == 2: #LINE# #TAB# #TAB# #TAB# quadratic = True #LINE# #TAB# #TAB# #TAB# for i, exponent in enumerate(monomial[:dim]): #LINE# #TAB# #TAB# #TAB# #TAB# if exponent == 2: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# found[i] += prefactor #LINE# #TAB# #TAB# elif sum(monomial) > 2: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return quadratic and found != [0] * dim and len(set(found)) == 1"
"Loads a dolfin mesh <code> def load_mesh(comm: MPI.Comm, filepath: Text) ->df.Mesh: ","#LINE# #TAB# with df.HDF5File(comm, filepath, 'r') as meshfile: #LINE# #TAB# #TAB# mesh = df.Mesh(comm) #LINE# #TAB# #TAB# meshfile.read(mesh, 'mesh', False) #LINE# #TAB# return mesh"
If freesurfer_home is not set try to make an intelligent guess at it <code> def guess_home(): ,#LINE# #TAB# global freesurfer_home #LINE# #TAB# if freesurfer_home != None: #LINE# #TAB# #TAB# return True #LINE# #TAB# fv = nl.which('freeview') #LINE# #TAB# if fv: #LINE# #TAB# #TAB# freesurfer_home = parpar_dir(os.path.realpath(fv)) #LINE# #TAB# #TAB# return True #LINE# #TAB# for guess_dir in guess_locations: #LINE# #TAB# #TAB# if os.path.exists(guess_dir): #LINE# #TAB# #TAB# #TAB# freesurfer_home = guess_dir #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"Deletes stage identified by stageName from API identified by restApiId <code> def delete_api_stage(restApiId, stageName, region=None, key=None, keyid=None, profile=None): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) #LINE# #TAB# #TAB# conn.delete_stage(restApiId=restApiId, stageName=stageName) #LINE# #TAB# #TAB# return {'deleted': True} #LINE# #TAB# except ClientError as e: #LINE# #TAB# #TAB# return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"
"Given an set of transformation angles , generate a series of transformation matrices <code> def transformation_matrix(angles): ","#LINE# #TAB# R_t = [] #LINE# #TAB# for axis, ang in angles: #LINE# #TAB# #TAB# r_t = [] #LINE# #TAB# #TAB# for row in R[axis]: #LINE# #TAB# #TAB# #TAB# r_t_i = [] #LINE# #TAB# #TAB# #TAB# for el in row: #LINE# #TAB# #TAB# #TAB# #TAB# r_t_i.append(eval(el)) #LINE# #TAB# #TAB# #TAB# r_t.append(r_t_i) #LINE# #TAB# #TAB# R_t.append(r_t) #LINE# #TAB# return R_t"
"Time a function multiple times and return results and statistics . Expects a function without arguments  <code> def time_repeat(f, repeats=3): ","#LINE# #TAB# results = [] #LINE# #TAB# times = np.zeros(repeats, dtype=float) #LINE# #TAB# for i in range(repeats): #LINE# #TAB# #TAB# start = time.monotonic() #LINE# #TAB# #TAB# res = f() #LINE# #TAB# #TAB# results.append(res) #LINE# #TAB# #TAB# end = time.monotonic() #LINE# #TAB# #TAB# times[i] = end - start #LINE# #TAB# return results, {'times': times.tolist(), 'mean': np.mean(times).item(), #LINE# #TAB# #TAB# 'min': np.min(times).item(), 'max': np.max(times).item()}"
"Generate list of ops to implement a SWAP gate along a coupling edge  <code> def swap_ops_from_edge(edge, layout): ","#LINE# #TAB# device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q') #LINE# #TAB# qreg_edge = [(device_qreg, i) for i in edge] #LINE# #TAB# return [ #LINE# #TAB# #TAB# DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'}) #LINE# #TAB# ]"
"from list of vectors of v in absolute frame , reflect vector on plane defined by its normal u angle in deg result is an array <code> def reflect_on_u(v, u): ","#LINE# #TAB# UU = np.array(u) #LINE# #TAB# nUU = 1.0 * np.sqrt(np.sum(UU ** 2)) #LINE# #TAB# unit_axis = np.array(UU) / nUU #LINE# #TAB# mat = np.eye(3) - 2.0 * np.outer(unit_axis, unit_axis) #LINE# #TAB# wholelistvecfiltered = np.dot(mat, v.T).T #LINE# #TAB# return wholelistvecfiltered"
Strips type information from rendered data . Useful for debugging  <code> def strip_type_info(rendered_data): ,"#LINE# if isinstance(rendered_data, (list, tuple)): #LINE# #TAB# return [strip_type_info(d) for d in rendered_data] #LINE# elif isinstance(rendered_data, dict): #LINE# #TAB# if ""value"" in rendered_data and ""type"" in rendered_data: #LINE# #TAB# return strip_type_info(rendered_data[""value""]) #LINE# #TAB# else: #LINE# #TAB# result = {} #LINE# #TAB# for k, v in iteritems(rendered_data): #LINE# #TAB# #TAB# result[k] = strip_type_info(v) #LINE# #TAB# return result #LINE# else: #LINE# #TAB# return rendered_data"
"preprocess data by removing extra space and normalize data  <code> def preprocess_text(inputs, remove_space=True, lower=False): ","#LINE# #TAB# outputs = inputs #LINE# #TAB# if remove_space: #LINE# #TAB# #TAB# outputs = ' '.join(inputs.strip().split()) #LINE# #TAB# outputs = unicodedata.normalize('NFKD', outputs) #LINE# #TAB# outputs = ''.join([c for c in outputs if not unicodedata.combining(c)]) #LINE# #TAB# if lower: #LINE# #TAB# #TAB# outputs = outputs.lower() #LINE# #TAB# return outputs"
Returns request data extracted from web . ctx  <code> def get_data_from_request(): ,"#LINE# #TAB# return { #LINE# #TAB# #TAB# 'request': { #LINE# #TAB# #TAB# #TAB# 'url': '%s://%s%s' % (web.ctx['protocol'], web.ctx['host'], web.ctx['path']), #LINE# #TAB# #TAB# #TAB# 'query_string': web.ctx.query, #LINE# #TAB# #TAB# #TAB# 'method': web.ctx.method, #LINE# #TAB# #TAB# #TAB# 'data': web.data(), #LINE# #TAB# #TAB# #TAB# 'headers': dict(get_headers(web.ctx.environ)), #LINE# #TAB# #TAB# #TAB# 'env': dict(get_environ(web.ctx.environ)), #LINE# #TAB# #TAB# } #LINE# #TAB# }"
"Return a list of code , name , description of valid encodings  <code> def get_valid(): ","#LINE# #TAB# valid_encodings = [] #LINE# #TAB# for i, item in enumerate(_encodings): #LINE# #TAB# #TAB# if is_valid_code(item[CODE]): #LINE# #TAB# #TAB# #TAB# valid_encodings.append(item) #LINE# #TAB# return valid_encodings"
Return open Word . Application handle or None if Word is not available on this system  <code> def get_word_app (): ,"#LINE# #TAB# if not has_word(): #LINE# #TAB# #TAB# return None #LINE# #TAB# pythoncom.CoInitialize() #LINE# #TAB# import win32com.client #LINE# #TAB# app = win32com.client.gencache.EnsureDispatch(""Word.Application"") #LINE# #TAB# app.Visible = False #LINE# #TAB# return app"
change extension to type : param extension_name : extension name : return : type <code> def extension_to_type(extension_name): ,#LINE# #TAB# if extension_name in extension_to_type_mapping_dict.keys(): #LINE# #TAB# #TAB# return extension_to_type_mapping_dict[extension_name] #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''
"Waits for page loaded <code> def wait_for_page_loaded(driver, seconds=NORMAL_LOAD_TIME): ","#LINE# #TAB# wait_document_status(driver, seconds) #LINE# #TAB# enable_jquery(driver) #LINE# #TAB# wait_jquery_status(driver, seconds) #LINE# #TAB# alert_is_present = EC.alert_is_present() #LINE# #TAB# if alert_is_present(driver): #LINE# #TAB# #TAB# alert_text = driver.switch_to_alert().text #LINE# #TAB# #TAB# driver.switch_to_alert().dismiss() #LINE# #TAB# #TAB# raise alert_text"
return either ' < path > ' or ' line N of < path > ' or ' lines M - N of < path > '  <code> def describe_source(d): ,"#LINE# #TAB# path = d.get('path') or '' #LINE# #TAB# if 'num_lines' in d and 'start_line' in d: #LINE# #TAB# #TAB# if d['num_lines'] == 1: #LINE# #TAB# #TAB# #TAB# return 'line %d of %s' % (d['start_line'] + 1, path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return 'lines %d-%d of %s' % (d['start_line'] + 1, d[ #LINE# #TAB# #TAB# #TAB# #TAB# 'start_line'] + d['num_lines'], path) #LINE# #TAB# else: #LINE# #TAB# #TAB# return path"
Return the binary 's architecture  <code> def get_arch(): ,"#LINE# #TAB# if is_alive(): #LINE# #TAB# #TAB# arch = gdb.selected_frame().architecture() #LINE# #TAB# #TAB# return arch.name() #LINE# #TAB# arch_str = gdb.execute('show architecture', to_string=True).strip() #LINE# #TAB# if 'The target architecture is set automatically (currently ' in arch_str: #LINE# #TAB# #TAB# arch_str = arch_str.split('(currently ', 1)[1] #LINE# #TAB# #TAB# arch_str = arch_str.split(')', 1)[0] #LINE# #TAB# elif 'The target architecture is assumed to be ' in arch_str: #LINE# #TAB# #TAB# arch_str = arch_str.replace('The target architecture is assumed to be ' #LINE# #TAB# #TAB# #TAB# , '') #LINE# #TAB# else: #LINE# #TAB# #TAB# raise RuntimeError('Unknown architecture: {}'.format(arch_str)) #LINE# #TAB# return arch_str"
# Electronic entropy evaluation # Depends on multiplicity Calculates the electronic entropic contribution ( J/(mol*K ) ) of the molecule S_elec = R(Ln(multiplicity ) <code> def calc_electronic_entropy(multiplicity): ,#LINE# #TAB# entropy = GAS_CONSTANT * np.log(multiplicity) #LINE# #TAB# return entropy
Returns a color scale to be used for a plotly figure <code> def get_colorscale(scale): ,"#LINE# #TAB# if type(scale) in string_types: #LINE# #TAB# #TAB# scale = get_scales(scale) #LINE# #TAB# else: #LINE# #TAB# #TAB# if type(scale) != list: #LINE# #TAB# #TAB# #TAB# raise Exception( #LINE# #TAB# #TAB# #TAB# #TAB# ""scale needs to be either a scale name or list of colors"") #LINE# #TAB# cs = [[1.0 * c / (len(scale) - 1), scale[c]] for c in range(len(scale))] #LINE# #TAB# cs.sort() #LINE# #TAB# return cs"
"Read parameters from TOML file . Parameters ---------- filename : str or Path The name of the file to read . Should have extension ' .toml ' . Returns ------- dict A dictionary representation of the parameters file  <code> def read_parameter_file(filename: Union[str, Path]) ->dict: ","#LINE# #TAB# if not pathlib.Path(filename).exists(): #LINE# #TAB# #TAB# raise ValueError('parameter file does not exist') #LINE# #TAB# with open(filename, 'r') as fp: #LINE# #TAB# #TAB# t = tomlkit.loads(fp.read()) #LINE# #TAB# d = dict() #LINE# #TAB# for key, val in t.items(): #LINE# #TAB# #TAB# if isinstance(val, list): #LINE# #TAB# #TAB# #TAB# val = tuple(val) #LINE# #TAB# #TAB# d[key] = val #LINE# #TAB# return d"
Return list of tasks from ` job_list ` that were unsuccessfully terminated  <code> def filter_unsuccessful(job_list): ,#LINE# #TAB# return [job for job in job_list if job.execution.returncode is not None and #LINE# #TAB# #TAB# job.execution.returncode != 0]
"Return all configs for given package . Args : _ _ pkg : Package name _ _ name : Configuration file name <code> def get_configs(__pkg: str, __name: str='config') ->List[Path]: ","#LINE# #TAB# dirs = [user_config(__pkg)] #LINE# #TAB# dirs.extend(Path(d).expanduser() / __pkg for d in getenv( #LINE# #TAB# #TAB# 'XDG_CONFIG_DIRS', '/etc/xdg').split(':')) #LINE# #TAB# configs = [] #LINE# #TAB# for dname in reversed(dirs): #LINE# #TAB# #TAB# test_path = dname / __name #LINE# #TAB# #TAB# if test_path.exists(): #LINE# #TAB# #TAB# #TAB# configs.append(test_path) #LINE# #TAB# return configs"
"Load ( or create ) dungeon layout . Returns : dict : dungeon layout in format { identifier : ( column , row ) } <code> def get_layout() ->dict: ","#LINE# #TAB# try: #LINE# #TAB# #TAB# layout = layoutstorage.load_dungeons() #LINE# #TAB# except (layoutstorage.NoConfig, configparser.Error): #LINE# #TAB# #TAB# layout = {} #LINE# #TAB# #TAB# for dungeon in DUNGEONS: #LINE# #TAB# #TAB# #TAB# layout[dungeon] = DUNGEONS[dungeon] #LINE# #TAB# #TAB# layoutstorage.new_dungeon(layout) #LINE# #TAB# #TAB# layout = layoutstorage.load_dungeons() #LINE# #TAB# return layout"
"Get suggestions for NameError exception  <code> def get_name_error_sugg(value, frame): ","#LINE# #TAB# assert isinstance(value, NameError) #LINE# #TAB# assert len(value.args) == 1 #LINE# #TAB# error_msg, = value.args #LINE# #TAB# error_re = re.UNBOUNDERROR_RE if isinstance(value, UnboundLocalError #LINE# #TAB# #TAB# ) else re.NAMENOTDEFINED_RE #LINE# #TAB# match = re.match(error_re, error_msg) #LINE# #TAB# if match: #LINE# #TAB# #TAB# name, = match.groups() #LINE# #TAB# #TAB# return get_name_suggestions(name, frame) #LINE# #TAB# return []"
"returns true if the compiler is gfortran-8 or later <code> def is_gfortran8plus(compiler, log): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# output, error, status = config.base.Configure.executeShellCommand( #LINE# #TAB# #TAB# #TAB# compiler + ' --version', log=log) #LINE# #TAB# #TAB# output = output + error #LINE# #TAB# #TAB# import re #LINE# #TAB# #TAB# strmatch = re.match('GNU Fortran\\s+\\(.*\\)\\s+(\\d+)\\.(\\d+)', #LINE# #TAB# #TAB# #TAB# output) #LINE# #TAB# #TAB# if strmatch: #LINE# #TAB# #TAB# #TAB# VMAJOR, VMINOR = strmatch.groups() #LINE# #TAB# #TAB# #TAB# if (int(VMAJOR), int(VMINOR)) >= (8, 0): #LINE# #TAB# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# except RuntimeError: #LINE# #TAB# #TAB# pass"
"Append text to file <code> def write_text_to_file(text, filename): ","#LINE# #TAB# with open(filename, mode='a') as text_file: #LINE# #TAB# #TAB# text_file.write(text) #LINE# #TAB# text_file.close() #LINE# #TAB# return filename"
"Find the indices of the n largest and smallest ( respectively ) values in an array . : param data : the data array : param n : the number of indices to find : return : two lists : one with the smallest indices and one with the largest <code> def n_largest_smallest(data, n): ","#LINE# #TAB# first_args = np.argpartition(data, n)[:n] #LINE# #TAB# last_args = np.argpartition(data, -n)[-n:] #LINE# #TAB# return [(arg, data[arg]) for arg in first_args], [(arg, data[arg]) for #LINE# #TAB# #TAB# arg in last_args]"
Get a Provider1Definition for the provider that knows all the categories <code> def get_categories_def(): ,"#LINE# #TAB# categories_def = Provider1Definition() #LINE# #TAB# categories_def.name = 'com.canonical.plainbox:categories' #LINE# #TAB# categories_def.version = '1.0' #LINE# #TAB# categories_def.description = N_('Common test category definitions') #LINE# #TAB# categories_def.secure = False #LINE# #TAB# categories_def.gettext_domain = 'plainbox-provider-categories' #LINE# #TAB# categories_def.location = os.path.join(get_plainbox_dir(), #LINE# #TAB# #TAB# 'impl/providers/categories') #LINE# #TAB# return categories_def"
Returns the minimum RAM required to run an IOS image . : param path : path to the IOS image : returns : minimum RAM in MB or 0 if there is an error <code> def get_minimum_required_ram(path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if isIOSCompressed(path): #LINE# #TAB# #TAB# #TAB# zip_file = zipfile.ZipFile(path, 'r') #LINE# #TAB# #TAB# #TAB# decompressed_size = 0 #LINE# #TAB# #TAB# #TAB# for zip_info in zip_file.infolist(): #LINE# #TAB# #TAB# #TAB# #TAB# decompressed_size += zip_info.file_size #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# decompressed_size = os.path.getsize(path) #LINE# #TAB# except (zipfile.BadZipFile, OSError, ValueError): #LINE# #TAB# #TAB# return 0 #LINE# #TAB# decompressed_size = decompressed_size / (1000 * 1000) + 1 #LINE# #TAB# return math.ceil(decompressed_size / 32) * 32"
"Stream an entire remote annotation file from physiobank Parameters ---------- file_name : str The name of the annotation file to be read . pb_dir : str The physiobank directory where the annotation file is located  <code> def stream_annotation(file_name, pb_dir): ","#LINE# #TAB# url = posixpath.join(config.db_index_url, pb_dir, file_name) #LINE# #TAB# response = requests.get(url) #LINE# #TAB# response.raise_for_status() #LINE# #TAB# ann_data = np.fromstring(response.content, dtype=np.dtype('<u1')) #LINE# #TAB# return ann_data"
Returns a parser for query parameters from swagger document parameters . : param params : swagger doc parameters : return : Query parameter parser <code> def get_parser(params): ,"#LINE# #TAB# parser = reqparse.RequestParser() #LINE# #TAB# for arg in get_parser_args(params): #LINE# #TAB# #TAB# parser.add_argument(arg[0], **arg[1]) #LINE# #TAB# return parser"
"Update the default requests HTTP headers with OSMnx info  <code> def get_http_headers(user_agent=None, referer=None, accept_language=None): ","#LINE# #TAB# if user_agent is None: #LINE# #TAB# #TAB# user_agent = settings.default_user_agent #LINE# #TAB# if referer is None: #LINE# #TAB# #TAB# referer = settings.default_referer #LINE# #TAB# if accept_language is None: #LINE# #TAB# #TAB# accept_language = settings.default_accept_language #LINE# #TAB# headers = requests.utils.default_headers() #LINE# #TAB# headers.update({'User-Agent': user_agent, 'referer': referer, 'Accept-Language': accept_language}) #LINE# #TAB# return headers"
Refresh an existing lock  <code> def refresh_lock(lock_file): ,"#LINE# #TAB# unique_id = '%s_%s_%s' % ( #LINE# #TAB# #TAB# os.getpid(), #LINE# #TAB# #TAB# ''.join([str(random.randint(0, 9)) for i in range(10)]), hostname) #LINE# #TAB# try: #LINE# #TAB# #TAB# lock_write = open(lock_file, 'w') #LINE# #TAB# #TAB# lock_write.write(unique_id + '\n') #LINE# #TAB# #TAB# lock_write.close() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# while get_lock.n_lock > 0: #LINE# #TAB# #TAB# #TAB# release_lock() #LINE# #TAB# #TAB# raise #LINE# #TAB# return unique_id"
Validate matplotlib axes or generate one if not provided <code> def get_matplotlib_ax(ax): ,"#LINE# #TAB# if ax: #LINE# #TAB# #TAB# assert isinstance(ax, matplotlib.axes.Axes #LINE# #TAB# #TAB# #TAB# ), 'You must pass a valid matplotlib.axes.Axes' #LINE# #TAB# else: #LINE# #TAB# #TAB# _, ax = plt.subplots() #LINE# #TAB# return ax"
"Return percentage of sample_count in total_samples  <code> def get_percentage(sample_count, total_samples): ","#LINE# #TAB# if total_samples != 0: #LINE# #TAB# #TAB# return 100 * round(float(sample_count) / total_samples, 3) #LINE# #TAB# return 0.0"
"Assuming that output has already been generated and stop executing the rest of the substep <code> def done_if(expr, msg=''): ","#LINE# #TAB# if expr: #LINE# #TAB# #TAB# raise StopInputGroup(msg=msg, keep_output=True) #LINE# #TAB# return 0"
"Returns a list of files that match the wildcard passed in ( honoring asterisks ) by recursively searching the starting_dir and below  <code> def find_files_recursively(starting_dir, wildcard): ","#LINE# #TAB# matches = [] #LINE# #TAB# for root, dirnames, filenames in os.walk(starting_dir): #LINE# #TAB# #TAB# for filename in fnmatch.filter(filenames, wildcard): #LINE# #TAB# #TAB# #TAB# matches.append(cleanse_path(os.path.join(root, filename), False)) #LINE# #TAB# return matches"
"Compute number of time frames of spectrogram <code> def num_frames(length, fsize, fshift): ",#LINE# #TAB# pad = fsize - fshift #LINE# #TAB# if length % fshift == 0: #LINE# #TAB# #TAB# M = (length + pad * 2 - fsize) // fshift + 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# M = (length + pad * 2 - fsize) // fshift + 2 #LINE# #TAB# return M
Return any ` ` MarkdownExtension ` ` plugins  <code> def load_markdown_extensions(): ,"#LINE# #TAB# load_plugins(os.path.join(os.path.abspath(os.curdir), 'plugins')) #LINE# #TAB# return [e() for e in MarkdownExtensionMixin.__subclasses__()]"
"Create id / token mappings for sequences . : param sequences : list of token sequences : return : mappings from i d to token , and token to i d <code> def create_dictionary(sequences): ","#LINE# #TAB# tokens = {} #LINE# #TAB# for s in sequences: #LINE# #TAB# #TAB# for token in s: #LINE# #TAB# #TAB# #TAB# tokens[token] = tokens.get(token, 0) + 1 #LINE# #TAB# sorted_tokens = sorted(tokens.items(), key=lambda x: -x[1]) #LINE# #TAB# id2token = [] #LINE# #TAB# token2id = {} #LINE# #TAB# for i, (t, _) in enumerate(sorted_tokens): #LINE# #TAB# #TAB# id2token.append(t) #LINE# #TAB# #TAB# token2id[t] = i #LINE# #TAB# return id2token, token2id"
"Calculate the average position of a list of lat , long coordinates . Parameters ---------- * coordinates : list or tuple List of ( lat , lng ) coordinates . Returns ------- tuple <code> def calc_avg_position(coordinates): ","#LINE# #TAB# if not all(isinstance(i, (list, tuple)) for i in coordinates): #LINE# #TAB# #TAB# error = 'Coordinates is not an iterable of [lat, lng] or (lat, lng)' #LINE# #TAB# #TAB# raise ValueError(error) #LINE# #TAB# n_coordinates = len(coordinates) #LINE# #TAB# avg_lat = sum(i[0] for i in coordinates) / n_coordinates #LINE# #TAB# avg_lng = sum(i[1] for i in coordinates) / n_coordinates #LINE# #TAB# avg_position = round(avg_lat, 6), round(avg_lng, 6) #LINE# #TAB# return avg_position"
Check if a group exists <code> def group_exists(groupname): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# grp.getgrnam(groupname) #LINE# #TAB# #TAB# group_exists = True #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# group_exists = False #LINE# #TAB# return group_exists
"lmfit 2nd order polynomial model <code> def poly2o_model(params, shape): ","#LINE# #TAB# mx = params['mx'].value #LINE# #TAB# my = params['my'].value #LINE# #TAB# mxy = params['mxy'].value #LINE# #TAB# ax = params['ax'].value #LINE# #TAB# ay = params['ay'].value #LINE# #TAB# off = params['off'].value #LINE# #TAB# bg = np.zeros(shape, dtype=float) + off #LINE# #TAB# x = np.arange(bg.shape[0]) - bg.shape[0] // 2 #LINE# #TAB# y = np.arange(bg.shape[1]) - bg.shape[1] // 2 #LINE# #TAB# x = x.reshape(-1, 1) #LINE# #TAB# y = y.reshape(1, -1) #LINE# #TAB# bg += ax * x ** 2 + ay * y ** 2 + mx * x + my * y + mxy * x * y #LINE# #TAB# return bg"
Checks if a node is a valid field or method in a embedded document  <code> def node_is_embedded_doc_attr(node): ,#LINE# #TAB# embedded_doc = get_field_embedded_doc(node.last_child()) #LINE# #TAB# name = node.attrname #LINE# #TAB# try: #LINE# #TAB# #TAB# r = bool(embedded_doc.lookup(name)[1][0]) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# r = False #LINE# #TAB# return r
"Import module from a file . Used to load models from a directory  <code> def import_file(name, loc): ","#LINE# #TAB# loc = path2str(loc) #LINE# #TAB# if is_python_pre_3_5: #LINE# #TAB# #TAB# import imp #LINE# #TAB# #TAB# return imp.load_source(name, loc) #LINE# #TAB# else: #LINE# #TAB# #TAB# import importlib.util #LINE# #TAB# #TAB# spec = importlib.util.spec_from_file_location(name, str(loc)) #LINE# #TAB# #TAB# module = importlib.util.module_from_spec(spec) #LINE# #TAB# #TAB# spec.loader.exec_module(module) #LINE# #TAB# #TAB# return module"
Credit : streamlink / plugins / ustreamtv.py : UHSClient : parse_proxy_url ( ) <code> def parse_proxy_url(purl): ,"#LINE# #TAB# proxy_options = {} #LINE# #TAB# if purl: #LINE# #TAB# #TAB# p = urlparse(purl) #LINE# #TAB# #TAB# proxy_options['proxy_type'] = p.scheme #LINE# #TAB# #TAB# proxy_options['http_proxy_host'] = p.hostname #LINE# #TAB# #TAB# if p.port: #LINE# #TAB# #TAB# #TAB# proxy_options['http_proxy_port'] = p.port #LINE# #TAB# #TAB# if p.username: #LINE# #TAB# #TAB# #TAB# proxy_options['http_proxy_auth'] = unquote_plus(p.username #LINE# #TAB# #TAB# #TAB# #TAB# ), unquote_plus(p.password or '') #LINE# #TAB# return proxy_options"
"Read 1 byte of data as ` char ` . Parameters ---------- data : io . BufferedReader File open to read in binary mode Returns ------- bytes Python string of length of 1 , encoded as bytes <code> def read_char(data): ","#LINE# #TAB# s_type = '=%s' % get_type('char') #LINE# #TAB# return struct.unpack(s_type, data.read(1))[0]"
returns True if all necessary components are available to support the .to_pcm ( ) method <code> def supports_to_pcm(cls): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# from audiotools.decoders import OpusDecoder #LINE# #TAB# #TAB# return True #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
"Resizes mini masks back to image size . Reverses the change of minimize_mask ( ) . See inspect_data.ipynb notebook for more details  <code> def expand_mask(bbox, mini_mask, image_shape): ","#LINE# #TAB# mask = np.zeros(image_shape[:2] + (mini_mask.shape[-1],), dtype=bool) #LINE# #TAB# for i in range(mask.shape[-1]): #LINE# #TAB# #TAB# m = mini_mask[:, :, (i)] #LINE# #TAB# #TAB# y1, x1, y2, x2 = bbox[i][:4] #LINE# #TAB# #TAB# h = y2 - y1 #LINE# #TAB# #TAB# w = x2 - x1 #LINE# #TAB# #TAB# m = resize(m, (h, w)) #LINE# #TAB# #TAB# mask[y1:y2, x1:x2, (i)] = np.around(m).astype(np.bool) #LINE# #TAB# return mask"
creates a datetime index <code> def make_datetime_index(df): ,"#LINE# #TAB# df['date'] = df[0].apply(lambda tvec: datetime_mtopy(tvec)) #LINE# #TAB# df.set_index('date', inplace=True) #LINE# #TAB# df.drop(columns=[0], axis=1, inplace=True) #LINE# #TAB# return"
Extract the contact info from a vCard . : param vcard : the vCard text to convert . : return : a list containing the extracted contact info  <code> def extract_contacts_from_vcard(vcard): ,#LINE# #TAB# contacts = [] #LINE# #TAB# for v_component in vobject.readComponents(vcard): #LINE# #TAB# #TAB# entry = extract_contact_from_component(v_component) #LINE# #TAB# #TAB# contacts.append(entry) #LINE# #TAB# return contacts
Check if the given request uses a whitelisted method  <code> def is_client_method_whitelisted(request) ->bool: ,#LINE# #TAB# if settings.AXES_NEVER_LOCKOUT_GET and request.method == 'GET': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Opens a simple yes / no message box , returns a bool <code> def get_yesno(parent=None, title='User Input', prompt='Yes or No:'): ","#LINE# #TAB# toplevel = tk.Tk() #LINE# #TAB# toplevel.withdraw() #LINE# #TAB# ret = messagebox.askyesno(title, prompt) #LINE# #TAB# toplevel.deiconify() #LINE# #TAB# toplevel.destroy() #LINE# #TAB# return ret"
"Rotated XOR cipher  <code> def rotated_xor(values, width, rotator): ",#LINE# #TAB# cs = 0 #LINE# #TAB# for value in values: #LINE# #TAB# #TAB# cs ^= value #LINE# #TAB# #TAB# cs = rotator(cs) #LINE# #TAB# return cs % 2 ** width
"Removes trailing characters from a string if that does not make it empty : param value : A string value that will be stripped . : param chars : Characters to remove . : return : Stripped value  <code> def safe_rstrip(value, chars=None): ","#LINE# #TAB# if not isinstance(value, six.string_types): #LINE# #TAB# #TAB# LOG.warning( #LINE# #TAB# #TAB# #TAB# 'Failed to remove trailing character. Returning original object. Supplied object is not a string: %s,' #LINE# #TAB# #TAB# #TAB# , value) #LINE# #TAB# #TAB# return value #LINE# #TAB# return value.rstrip(chars) or value"
"Adds a highlight tag to the given edges  <code> def highlight_edges(graph: BELGraph, edges=None, color: Optional[str]=None) -> None: ","#LINE# #TAB# color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR #LINE# #TAB# for u, v, k, d in edges if edges is not None else graph.edges(keys=True, data=True): #LINE# #TAB# #TAB# graph[u][v][k][EDGE_HIGHLIGHT] = color"
"Resizes an image by fitting it into the box without changing the aspect ratio  <code> def resize_cmd(images, width, height): ","#LINE# #TAB# for image in images: #LINE# #TAB# #TAB# w, h = width or image.size[0], height or image.size[1] #LINE# #TAB# #TAB# click.echo('Resizing ""%s"" to %dx%d' % (image.filename, w, h)) #LINE# #TAB# #TAB# image.thumbnail((w, h)) #LINE# #TAB# #TAB# yield image"
"Intended primarily for testing _ step_function_fast correctness <code> def step_function_correct(eigs: ndarray, x: ndarray) ->ndarray: ","#LINE# #TAB# ret = np.empty(len(x), dtype=np.float64) #LINE# #TAB# for i in prange(len(x)): #LINE# #TAB# #TAB# ret[i] = np.sum(eigs <= x[i]) #LINE# #TAB# return ret"
"Generate harmonics of a given frequency for least squares fitting  <code> def har_n(t, f, N, A=1): ","#LINE# #TAB# cols = [] #LINE# #TAB# for i in xrange(2, N + 1): #LINE# #TAB# #TAB# cols.extend([A * np.cos(t * 2 * np.pi * f * i), A * np.sin(t * 2 * #LINE# #TAB# #TAB# #TAB# np.pi * f * i)]) #LINE# #TAB# return np.c_[cols].T"
Recover the backup config  <code> def get_backup_config(): ,"#LINE# #TAB# backup_config_filepath = join(PREFS_PATH, BACKUP_CONFIG_FILENAME) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(backup_config_filepath) as f: #LINE# #TAB# #TAB# #TAB# config_str = f.read() #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# with open(backup_config_filepath, 'w+') as f: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# config_str = '' #LINE# #TAB# locals_ = {} #LINE# #TAB# exec(config_str, {}, locals_) #LINE# #TAB# return locals_"
Generate a unique string <code> def unique_string(length=UUID_LENGTH): ,#LINE# #TAB# string = str(uuid4()) * int(math.ceil(length / float(UUID_LENGTH))) #LINE# #TAB# return string[:length] if length else string
"Parse the url to decide which driver should be loaded . : param url : str : return : Module name , str <code> def parse_url(url): ","#LINE# #TAB# if 'core.windows.net' in url: #LINE# #TAB# #TAB# logger.info(f'Loading azure driver, url={url}.') #LINE# #TAB# #TAB# return 'azure' #LINE# #TAB# elif 's3://' in url: #LINE# #TAB# #TAB# logger.info(f'Loading aws driver, url={url}.') #LINE# #TAB# #TAB# return 'aws' #LINE# #TAB# elif 'ipfs://' in url: #LINE# #TAB# #TAB# logger.info(f'Loading IPFS driver, url={url}') #LINE# #TAB# #TAB# return 'ipfs' #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.info(f'Loading on_premise driver, url={url}.') #LINE# #TAB# #TAB# return 'on_premise'"
"Get the right text based on the number of ` ` items ` `  <code> def get_many_text(items, one, two, many): ","#LINE# #TAB# message = '' #LINE# #TAB# if not items: #LINE# #TAB# #TAB# return message #LINE# #TAB# items_count = len(items) #LINE# #TAB# if items_count == 1: #LINE# #TAB# #TAB# message = one.format(item=items[0]) #LINE# #TAB# elif items_count == 2: #LINE# #TAB# #TAB# message = two.format(first=items[0], second=items[1]) #LINE# #TAB# else: #LINE# #TAB# #TAB# left = ', '.join(items[:-1]) #LINE# #TAB# #TAB# last = items[-1] #LINE# #TAB# #TAB# message = many.format(left=left, last=last) #LINE# #TAB# return message"
makes a sorted list of all roi.zips . Requires zips to be named with suffix SeriesXXX.zip <code> def get_sorted_zipfile_list(dirpath): ,"#LINE# #TAB# zips = filebrowser.GetROIZIPList(dirpath) #LINE# #TAB# if zips: #LINE# #TAB# #TAB# series_ids = [int(re.search('.*Series(\\d+)\\.zip', i).group(1)) for #LINE# #TAB# #TAB# #TAB# i in zips] #LINE# #TAB# #TAB# return sorted(list(zip(series_ids, zips))) #LINE# #TAB# else: #LINE# #TAB# #TAB# return []"
"Check if input items are unique Parameters ---------- values : set set of all values Returns ------- True / False , MULTI / unique value <code> def is_uniq(values): ","#LINE# #TAB# if len(values) == 0: #LINE# #TAB# #TAB# return True, '' #LINE# #TAB# elif len(values) == 1: #LINE# #TAB# #TAB# return True, list(values)[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return False, 'MULTI'"
Return fieldnames of either a namedtuple or GOEnrichmentRecord  <code> def _get_fieldnames(item): ,"#LINE# #TAB# #TAB# if hasattr(item, ""_fldsdefprt""): #LINE# #TAB# #TAB# #TAB# return item.get_prtflds_all() #LINE# #TAB# #TAB# if hasattr(item, ""_fields""): #LINE# #TAB# #TAB# #TAB# return item._fields"
"Submit a batch job to combine the outputs of a reading job  <code> def submit_combine(basename, readers, job_ids=None, project_name=None): ","#LINE# #TAB# sub = PmidSubmitter(basename, readers, project_name) #LINE# #TAB# sub.job_list = job_ids #LINE# #TAB# sub.submit_combine() #LINE# #TAB# return sub"
Get the readings meta - data  <code> def get_meta_data(): ,"#LINE# #TAB# return {'air_temperature': {'type': 'gauge', 'unit': 'C', 'precision': #LINE# #TAB# #TAB# 2, 'range_low': 0, 'range_high': 100, 'sensor': 'flower_power'}, #LINE# #TAB# #TAB# 'soil_temperature': {'type': 'gauge', 'unit': 'C', 'precision': 2, #LINE# #TAB# #TAB# 'range_low': 0, 'range_high': 100, 'sensor': 'flower_power'}}"
"Creates an suffix property transducer based on given alphabet : param bool preserving : input preserving transducer , else input altering : param list|set alphabet : alphabet : rtype : SFT <code> def suffix_transducer(alphabet, preserving=False): ","#LINE# #TAB# t = SFT() #LINE# #TAB# t.setSigma(alphabet) #LINE# #TAB# t.setOutput(alphabet) #LINE# #TAB# initial = t.stateIndex(0, True) #LINE# #TAB# final = t.stateIndex(1, True) #LINE# #TAB# t.addInitial(initial) #LINE# #TAB# t.addFinal(final) #LINE# #TAB# for i in alphabet: #LINE# #TAB# #TAB# t.addTransition(initial, i, Epsilon, initial) #LINE# #TAB# #TAB# t.addTransition(initial, i, Epsilon, final) #LINE# #TAB# #TAB# t.addTransition(final, i, i, final) #LINE# #TAB# if preserving: #LINE# #TAB# #TAB# t.addFinal(initial) #LINE# #TAB# return t"
Ensure the dependencies are inline tables <code> def format_toml(pdm_settings): ,"#LINE# #TAB# for section in pdm_settings: #LINE# #TAB# #TAB# if not section.endswith('dependencies'): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for name in pdm_settings[section]: #LINE# #TAB# #TAB# #TAB# if getattr(pdm_settings[section][name], 'items', None): #LINE# #TAB# #TAB# #TAB# #TAB# table = tomlkit.inline_table() #LINE# #TAB# #TAB# #TAB# #TAB# table.update(pdm_settings[section][name]) #LINE# #TAB# #TAB# #TAB# #TAB# pdm_settings[section][name] = table"
"Inspect a log line and groups it by connection being openend or closed . If neither , return False  <code> def opened_closed(logevent): ",#LINE# #TAB# if 'connection accepted' in logevent.line_str: #LINE# #TAB# #TAB# return 'opened' #LINE# #TAB# elif 'end connection' in logevent.line_str: #LINE# #TAB# #TAB# return 'closed' #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
Map numpy types to MPI datatypes  <code> def dtype_to_mpitype(dtype): ,"#LINE# #TAB# return {np.int32: 'MPI_INT', np.float32: 'MPI_FLOAT', np.int64: #LINE# #TAB# #TAB# 'MPI_LONG', np.float64: 'MPI_DOUBLE'}[dtype]"
Return the release date for this printing as a date object  <code> def get_printing_date(oPrinting): ,"#LINE# #TAB# for oProp in oPrinting.properties: #LINE# #TAB# #TAB# if oProp.value.startswith('Release Date:'): #LINE# #TAB# #TAB# #TAB# sDate = oProp.value.split(':', 1)[1].strip() #LINE# #TAB# #TAB# #TAB# oDate = datetime.datetime.strptime(sDate, '%Y-%m-%d').date() #LINE# #TAB# #TAB# #TAB# return oDate #LINE# #TAB# return None"
Returns the grizzly s internal address value . Returns a negative error value in case of error  <code> def get_device_address(usb_device): ,"#LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# usb_device.ctrl_transfer(0x21, 0x09, 0x0300, 0, GrizzlyUSB.COMMAND_GET_ADDR) #LINE# #TAB# #TAB# #TAB# internal_addr = usb_device.ctrl_transfer(0xa1, 0x01, 0x0301, 0, 2)[1] #LINE# #TAB# #TAB# #TAB# return internal_addr >> 1 #LINE# #TAB# #TAB# except usb.USBError as e: #LINE# #TAB# #TAB# #TAB# return GrizzlyUSB.USB_DEVICE_ERROR"
"Return a list of netflix optimized servers <code> def filter_netflix_servers(servers, countries_): ","#LINE# #TAB# servers = servers.copy() #LINE# #TAB# servers = countries.filter_servers(servers, NETFLIX) #LINE# #TAB# return servers"
"Detect if running on the Raspberry Pi or Beaglebone Black and return the platform type . Will return RASPBERRY_PI , BEAGLEBONE_BLACK , or UNKNOWN  <code> def platform_detect(): ",#LINE# #TAB# pi = pi_version() #LINE# #TAB# if pi is not None: #LINE# #TAB# #TAB# return RASPBERRY_PI #LINE# #TAB# plat = platform.platform() #LINE# #TAB# if plat.lower().find('armv7l-with-debian') > -1: #LINE# #TAB# #TAB# return BEAGLEBONE_BLACK #LINE# #TAB# elif plat.lower().find('armv7l-with-ubuntu') > -1: #LINE# #TAB# #TAB# return BEAGLEBONE_BLACK #LINE# #TAB# elif plat.lower().find('armv7l-with-glibc2.4') > -1: #LINE# #TAB# #TAB# return BEAGLEBONE_BLACK #LINE# #TAB# elif plat.lower().find('armv7l-with-arch') > -1: #LINE# #TAB# #TAB# return BEAGLEBONE_BLACK #LINE# #TAB# return UNKNOWN
"Download a single file from GDC  <code> def py_download_file(uuid, file_name, chunk_size=4096): ","#LINE# #TAB# url = GDCQuery.GDC_ROOT #LINE# #TAB# if __legacy: #LINE# #TAB# #TAB# url += 'legacy/' #LINE# #TAB# url += 'data/' + uuid #LINE# #TAB# r = requests.get(url, stream=True) #LINE# #TAB# with open(file_name, 'wb') as f: #LINE# #TAB# #TAB# for chunk in r.iter_content(chunk_size=chunk_size): #LINE# #TAB# #TAB# #TAB# if chunk: #LINE# #TAB# #TAB# #TAB# #TAB# f.write(chunk) #LINE# #TAB# return r"
Load string from txt file Args : filepath : the path specified <code> def load_string_from_file(filepath): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(filepath, 'r') as myfile: #LINE# #TAB# #TAB# #TAB# data = myfile.read() #LINE# #TAB# #TAB# #TAB# return data #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# raise"
Check if repository is local <code> def check_for_local_repos(repo): ,"#LINE# #TAB# repos_dict = Repo().default_repository() #LINE# #TAB# if repo in repos_dict: #LINE# #TAB# #TAB# repo_url = repos_dict[repo] #LINE# #TAB# #TAB# if repo_url.startswith(""file:///""): #LINE# #TAB# #TAB# #TAB# return True"
"Return a list of method ( name , object ) tuples  <code> def find_methods(kls): ","#LINE# #TAB# mthdmembers = inspect.getmembers(kls, predicate=lambda obj: inspect. #LINE# #TAB# #TAB# isfunction(obj) or inspect.ismethod(obj)) #LINE# #TAB# return mthdmembers"
Check input vector items type  <code> def vector_check(vector): ,"#LINE# #TAB# for i in vector: #LINE# #TAB# #TAB# if isinstance(i, int) is False: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if i < 0: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Return a list of : class:`XAttr ` for a scheme definition , with some proper values specified if a scheme is given  <code> def xsd_attributes_scheme(cls, scheme=None): ","#LINE# #TAB# attributes = [XAttr('schemeID', 'xsd:token'), XAttr('schemeName', #LINE# #TAB# #TAB# 'xsd:string', fixed_value=scheme.title if scheme and scheme.title else #LINE# #TAB# #TAB# None), XAttr('schemeAgencyName', 'xsd:string'), XAttr( #LINE# #TAB# #TAB# 'schemeVersionID', 'xsd:token'), XAttr('schemeDataURI', #LINE# #TAB# #TAB# 'xsd:anyURI'), XAttr('schemeURI', 'xsd:anyURI', fixed_value=cls. #LINE# #TAB# #TAB# cwuri_url(scheme) if scheme else None)] #LINE# #TAB# return attributes"
"Pick an embedding size for ` n ` depending on ` classes ` if not given in ` sz_dict `  <code> def one_emb_sz(classes, n, sz_dict=None): ","#LINE# #TAB# sz_dict = ifnone(sz_dict, {}) #LINE# #TAB# n_cat = len(classes[n]) #LINE# #TAB# sz = sz_dict.get(n, int(emb_sz_rule(n_cat))) #LINE# #TAB# return n_cat, sz"
"chekcs if a key exists . Works only for files , and not directory . Cheaper than exists , which employs a list  <code> def file_exists(s3_path): ","#LINE# #TAB# s3 = boto3.resource('s3') #LINE# #TAB# bucket_, key_ = get_both(s3_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# s3.Object(bucket_, key_).load() #LINE# #TAB# except botocore.exceptions.ClientError as e: #LINE# #TAB# #TAB# if e.response['Error']['Code'] == '404': #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Generate a unique file ID for OutboxFile  <code> def make_file_id(): ,#LINE# #TAB# global _LastFileID #LINE# #TAB# newid = int(str(int(time.time() * 100.0))[4:]) #LINE# #TAB# if _LastFileID is None: #LINE# #TAB# #TAB# _LastFileID = newid #LINE# #TAB# elif _LastFileID >= newid: #LINE# #TAB# #TAB# _LastFileID += 1 #LINE# #TAB# else: #LINE# #TAB# #TAB# _LastFileID = newid #LINE# #TAB# return _LastFileID
"Retrieve names of components or predicates from nested data : param value : data to extract name or to retrieve predicates from : return : names of components or predicates as a string <code> def retrieve_data(cls, value): ","#LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return ', '.join([(v['name'] if 'name' in v else cls. #LINE# #TAB# #TAB# #TAB# retrieve_predicates(v)) for v in value]) #LINE# #TAB# return value"
"Set the current ' measure level ' used in deciding how much effort to expend creating FFTW plans . Must be an integer from 0 ( least effort , shortest time ) to 3 ( most effort and time )  <code> def set_measure_level(mlvl): ","#LINE# #TAB# global _default_measurelvl #LINE# #TAB# if mlvl not in (0, 1, 2, 3): #LINE# #TAB# #TAB# raise ValueError('Measure level can only be one of 0, 1, 2, or 3') #LINE# #TAB# _default_measurelvl = mlvl"
Return supported template names  <code> def get_templates(parentPage): ,"#LINE# #TAB# if cfg.get('page', 'factory', 'articlelist', 'enabled', default=True): #LINE# #TAB# #TAB# return {'articlelist': _('Articles list')} #LINE# #TAB# else: #LINE# #TAB# #TAB# return {}"
"Removes the target file name , catching and ignoring errors that indicate that the file does not exist . @param filename : The file to remove . @param disable : boolean to flag if want to disable removal <code> def silent_remove(filename, disable=False): ",#LINE# #TAB# if not disable: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.remove(filename) #LINE# #TAB# #TAB# except OSError as e: #LINE# #TAB# #TAB# #TAB# if e.errno != errno.ENOENT: #LINE# #TAB# #TAB# #TAB# #TAB# raise
"Return a Sky Hub scanner if successful  <code> def get_scanner(hass, config): ",#LINE# #TAB# scanner = SkyHubDeviceScanner(config[DOMAIN]) #LINE# #TAB# return scanner if scanner.success_init else None
"selects up , down or equal and tone group length <code> def get_up_down_equal_characteristics(rndm_local): ","#LINE# #TAB# up_down_equal_probs = rndm_local.rndm_choice(mp.UP_DOWN_EQUAL_PROBS) #LINE# #TAB# if up_down_equal_probs == 0: #LINE# #TAB# #TAB# tone_group_length = int(rndm_local.rndm_gauss_limit(mp. #LINE# #TAB# #TAB# #TAB# TONE_GROUP_LENGTH_EQUAL) + 0.5) #LINE# #TAB# else: #LINE# #TAB# #TAB# tone_group_length = int(rndm_local.rndm_gauss_limit(mp. #LINE# #TAB# #TAB# #TAB# TONE_GROUP_LENGTH_UP_DOWN) + 0.5) #LINE# #TAB# return tone_group_length, up_down_equal_probs"
""" c - a : potassium W(CN)8 [ PM73 ]  <code> def bc_k_wcn8_pm73(T, P): ","#LINE# #TAB# b0 = 1.032 * 5 / 8 #LINE# #TAB# b1 = 18.49 * 5 / 8 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.4937 * 5 / 16 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['K'] * i2c['WCN8']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
Create directory but first delete it if it exists <code> def create_directory(directory): ,#LINE# #TAB# if os.path.exists(directory): #LINE# #TAB# #TAB# rmtree(directory) #LINE# #TAB# os.makedirs(directory) #LINE# #TAB# return directory
"Retrieve and parse a JSON schema  <code> def get_schema(schemas_dir, section_name): ","#LINE# #TAB# path = _path(schemas_dir, section_name) #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# raise web.HTTPError(404, 'Schema not found: %r' % path) #LINE# #TAB# with open(path) as fid: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# schema = json.load(fid) #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# name = section_name #LINE# #TAB# #TAB# #TAB# message = 'Failed parsing schema ({}): {}'.format(name, str(e)) #LINE# #TAB# #TAB# #TAB# raise web.HTTPError(500, message) #LINE# #TAB# return schema"
"Returns a menu to switch users quickly . Settings required : : settings . TEST_USERS = ( ( ' user1 ' , ' pass1 ' ) , ... ) settings . DEBUG = True FOR TESTING PURPOSE ONLY <code> def switch_users(): ","#LINE# #TAB# if not settings.DEBUG: #LINE# #TAB# #TAB# return '' #LINE# #TAB# try: #LINE# #TAB# #TAB# if settings.TEST_USERS: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return '' #LINE# #TAB# content = 'switch user:' #LINE# #TAB# for item in settings.TEST_USERS: #LINE# #TAB# #TAB# u, p = item #LINE# #TAB# #TAB# content += ' [<a href=switch/%s/%s/>%s</a>]' % (u, p, u) #LINE# #TAB# return content"
"Compute a linear regression model , regressing y onto x Input --- x : ndarray independent variable data y : array dependent variable data verbose : bool , optional whether to print results of linear model fit Returns --- fitted statsmodels OLS object <code> def run_linear_model(x, y, verbose=True): ","#LINE# #TAB# X = sm.add_constant(x) #LINE# #TAB# lm = sm.OLS(y, X).fit() #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print(lm.summary()) #LINE# #TAB# #TAB# print('Slope = {:.2f}'.format(lm.params[-1])) #LINE# #TAB# #TAB# print('t({:d}) = {:.2f}'.format(int(lm.df_resid), lm.tvalues[-1])) #LINE# #TAB# #TAB# print('P = {:.10f}'.format(lm.pvalues[-1])) #LINE# #TAB# return lm"
Perform a forward lookup of a hostname  <code> def lookup_forward(name): ,"#LINE# #TAB# ip_addresses = {} #LINE# #TAB# addresses = list(set(str(ip[4][0]) for ip in socket.getaddrinfo(name, #LINE# #TAB# #TAB# None))) #LINE# #TAB# if addresses is None: #LINE# #TAB# #TAB# return ip_addresses #LINE# #TAB# for address in addresses: #LINE# #TAB# #TAB# if type(ipaddress.ip_address(address)) is ipaddress.IPv4Address: #LINE# #TAB# #TAB# #TAB# ip_addresses['ipv4'] = address #LINE# #TAB# #TAB# if type(ipaddress.ip_address(address)) is ipaddress.IPv6Address: #LINE# #TAB# #TAB# #TAB# ip_addresses['ipv6'] = address #LINE# #TAB# return ip_addresses"
"Print class prediction to STDOUT Args : samples(pandas DataFrame index ) : List of samples classes(list ) : list of classes pred(list ) : list of probablities predictions <code> def print_class(samples, classes, pred): ","#LINE# #TAB# print('\t----------------------') #LINE# #TAB# for ix, sample in enumerate(samples): #LINE# #TAB# #TAB# print(f'\t- Sample: {sample}') #LINE# #TAB# #TAB# [print(f'\t\t {i}:{round(j * 100, 2)}%') for i, j in zip(list( #LINE# #TAB# #TAB# #TAB# classes), list(pred[(ix), :]))]"
"Calculate a direction between two positions pos1 is current position pos2 is previous position <code> def calc_direction(pos1, pos2): ","#LINE# #TAB# theta = math.atan2(pos1[0] - pos2[0], pos1[1] - pos2[1]) / math.pi #LINE# #TAB# return theta"
Converts a SciPy sparse matrix in * * Compressed Sparse Column * * format to an iterable of weighted edge triples  <code> def csc_gen_triples(A): ,"#LINE# #TAB# ncols = A.shape[1] #LINE# #TAB# data, indices, indptr = A.data, A.indices, A.indptr #LINE# #TAB# for i in range(ncols): #LINE# #TAB# #TAB# for j in range(indptr[i], indptr[i + 1]): #LINE# #TAB# #TAB# #TAB# yield indices[j], i, data[j]"
"Pass a command list to subprocess.check_output . Returning None if an expected exception is raised : param cmd : The command to execute : return : Stripped string output of the command , or None if error <code> def check_output_and_strip(cmd): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# result = subprocess.check_output(cmd, stderr=subprocess.STDOUT, #LINE# #TAB# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# #TAB# return result.strip() #LINE# #TAB# except (OSError, subprocess.CalledProcessError, TypeError, AttributeError): #LINE# #TAB# #TAB# return None"
Validating if the instance should be logged or is excluded <code> def validate_instance(instance): ,"#LINE# #TAB# excludes = settings.AUTOMATED_LOGGING['exclude']['model'] #LINE# #TAB# for excluded in excludes: #LINE# #TAB# #TAB# if (excluded in [instance._meta.app_label.lower(), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# instance.__class__.__name__.lower()] or #LINE# #TAB# #TAB# #TAB# #TAB# instance.__module__.lower().startswith(excluded)): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Determines if a namestack is an enum namestack <code> def is_enum_namestack(nameStack): ,#LINE# #TAB# if not nameStack: #LINE# #TAB# #TAB# return False #LINE# #TAB# if nameStack[0] == 'enum': #LINE# #TAB# #TAB# return True #LINE# #TAB# if len(nameStack) > 1 and nameStack[0] == 'typedef' and nameStack[1 #LINE# #TAB# #TAB# ] == 'enum': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Generate local path view and total size required : param blobxfer.models.azure . StorageEntity ase : Storage Entity : rtype : tuple : return : ( local path view , allocation size ) <code> def generate_view(ase): ","#LINE# #TAB# slicesize = blobxfer.models.download.Descriptor.compute_allocated_size(ase #LINE# #TAB# #TAB# .size, ase.is_encrypted) #LINE# #TAB# if ase.vectored_io is None: #LINE# #TAB# #TAB# view = LocalPathView(fd_start=0, fd_end=slicesize) #LINE# #TAB# #TAB# total_size = slicesize #LINE# #TAB# else: #LINE# #TAB# #TAB# view = LocalPathView(fd_start=ase.vectored_io.offset_start, fd_end= #LINE# #TAB# #TAB# #TAB# ase.vectored_io.offset_start + slicesize) #LINE# #TAB# #TAB# total_size = ase.vectored_io.total_size #LINE# #TAB# return view, total_size"
"Get intersection of communities solutions Args : grounding ( bytes ) : grounded model optimum ( str ) : optimal score Returns : TermSet : intersection <code> def get_exchanged_metabolites_intersection(grounding, optimum): ","#LINE# #TAB# options = ( #LINE# #TAB# #TAB# '--configuration jumpy --opt-strategy=usc,5 --enum-mode cautious --opt-mode=optN --opt-bound=' #LINE# #TAB# #TAB# + str(optimum)) #LINE# #TAB# solver = Clasp(clasp_options=options) #LINE# #TAB# intersec = solver.run(grounding, collapseTerms=True, collapseAtoms=False) #LINE# #TAB# return intersec[0]"
Add the version of wily to the help heading . : param f : function to decorate : return : decorated function <code> def add_version(f): ,#LINE# #TAB# doc = f.__doc__ #LINE# #TAB# f.__doc__ = 'Version: ' + __version__ + '\n\n' + doc #LINE# #TAB# return f
Method to be called from within a parse action to determine the end location of the parsed tokens  <code> def get_tokens_end_loc(): ,"#LINE# #TAB# import inspect #LINE# #TAB# fstack = inspect.stack() #LINE# #TAB# try: #LINE# #TAB# #TAB# for f in fstack[2:]: #LINE# #TAB# #TAB# #TAB# if f[3] == ""_parseNoCache"": #LINE# #TAB# #TAB# #TAB# #TAB# endloc = f[0].f_locals[""loc""] #LINE# #TAB# #TAB# #TAB# #TAB# return endloc #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ParseFatalException(""incorrect usage of get_tokens_end_loc - may only be called from within a parse action"") #LINE# #TAB# finally: #LINE# #TAB# #TAB# del fstack"
"Normalize some of the string characters , preserving original length : param text : string to normalize : return : normalized string <code> def normalize_smb_preserve_len(text: str) ->str: ","#LINE# #TAB# if not text: #LINE# #TAB# #TAB# return text #LINE# #TAB# resulted = '' #LINE# #TAB# for c in text: #LINE# #TAB# #TAB# if c in TextBeautifier.QUOTES: #LINE# #TAB# #TAB# #TAB# c = '""' #LINE# #TAB# #TAB# resulted += c #LINE# #TAB# return resulted"
Returns sparse matrix with edges as an adjacency matrix Parameters ---------- faces : array of shape [ n_triangles x 3 ] The mesh faces Returns ------- edges : sparse matrix The adjacency matrix <code> def mesh_edges(faces): ,"#LINE# #TAB# npoints = np.max(faces) + 1 #LINE# #TAB# nfaces = len(faces) #LINE# #TAB# a, b, c = faces.T #LINE# #TAB# edges = sparse.coo_matrix((np.ones(nfaces), (a, b)), shape=(npoints, #LINE# #TAB# #TAB# npoints)) #LINE# #TAB# edges = edges + sparse.coo_matrix((np.ones(nfaces), (b, c)), shape=( #LINE# #TAB# #TAB# npoints, npoints)) #LINE# #TAB# edges = edges + sparse.coo_matrix((np.ones(nfaces), (c, a)), shape=( #LINE# #TAB# #TAB# npoints, npoints)) #LINE# #TAB# edges = edges + edges.T #LINE# #TAB# edges = edges.tocoo() #LINE# #TAB# return edges"
"Returns a tuple ( width , height ) <code> def get_dims(filename): ","#LINE# #TAB# hdulist = fits.open(filename) #LINE# #TAB# hdu = hdulist[0] #LINE# #TAB# ret = hdu.header['NAXIS1'], hdu.header['NAXIS2'] #LINE# #TAB# hdulist.close() #LINE# #TAB# return ret"
Get running notebook server data . ( Anticipating server / notebook split ) . Returns ------- data : dict Keys are ports and values are server data  <code> def get_running_servers(): ,"#LINE# #TAB# output = subprocess.run(['jupyter', 'server', 'list', '--json'], #LINE# #TAB# #TAB# capture_output=True, text=True) #LINE# #TAB# if output.returncode != 0: #LINE# #TAB# #TAB# print(output.stderr) #LINE# #TAB# #TAB# return #LINE# #TAB# elif output.stdout is None: #LINE# #TAB# #TAB# return #LINE# #TAB# text = output.stdout.strip() #LINE# #TAB# server_list = text.split('\n') #LINE# #TAB# data = {} #LINE# #TAB# for server in server_list: #LINE# #TAB# #TAB# server = json.loads(server) #LINE# #TAB# #TAB# data[server['port']] = server #LINE# #TAB# return data"
"Returns all entities for the given : class:`Entity ` type  <code> def get_all_entities(client: Client, etype: Type[E]) ->List[E]: ","#LINE# #TAB# return [etype.of(d) for d in client.get(getattr(etype, #LINE# #TAB# #TAB# '_Entity__endpoint'), params={'page_size': '-1'})]"
"This function converts the bil files into tiff files Keyword Arguments : input_bil -- name , name of the bil file output_tiff -- Name of the output tiff file <code> def convert_bil_to_tiff(input_bil, output_tiff): ","#LINE# #TAB# import gdalconst #LINE# #TAB# gdal.GetDriverByName('EHdr').Register() #LINE# #TAB# dest = gdal.Open(input_bil, gdalconst.GA_ReadOnly) #LINE# #TAB# Array = dest.GetRasterBand(1).ReadAsArray() #LINE# #TAB# geo_out = dest.GetGeoTransform() #LINE# #TAB# Save_as_tiff(output_tiff, Array, geo_out, 'WGS84') #LINE# #TAB# return output_tiff"
"Reads a Mechbase file found in file_path , returning a tuple with the header and a list of numpy arrays with the actual measurement values  <code> def dr_to_values(file_path: str) ->(MeasurementHeader, typing.List[np.array]): ","#LINE# #TAB# header, raw_signals = dr_to_raw_values(file_path) #LINE# #TAB# signals = list() #LINE# #TAB# converters = list(map(value_converter, header.channels)) #LINE# #TAB# for index, s in enumerate(raw_signals): #LINE# #TAB# #TAB# signals.append(np.array(list(map(converters[index % len(converters) #LINE# #TAB# #TAB# #TAB# ], raw_signals[index])))) #LINE# #TAB# return header, signals"
"Blocking function to populate data in the heap . This is run in an executor  <code> def fill_buffer(heap_data, i_chan): ","#LINE# #TAB# #TAB# now = datetime.datetime.utcnow() #LINE# #TAB# #TAB# time_full = now.timestamp() #LINE# #TAB# #TAB# time_count = int(time_full) #LINE# #TAB# #TAB# time_fraction = int((time_full - time_count) * (2**32 - 1)) #LINE# #TAB# #TAB# diff = now - (now.replace(hour=0, minute=0, second=0, microsecond=0)) #LINE# #TAB# #TAB# time_data = diff.seconds + 1e-6 * diff.microseconds #LINE# #TAB# #TAB# heap_data['visibility_timestamp_count'] = time_count #LINE# #TAB# #TAB# heap_data['visibility_timestamp_fraction'] = time_fraction #LINE# #TAB# #TAB# heap_data['correlator_output_data']['VIS'][:][:] = \ #LINE# #TAB# #TAB# #TAB# time_data + i_chan * 1j"
As same as * nix command ` touch ` : param path : path string <code> def touch_file(path): ,#LINE# #TAB# with safe_open_for_write(path): #LINE# #TAB# #TAB# pass
Read an extracted line and return the artist and song part : param ligne : Line from the server to parse : return : The title and the artist included in the line in a tuple <code> def parse_line(ligne): ,"#LINE# #TAB# regexp = re.compile('(.*?)\t(.*?)\t(.*?)\t.*') #LINE# #TAB# if regexp.match(ligne): #LINE# #TAB# #TAB# playing_date, title, artist = regexp.findall(ligne)[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# playing_date, title, artist = None, None, None #LINE# #TAB# #TAB# print('The following line cannot be parsed: {0}'.format(ligne[:-1])) #LINE# #TAB# return int(playing_date), title, artist"
"For functions that return a c_char_p array  <code> def chararray_output(func, argtypes, errcheck=True): ",#LINE# #TAB# func.argtypes = argtypes #LINE# #TAB# func.restype = POINTER(c_char_p) #LINE# #TAB# if errcheck: #LINE# #TAB# #TAB# func.errcheck = check_pointer #LINE# #TAB# return func
"number of unit extracted from unit_name Parameters ---------- unit_name : str , unit # # # Returns ------- int <code> def parse_unit_number(unit_name): ",#LINE# #TAB# pattern = 'unit(\\d*)' #LINE# #TAB# parser = re.compile(pattern) #LINE# #TAB# out = int(parser.match(unit_name)[1]) #LINE# #TAB# return out
Find all teams that have performed releases : param platform : Platform to filter on <code> def team_list(platform=None): ,#LINE# #TAB# query = db.session.query(Release.team.distinct()) #LINE# #TAB# if platform: #LINE# #TAB# #TAB# query = query.filter(Release.platforms.any(Platform.name == platform)) #LINE# #TAB# return query
"Returns a label string for the selected axis <code> def subplot_label(axis, N, plot_idx, varlabels): ",#LINE# #TAB# label = '' #LINE# #TAB# if axis == 'x': #LINE# #TAB# #TAB# if plot_idx % N == 0: #LINE# #TAB# #TAB# #TAB# label = varlabels[0] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# label = varlabels[N - plot_idx % N] #LINE# #TAB# elif axis == 'y': #LINE# #TAB# #TAB# label = varlabels[N - 1 - (plot_idx - 1) // N] #LINE# #TAB# return label
Returns a cached copy of the user if it exists or calls auth_get_user otherwise  <code> def get_user(request): ,"#LINE# #TAB# if not hasattr(request, '_cached_user'): #LINE# #TAB# #TAB# request._cached_user = auth_get_user(request) #LINE# #TAB# return request._cached_user"
"compute butt cap of width 2*hw for [ p0,p1 ]  <code> def caps_butt(p0, p1, hw, du=1, start=True): ","#LINE# #TAB# aw, bw = _offset(p0, p1, hw) #LINE# #TAB# if start: #LINE# #TAB# #TAB# x, y = p0 #LINE# #TAB# else: #LINE# #TAB# #TAB# x, y = p1 #LINE# #TAB# return [(x + aw, y + bw), (x - aw, y - bw)]"
Return list of valid context names  <code> def context_names(): ,#LINE# #TAB# from renku.core.models.jsonld import JSONLDMixin #LINE# #TAB# return [cls.__name__ for cls in JSONLDMixin.__type_registry__.values()]
Convert an integer to a roman numeral <code> def to_roman(number): ,"#LINE# #TAB# romans = list(itertools.islice(roman_nums(), 0, number + 1)) #LINE# #TAB# return romans[number - 1]"
"Download filename from url unless it 's already in directory  <code> def maybe_download(directory, filename, url): ","#LINE# #TAB# if not os.path.exists(directory): #LINE# #TAB# #TAB# print('Creating directory %s' % directory) #LINE# #TAB# #TAB# os.mkdir(directory) #LINE# #TAB# filepath = os.path.join(directory, filename) #LINE# #TAB# if not os.path.exists(filepath): #LINE# #TAB# #TAB# print('Downloading %s to %s' % (url, filepath)) #LINE# #TAB# #TAB# filepath, _ = urllib.request.urlretrieve(url, filepath) #LINE# #TAB# #TAB# statinfo = os.stat(filepath) #LINE# #TAB# #TAB# print('Successfully downloaded', filename, statinfo.st_size, 'bytes') #LINE# #TAB# return filepath"
"The value of example e under disjunction d <code> def disjunction_value(e, d): ","#LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if v[0] == '!': #LINE# #TAB# #TAB# #TAB# if e[k] == v[1:]: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif e[k] != v: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Append new ( unique ) value to the list of ' key ' . If ' key ' does not exist , create new list  <code> def append_add(dct, key, val, unique=False): ",#LINE# #TAB# if key in dct: #LINE# #TAB# #TAB# if unique: #LINE# #TAB# #TAB# #TAB# if val in dct[key]: #LINE# #TAB# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dct[key].append(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dct[key].append(val) #LINE# #TAB# else: #LINE# #TAB# #TAB# dct[key] = [val]
"any elements of ret not indexed by idx are set to fill_value  <code> def fill_untouched(idx, ret, fill_value): ","#LINE# #TAB# untouched = np.ones_like(ret, dtype=bool) #LINE# #TAB# untouched[idx] = False #LINE# #TAB# ret[untouched] = fill_value"
"After resolving symlinks , is path lhs either equal to or nested within path rhs ? <code> def path_really_within(lhs: str, rhs: str) ->bool: ",#LINE# #TAB# lhs_cmp = splitall(os.path.realpath(lhs)) #LINE# #TAB# rhs_cmp = splitall(os.path.realpath(rhs)) #LINE# #TAB# return len(lhs_cmp) >= len(rhs_cmp) and lhs_cmp[:len(rhs_cmp)] == rhs_cmp
"Get JOINING TYPE property  <code> def get_joining_type_property(value, is_bytes=False): ","#LINE# #TAB# obj = unidata.ascii_joining_type if is_bytes else unidata.unicode_joining_type #LINE# #TAB# if value.startswith('^'): #LINE# #TAB# #TAB# negated = value[1:] #LINE# #TAB# #TAB# value = '^' + unidata.unicode_alias['joiningtype'].get(negated, negated) #LINE# #TAB# else: #LINE# #TAB# #TAB# value = unidata.unicode_alias['joiningtype'].get(value, value) #LINE# #TAB# return obj[value]"
"Validate that kwargs are at least in one signature of the methods Raises an error if it 's not the case <code> def validate_kwargs(kwargs: dict, t: Optional[TypeEnum]) ->bool: ","#LINE# #TAB# types: List[TypeEnum] = [t] if t is not None else [TypeEnum(t) for t in #LINE# #TAB# #TAB# SUPPORTED_FILE_TYPES] #LINE# #TAB# allowed_kwargs: List[str] = [] #LINE# #TAB# for t in types: #LINE# #TAB# #TAB# allowed_kwargs += get_reader_allowed_params(t) #LINE# #TAB# #TAB# allowed_kwargs += SUPPORTED_FILE_TYPES[t].reader_kwargs #LINE# #TAB# bad_kwargs = set(kwargs) - set(allowed_kwargs) #LINE# #TAB# if bad_kwargs: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f""Unsupported kwargs: {', '.join(map(repr, bad_kwargs))}"") #LINE# #TAB# return True"
Get the index path from the current application  <code> def get_component_index_path() ->str: ,"#LINE# #TAB# config = get_application_config() #LINE# #TAB# _path = config.get('INDEX_NAME', 'idx') #LINE# #TAB# return f'{_path}_components'"
Find the last sql keyword in an SQL statement Returns the value of the last keyword and the text of the query with everything after the last keyword stripped <code> def find_prev_keyword(sql): ,"#LINE# #TAB# if not sql.strip(): #LINE# #TAB# #TAB# return None, '' #LINE# #TAB# parsed = sqlparse.parse(sql)[0] #LINE# #TAB# flattened = list(parsed.flatten()) #LINE# #TAB# logical_operators = ('AND', 'OR', 'NOT', 'BETWEEN') #LINE# #TAB# for t in reversed(flattened): #LINE# #TAB# #TAB# if t.value == '(' or (t.is_keyword and ( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# t.value.upper() not in logical_operators)): #LINE# #TAB# #TAB# #TAB# idx = flattened.index(t) #LINE# #TAB# #TAB# #TAB# text = ''.join(tok.value for tok in flattened[:idx+1]) #LINE# #TAB# #TAB# #TAB# return t, text #LINE# #TAB# return None, ''"
sets the path to the global variable <code> def set_workspace_path(path: str): ,#LINE# #TAB# global workspace_path #LINE# #TAB# workspace_path = path
"Return the projection of the point p on the line defined by a , b and c with $ x + by + c = 0$ <code> def orthogonal_projection_point_to_line(a, b, c, p): ","#LINE# #TAB# p2 = (b * (b * p[0] - a * p[1]) - a * c) / (math.pow(a, 2.0) + math.pow #LINE# #TAB# #TAB# (b, 2.0)), (a * (-b * p[0] + a * p[1]) - b * c) / (math.pow(a, 2.0) + #LINE# #TAB# #TAB# math.pow(b, 2.0)) #LINE# #TAB# return p2"
A low latency check for an internet connection . : param str hostname : Hostname to use for connection check . : returns : Connection availability . : rtype : bool <code> def check_connection(hostname): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# host = socket.gethostbyname(hostname) #LINE# #TAB# #TAB# s = socket.create_connection((host, 80), 2) #LINE# #TAB# #TAB# s.close() #LINE# #TAB# #TAB# return True #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False"
Create and initialise the Overpass API object . Passing the endpoint argument will override the default endpoint URL  <code> def generate_overpass_api(endpoint=None): ,#LINE# #TAB# api = overpass.API() #LINE# #TAB# if not endpoint is None: #LINE# #TAB# #TAB# api.endpoint = endpoint #LINE# #TAB# return api
"Return the line spacing rule value calculated from the combination of * line * and * lineRule * . Returns special members of the : ref:`WdLineSpacing ` enumeration when line spacing is single , double , or 1.5 lines  <code> def line_spacing_rule(line, lineRule): ",#LINE# #TAB# if lineRule == WD_LINE_SPACING.MULTIPLE: #LINE# #TAB# #TAB# if line == Twips(240): #LINE# #TAB# #TAB# #TAB# return WD_LINE_SPACING.SINGLE #LINE# #TAB# #TAB# if line == Twips(360): #LINE# #TAB# #TAB# #TAB# return WD_LINE_SPACING.ONE_POINT_FIVE #LINE# #TAB# #TAB# if line == Twips(480): #LINE# #TAB# #TAB# #TAB# return WD_LINE_SPACING.DOUBLE #LINE# #TAB# return lineRule
Given an input string part of speech tags the string then generates a list of ngrams that appear in the string . Used to define grammatically correct part of speech tag sequences . Returns a list of part of speech tag sequences  <code> def regenerate_good_tokens(string): ,"#LINE# #TAB# toks = nltk.word_tokenize(string) #LINE# #TAB# pos_string = nltk.pos_tag(toks) #LINE# #TAB# pos_seq = [tag[1] for tag in pos_string] #LINE# #TAB# pos_ngrams = ngrams(pos_seq, 2, 4) #LINE# #TAB# sel_pos_ngrams = f7(pos_ngrams) #LINE# #TAB# return sel_pos_ngrams"
Make a deep copy of list and dict objects . Intentionally do not copy attributes . This is to discard CommentedMap and CommentedSeq metadata which is very expensive with regular copy.deepcopy  <code> def deepcopy_strip(item): ,"#LINE# #TAB# if isinstance(item, MutableMapping): #LINE# #TAB# #TAB# return {k: deepcopy_strip(v) for k, v in item.items()} #LINE# #TAB# if isinstance(item, MutableSequence): #LINE# #TAB# #TAB# return [deepcopy_strip(k) for k in item] #LINE# #TAB# return item"
"Remove all pods with these labels in this namespace <code> def cleanup_pods(namespace, labels): ","#LINE# #TAB# api = kubernetes.client.CoreV1Api() #LINE# #TAB# pods = api.list_namespaced_pod(namespace, label_selector=format_labels(labels)) #LINE# #TAB# for pod in pods.items: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# api.delete_namespaced_pod(pod.metadata.name, namespace) #LINE# #TAB# #TAB# #TAB# logger.info('Deleted pod: %s', pod.metadata.name) #LINE# #TAB# #TAB# except kubernetes.client.rest.ApiException as e: #LINE# #TAB# #TAB# #TAB# if e.status != 404: #LINE# #TAB# #TAB# #TAB# #TAB# raise"
"If a timezone is inferred from data check that it is compatible with the user - provided timezone if any  <code> def maybe_infer_tz(tz, inferred_tz): ","#LINE# #TAB# if tz is None: #LINE# #TAB# #TAB# tz = inferred_tz #LINE# #TAB# elif inferred_tz is None: #LINE# #TAB# #TAB# pass #LINE# #TAB# elif not timezones.tz_compare(tz, inferred_tz): #LINE# #TAB# #TAB# raise TypeError('data is already tz-aware {inferred_tz}, unable to ' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'set specified tz: {tz}' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# .format(inferred_tz=inferred_tz, tz=tz)) #LINE# #TAB# return tz"
Takes a query tree and returns the elements in list form . Args : query_tree ( dict ) : A nested dictionary that organizes queries by domain then intent . Returns : list : A list of Query objects  <code> def flatten_query_tree(query_tree): ,"#LINE# #TAB# flattened = [] #LINE# #TAB# for _, intent_queries in query_tree.items(): #LINE# #TAB# #TAB# for _, queries in intent_queries.items(): #LINE# #TAB# #TAB# #TAB# flattened.extend(queries) #LINE# #TAB# return flattened"
"Given two points , gives us the coefficients for a parabola . args : pt1 : The first point . pt2 : The second point  <code> def two_pt_parabola(pt1, pt2): ","#LINE# #TAB# [x1, y1] = pt1 * 1.0 #LINE# #TAB# [x2, y2] = pt2 * 1.0 #LINE# #TAB# a = (y2 - y1) / (x2 - x1) ** 2 #LINE# #TAB# c = y1 + a * x1 * x1 #LINE# #TAB# b = -2 * a * x1 #LINE# #TAB# return [a, b, c]"
Don t look I m hideous! <code> def extract_authenticity_token(data): ,"#LINE# #TAB# if not isinstance(data, str): #LINE# #TAB# #TAB# data = str(data, 'utf-8') #LINE# #TAB# pos = data.find(""authenticity_token"") #LINE# #TAB# authtok = str(data[pos + 41:pos + 41 + 88]) #LINE# #TAB# return authtok"
Create / return ` ` Classification ` ` object from JSON <code> def from_json(json_obj): ,"#LINE# #TAB# cl = Classification() #LINE# #TAB# cl.json = json_obj #LINE# #TAB# cl.primary = json_obj.get('primary') #LINE# #TAB# if 'segment' in json_obj: #LINE# #TAB# #TAB# cl.segment = Segment.from_json(json_obj['segment']) #LINE# #TAB# if 'type' in json_obj: #LINE# #TAB# #TAB# cl_t = json_obj['type'] #LINE# #TAB# #TAB# cl.type = ClassificationType(cl_t['id'], cl_t['name']) #LINE# #TAB# if 'subType' in json_obj: #LINE# #TAB# #TAB# cl_st = json_obj['subType'] #LINE# #TAB# #TAB# cl.subtype = ClassificationSubType(cl_st['id'], cl_st['name']) #LINE# #TAB# _assign_links(cl, json_obj) #LINE# #TAB# return cl"
Convenience function for GetConsoleMode filehandle is a Windows filehandle object returned by msvcrt.get_osfhandle ( ) <code> def get_console_mode(filehandle): ,"#LINE# #TAB# mode = wintypes.DWORD() #LINE# #TAB# KERNEL32.GetConsoleMode(filehandle, ctypes.byref(mode)) #LINE# #TAB# return mode.value"
"Register verb implementations in the module Parameters ---------- module : module Module with the implementations . verb_names : list Names of verbs implemented in the module . datatype : str A name of the datatype implemented . e.g ' dataframe ' , ' dict ' <code> def register_implementations(module, verb_names, datatype): ",#LINE# #TAB# for name in verb_names: #LINE# #TAB# #TAB# REGISTRY[datatype][name] = module[name]
"Return the closest matching playlist name that starts with begin  <code> def get_near_name(begin, items): ",#LINE# #TAB# for name in sorted(items): #LINE# #TAB# #TAB# if name.lower().startswith(begin.lower()): #LINE# #TAB# #TAB# #TAB# return name #LINE# #TAB# return begin
Used to get cache client <code> def get_cache(conf): ,#LINE# #TAB# if conf.cache.enabled: #LINE# #TAB# #TAB# return _get_cache_region(conf) #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Return an available ND given an ID and a schedule  <code> def nd_sdr_available(ID, SDR): ",#LINE# #TAB# for i in range(len(np.array(ID_SDR_all_available(SDR)))): #LINE# #TAB# #TAB# if np.array(ID_SDR_all_available(SDR))[i] >= (ID.to(u.inch)).magnitude: #LINE# #TAB# #TAB# #TAB# return ND_all_available()[i]
"Return HSL color components for given weight where the max absolute weight is given by weight_range  <code> def weight_color_hsl(weight, weight_range, min_lightness=0.8): ","#LINE# #TAB# hue = _hue(weight) #LINE# #TAB# saturation = 1 #LINE# #TAB# rel_weight = (abs(weight) / weight_range) ** 0.7 #LINE# #TAB# lightness = 1.0 - (1 - min_lightness) * rel_weight #LINE# #TAB# return hue, saturation, lightness"
Get the workload from the storagegroup object . : param storage_group : storagegroup -- object : returns : workload -- str <code> def get_workload(storage_group): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# workload = storage_group['workload'] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# workload = 'NONE' #LINE# #TAB# return workload
"Forces compilation cache to be generated for every subresource script . : param enabled : : type enabled : bool <code> def set_produce_compilation_cache(cls, enabled: Union['bool']): ","#LINE# #TAB# return cls.build_send_payload('set_produce_compilation_cache', {'enabled': #LINE# #TAB# #TAB# enabled}), None"
"AB test ( Issue 3712 ) alters the string for media edge , this resoves it <code> def get_media_edge_comment_string(media): ","#LINE# #TAB# options = ['edge_media_to_comment', 'edge_media_preview_comment'] #LINE# #TAB# for option in options: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# media[option] #LINE# #TAB# #TAB# except KeyError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return option"
"Define a vector 's view  <code> def define_view(name, ctype, mixins, size): ","#LINE# #TAB# mixins = (_ViewMixin,) + mixins #LINE# #TAB# definition = _get_definition(mixins, size) #LINE# #TAB# out = type(name, (), definition) #LINE# #TAB# out._class = ctype #LINE# #TAB# return out"
Parses a string for a valid gauge ID Args : s : The string to be parsed Returns : Returns a matching gauge ID or None if not found <code> def parse_for_gauge(s): ,#LINE# #TAB# g = GAUGE_PATTERN.search(s) #LINE# #TAB# if g: #LINE# #TAB# #TAB# return g.group(0) #LINE# #TAB# return None
"Create the SQL database engine . Parameters ---------- path : string , default None Path for the database engine . Returns ------- data_str : Engine Engine from database  <code> def db_engine(path=None): ","#LINE# #TAB# if path is None: #LINE# #TAB# #TAB# home = os.path.expanduser('~') #LINE# #TAB# #TAB# path = os.path.join(home, '.cache', 'pynmet') #LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# f_path = os.path.join(path, 'inmet.db') #LINE# #TAB# engine = create_engine('sqlite:///' + f_path, echo=False) #LINE# #TAB# return engine"
Generate a configuration file with sample data  <code> def init_config(): ,"#LINE# #TAB# config = find_config() #LINE# #TAB# if config: #LINE# #TAB# #TAB# log.info(f'Config file already exists: {config.path}') #LINE# #TAB# #TAB# return config, False #LINE# #TAB# log.info('Generating config file...') #LINE# #TAB# config = Config.new() #LINE# #TAB# config.sourcefiles = [SourceFile('app.json'), SourceFile('.env')] #LINE# #TAB# config.environments = [Environment('localhost'), Environment( #LINE# #TAB# #TAB# 'production', command='heroku run env')] #LINE# #TAB# config.save() #LINE# #TAB# return config, True"
Return a shell - escaped version of the string * s *  <code> def single_quote(s): ,"#LINE# #TAB# if not s: #LINE# #TAB# #TAB# return ""''"" #LINE# #TAB# if find_unsafe(s) is None: #LINE# #TAB# #TAB# return s #LINE# #TAB# return ""'"" + s.replace(""'"", '\'""\'""\'') + ""'"""
Normalize a path obtained from the database  <code> def normalize_path(path): ,#LINE# #TAB# path = PosixPath(path) #LINE# #TAB# if path.path.startswith(path._sep + path._sep): #LINE# #TAB# #TAB# path = PosixPath(path.path[1:]) #LINE# #TAB# return path
"Prompt for an input for temperature and automatically resolve unit ( Celcius or Fahrenheit ) : arg ctx : Click context : arg prompt : User prompt . Correct unit will be appended : return : entered value as float <code> def get_unit_input(unit, prompt): ","#LINE# #TAB# prompt = '{prompt} ({units}): '.format(prompt=prompt, units=unit) #LINE# #TAB# value = get_input(prompt, lambda x: float(x)) #LINE# #TAB# return value"
"Returns deprecated backend names  <code> def deprecated_backend_names() ->Dict[str, str]: ","#LINE# #TAB# return {'ibmqx_qasm_simulator': 'ibmq_qasm_simulator', #LINE# #TAB# #TAB# 'ibmqx_hpc_qasm_simulator': 'ibmq_qasm_simulator', 'real': 'ibmqx1'}"
"Reads a private key PEM block and returns a RSAPrivatekey : param private_key_pem : The private key PEM block : param passphrase : Optional passphrase needed to decrypt the private key : returns : a RSAPrivatekey object <code> def read_private_key(private_key_pem, passphrase=None): ","#LINE# #TAB# if passphrase and type(passphrase) == six.text_type: #LINE# #TAB# #TAB# passphrase = passphrase.encode('utf-8') #LINE# #TAB# if type(private_key_pem) == six.text_type: #LINE# #TAB# #TAB# private_key_pem = private_key_pem.encode('utf-8') #LINE# #TAB# try: #LINE# #TAB# #TAB# return serialization.load_pem_private_key(private_key_pem, #LINE# #TAB# #TAB# #TAB# passphrase, backends.default_backend()) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# LOG.exception('Passphrase required.') #LINE# #TAB# #TAB# raise exceptions.NeedsPassphrase"
Parse a list of option strings like foo = bar baz = qux and return a dictionary . Will raise if keys are repeated  <code> def parse_option_dict(options): ,"#LINE# #TAB# ret = {} #LINE# #TAB# for option_pair in (options or []): #LINE# #TAB# #TAB# key, value = option_pair.split('=', 1) #LINE# #TAB# #TAB# if key in ret: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# ""Multiple values provided for the same key '%s'"" % key) #LINE# #TAB# #TAB# ret[key] = value #LINE# #TAB# return ret"
""" c - a : sodium bromate [ SP78 ]  <code> def bc_na_bro3_sp78(T, P): ","#LINE# #TAB# b0, b1, b2, C0, C1, alph1, alph2, omega, _ = bC_Na_BrO3_PM73(T, P) #LINE# #TAB# b0 = b0 + (T - 298.15) * 0.000559 #LINE# #TAB# b1 = b1 + (T - 298.15) * 0.003437 #LINE# #TAB# valid = logical_and(T >= 273.15, T <= 323.15) #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
"Compute the Euclidean distance between : math:`x ` and : math:`y ` . Expected dimension of x is 2 . Expected dimension of y is 2  <code> def euclidean_distance(x, y): ","#LINE# #TAB# distances_raised = squared_euclidean_distance(x, y) #LINE# #TAB# distances = tf.math.pow(distances_raised, 1.0 / 2.0) #LINE# #TAB# return distances"
"Get credentials or prompt for them from options <code> def get_credentials(options, environment): ","#LINE# #TAB# if options['--username'] or options['--auth']: #LINE# #TAB# #TAB# if not options['--username']: #LINE# #TAB# #TAB# #TAB# options['<username>'] = lib.prompt(""Please enter the username for %s..."" % environment) #LINE# #TAB# #TAB# if not options['--password']: #LINE# #TAB# #TAB# #TAB# options['<password>'] = lib.prompt(""Please enter the password for %s..."" % environment, secret=True) #LINE# #TAB# return options"
"Python 2 compatible version of traceback . format_exception Accepts negative limits like the Python 3 version <code> def format_exception(etype, value, tback, limit=None): ","#LINE# #TAB# rtn = ['Traceback (most recent call last):\n'] #LINE# #TAB# if limit is None or limit >= 0: #LINE# #TAB# #TAB# rtn.extend(traceback.format_tb(tback, limit)) #LINE# #TAB# else: #LINE# #TAB# #TAB# rtn.extend(traceback.format_list(traceback.extract_tb(tback)[limit:])) #LINE# #TAB# rtn.extend(traceback.format_exception_only(etype, value)) #LINE# #TAB# return rtn"
Return the json data <code> def fetch_json_data(url): ,#LINE# #TAB# got = requests.get(url) #LINE# #TAB# if got.status_code != 200: #LINE# #TAB# #TAB# got.raise_for_status() #LINE# #TAB# data = got.json() #LINE# #TAB# return data
"Calculate the distance between home coordinates and geometry  <code> def distance_to_geometry(home_coordinates: Tuple[float, float], geometry: ","#LINE# #TAB# Geometry) ->float: #LINE# #TAB# distance = float('inf') #LINE# #TAB# if isinstance(geometry, Point): #LINE# #TAB# #TAB# distance = GeoRssDistanceHelper._distance_to_point(home_coordinates, #LINE# #TAB# #TAB# #TAB# geometry) #LINE# #TAB# elif isinstance(geometry, Polygon): #LINE# #TAB# #TAB# distance = GeoRssDistanceHelper._distance_to_polygon(home_coordinates, #LINE# #TAB# #TAB# #TAB# geometry) #LINE# #TAB# elif isinstance(geometry, BoundingBox): #LINE# #TAB# #TAB# distance = GeoRssDistanceHelper._distance_to_bounding_box( #LINE# #TAB# #TAB# #TAB# home_coordinates, geometry) #LINE# #TAB# else: #LINE# #TAB# #TAB# _LOGGER.debug('Not implemented: %s', type(geometry)) #LINE# #TAB# return distance"
"Build SLO i d from SLO configuration . Args : slo_config ( dict ) : SLO configuration . full ( bool ) : If True , return full resource i d including project . Returns : str : SLO i d  <code> def build_slo_id(window, slo_config, full=False): ","#LINE# #TAB# if 'slo_id' in slo_config: #LINE# #TAB# #TAB# slo_id_part = slo_config['slo_id'] #LINE# #TAB# #TAB# slo_id = f'{slo_id_part}-{window}' #LINE# #TAB# else: #LINE# #TAB# #TAB# slo_name = slo_config['slo_name'] #LINE# #TAB# #TAB# slo_id = f'{slo_name}-{window}' #LINE# #TAB# if full: #LINE# #TAB# #TAB# service_path = SSM.build_service_id(slo_config, full=True) #LINE# #TAB# #TAB# return f'{service_path}/serviceLevelObjectives/{slo_id}' #LINE# #TAB# return slo_id"
"Get the settings from a given parser , for a given script . : param parser : : param script : : return : <code> def get_settings(parser, script): ",#LINE# #TAB# d = ns_to_d(parser.parse_args()) #LINE# #TAB# if 'profile' not in d and 'CAPO_PROFILE' not in os.environ: #LINE# #TAB# #TAB# sys.stderr.write(_ERR_MISSING_PROFILE.format(script=script)) #LINE# #TAB# #TAB# parser.print_help() #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# d['profile'] = os.environ['CAPO_PROFILE'] if 'profile' not in d else d[ #LINE# #TAB# #TAB# 'profile'] #LINE# #TAB# return d
"Computes the version of packages after installing to_install  <code> def simulate_installation_of(to_install, package_set): ","#LINE# #TAB# installed = set() #LINE# #TAB# for inst_req in to_install: #LINE# #TAB# #TAB# dist = make_abstract_dist(inst_req).dist() #LINE# #TAB# #TAB# name = canonicalize_name(dist.key) #LINE# #TAB# #TAB# package_set[name] = PackageDetails(dist.version, dist.requires()) #LINE# #TAB# #TAB# installed.add(name) #LINE# #TAB# return installed"
Process the min / max occurrence indicators <code> def process_occurs_attrs(node): ,"#LINE# #TAB# max_occurs = node.get('maxOccurs', '1') #LINE# #TAB# min_occurs = int(node.get('minOccurs', '1')) #LINE# #TAB# if max_occurs == 'unbounded': #LINE# #TAB# #TAB# max_occurs = 'unbounded' #LINE# #TAB# else: #LINE# #TAB# #TAB# max_occurs = int(max_occurs) #LINE# #TAB# return min_occurs, max_occurs"
"Returns a dataframe with the mean absolute shap values for each feature  <code> def mean_absolute_shap_values(columns, shap_values, cats=None): ","#LINE# #TAB# feature_dict = get_feature_dict(columns, cats) #LINE# #TAB# shap_abs_mean_dict = {} #LINE# #TAB# for col_name, col_list in feature_dict.items(): #LINE# #TAB# #TAB# shap_abs_mean_dict[col_name] = np.absolute(shap_values[:, ([columns #LINE# #TAB# #TAB# #TAB# .index(col) for col in col_list])].sum(axis=1)).mean() #LINE# #TAB# shap_df = pd.DataFrame({'Feature': list(shap_abs_mean_dict.keys()), #LINE# #TAB# #TAB# 'MEAN_ABS_SHAP': list(shap_abs_mean_dict.values())}).sort_values( #LINE# #TAB# #TAB# 'MEAN_ABS_SHAP', ascending=False).reset_index(drop=True) #LINE# #TAB# return shap_df"
Create public and private ssh - keys  <code> def generate_rsa_key_pair(): ,"#LINE# #TAB# key = rsa.generate_private_key( #LINE# #TAB# #TAB# backend=default_backend(), public_exponent=65537, key_size=2048) #LINE# #TAB# public_key = key.public_key().public_bytes( #LINE# #TAB# #TAB# serialization.Encoding.OpenSSH, #LINE# #TAB# #TAB# serialization.PublicFormat.OpenSSH).decode(""utf-8"") #LINE# #TAB# pem = key.private_bytes( #LINE# #TAB# #TAB# encoding=serialization.Encoding.PEM, #LINE# #TAB# #TAB# format=serialization.PrivateFormat.TraditionalOpenSSL, #LINE# #TAB# #TAB# encryption_algorithm=serialization.NoEncryption()).decode(""utf-8"") #LINE# #TAB# return public_key, pem"
"Remove logic brick connections : param path : path : type path : typing . List['bpy.types . OperatorMousePath ' ] : param cursor : Cursor : type cursor : int <code> def links_cut(path: typing.List['bpy.types.OperatorMousePath']=None, cursor: ",#LINE# #TAB# int=9): #LINE# #TAB# pass
"Normalize a numpy array of 3 component vectors shape=(n,3 ) <code> def normalize_v3(arr): ","#LINE# #TAB# lens = np.sqrt(arr[:, (0)] ** 2 + arr[:, (1)] ** 2 + arr[:, (2)] ** 2) #LINE# #TAB# arr[:, (0)] /= lens #LINE# #TAB# arr[:, (1)] /= lens #LINE# #TAB# arr[:, (2)] /= lens #LINE# #TAB# return arr"
Verify that a tar.xz file is valid and working . : param filepath : Filename . : type filepath : str <code> def txz_verify(filepath): ,"#LINE# #TAB# if not utilities.new_enough(3): #LINE# #TAB# #TAB# sentinel = None #LINE# #TAB# else: #LINE# #TAB# #TAB# sentinel = generic_tarfile_verify(filepath, 'r:xz') #LINE# #TAB# return sentinel"
"Return parent node of leaf  <code> def get_parent(leaf, tree): ","#LINE# #TAB# if tree[1] == (): #LINE# #TAB# #TAB# return None #LINE# #TAB# elif tree[1][0] == leaf or tree[2][0] == leaf: #LINE# #TAB# #TAB# return tree[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# left = get_parent(leaf, tree[1]) #LINE# #TAB# #TAB# right = get_parent(leaf, tree[2]) #LINE# #TAB# #TAB# if left != None: #LINE# #TAB# #TAB# #TAB# return left #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return right"
Returns dictionary of some basic features that can be calculated for segmented time series data <code> def base_features(): ,"#LINE# #TAB# features = {'mean': mean, 'median': median, 'abs_energy': abs_energy, #LINE# #TAB# #TAB# 'std': std, 'var': var, 'min': minimum, 'max': maximum, 'skew': #LINE# #TAB# #TAB# skew, 'kurt': kurt, 'mse': mse, 'mnx': mean_crossings} #LINE# #TAB# return features"
Utility that detects and returns the columns that are forward returns <code> def get_forward_returns_columns(columns): ,"#LINE# #TAB# pattern = re.compile('^(\\d+([Dhms]|ms|us|ns))+$', re.IGNORECASE) #LINE# #TAB# valid_columns = [(pattern.match(col) is not None) for col in columns] #LINE# #TAB# return columns[valid_columns]"
"Check if the Class supports the operation . : param path : Path of the collection or non - collection class . : param method : Method name . : return : True if the method is defined , false otherwise  <code> def check_class_op(path: str, method: str) ->bool: ",#LINE# #TAB# for supportedOp in get_doc().parsed_classes[path]['class' #LINE# #TAB# #TAB# ].supportedOperation: #LINE# #TAB# #TAB# if supportedOp.method == method: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
dynamically build autocomplete options based on an external file <code> def get_default_preds(): ,"#LINE# #TAB# g = ontospy.Ontospy(rdfsschema, text=True, verbose=False, hide_base_schemas=False) #LINE# #TAB# classes = [(x.qname, x.bestDescription()) for x in g.all_classes] #LINE# #TAB# properties = [(x.qname, x.bestDescription()) for x in g.all_properties] #LINE# #TAB# commands = [('exit', 'exits the terminal'), ('show', 'show current buffer')] #LINE# #TAB# return rdfschema + owlschema + classes + properties + commands"
"globs_to_include is a list of file name globs If the number of found files does not match num_files then no files will be included  <code> def find_all_or_none(globs_to_include, num_files, qt_library_info): ","#LINE# #TAB# to_include = [] #LINE# #TAB# dst_dll_path = '.' #LINE# #TAB# for dll in globs_to_include: #LINE# #TAB# #TAB# dll_path = os.path.join(qt_library_info.location['BinariesPath' if #LINE# #TAB# #TAB# #TAB# qt_library_info.is_PyQt5 else 'PrefixPath'], dll) #LINE# #TAB# #TAB# dll_file_paths = glob.glob(dll_path) #LINE# #TAB# #TAB# for dll_file_path in dll_file_paths: #LINE# #TAB# #TAB# #TAB# to_include.append((dll_file_path, dst_dll_path)) #LINE# #TAB# if len(to_include) == num_files: #LINE# #TAB# #TAB# return to_include #LINE# #TAB# return []"
Return user name stored in JSON body . : raises InvalidTokenError in case user name does not exists  <code> def user_name(json_body: dict) ->str: ,"#LINE# #TAB# upn = get(json_body, 'upn', description='upn (i.e. User ID)') #LINE# #TAB# return upn.split('@')[0]"
"Waits for nb_name to update waiting up to TIMEOUT seconds . Returns True if a save was detected and False otherwise  <code> def wait_for_save(nb_name, timeout=5): ",#LINE# #TAB# modification_time = os.path.getmtime(nb_name) #LINE# #TAB# start_time = time.time() #LINE# #TAB# while time.time() < start_time + timeout: #LINE# #TAB# #TAB# if ( #LINE# #TAB# #TAB# #TAB# os.path.getmtime(nb_name) > modification_time #LINE# #TAB# #TAB# #TAB# and os.path.getsize(nb_name) > 0 #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# time.sleep(0.2) #LINE# #TAB# return False
"solves lap_sparse X_i = B_i for each phase i , using the conjugate gradient method . For each pixel , the label i corresponding to the maximal X_i is returned  <code> def solve_cg(lap_sparse, B, tol, return_full_prob=False): ","#LINE# #TAB# lap_sparse = lap_sparse.tocsc() #LINE# #TAB# X = [] #LINE# #TAB# for i in range(len(B)): #LINE# #TAB# #TAB# x0 = cg(lap_sparse, -B[i].toarray(), tol=tol)[0] #LINE# #TAB# #TAB# X.append(x0) #LINE# #TAB# if not return_full_prob: #LINE# #TAB# #TAB# X = np.array(X) #LINE# #TAB# #TAB# X = np.argmax(X, axis=0) #LINE# #TAB# return X"
"Tries to convert value into boolean  <code> def try_value_to_bool(value, strict_mode=True): ","#LINE# #TAB# if strict_mode: #LINE# #TAB# #TAB# true_list = ('True',) #LINE# #TAB# #TAB# false_list = ('False',) #LINE# #TAB# #TAB# val = value #LINE# #TAB# else: #LINE# #TAB# #TAB# true_list = ('true', 'on', 'yes') #LINE# #TAB# #TAB# false_list = ('false', 'off', 'no') #LINE# #TAB# #TAB# val = str(value).lower() #LINE# #TAB# if val in true_list: #LINE# #TAB# #TAB# return True #LINE# #TAB# elif val in false_list: #LINE# #TAB# #TAB# return False #LINE# #TAB# return value"
Checks whether Link action is disabled  <code> def is_disabled_action(view): ,"#LINE# #TAB# if not isinstance(view, core_views.ActionsViewSet): #LINE# #TAB# #TAB# return False #LINE# #TAB# action = getattr(view, 'action', None) #LINE# #TAB# return action in view.disabled_actions if action is not None else False"
Returns unassigned port ranges according to IANA  <code> def iana_unassigned_port_ranges(): ,#LINE# #TAB# page = urllib2.urlopen(IANA_DOWNLOAD_URL).read() #LINE# #TAB# xml = ElementTree.fromstring(page) #LINE# #TAB# records = xml.findall('{%s}record' % IANA_NS) #LINE# #TAB# for record in records: #LINE# #TAB# #TAB# description = record.find('{%s}description' % IANA_NS).text #LINE# #TAB# #TAB# if description == 'Unassigned': #LINE# #TAB# #TAB# #TAB# numbers = record.find('{%s}number' % IANA_NS).text #LINE# #TAB# #TAB# #TAB# yield numbers
"Converts the RGB colors we get from ImageGrab into HSV for the NanoLeaf ` colorsys.py ` expects and provides values in range [ 0 , 1 ] , but we get and need them in a different range <code> def rgb_to_hsv(rgb_color: Color) ->Color: ","#LINE# #TAB# r, g, b = map(lambda v: v / 255.0, rgb_color) #LINE# #TAB# h, s, v = rgb_to_hsv(r, g, b) #LINE# #TAB# h, s, v = int(h * 360), int(s * 100), int(v * 100) #LINE# #TAB# return h, s, v"
"Return HTML report file dataset type  <code> def html_dataset_type(is_binary, is_imbalanced): ","#LINE# #TAB# result = ""<h2>Dataset Type : </h2>\n"" #LINE# #TAB# balance_type = ""Balanced"" #LINE# #TAB# class_type = ""Binary Classification"" #LINE# #TAB# if is_imbalanced: #LINE# #TAB# #TAB# balance_type = ""Imbalanced"" #LINE# #TAB# if not is_binary: #LINE# #TAB# #TAB# class_type = ""Multi-Class Classification"" #LINE# #TAB# result += ""<ul>\n\n<li>{0}</li>\n\n<li>{1}</li>\n</ul>\n"".format( #LINE# #TAB# #TAB# class_type, balance_type) #LINE# #TAB# result += ""<p>{0}</p>\n"".format(RECOMMEND_HTML_MESSAGE) #LINE# #TAB# result += ""<p>{0}</p>\n"".format(RECOMMEND_HTML_MESSAGE2) #LINE# #TAB# return result"
"Get a reference to a file and make sure it exists Returns : path : The path to the file , if it exists Raises : ValueError : If the provided file does not exist <code> def get_file(message, default=None): ","#LINE# #TAB# if default: #LINE# #TAB# #TAB# path = input('{} ({}): '.format(message, default)) #LINE# #TAB# else: #LINE# #TAB# #TAB# path = input('{}: '.format(message)) #LINE# #TAB# return path"
redis slave has a slaveof statement referencing master <code> def configure_redis_slave(): ,"#LINE# #TAB# role_conf = 'slaveof {other} 6379'.format(other=OTHER_SERVER) #LINE# #TAB# backup_path = backup_conf(REDIS_CONFIG_PATH) #LINE# #TAB# backup_restored = False #LINE# #TAB# try: #LINE# #TAB# #TAB# config_replace(REDIS_CONFIG_PATH, 'redis', role_conf) #LINE# #TAB# #TAB# success = True #LINE# #TAB# except: #LINE# #TAB# #TAB# success = False #LINE# #TAB# #TAB# backup_restored = restore_conf(backup_path, REDIS_CONFIG_PATH) #LINE# #TAB# else: #LINE# #TAB# #TAB# release_backup(backup_path) #LINE# #TAB# return success, backup_path, backup_restored"
Calculate the average gradient for each shared variable across all towers  <code> def avg_grads(tower_grads): ,"#LINE# average_grads = [] #LINE# for grad_and_vars in zip(*tower_grads): #LINE# #TAB# grads = [] #LINE# #TAB# for g, _ in grad_and_vars: #LINE# #TAB# expanded_g = tf.expand_dims(g, 0) #LINE# #TAB# grads.append(expanded_g) #LINE# #TAB# grad = tf.concat(0, grads) #LINE# #TAB# grad = tf.reduce_mean(grad, 0) #LINE# #TAB# v = grad_and_vars[0][1] #LINE# #TAB# grad_and_var = (grad, v) #LINE# #TAB# average_grads.append(grad_and_var) #LINE# return average_grads"
"Return a message header fragment dict  <code> def create_header(cls, request_id=None): ","#LINE# #TAB# #TAB# header = { #LINE# #TAB# #TAB# #TAB# 'msgid' : bkserial.make_id(), #LINE# #TAB# #TAB# #TAB# 'msgtype' : cls.msgtype #LINE# #TAB# #TAB# } #LINE# #TAB# #TAB# if request_id is not None: #LINE# #TAB# #TAB# #TAB# header['reqid'] = request_id #LINE# #TAB# #TAB# return header"
"Encodes a numpy array into a PNG string . Args : image : a numpy array with shape [ height , width , 3 ] . Returns : PNG encoded image string  <code> def encode_image_array_as_png_str(image): ","#LINE# #TAB# image_pil = Image.fromarray(numpy.uint8(image)) #LINE# #TAB# output = six.BytesIO() #LINE# #TAB# image_pil.save(output, format='PNG') #LINE# #TAB# png_string = output.getvalue() #LINE# #TAB# output.close() #LINE# #TAB# return png_string"
Split a Path type containing a single NaN separated path into multiple subpaths  <code> def split_path(path): ,"#LINE# #TAB# path = path.split()[0] #LINE# #TAB# values = path.dimension_values(0) #LINE# #TAB# splits = np.concatenate([[0], np.where(np.isnan(values))[0] + 1, [0]]) #LINE# #TAB# subpaths = [] #LINE# #TAB# data = PandasInterface.as_dframe(path) if pd else path.array() #LINE# #TAB# for i in range(len(splits) - 1): #LINE# #TAB# #TAB# slc = slice(splits[i], splits[i + 1] - 1) #LINE# #TAB# #TAB# subpath = data.iloc[slc] if pd else data[slc] #LINE# #TAB# #TAB# if len(subpath): #LINE# #TAB# #TAB# #TAB# subpaths.append(subpath) #LINE# #TAB# return subpaths"
"Create a new container in LIMS  <code> def make_container(lims_api, container_name): ","#LINE# #TAB# if container_name.startswith('tube_'): #LINE# #TAB# #TAB# container_type = Containertype(lims=lims_api, id='2') #LINE# #TAB# #TAB# container_name = container_name.replace('tube_', '') #LINE# #TAB# else: #LINE# #TAB# #TAB# container_type = Containertype(lims=lims_api, id='1') #LINE# #TAB# lims_container = Container.create(lims=lims_api, name=container_name, #LINE# #TAB# #TAB# type=container_type) #LINE# #TAB# log.info('added new LIMS container: %s', lims_container.id) #LINE# #TAB# return lims_container"
"Parse API data into the Python types our binding expects <code> def parse_row(row, binding, srid): ","#LINE# #TAB# parsers = {DateTime: parse_datetime, Geometry: parse_geom, Text: parse_str} #LINE# #TAB# parsed = {} #LINE# #TAB# binding_columns = binding.__mapper__.columns #LINE# #TAB# for col_name, col_val in row.items(): #LINE# #TAB# #TAB# col_name = col_name.lower() #LINE# #TAB# #TAB# if col_name not in binding_columns: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# mapper_col_type = type(binding_columns[col_name].type) #LINE# #TAB# #TAB# if mapper_col_type in parsers: #LINE# #TAB# #TAB# #TAB# parsed[col_name] = parsers[mapper_col_type](col_val, srid) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# parsed[col_name] = col_val #LINE# #TAB# return parsed"
Create a window with a figure widget  <code> def new_figure(): ,"#LINE# #TAB# size = visvis.settings.figureSize #LINE# #TAB# figure = Figure(size[0], size[1], 'Figure') #LINE# #TAB# figure._widget.size_range(100, 100, 0, 0, 0, 0) #LINE# #TAB# figure._widget.show() #LINE# #TAB# figure.DrawNow() #LINE# #TAB# figure._widget.draw() #LINE# #TAB# return figure"
"Pre - compute some values  <code> def agg_sum(df, features): ","#LINE# #TAB# sum_partial = [np.nansum(df[features].values, axis=0), len(df)] #LINE# #TAB# return sum_partial"
Returns the hex color for any valid css color name > > > lookup_color('aliceblue ' ) ' F0F8FF ' <code> def lookup_color(color): ,#LINE# #TAB# if color is None: #LINE# #TAB# #TAB# return #LINE# #TAB# color = color.lower() #LINE# #TAB# if color in COLOR_MAP: #LINE# #TAB# #TAB# return COLOR_MAP[color] #LINE# #TAB# return color
Check internal deps for dep_list <code> def exist_deps(deps_list): ,#LINE# #TAB# deps_set = set(deps_list) #LINE# #TAB# active_components = set(server_packages) | set(server_modules) #LINE# #TAB# if len(active_components.intersection(deps_set)) != len(deps_set): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"Generates all valid Partitions that differ from the given partition by one flip . These are the given partition 's neighbors in the metagraph of partitions  <code> def all_valid_states_one_flip_away(partition, constraints): ",#LINE# #TAB# if callable(constraints): #LINE# #TAB# #TAB# is_valid = constraints #LINE# #TAB# else: #LINE# #TAB# #TAB# is_valid = Validator(constraints) #LINE# #TAB# for flip in all_cut_edge_flips(partition): #LINE# #TAB# #TAB# next_state = partition.flip(flip) #LINE# #TAB# #TAB# if is_valid(next_state): #LINE# #TAB# #TAB# #TAB# yield next_state
"Ask a question to the user , retry until the reply is valid <code> def ask_bool(prompt): ",#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return strtobool(input('%s (yes/no) ' % prompt).strip().casefold()) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# continue
"Generate links for loan  <code> def loan_links_factory(pid, record=None): ","#LINE# #TAB# links = {} #LINE# #TAB# record = record or Loan.get_record_by_pid(pid.pid_value) #LINE# #TAB# actions = {} #LINE# #TAB# transitions_config = current_app.config.get('CIRCULATION_LOAN_TRANSITIONS', #LINE# #TAB# #TAB# {}) #LINE# #TAB# for transition in transitions_config.get(record['state']): #LINE# #TAB# #TAB# action = transition.get('trigger', 'next') #LINE# #TAB# #TAB# actions[action] = build_url_action_for_pid(pid, action) #LINE# #TAB# links.setdefault('actions', actions) #LINE# #TAB# return links"
"Split and strip and discard empties . Turns the following : A , B , into [ "" A "" , "" B "" ] <code> def parse_multi_options(options, split_token=','): ",#LINE# #TAB# if options: #LINE# #TAB# #TAB# return [o.strip() for o in options.split(split_token) if o.strip()] #LINE# #TAB# else: #LINE# #TAB# #TAB# return options
Get a set of experiment IDs from the NDA GUID API . Args : data_structure_row : a dictionary returned by the NDA GUID data API . Returns : a set of experiment IDs as integers  <code> def get_experiment_ids_from_links(data_structure_row: dict) ->set: ,#LINE# #TAB# experiment_ids = set() #LINE# #TAB# for link_row in data_structure_row['links']['link']: #LINE# #TAB# #TAB# if link_row['rel'].lower() == 'experiment_id': #LINE# #TAB# #TAB# #TAB# experiment_ids.add(int(link_row['href'].split('=')[1])) #LINE# #TAB# if len(experiment_ids) > 1: #LINE# #TAB# #TAB# logger.warn(f'Found different collection ids: {experiment_ids}') #LINE# #TAB# return experiment_ids
"Compile the LLVM IR string with the given engine . The compiled module object is returned  <code> def compile_ir(engine, llvm_ir): ",#LINE# #TAB# mod = llvm.parse_assembly(llvm_ir) #LINE# #TAB# mod.verify() #LINE# #TAB# engine.add_module(mod) #LINE# #TAB# engine.finalize_object() #LINE# #TAB# return mod
"Describe whether the policy represented by this class must be included  <code> def policy_is_required(cls, pcluster_config): ",#LINE# #TAB# return pcluster_config.get_section('cluster').get_param_value('scheduler' #LINE# #TAB# #TAB# ) == 'awsbatch'
Test if a file exists at path on a host accessible with SSH  <code> def exists_remote(host_with_path): ,"#LINE# #TAB# host, path = host_with_path.split(':', 1) #LINE# #TAB# return subprocess.call(['ssh', host, 'test -f {}'.format(pipes.quote( #LINE# #TAB# #TAB# path))]) == 0"
"Uploads a . jpg image with the given filepath via the AdWords MediaService  <code> def upload_image(client, filepath): ","#LINE# media_service = client.GetService('MediaService', 'v201809') #LINE# with open(filepath, 'rb') as image_handle: #LINE# #TAB# image_data = image_handle.read().decode('utf-8') #LINE# image = [{ #LINE# #TAB# 'xsi_type': 'Image', #LINE# #TAB# 'data': image_data, #LINE# #TAB# 'type': 'IMAGE' #LINE# }] #LINE# image = media_service.upload(image)[0] #LINE# return image"
"Convert resource 's path and name to storage 's table name . Args : path ( str ) : resource path name ( str ) : resource name Returns : str : table name <code> def convert_path(path, name): ","#LINE# #TAB# table = os.path.splitext(path)[0] #LINE# #TAB# table = table.replace(os.path.sep, '__') #LINE# #TAB# if name is not None: #LINE# #TAB# #TAB# table = '___'.join([table, name]) #LINE# #TAB# table = re.sub('[^0-9a-zA-Z_]+', '_', table) #LINE# #TAB# table = table.lower() #LINE# #TAB# return table"
"Accepts a hex text string and translates it into a binary text string . Returns the binary text string  <code> def hex2_bin_str(hexstr, endian='little'): ",#LINE# #TAB# binstr = '' #LINE# #TAB# dir = 1 #LINE# #TAB# for ch in hexstr[::dir]: #LINE# #TAB# #TAB# binstr += HEXBIN_TRANS[ch] #LINE# #TAB# return binstr
"checking Coo - Coo hydrogen - bond exception <code> def check_coo_coo_exception(group1, group2, version): ","#LINE# #TAB# exception = True #LINE# #TAB# [closest_atom1, distance, closest_atom2] = get_smallest_distance(group1 #LINE# #TAB# #TAB# .get_interaction_atoms(group2), group2.get_interaction_atoms(group1)) #LINE# #TAB# [dpka_max, cutoff] = version.get_hydrogen_bond_parameters(closest_atom1, #LINE# #TAB# #TAB# closest_atom2) #LINE# #TAB# f_angle = 1.0 #LINE# #TAB# value = HydrogenBondEnergy(distance, dpka_max, cutoff, f_angle) #LINE# #TAB# weight = calculatePairWeight(version.parameters, group1.Nmass, group2.Nmass #LINE# #TAB# #TAB# ) #LINE# #TAB# value = value * (1.0 + weight) #LINE# #TAB# return exception, value"
Returns a bool whether or not this node is a bhyve hypervisor <code> def is_bhyve_hyper(): ,"#LINE# #TAB# sysctl_cmd = 'sysctl hw.vmm.create' #LINE# #TAB# vmm_enabled = False #LINE# #TAB# try: #LINE# #TAB# #TAB# stdout = subprocess.Popen(sysctl_cmd, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# shell=True, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=subprocess.PIPE).communicate()[0] #LINE# #TAB# #TAB# vmm_enabled = len(salt.utils.stringutils.to_str(stdout).split('""')[1]) != 0 #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return vmm_enabled"
Returns the next state needed for reach the state Operation Enabled : param string target : Target state : return string : Next target to chagne <code> def next_state_for_enabling(_from): ,"#LINE# #TAB# for cond, next_state in State402.NEXTSTATE2ENABLE.items(): #LINE# #TAB# #TAB# if _from in cond: #LINE# #TAB# #TAB# #TAB# return next_state"
"Generate documentation for single parameter of function : param param : dict contains info about parameter : param sub : prefix string for recursive purposes <code> def create_param_doc(cls, param, prefix=None): ","#LINE# #TAB# desc = cls.exclude_html_reg.sub('', param['description']).strip() #LINE# #TAB# if not desc: #LINE# #TAB# #TAB# desc = '<no description>' #LINE# #TAB# name = param['name'] #LINE# #TAB# if prefix: #LINE# #TAB# #TAB# name = '%s[%s]' % (prefix, name) #LINE# #TAB# doc_ = ':param %s: %s; %s' % (name, desc, param['validator']) #LINE# #TAB# if param['required']: #LINE# #TAB# #TAB# doc_ += ' (REQUIRED)' #LINE# #TAB# else: #LINE# #TAB# #TAB# doc_ += ' (OPTIONAL)' #LINE# #TAB# for param in param.get('params', []): #LINE# #TAB# #TAB# doc_ += '\n' + cls.create_param_doc(param, name) #LINE# #TAB# return doc_"
"Find common attributes for SQL join condition : param sql_obj1 : : param sql_obj2 : : return : <code> def find_common_cond_attrs(sql_obj1, sql_obj2): ",#LINE# #TAB# commonpr_list = [] #LINE# #TAB# for project1 in sql_obj1.pr_list: #LINE# #TAB# #TAB# for project2 in sql_obj2.pr_list: #LINE# #TAB# #TAB# #TAB# if project1.alias == project2.alias: #LINE# #TAB# #TAB# #TAB# #TAB# commonpr_list.append(project1) #LINE# #TAB# return commonpr_list
"take the similarity index from a two list of synonyms <code> def similarity_synonyms(list1, list2): ","#LINE# #TAB# current_index = 0.0 #LINE# #TAB# for i in range(len(list1)): #LINE# #TAB# #TAB# index = max(list(map(lambda a: similarity(a, list1[i]), list2))) #LINE# #TAB# #TAB# if index > current_index: #LINE# #TAB# #TAB# #TAB# current_index = index #LINE# #TAB# return current_index"
"Return schedule from number , timedelta , or actual schedule  <code> def maybe_schedule(s, relative=False, app=None): ","#LINE# #TAB# if s is not None: #LINE# #TAB# #TAB# if isinstance(s, numbers.Number): #LINE# #TAB# #TAB# #TAB# s = timedelta(seconds=s) #LINE# #TAB# #TAB# if isinstance(s, timedelta): #LINE# #TAB# #TAB# #TAB# return schedule(s, relative, app=app) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# s.app = app #LINE# #TAB# return s"
EXTENDED MEASUREMENT REPORT Section 9 . 1 . 52 <code> def extended_measurement_report(): ,#LINE# #TAB# a = TpPd(pd=0x6) #LINE# #TAB# b = MessageType(mesType=0x36) #LINE# #TAB# c = ExtendedMeasurementResults() #LINE# #TAB# packet = a / b / c #LINE# #TAB# return packet
Sets a Cache - Control header  <code> def cache_control(value): ,"#LINE# #TAB# response = view_get() #LINE# #TAB# response.headers['Cache-Control'] = 'public, max-age={0}'.format(value) #LINE# #TAB# return response"
"Returns all trace of error as list , which consists with tuples ( error line location , an error ) . : param exception _ : is the exception that whose trace is needed . : return : the list of tuples  <code> def get_trace(exception_: Exception) ->List[Tuple]: ","#LINE# #TAB# result = [] #LINE# #TAB# for tb in (e for e in traceback.extract_tb(exception_.__traceback__)): #LINE# #TAB# #TAB# first = f'File ""{tb.filename}"", line {tb.lineno}, in {tb.name}' #LINE# #TAB# #TAB# second = f'-->#TAB# {tb.line}' #LINE# #TAB# #TAB# result.append((first, second)) #LINE# #TAB# return result"
"Find the path to a dynamic library in a subfolder . Returns the full filename  <code> def find_library(name, libname): ","#LINE# #TAB# paths = [os.path.join(usrdir, 'lib'), os.path.join(sysdir, 'lib')] #LINE# #TAB# for path in paths: #LINE# #TAB# #TAB# fname = os.path.join(path, libname) #LINE# #TAB# #TAB# if os.path.isfile(fname): #LINE# #TAB# #TAB# #TAB# return fname #LINE# #TAB# return None"
"Converts "" True "" to True None to False . Case - insensitive  <code> def string_to_bool(_value): ",#LINE# #TAB# if _value is not None and _value.lower() == 'true': #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
This method returns the floor on which the apartment is located . : param html_parser : a BeautifulSoup object : rtype : string : return : The floor number <code> def get_offer_floor(html_parser): ,"#LINE# #TAB# floor_number_raw_data = html_parser.find(class_='param_floor_no') #LINE# #TAB# floor = '' #LINE# #TAB# if hasattr(floor_number_raw_data, 'strong'): #LINE# #TAB# #TAB# floor = floor_number_raw_data.strong.text #LINE# #TAB# return '0' if floor == 'parter' else floor"
fetch the page parameter from request query <code> def get_page(page): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# page = int(page) #LINE# #TAB# #TAB# return page if page > 0 else -page #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 1
Returns true if the first word in status is ' select '  <code> def is_select(status): ,"#LINE# #TAB# if not status: #LINE# #TAB# #TAB# return False #LINE# #TAB# return status.split(None, 1)[0].lower() == 'select'"
"Apply appropriate multiplication factors to parameters . Parameters ---------- params : dict A dictionary of parameters and their values for running the software . See "" Use "" documentation for further details  <code> def apply_factors(params): ",#LINE# #TAB# params['Rs'] = params['Rs'] * R_sun #LINE# #TAB# params['Mp'] = params['Mp'] * M_jup #LINE# #TAB# params['Rp'] = params['Rp'] * R_jup #LINE# #TAB# return params
update_table_statement : UPDATE identifier SET col_expr_list opt_WHERE opt_USE_LOCK <code> def p_update_table_statement(p): ,#LINE# #TAB# tree = SQLTree() #LINE# #TAB# tree.query_type = 'UPDATE' #LINE# #TAB# tree.table = p[2] #LINE# #TAB# tree.expressions = p[4] #LINE# #TAB# tree.where = p[5] #LINE# #TAB# tree.lock = p[6] #LINE# #TAB# p[0] = tree
If there is a pending timeout remove it from the IOLoop and set the _timeout global to None  <code> def maybe_stop_timeout(): ,"#LINE# #TAB# global _timeout #LINE# #TAB# if _timeout is not None: #LINE# #TAB# #TAB# LOGGER.debug('Removing the pending timeout (%r)', _timeout) #LINE# #TAB# #TAB# ioloop.IOLoop.current().remove_timeout(_timeout) #LINE# #TAB# #TAB# _timeout = None"
Get list of known indexable filenames from pygment lexer internals <code> def get_index_filenames(): ,"#LINE# #TAB# filenames = [] #LINE# #TAB# for lx, t in sorted(lexers.LEXERS.items()): #LINE# #TAB# #TAB# for f in t[-2]: #LINE# #TAB# #TAB# #TAB# if '*' not in f and '[' not in f: #LINE# #TAB# #TAB# #TAB# #TAB# filenames.append(f) #LINE# #TAB# return filenames"
"Retain the top N largest components <code> def retain_top_n(vals, num_keep): ","#LINE# #TAB# if num_keep < 1: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""Must select num_keep >= 1 when using 'top_n' retention criterion. Currently, num_keep = {}"" #LINE# #TAB# #TAB# #TAB# .format(num_keep)) #LINE# #TAB# absmag_order = np.argsort(-np.abs(vals)) #LINE# #TAB# retain_idx = absmag_order[:num_keep] #LINE# #TAB# return retain_idx"
"Gets the value of the specified key in the config file  <code> def get_value(repo_directory, key, expect_type=None): ","#LINE# #TAB# config = read_config(repo_directory) #LINE# #TAB# value = config.get(key) #LINE# #TAB# if expect_type and value is not None and not isinstance(value, expect_type): #LINE# #TAB# #TAB# raise ConfigSchemaError('Expected config variable %s to be type %s, got %s' #LINE# #TAB# #TAB# #TAB# % (repr(key), repr(expect_type), repr(type(value)))) #LINE# #TAB# return value"
Convert a GPS timestamp to datetime object : param timestamp : GPS timestamp in seconds . : return : datetime object  <code> def gps_to_datetime(timestamp): ,#LINE# #TAB# gps_dt = datetime.datetime.utcfromtimestamp(timestamp) #LINE# #TAB# return gps_dt
Extracts the paths from the thin archive  <code> def extract_from_thin_archive(inputFile): ,"#LINE# #TAB# retval = None #LINE# #TAB# arCmd = ['ar', '-t', inputFile] #LINE# #TAB# arProc = Popen(arCmd, stdout=sp.PIPE) #LINE# #TAB# arOutput = arProc.communicate()[0] #LINE# #TAB# if arProc.returncode != 0: #LINE# #TAB# #TAB# _logger.error('ar failed on %s', inputFile) #LINE# #TAB# else: #LINE# #TAB# #TAB# retval = arOutput.splitlines() #LINE# #TAB# return retval"
"Return the special character that ` c ` encodes . > > > escape_char(""n "" ) "" "" <code> def escape_char(c): ","#LINE# #TAB# if c == 'n': #LINE# #TAB# #TAB# return '\n' #LINE# #TAB# elif c == 't': #LINE# #TAB# #TAB# return '\t' #LINE# #TAB# elif c == '\\': #LINE# #TAB# #TAB# return '\\' #LINE# #TAB# elif c == '""': #LINE# #TAB# #TAB# return '""' #LINE# #TAB# else: #LINE# #TAB# #TAB# return '\\' + c"
"Get the root form without markers  <code> def get_root(root, phonetic, compound): ",#LINE# #TAB# global compound_regex #LINE# #TAB# if not phonetic: #LINE# #TAB# #TAB# root = trim_phonetics(root) #LINE# #TAB# if not compound: #LINE# #TAB# #TAB# root = trim_compounds(root) #LINE# #TAB# return root
Merge a list of dataframes <code> def merge_list_dataframes(list_dataframes): ,"#LINE# #TAB# df = reduce(lambda df1, df2: pd.merge(df1, df2, left_index=True, #LINE# #TAB# #TAB# right_index=True), list_dataframes) #LINE# #TAB# return df"
"Iterate over the validation errors , print to log.warn : param json_dict : : param schema_file : : return : <code> def list_errors(json_dict, schema_file): ","#LINE# #TAB# validator = validator_file(schema_file) #LINE# #TAB# errors = sorted(validator.iter_errors(json_dict), key=lambda e: e.path) #LINE# #TAB# error_summary = list() #LINE# #TAB# for i, err in enumerate(errors): #LINE# #TAB# #TAB# stack_path = list(err.relative_path) #LINE# #TAB# #TAB# stack_path = [str(p) for p in stack_path] #LINE# #TAB# #TAB# error_string = 'Error {} at {}'.format(i, '/'.join(stack_path)) #LINE# #TAB# #TAB# error_summary.append((error_string, err)) #LINE# #TAB# return error_summary"
ports should be of the format < int : port_number>/<str : protocol > where protocol is either tcp or udp : param port_list : < list > : return : < bool > <code> def verify_ports(port_list): ,"#LINE# #TAB# result = True #LINE# #TAB# for item in port_list: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# port, protocol = item.split(PORT_SEPARATOR) #LINE# #TAB# #TAB# #TAB# port_num = int(port) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# result = False #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# result &= protocol in ALLOWED_PROTOCOLS #LINE# #TAB# #TAB# result &= port == str(port_num) #LINE# #TAB# return result"
"Computes eigenvector which corresponds to minimum eigenvalue  <code> def minimum_eigen_vector(x, num_steps, learning_rate, vector_prod_fn): ","#LINE# x = tf.nn.l2_normalize(x) #LINE# for _ in range(num_steps): #LINE# #TAB# x = eig_one_step(x, learning_rate, vector_prod_fn) #LINE# return x"
"Get a template path to compile a message  <code> def get_template(cls, message, messenger): ","#LINE# #TAB# #TAB# template = message.context.get('tpl', None) #LINE# #TAB# #TAB# if template: #LINE# #TAB# #TAB# #TAB# return template #LINE# #TAB# #TAB# if cls.template is None: #LINE# #TAB# #TAB# #TAB# cls.template = 'sitemessage/messages/%s__%s.%s' % ( #LINE# #TAB# #TAB# #TAB# #TAB# cls.get_alias(), messenger.get_alias(), cls.template_ext #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# return cls.template"
A context manager that provides a temporary directory and cleans it up . Note : This context manager duplicates the base functionality of the tempfile . TemporaryDirectory ( ) context manager in Python 3.2 + Returns ------- str The path to the temporary directory <code> def temporary_directory(): ,#LINE# #TAB# name = tempfile.mkdtemp() #LINE# #TAB# try: #LINE# #TAB# #TAB# yield name #LINE# #TAB# finally: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# shutil.rmtree(name) #LINE# #TAB# #TAB# except OSError as error: #LINE# #TAB# #TAB# #TAB# if error.errno != errno.ENOENT: #LINE# #TAB# #TAB# #TAB# #TAB# raise
Get config with url links to models datas  <code> def get_links_config(): ,"#LINE# #TAB# name_config = 'links_to_models.ini' #LINE# #TAB# path_config = pkg_resources.resource_filename('facelib._utils', name_config #LINE# #TAB# #TAB# ) #LINE# #TAB# config = ConfigParser(interpolation=ExtendedInterpolation()) #LINE# #TAB# config.read(path_config) #LINE# #TAB# assert config, 'Error opening config file: ' + name_config #LINE# #TAB# return config"
"Convert options to list of float values and add to < extent > dict <code> def add_to_dict(extent, option): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# parameters = [float(el.strip()) for el in option[1:]] #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValueError('Input values must be int or float') #LINE# #TAB# key = option[0].strip().replace('-', '') #LINE# #TAB# extent[key] = parameters #LINE# #TAB# return key, extent"
Return a list of well indices that correspond to the correct quadrant on a 384 well plate Parameters ---------- quad : Int The wells that need to be changed into a binary list Returns ------- List All the wells inside the desired quadrant <code> def get_quadrant_indices(quad): ,"#LINE# #TAB# assert quad in [0, 1, 2, 3] #LINE# #TAB# start_well = [0, 1, 24, 25] #LINE# #TAB# wells = [] #LINE# #TAB# for row_offset in range(start_well[quad], 384, 48): #LINE# #TAB# #TAB# for col_offset in range(0, 24, 2): #LINE# #TAB# #TAB# #TAB# wells.append(row_offset + col_offset) #LINE# #TAB# return wells"
Check if we are in IPython  <code> def runs_in_ipython(): ,#LINE# #TAB# import __builtin__ #LINE# #TAB# return '__IPYTHON__' in __builtin__.__dict__ and __builtin__.__dict__[ #LINE# #TAB# #TAB# '__IPYTHON__']
"Assign IDs to all the symbols that will be involved in the SDD <code> def compute_symbol_ids(count, selects_, fluents): ","#LINE# #TAB# symbols = {} #LINE# #TAB# scores = {x: (0) for x in selects_.keys()} #LINE# #TAB# scores.update(count) #LINE# #TAB# scored = sorted(scores.items(), key=lambda x: x[1], reverse=True) #LINE# #TAB# for x, _ in scored: #LINE# #TAB# #TAB# for s in selects_[x]: #LINE# #TAB# #TAB# #TAB# sdd_var_id = len(symbols) + 1 #LINE# #TAB# #TAB# #TAB# symbols[s] = sdd_var_id #LINE# #TAB# for p in fluents: #LINE# #TAB# #TAB# sdd_var_id = len(symbols) + 1 #LINE# #TAB# #TAB# symbols[p] = sdd_var_id #LINE# #TAB# return symbols"
"Return a txid : vout str <code> def utxo_str_from_utxo(utxo: Union[dict, str]) ->str: ","#LINE# #TAB# if isinstance(utxo, dict): #LINE# #TAB# #TAB# return '{}:{}'.format(utxo['prevout_hash'], utxo['prevout_n']) #LINE# #TAB# assert isinstance(utxo, str), f'utxo should be a str, not {type(utxo)}' #LINE# #TAB# return utxo"
Returns filename without extension > > > file_basename_no_extension('/home / me / file.txt ' ) ' file ' > > > file_basename_no_extension('file ' ) ' file ' <code> def file_basename_no_extension(filename): ,"#LINE# #TAB# base = os.path.basename(filename) #LINE# #TAB# name, extension = os.path.splitext(base) #LINE# #TAB# return name"
"Rate one or a list of force - distance curves Parameters ---------- data : nanite . Indentation or a list of those <code> def rate_fdist(data, scheme_id): ","#LINE# #TAB# if isinstance(data, nindent.Indentation): #LINE# #TAB# #TAB# data = [data] #LINE# #TAB# #TAB# return_single = True #LINE# #TAB# else: #LINE# #TAB# #TAB# return_single = False #LINE# #TAB# schemes = get_rating_schemes() #LINE# #TAB# scheme_key = list(schemes.keys())[scheme_id] #LINE# #TAB# training_set, regressor = schemes[scheme_key] #LINE# #TAB# rates = [] #LINE# #TAB# for fdist in data: #LINE# #TAB# #TAB# rt = fdist.rate_quality(regressor=regressor, training_set=training_set) #LINE# #TAB# #TAB# rates.append(rt) #LINE# #TAB# if return_single: #LINE# #TAB# #TAB# return rates[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return rates"
"Creates a subclass with value range constraint  <code> def withRange(cls, minimum, maximum): ","#LINE# #TAB# #TAB# class X(cls): #LINE# #TAB# #TAB# #TAB# subtypeSpec = cls.subtypeSpec + constraint.ValueRangeConstraint( #LINE# #TAB# #TAB# #TAB# #TAB# minimum, maximum) #LINE# #TAB# #TAB# X.__name__ = cls.__name__ #LINE# #TAB# #TAB# return X"
"R_d : daily portfolio return series S_m : monthly portfolio std series times : month / day <code> def bs_std(R_d, S_m, times=21): ",#LINE# #TAB# b = R_d / S_m * np.sqrt(times) #LINE# #TAB# bs_tmp = b.std() #LINE# #TAB# return bs_tmp
"Given a token , get the token information for it  <code> def get_token_information(token, information_class): ","#LINE# #TAB# data_size = ctypes.wintypes.DWORD() #LINE# #TAB# ctypes.windll.advapi32.get_token_information(token, information_class.num, #LINE# #TAB# #TAB# 0, 0, ctypes.byref(data_size)) #LINE# #TAB# data = ctypes.create_string_buffer(data_size.value) #LINE# #TAB# handle_nonzero_success(ctypes.windll.advapi32.get_token_information(token, #LINE# #TAB# #TAB# information_class.num, ctypes.byref(data), ctypes.sizeof(data), #LINE# #TAB# #TAB# ctypes.byref(data_size))) #LINE# #TAB# return ctypes.cast(data, ctypes.POINTER(security.TOKEN_USER)).contents"
Get the name of all available gpus . : return : a list of all available gpus  <code> def get_available_gpus(): ,#LINE# #TAB# from tensorflow.python.client import device_lib #LINE# #TAB# all_devices = device_lib.list_local_devices() #LINE# #TAB# gpus = [x.name for x in all_devices if x.device_type == 'GPU'] #LINE# #TAB# return gpus
"Returns the closest ancestor to cls in bases  <code> def nearest_base(cls, bases): ","#LINE# #TAB# if cls in bases: #LINE# #TAB# #TAB# return cls #LINE# #TAB# dists = {base: index(mro(cls), base) for base in bases} #LINE# #TAB# dists2 = {dist: base for base, dist in dists.items() if dist is not None} #LINE# #TAB# if not dists2: #LINE# #TAB# #TAB# return None #LINE# #TAB# return dists2[min(dists2)]"
"Adds random noise to a data series <code> def add_jitter(data, std_ratio=0.03): ","#LINE# #TAB# std = np.std(data) #LINE# #TAB# data_out = data + np.random.normal(0, std * std_ratio, size=data.shape) #LINE# #TAB# return data_out"
"Unpack byte object to text string , assuming big - endian byte order  <code> def bytes_to_text(bytestring): ","#LINE# #TAB# enc = 'utf-8' #LINE# #TAB# errorMode = 'strict' #LINE# #TAB# try: #LINE# #TAB# #TAB# string = bytestring.decode(encoding=enc, errors=errorMode) #LINE# #TAB# #TAB# result = removeControlCharacters(string) #LINE# #TAB# except: #LINE# #TAB# #TAB# result = '' #LINE# #TAB# return result"
"c - c ' : potassium sodium [ GM89 ]  <code> def theta_k_na_gm89(T, P): ","#LINE# #TAB# theta = GM89_eq3(T, float_([-0.0502312111, 0, 14.0213141, 0, 0, 0, 0, 0])) #LINE# #TAB# valid = logical_and(T >= 273.15, T <= 523.15) #LINE# #TAB# return theta, valid"
Clean package data preliminarily to initializing package objects  <code> def clean_package_data(csv_data): ,"#LINE# #TAB# for row in csv_data: #LINE# #TAB# #TAB# if row[5] == 'EOD': #LINE# #TAB# #TAB# #TAB# row[5] = None #LINE# #TAB# #TAB# row[1] = row[1].replace('South', 'S').replace('North', 'N').replace( #LINE# #TAB# #TAB# #TAB# 'East', 'E').replace('West', 'W') #LINE# #TAB# #TAB# del row[8:]"
"Like rstrip , but also returns the whitespace that was stripped off <code> def rstrip_keep(text): ","#LINE# #TAB# text_length = len(text) #LINE# #TAB# new_text = text.rstrip() #LINE# #TAB# if text_length != len(new_text): #LINE# #TAB# #TAB# suffix = text[-(text_length - len(new_text)):] #LINE# #TAB# else: #LINE# #TAB# #TAB# suffix = '' #LINE# #TAB# return new_text, suffix"
Return whether we should cache a page render <code> def do_not_cache(): ,#LINE# #TAB# from . import index #LINE# #TAB# if index.in_progress(): #LINE# #TAB# #TAB# return True #LINE# #TAB# if request.if_none_match or request.if_modified_since: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Applicable to Brahmi derived Indic scripts <code> def is_indiclang_char(c, lang): ","#LINE# #TAB# o = get_offset(c, lang) #LINE# #TAB# return o >= 0 and o <= 127 or ord(c) == DANDA or ord(c) == DOUBLE_DANDA"
"reverse of _ ensure_data Parameters ---------- values : ndarray dtype : pandas_dtype original : ndarray - like Returns ------- Index for extension types , otherwise ndarray casted to dtype <code> def reconstruct_data(values, dtype, original): ","#LINE# #TAB# if is_extension_array_dtype(dtype): #LINE# #TAB# #TAB# values = dtype.construct_array_type()._from_sequence(values) #LINE# #TAB# elif is_bool_dtype(dtype): #LINE# #TAB# #TAB# values = values.astype(dtype, copy=False) #LINE# #TAB# #TAB# if isinstance(original, ABCIndexClass): #LINE# #TAB# #TAB# #TAB# values = values.astype(object, copy=False) #LINE# #TAB# elif dtype is not None: #LINE# #TAB# #TAB# if is_datetime64_dtype(dtype): #LINE# #TAB# #TAB# #TAB# dtype = 'datetime64[ns]' #LINE# #TAB# #TAB# elif is_timedelta64_dtype(dtype): #LINE# #TAB# #TAB# #TAB# dtype = 'timedelta64[ns]' #LINE# #TAB# #TAB# values = values.astype(dtype, copy=False) #LINE# #TAB# return values"
Read a block header but do not unpack . This is a replacement for get_blockheader . It skips f ahead 28 bytes  <code> def skip_blockheader(f): ,#LINE# #TAB# f.read(28) #LINE# #TAB# return
"Do some cleaning , remove double spaces new lines , clean commas and others : param str : : return : <code> def clean_str(str): ","#LINE# #TAB# str = str.replace('\n', ' ') #LINE# #TAB# clean_space_from = ['( ', ' )', ', ', '[ ', ' ]'] #LINE# #TAB# for str_to_replace in clean_space_from: #LINE# #TAB# #TAB# actual = str_to_replace.replace(' ', '') #LINE# #TAB# #TAB# str = str.replace(actual, str_to_replace) #LINE# #TAB# str = ' (' + str + ') ' #LINE# #TAB# str = ' '.join(str.split()) #LINE# #TAB# return str"
Get repo environment overrides dictionary to use in repo options process <code> def get_repo_options_env(env): ,"#LINE# #TAB# env_options = '' #LINE# #TAB# if env is None: #LINE# #TAB# #TAB# return env_options #LINE# #TAB# if not isinstance(env, dict): #LINE# #TAB# #TAB# raise SaltInvocationError( #LINE# #TAB# #TAB# #TAB# '\'env\' must be a Python dictionary' #LINE# #TAB# #TAB# ) #LINE# #TAB# for key, value in env.items(): #LINE# #TAB# #TAB# if key == 'OPTIONS': #LINE# #TAB# #TAB# #TAB# env_options += '{0}\n'.format(value) #LINE# #TAB# return env_options"
Iterate through the stack of decorated functions until the original function  <code> def walk_decorator_stack(func: CallableT) -> Iterable['CallableT']: ,"#LINE# #TAB# while hasattr(func, ""__wrapped__""): #LINE# #TAB# #TAB# yield func #LINE# #TAB# #TAB# func = getattr(func, ""__wrapped__"") #LINE# #TAB# yield func"
Lists all licenses on a vCenter  <code> def list_licenses(service_instance=None): ,"#LINE# #TAB# log.trace('Retrieving all licenses') #LINE# #TAB# licenses = salt.utils.vmware.get_licenses(service_instance) #LINE# #TAB# ret_dict = [{'key': l.licenseKey, #LINE# #TAB# #TAB# #TAB# #TAB# 'name': l.name, #LINE# #TAB# #TAB# #TAB# #TAB# 'description': l.labels[0].value if l.labels else None, #LINE# #TAB# #TAB# #TAB# #TAB# 'capacity': l.total if l.total > 0 else sys.maxsize, #LINE# #TAB# #TAB# #TAB# #TAB# 'used': l.used if l.used else 0} #LINE# #TAB# #TAB# #TAB# #TAB# for l in licenses] #LINE# #TAB# return ret_dict"
"extract and format certification information as comment Args : cert(cryptography.x509.Certificate , required ) : a x509 formatted certificate to process Returns : certification information as a string <code> def describe_cert(cert): ","#LINE# #TAB# assert isinstance(cert, Certificate) #LINE# #TAB# info = ( #LINE# #TAB# #TAB# '\n# Subject: {}\n# Issued by: {} {}\n# Signed with: {}\n# Expires: {}\n' #LINE# #TAB# #TAB# .format(cert.subject.get_attributes_for_oid(NameOID.COMMON_NAME)[0] #LINE# #TAB# #TAB# .value, *[cert.issuer.get_attributes_for_oid(oid)[0].value for oid in #LINE# #TAB# #TAB# [NameOID.ORGANIZATION_NAME, NameOID.ORGANIZATIONAL_UNIT_NAME, #LINE# #TAB# #TAB# NameOID.COMMON_NAME]], cert.not_valid_after)) #LINE# #TAB# return info"
"Detach an iface from a vm  <code> def _detach(cls, iface_id): ","#LINE# #TAB# #TAB# iface = cls._info(iface_id) #LINE# #TAB# #TAB# opers = [] #LINE# #TAB# #TAB# vm_id = iface.get('vm_id') #LINE# #TAB# #TAB# if vm_id: #LINE# #TAB# #TAB# #TAB# cls.echo('The iface is still attached to the vm %s.' % vm_id) #LINE# #TAB# #TAB# #TAB# cls.echo('Will detach it.') #LINE# #TAB# #TAB# #TAB# opers.append(cls.call('hosting.vm.iface_detach', vm_id, iface_id)) #LINE# #TAB# #TAB# return opers"
"Unify index masks and bool masks into bool masks <code> def to_column_indices(X, columns): ",#LINE# #TAB# if columns is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# columns = np.array(columns).reshape(-1) #LINE# #TAB# if X.shape[1] == columns.shape[0]: #LINE# #TAB# #TAB# return np.where(columns)[0] #LINE# #TAB# return columns
"Recursively merge two dicts to create a new dict . Does not modify either dict . The second dict 's values will take priority over the first 's  <code> def dict_merge(d1, d2): ","#LINE# #TAB# new = dict(d1) #LINE# #TAB# for k, v in d2.items(): #LINE# #TAB# #TAB# if k in new and isinstance(new[k], dict) and isinstance(d2[k], dict): #LINE# #TAB# #TAB# #TAB# new[k] = dict_merge(new[k], d2[k]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new[k] = d2[k] #LINE# #TAB# return new"
"Return value or ( value , value ) if value is not a tuple  <code> def to_2tuple(value): ","#LINE# #TAB# if isinstance(value, tuple): #LINE# #TAB# #TAB# return value #LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return tuple(value) #LINE# #TAB# return value, value"
Read the configuration files in config_file_sequence and return a dictionary of the parameter specified in their ' default ' section  <code> def get_smtp_parameters(config_file_sequence): ,#LINE# #TAB# config_parser = configparser.SafeConfigParser() #LINE# #TAB# config_parser.read(config_file_sequence) #LINE# #TAB# return {i[0].lower(): i[1] for i in config_parser.items('default')}
"The refs in the schemas are arbitrary identifiers , and can not be used as - is as real network locations . This rewrites any of those arbitrary refs to be real urls servable by this endpoint  <code> def rewrite_ref(identifier, base_url): ",#LINE# #TAB# if not base_url.endswith('/'): #LINE# #TAB# #TAB# base_url += '/' #LINE# #TAB# if identifier.startswith('/schema/'): #LINE# #TAB# #TAB# return base_url + identifier[1:] #LINE# #TAB# return identifier
Checks if a np.matrix is a valid SO2 pose  <code> def is_valid(obj): ,"#LINE# #TAB# if type(obj) is np.matrix and obj.shape == (2, 2) and abs(np.linalg.det #LINE# #TAB# #TAB# (obj) - 1) < np.spacing([1])[0]: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Ensure default test settings are acceptable . Raises GabbiFormatError for invalid settings  <code> def validate_defaults(defaults): ,"#LINE# #TAB# if any(_is_method_shortcut(key) for key in defaults): #LINE# #TAB# #TAB# raise GabbiFormatError('""METHOD: url"" pairs not allowed in defaults') #LINE# #TAB# return defaults"
"Divides given sequence to n chunks <code> def get_chunk(seq: list, n_chunks: int) ->list: ","#LINE# #TAB# seq_size = len(seq) #LINE# #TAB# result_list = list() #LINE# #TAB# if n_chunks > seq_size: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'The number of chunks ({n_chunks}) exceeds the length of the sequence ({seq_size})' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# [result_list.append([]) for _ in range(n_chunks)] #LINE# #TAB# for idx, chunk in enumerate(itertools.cycle(result_list)): #LINE# #TAB# #TAB# if idx == seq_size: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# chunk.append(seq[idx]) #LINE# #TAB# return result_list"
Force utf8 string entries in the given datas <code> def to_utf8(datas): ,"#LINE# #TAB# res = datas #LINE# #TAB# if isinstance(datas, dict): #LINE# #TAB# #TAB# res = {} #LINE# #TAB# #TAB# for key, value in datas.items(): #LINE# #TAB# #TAB# #TAB# key = to_utf8(key) #LINE# #TAB# #TAB# #TAB# value = to_utf8(value) #LINE# #TAB# #TAB# #TAB# res[key] = value #LINE# #TAB# elif isinstance(datas, (list, tuple)): #LINE# #TAB# #TAB# res = [] #LINE# #TAB# #TAB# for data in datas: #LINE# #TAB# #TAB# #TAB# res.append(to_utf8(data)) #LINE# #TAB# elif isinstance(datas, unicode): #LINE# #TAB# #TAB# res = datas.encode('utf-8') #LINE# #TAB# return res"
"Calculates RMS of rates from fitted_rates before and after correction <code> def calculate_rms_rates(rates, fitted_rates, corrected_rates): ","#LINE# #TAB# rms_rates = np.sqrt(np.mean((rates - fitted_rates)**2)) #LINE# #TAB# rms_corrected_rates = np.sqrt(np.mean((corrected_rates - fitted_rates)**2)) #LINE# #TAB# return rms_rates, rms_corrected_rates"
"Returns all the pixels neighbours of ipix <code> def _neighbour_pixels(hp, ipix): ",#LINE# #TAB# #TAB# neigh_ipix = np.unique(hp.neighbours(ipix).ravel()) #LINE# #TAB# #TAB# return neigh_ipix[np.where(neigh_ipix >= 0)]
Returns a message indicating that no data was found in the given clusters <code> def no_data_message(clusters): ,#LINE# #TAB# clusters_text = ' / '.join([c['name'] for c in clusters]) #LINE# #TAB# message = terminal.failed(f'No matching data found in {clusters_text}.') #LINE# #TAB# message = ( #LINE# #TAB# #TAB# f'{message}\nDo you need to add another cluster to your configuration?' #LINE# #TAB# #TAB# ) #LINE# #TAB# return message
Securely avaluate a Gdk . RGBA string  <code> def eval_rgba(rgba_str): ,"#LINE# #TAB# amatch = re.match(RGBA_re, rgba_str.strip()) #LINE# #TAB# if amatch: #LINE# #TAB# #TAB# return eval(rgba_str.strip()) #LINE# #TAB# #TAB# return Gdk.RGBA(red=float(thedict['RED']), green=float(thedict[ #LINE# #TAB# #TAB# #TAB# 'GREEN']), blue=float(thedict['BLUE']), alpha=float(thedict[ #LINE# #TAB# #TAB# #TAB# 'ALPHA'])) #LINE# #TAB# return None"
Creates a parser suitable to parse the argument describing features ids in different subparsers <code> def features_ids_argument_parser() ->ArgumentParser: ,"#LINE# #TAB# parser = ArgumentParser(add_help=False, parents=[ #LINE# #TAB# #TAB# collection_option_parser()]) #LINE# #TAB# parser.add_argument(FEATURES_IDS_ARGNAME, nargs='+', help= #LINE# #TAB# #TAB# 'features identifiers or features UUIDs') #LINE# #TAB# return parser"
Parse a PKCS7 certificate bundle in DER or PEM format : param pkcs7 : A pkcs7 bundle in DER or PEM format : returns : A list of individual DER - encoded certificates <code> def parse_pkcs7_bundle(pkcs7): ,#LINE# #TAB# if PKCS7_BEG in pkcs7: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# for substrate in _read_pem_blocks(pkcs7): #LINE# #TAB# #TAB# #TAB# #TAB# for cert in _get_certs_from_pkcs7_substrate(substrate): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield cert #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# LOG.exception('Unreadable Certificate.') #LINE# #TAB# #TAB# #TAB# raise exceptions.UnreadableCert #LINE# #TAB# else: #LINE# #TAB# #TAB# for cert in _get_certs_from_pkcs7_substrate(pkcs7): #LINE# #TAB# #TAB# #TAB# yield cert
To sort file names numerically <code> def numerical_sort(value): ,"#LINE# #TAB# numbers = re.compile('(\\d+)') #LINE# #TAB# parts = numbers.split(value) #LINE# #TAB# parts[1::2] = map(int, parts[1::2]) #LINE# #TAB# return parts"
Return the children of ` m ` and its direct parameters not registered in modules  <code> def children_and_parameters(m: nn.Module): ,"#LINE# #TAB# children = list(m.children()) #LINE# #TAB# children_p = sum([[id(p) for p in c.parameters()] for c in m.children() #LINE# #TAB# #TAB# ], []) #LINE# #TAB# for p in m.parameters(): #LINE# #TAB# #TAB# if id(p) not in children_p: #LINE# #TAB# #TAB# #TAB# children.append(ParameterModule(p)) #LINE# #TAB# return children"
"Return 1 iff text[start : stop ] only contains whitespace characters ( as defined in Constants / Sets.py ) , 0 otherwise  <code> def is_whitespace(text, start=0, stop=None, charset=nonwhitespace_charset): ","#LINE# #TAB# if stop is None: #LINE# #TAB# #TAB# stop = len(text) #LINE# #TAB# return charset.search(text, 1, start, stop) is None"
"Build relation between districts numbers and mps <code> def build_mps(mps: Iterable[MP]) ->Dict[CodedDistrict, ReducedMP]: ",#LINE# #TAB# result = dict() #LINE# #TAB# for mp in mps: #LINE# #TAB# #TAB# result[mp.coded_district] = mp.reduce() #LINE# #TAB# return result
"Generate a CADF tag in the format name?value=<value > : param name : name of tag : param valuue : optional value tag <code> def generate_name_value_tag(name, value): ",#LINE# #TAB# if name is None or value is None: #LINE# #TAB# #TAB# raise ValueError('Invalid name and/or value. Values cannot be None') #LINE# #TAB# tag = name + '?value=' + value #LINE# #TAB# return tag
Raise TypeError if x is a str containing non - utf8 bytes or if x is an iterable which contains such a str  <code> def to_unicode_optional_iterator(x): ,"#LINE# #TAB# if isinstance(x, STRING_TYPES): #LINE# #TAB# #TAB# return to_unicode(x) #LINE# #TAB# try: #LINE# #TAB# #TAB# l = list(x) #LINE# #TAB# except TypeError as e: #LINE# #TAB# #TAB# assert 'is not iterable' in str(e) #LINE# #TAB# #TAB# return x #LINE# #TAB# else: #LINE# #TAB# #TAB# return [ to_unicode(e) for e in l ]"
"Utility for deriving shared key in ECDH procedure <code> def derive_shared_key(private_key, serialized_pubkey, shared_info): ","#LINE# #TAB# deserialized_public_numbers = (ec.EllipticCurvePublicNumbers. #LINE# #TAB# #TAB# from_encoded_point(ec.SECP384R1(), serialized_pubkey)) #LINE# #TAB# deserialized_pubkey = deserialized_public_numbers.public_key( #LINE# #TAB# #TAB# default_backend()) #LINE# #TAB# shared_key = private_key.exchange(ec.ECDH(), deserialized_pubkey) #LINE# #TAB# derived_key = HKDF(algorithm=hashes.SHA256(), length=32, salt=None, #LINE# #TAB# #TAB# info=shared_info, backend=default_backend()).derive(shared_key) #LINE# #TAB# return derived_key"
Check if a file should be group executable <code> def file_group_executable(fi): ,#LINE# #TAB# mode = fi['mode'] #LINE# #TAB# if stat.S_ISDIR(mode): #LINE# #TAB# #TAB# return #LINE# #TAB# if bool(stat.S_IXUSR & mode) and not bool(stat.S_IXGRP & mode): #LINE# #TAB# #TAB# return 'PROB_FILE_NOT_GRPEXEC'
"Return the True if the ` arg_name ` is found in ` crumb_path `  <code> def has_arg(crumb_path: str, arg_name: str) ->bool: ","#LINE# #TAB# for txt, fld, rgx, conv in _yield_items(crumb_path): #LINE# #TAB# #TAB# if fld == arg_name: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Generates GitHub Markdown formatted API documentation using provided schemas in RequestHandler methods and their docstrings  <code> def get_api_docs(routes): ,"#LINE# #TAB# routes = map(_get_tuple_from_route, routes) #LINE# #TAB# documentation = [] #LINE# #TAB# for url, rh, methods in sorted(routes, key=lambda a: a[0]): #LINE# #TAB# #TAB# if issubclass(rh, APIHandler): #LINE# #TAB# #TAB# #TAB# documentation.append(_get_route_doc(url, rh, methods)) #LINE# #TAB# documentation = ( #LINE# #TAB# #TAB# ""**This documentation is automatically generated.**\n\n"" + #LINE# #TAB# #TAB# ""**Output schemas only represent `data` and not the full output; "" + #LINE# #TAB# #TAB# ""see output examples and the JSend specification.**\n"" + #LINE# #TAB# #TAB# ""\n<br>\n<br>\n"".join(documentation) #LINE# #TAB# ) #LINE# #TAB# return documentation"
"Utility function to join a list by "" . "" ensuring that there are no repeated periods and that the final item ends in a period  <code> def period_join(period_list: list): ",#LINE# #TAB# period_list = [x.strip('.') for x in period_list] #LINE# #TAB# return '. '.join(period_list) + '.'
Find the conversion of a unit string . Parameters ---------- unit : str The unit string Returns ------- float or None The unit value relative to the standard or None if not found  <code> def find_unit(unit): ,#LINE# #TAB# tp = find_unittype(unit) #LINE# #TAB# if tp: #LINE# #TAB# #TAB# return UNITS[tp][unit] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
Get a NumPy array view of a VNL matrix  <code> def get_array_view_from_vnl_matrix(vnl_matrix): ,"#LINE# #TAB# if not HAVE_NUMPY: #LINE# #TAB# #TAB# raise ImportError('Numpy not available.') #LINE# #TAB# cols = vnl_matrix.columns() #LINE# #TAB# rows = vnl_matrix.rows() #LINE# #TAB# dim = 2 #LINE# #TAB# shape = [rows, cols] #LINE# #TAB# pixelType = 'D' #LINE# #TAB# numpy_dtype = _get_numpy_pixelid(pixelType) #LINE# #TAB# memview = itkPyVnlD._get_array_view_from_vnl_matrix(vnl_matrix) #LINE# #TAB# ndarr_view = np.asarray(memview).view(dtype=numpy_dtype).reshape(shape #LINE# #TAB# #TAB# ).view(np.ndarray) #LINE# #TAB# itk_view = NDArrayITKBase(ndarr_view, vnl_matrix) #LINE# #TAB# return itk_view"
Returns the number of bytes necessary to store the integer n <code> def integer_byte_size(n): ,"#LINE# #TAB# quanta, mod = divmod(integer_bit_size(n), 8) #LINE# #TAB# if mod or n == 0: #LINE# #TAB# #TAB# quanta += 1 #LINE# #TAB# return quanta"
Fetch the current process local data dictionary  <code> def process_data(name=None): ,"#LINE# #TAB# ct = current_process() #LINE# #TAB# if not hasattr(ct, '_pulsar_local'): #LINE# #TAB# #TAB# ct._pulsar_local = {} #LINE# #TAB# loc = ct._pulsar_local #LINE# #TAB# return loc.get(name) if name else loc"
Sets up debug mode if there is a --debug argument on the commandline <code> def get_debug_mode(settings: dict) ->dict: ,"#LINE# #TAB# provider = ArgumentParser(add_help=False) #LINE# #TAB# provider.add_argument('--debug', action='store_true') #LINE# #TAB# arguments = vars(provider.parse_known_args()[0]) #LINE# #TAB# if arguments.get('debug'): #LINE# #TAB# #TAB# settings['global'] = {'debug': True} #LINE# #TAB# #TAB# for logger in settings['logging']['handlers'].values(): #LINE# #TAB# #TAB# #TAB# logger['level'] = 'DEBUG' #LINE# #TAB# #TAB# log_module.log.info('Debug mode activated!') #LINE# #TAB# return settings"
"Collects server identifiers : param connection_pool : The connection pool to iterate through : param server_to_exclude : Server identifier to exclude <code> def fetch_identifiers(connection_pool, server_to_exclude): ",#LINE# #TAB# for identifier in connection_pool.get_identifiers(): #LINE# #TAB# #TAB# server_address = connection_pool.get_connection(identifier)[ #LINE# #TAB# #TAB# #TAB# 'server_address'] #LINE# #TAB# #TAB# if server_address != server_to_exclude: #LINE# #TAB# #TAB# #TAB# yield server_address
"Returns the merged configuration , from the current directory and the user directory  <code> def get_config(): ",#LINE# #TAB# config = get_user_config() #LINE# #TAB# update = get_project_config() #LINE# #TAB# for key in update: #LINE# #TAB# #TAB# if key in config and type(config[key]) == dict: #LINE# #TAB# #TAB# #TAB# config[key].update(update[key]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# config[key] = update[key] #LINE# #TAB# return config
muck with the date because EPW starts counting from 1 and goes to 24  <code> def muck_w_date(record): ,"#LINE# #TAB# temp_d = datetime.datetime(int(record['Year']), int(record['Month']), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# int(record['Day']), int(record['Hour']) % 24, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# int(record['Minute']) % 60) #LINE# #TAB# d_off = int(record['Hour'])//24 #LINE# #TAB# if d_off > 0: #LINE# #TAB# #TAB# temp_d += datetime.timedelta(days=d_off) #LINE# #TAB# return temp_d"
Neg tuple t_a returning tuple <code> def fq2_neg(t_a): ,"#LINE# #TAB# a, b = t_a #LINE# #TAB# return -a % Q, -b % Q"
"Given an Elastic client and and index , attempt to open the index : param elastic_client : Elastic client : param index : Index which is to be opened : return : Response from Elastic <code> def open_index(elastic_client, index): ",#LINE# #TAB# response = elastic_client.indices.open(index=index) #LINE# #TAB# if response is None: #LINE# #TAB# #TAB# raise Exception('Problem occurred opening index. Index: {}'.format( #LINE# #TAB# #TAB# #TAB# index)) #LINE# #TAB# return response
Test save and load Python pickle : param ods : ods : return : ods <code> def through_omas_pkl(ods): ,"#LINE# #TAB# filename = omas_testdir(__file__) + '/test.pkl' #LINE# #TAB# save_omas_pkl(ods, filename) #LINE# #TAB# ods1 = load_omas_pkl(filename) #LINE# #TAB# return ods1"
"Run the specified process and wait until it finishes  <code> def run_process(command, environ): ","#LINE# #TAB# log.info('running %r with %r', command, environ) #LINE# #TAB# env = dict(os.environ) #LINE# #TAB# env.update(environ) #LINE# #TAB# try: #LINE# #TAB# #TAB# p = subprocess.Popen(args=command, env=env) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# raise OSError('cannot run %r: %s' % (command, e)) #LINE# #TAB# log.debug('subprocess %d is running', p.pid) #LINE# #TAB# ret = p.wait() #LINE# #TAB# log.debug('subprocess %d exited: %d', p.pid, ret) #LINE# #TAB# return ret"
"Callback for parsing to a ` ` datetime ` ` with ` ` opt_date_format ` ` <code> def cb_time(ctx, param, value): ","#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return value #LINE# #TAB# _format = ctx.params[_KEY_DATE_FORMAT] #LINE# #TAB# try: #LINE# #TAB# #TAB# time = dt.datetime.strptime(value, _format) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# raise click.ClickException( #LINE# #TAB# #TAB# #TAB# f'Need to use `--{_KEY_DATE_FORMAT}` when using `cb_time`.') #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# raise click.BadParameter( #LINE# #TAB# #TAB# #TAB# f'Cannot parse ""{value}"" to date with format ""{_format}""') #LINE# #TAB# else: #LINE# #TAB# #TAB# return time"
"Remap coordinates from polar to cartesian <code> def map_pol_to_crt(aryTht, aryRad): ","#LINE# #TAB# aryXCrds = aryRad * np.cos(aryTht) #LINE# #TAB# aryYrds = aryRad * np.sin(aryTht) #LINE# #TAB# return aryXCrds, aryYrds"
Create a definition dictionary for the VNI input method . Returns a dictionary to be passed into process_key ( )  <code> def get_vni_definition(): ,"#LINE# #TAB# return {'6': ['a^', 'o^', 'e^'], '7': ['u*', 'o*'], '8': 'a+', '9': #LINE# #TAB# #TAB# 'd-', '2': '\\', '1': '/', '3': '?', '4': '~', '5': '.'}"
"timestamp to date . : param timestamp : int , e.g.1537535021 : return : Year - Month - Day Hour : Minute : Second <code> def time_to_date(timestamp): ","#LINE# #TAB# timearr = time.localtime(timestamp) #LINE# #TAB# otherStyleTime = time.strftime('%Y-%m-%d %H:%M:%S', timearr) #LINE# #TAB# return otherStyleTime"
"Fortpy caches many things , that should be completed after each completion finishes . : param delete_all : Deletes also the cache that is normally not deleted , like parser cache , which is important for faster parsing  <code> def clear_caches(delete_all=False): ","#LINE# #TAB# global _time_caches #LINE# #TAB# if delete_all: #LINE# #TAB# #TAB# _time_caches = [] #LINE# #TAB# #TAB# _parser = {'default': CodeParser()} #LINE# #TAB# else: #LINE# #TAB# #TAB# for tc in _time_caches: #LINE# #TAB# #TAB# #TAB# for key, (t, value) in list(tc.items()): #LINE# #TAB# #TAB# #TAB# #TAB# if t < time.time(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# del tc[key]"
Compress file under zip mode when compress failed with return source path : param source : source file path : return : zip file path <code> def zip_file(source): ,"#LINE# #TAB# target = source[0:source.rindex('.')] + '.zip' #LINE# #TAB# try: #LINE# #TAB# #TAB# with zipfile.ZipFile(target, 'w') as zip_file: #LINE# #TAB# #TAB# #TAB# zip_file.write(source, source[source.startswith('/'):], zipfile #LINE# #TAB# #TAB# #TAB# #TAB# .ZIP_DEFLATED) #LINE# #TAB# #TAB# #TAB# zip_file.close() #LINE# #TAB# #TAB# #TAB# __cps_rate__(source, target) #LINE# #TAB# except IOError as e: #LINE# #TAB# #TAB# logger.error('Compress file[%s] with zip mode failed. Case: %s', #LINE# #TAB# #TAB# #TAB# source, str(e)) #LINE# #TAB# #TAB# target = source #LINE# #TAB# return target"
"add simple button with bound action <code> def add_button(parent, label, size=(-1, -1), action=None): ","#LINE# #TAB# thisb = wx.Button(parent, label=label, size=size) #LINE# #TAB# if callable(action): #LINE# #TAB# #TAB# thisb.Bind(wx.EVT_BUTTON, action) #LINE# #TAB# return thisb"
"Validate the configuration and return an Xfinity Gateway scanner  <code> def get_scanner(hass, config): ","#LINE# #TAB# _LOGGER.warning( #LINE# #TAB# #TAB# 'The Xfinity Gateway has been deprecated and will be removed from Home Assistant in version 0.109. Please remove it from your configuration. ' #LINE# #TAB# #TAB# ) #LINE# #TAB# gateway = XfinityGateway(config[DOMAIN][CONF_HOST]) #LINE# #TAB# scanner = None #LINE# #TAB# try: #LINE# #TAB# #TAB# gateway.scan_devices() #LINE# #TAB# #TAB# scanner = XfinityDeviceScanner(gateway) #LINE# #TAB# except (RequestException, ValueError): #LINE# #TAB# #TAB# _LOGGER.error( #LINE# #TAB# #TAB# #TAB# 'Error communicating with Xfinity Gateway. Check host: %s', #LINE# #TAB# #TAB# #TAB# gateway.host) #LINE# #TAB# return scanner"
"Create a peer table from the peer DB <code> def atlasdb_load_peer_table( con=None, path=None ): ","#LINE# #TAB# peer_table = {} #LINE# #TAB# with AtlasDBOpen(con=con, path=path) as dbcon: #LINE# #TAB# #TAB# sql = ""SELECT * FROM peers;"" #LINE# #TAB# #TAB# args = () #LINE# #TAB# #TAB# cur = dbcon.cursor() #LINE# #TAB# #TAB# res = atlasdb_query_execute( cur, sql, args ) #LINE# #TAB# #TAB# count = 0 #LINE# #TAB# #TAB# for row in res: #LINE# #TAB# #TAB# if count > 0 and count % 100 == 0: #LINE# #TAB# #TAB# #TAB# log.debug(""Loaded %s peers..."" % count) #LINE# #TAB# #TAB# atlas_init_peer_info( peer_table, row['peer_hostport'] ) #LINE# #TAB# #TAB# count += 1 #LINE# #TAB# return peer_table"
"Retrieve ' next ' link , if applicable  <code> def get_collection_links(request, items): ","#LINE# #TAB# links = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# limit = int(request.params.get('limit') or 0) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# limit = 0 #LINE# #TAB# if limit > 0 and limit == len(items): #LINE# #TAB# #TAB# last_item = items[-1] #LINE# #TAB# #TAB# last_item_id = last_item['id'] #LINE# #TAB# #TAB# links.append({'rel': 'next', 'href': _get_next_link(request, #LINE# #TAB# #TAB# #TAB# last_item_id)}) #LINE# #TAB# return links"
"Converts a human readable sting to regModeS <code> def string_to_regulation_mode(cls, s): ",#LINE# #TAB# reg_mode_s = 0 #LINE# #TAB# if s == u'uPv': #LINE# #TAB# #TAB# reg_mode_s = 1 #LINE# #TAB# elif s == u'iPv': #LINE# #TAB# #TAB# reg_mode_s = 2 #LINE# #TAB# elif s == u'pPv': #LINE# #TAB# #TAB# reg_mode_s = 4 #LINE# #TAB# elif s == u'iBat': #LINE# #TAB# #TAB# reg_mode_s = 8 #LINE# #TAB# elif s == u'PVSim': #LINE# #TAB# #TAB# reg_mode_s = 16 #LINE# #TAB# elif s == u'IUcurve': #LINE# #TAB# #TAB# reg_mode_s = 32 #LINE# #TAB# return reg_mode_s
Updates a Jinja2 env by adding custom functions and filters  <code> def update_jinja2_env(env): ,"#LINE# #TAB# for name, func in humilis.config.config.jinja2_filters.items(): #LINE# #TAB# #TAB# env.filters[name] = func"
"Converts EmailAddress to SendGrid API { email , name } dict <code> def email_object(email, workaround_name_quote_bug=False): ","#LINE# #TAB# obj = {'email': email.addr_spec} #LINE# #TAB# if email.display_name: #LINE# #TAB# #TAB# if workaround_name_quote_bug: #LINE# #TAB# #TAB# #TAB# obj['name'] = '""%s""' % rfc822_quote(email.display_name) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# obj['name'] = email.display_name #LINE# #TAB# return obj"
Enable gradients sharing globally . Parameters ---------- enabled : boolean Whether to share grads . Returns ------- None Examples -------- > > > import dragon.memonger as opt > > > opt . ShareGrads ( ) <code> def share_grads(enabled=True): ,#LINE# #TAB# from dragon.config import option #LINE# #TAB# option['share_grads'] = enabled
"Validates the given resource id  <code> def is_valid_resource_id(rid, exception_type=None): ",#LINE# #TAB# is_valid = False #LINE# #TAB# try: #LINE# #TAB# #TAB# is_valid = rid and resource_id(**parse_resource_id(rid)).lower() == rid.lower() #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# if not is_valid and exception_type: #LINE# #TAB# #TAB# raise exception_type() #LINE# #TAB# return is_valid
"Parse file iteration task factory object , and return task list . : param dict factory : A loaded JSON task factory object  <code> def expand_task_per_file(factory, fileutils): ","#LINE# #TAB# files = fileutils.get_container_list(factory.source) #LINE# #TAB# task_objs = [_transform_repeat_task(factory.repeat_task, f, i, #LINE# #TAB# #TAB# _transform_file_str) for i, f in enumerate(files)] #LINE# #TAB# try: #LINE# #TAB# #TAB# factory.merge_task.id = 'merge' #LINE# #TAB# #TAB# factory.merge_task.depends_on = models.TaskDependencies(task_id_ranges #LINE# #TAB# #TAB# #TAB# =[models.TaskIdRange(start=0, end=len(task_objs) - 1)]) #LINE# #TAB# #TAB# task_objs.append(_transform_merge_task(factory.merge_task)) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return task_objs"
Gets all properties for all profiles in the specified store <code> def get_all_profiles(store='local'): ,"#LINE# #TAB# return { #LINE# #TAB# #TAB# 'Domain Profile': get_all_settings(profile='domain', store=store), #LINE# #TAB# #TAB# 'Private Profile': get_all_settings(profile='private', store=store), #LINE# #TAB# #TAB# 'Public Profile': get_all_settings(profile='public', store=store) #LINE# #TAB# }"
Convert JSON Unicode escapes in ' txt ' into actual characters <code> def parse_unicode_escapes(txt): ,"#LINE# #TAB# if '\\u' in txt: #LINE# #TAB# #TAB# return re.sub('\\\\u([0-9a-fA-F]{4})', _hex_to_char, txt) #LINE# #TAB# return txt"
"Returns list of project service classes . The list contains services whose resources need to be deleted prior , the project they are associated with , deletion . The resources can not be most likely deleted after the project is deleted first  <code> def get_project_associated_cleanup_services(): ",#LINE# #TAB# project_associated_services = [] #LINE# #TAB# if IS_NOVA: #LINE# #TAB# #TAB# project_associated_services.append(NovaQuotaService) #LINE# #TAB# if IS_CINDER: #LINE# #TAB# #TAB# project_associated_services.append(VolumeQuotaService) #LINE# #TAB# if IS_NEUTRON: #LINE# #TAB# #TAB# project_associated_services.append(NetworkQuotaService) #LINE# #TAB# return project_associated_services
Set the color map of the current color scale  <code> def set_cmap(cmap): ,"#LINE# #TAB# scale = _context['scales']['color'] #LINE# #TAB# for k, v in _process_cmap(cmap).items(): #LINE# #TAB# #TAB# setattr(scale, k, v) #LINE# #TAB# return scale"
"Get all sales for a given time period <code> def sales_for_time_period(from_date, to_date): ",#LINE# #TAB# sales = Order.objects.filter( #LINE# #TAB# #TAB# Q(payment_date__lte=to_date) & Q(payment_date__gte=from_date) #LINE# #TAB# ).exclude(status=Order.CANCELLED) #LINE# #TAB# return sales
Build map from elements to element numbers  <code> def compute_element_numbers(elements): ,"#LINE# #TAB# element_numbers = {} #LINE# #TAB# for i, element in enumerate(elements): #LINE# #TAB# #TAB# element_numbers[element] = i #LINE# #TAB# return element_numbers"
Check if file gzipped  <code> def is_gzipped(f): ,"#LINE# #TAB# with open(f, 'rb') as rpkm_file: #LINE# #TAB# #TAB# magic = rpkm_file.read(2) #LINE# #TAB# return magic == '\x1f\x8b'"
Returns a list of n - grams read from the file at ` path `  <code> def get_ngrams(path): ,"#LINE# #TAB# with open(path, encoding='utf-8') as fh: #LINE# #TAB# #TAB# ngrams = [ngram.strip() for ngram in fh.readlines()] #LINE# #TAB# return ngrams"
Workaround for loading a big pickle file . Files over 2 GB cause pickle errors on certin Mac and Windows distributions . : param f : : return : <code> def load_big_file(f): ,"#LINE# #TAB# logger.info(f'loading file {f}') #LINE# #TAB# with open(f, 'r+b') as f_in: #LINE# #TAB# #TAB# bf = mmap.mmap(f_in.fileno(), 0) #LINE# #TAB# #TAB# f_in.close() #LINE# #TAB# return bf"
Returns the count of emojis in a string <code> def emoji_count(string): ,#LINE# #TAB# c = 0 #LINE# #TAB# for i in string: #LINE# #TAB# #TAB# if i in unicode_codes.UNICODE_EMOJI: #LINE# #TAB# #TAB# #TAB# c = c + 1 #LINE# #TAB# return c
"Read a gauge file and return time series and values with outside gauges removed Supporting function to read_gauges_file <code> def read_one_gauge_file(file_name, gauge_ind=None): ","#LINE# #TAB# t_value = np.loadtxt(file_name, dtype='float64', ndmin=2) #LINE# #TAB# times = t_value[:, (0)] #LINE# #TAB# values = t_value[:, 1:] #LINE# #TAB# if 'hU_gauges.dat' in file_name: #LINE# #TAB# #TAB# values = np.array([values[:, 0::2], values[:, 1::2]]) #LINE# #TAB# if gauge_ind is not None: #LINE# #TAB# #TAB# if values.ndim == 2: #LINE# #TAB# #TAB# #TAB# values = values[:, (gauge_ind)] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# values = values[:, :, (gauge_ind)] #LINE# #TAB# return times, values"
Guess the optimal depth to use for the given list of arguments  <code> def guess_depth(packages): ,#LINE# #TAB# if len(packages) == 1: #LINE# #TAB# #TAB# return packages[0].count('.') + 2 #LINE# #TAB# return min(p.count('.') for p in packages) + 1
Knows if the server is streaming <code> def is_streaming(): ,#LINE# #TAB# global STREAMING #LINE# #TAB# return STREAMING
"Split large message into smaller messages each smaller than the max_length  <code> def split_message(message, max_length): ",#LINE# #TAB# ms = [] #LINE# #TAB# while len(message) > max_length: #LINE# #TAB# #TAB# ms.append(message[:max_length]) #LINE# #TAB# #TAB# message = message[max_length:] #LINE# #TAB# ms.append(message) #LINE# #TAB# return ms
"Apply a list of decorators to a given function  <code> def apply_decorators(func, decorators): ","#LINE# #TAB# decorators = filter(_is_not_none_or_false, reversed(decorators)) #LINE# #TAB# for decorator in decorators: #LINE# #TAB# #TAB# func = decorator(func) #LINE# #TAB# return func"
"Takes in path arg , if string , casts to PurePath . Path objects are passed through , not cast back to PurePath  <code> def init_pure_path(path: Union[str, PurePath]) ->PurePath: ","#LINE# #TAB# if not isinstance(path, PurePath): #LINE# #TAB# #TAB# path = PurePath(path) #LINE# #TAB# return path"
"Delete a splunk user by email <code> def delete_user(email, profile=""splunk""): ","#LINE# #TAB# client = _get_splunk(profile) #LINE# #TAB# user = list_users(profile).get(email) #LINE# #TAB# if user: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# client.users.delete(user.name) #LINE# #TAB# #TAB# except (AuthenticationError, HTTPError) as e: #LINE# #TAB# #TAB# #TAB# log.info('Exception: %s', e) #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return False #LINE# #TAB# return user.name not in client.users"
Estimates a tree using fasttree : param alignment_fn : The aligned fasta format file to estimate a tree from . : return : The estimated tree filename if its created . Else False <code> def estimate_tree(alignment_fn): ,"#LINE# #TAB# base_path = os.path.split(alignment_fn)[0] #LINE# #TAB# filenamenoext = os.path.splitext(os.path.split(alignment_fn)[1])[0] #LINE# #TAB# outfn = os.path.join(base_path, filenamenoext + '_TREE.nwk') #LINE# #TAB# cmd = 'fasttree -nt -gtr < {} > {}'.format(alignment_fn, outfn) #LINE# #TAB# try: #LINE# #TAB# #TAB# print('Going to call cmd:\n' + cmd) #LINE# #TAB# #TAB# subprocess.call(cmd, shell=True) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print(e) #LINE# #TAB# #TAB# print('Something went wrong calling fasttree.') #LINE# #TAB# #TAB# return False #LINE# #TAB# return outfn"
Parse a configuration file / text using the given iterable  <code> def parse_words(iterable): ,#LINE# #TAB# p = MCUDefintitions() #LINE# #TAB# p.interpret(iterable) #LINE# #TAB# return p.memory_maps
"Given a list of parnames , including possible duplicates , returns a new list of parnames with duplicates prepended by one or more underscores to make them unique . This is also case insensitive  <code> def unique_parnames(names): ",#LINE# #TAB# upper_names = set() #LINE# #TAB# unique_names = [] #LINE# #TAB# for name in names: #LINE# #TAB# #TAB# name_upper = name.upper() #LINE# #TAB# #TAB# while name_upper in upper_names: #LINE# #TAB# #TAB# #TAB# name = '_' + name #LINE# #TAB# #TAB# #TAB# name_upper = '_' + name_upper #LINE# #TAB# #TAB# unique_names.append(name) #LINE# #TAB# #TAB# upper_names.add(name_upper) #LINE# #TAB# return unique_names
"Return the keyid of ' key_value '  <code> def get_keyid(keytype, scheme, key_value, hash_algorithm='sha256'): ","#LINE# #TAB# key_meta = format_keyval_to_metadata(keytype, scheme, key_value, #LINE# #TAB# #TAB# private=False) #LINE# #TAB# key_update_data = securesystemslib.formats.encode_canonical(key_meta) #LINE# #TAB# digest_object = securesystemslib.hash.digest(hash_algorithm) #LINE# #TAB# digest_object.update(key_update_data.encode('utf-8')) #LINE# #TAB# keyid = digest_object.hexdigest() #LINE# #TAB# return keyid"
Special behaviour when we have a multi line options <code> def split_multiline(value): ,#LINE# #TAB# value = [element for element in (line.strip() for line in value.split( #LINE# #TAB# #TAB# '\n')) if element and not element.startswith('#')] #LINE# #TAB# return value
Simple wrapper for graph to adjacency matrix . Parameters ---------- graph : object The networkx graph object . Returns ------- matrix : array The numpy adjacency matrix  <code> def networkx_to_adjacency(graph): ,"#LINE# #TAB# msg = 'Please pass an networkx graph object, not a {}'.format(type(graph)) #LINE# #TAB# assert isinstance(graph, nx.Graph), msg #LINE# #TAB# atomic_numbers = list(dict(graph.nodes('atomic_number')).values()) #LINE# #TAB# adjacency = np.asarray(nx.to_numpy_matrix(graph, dtype='f')) #LINE# #TAB# assert np.shape(adjacency) == (len(atomic_numbers), len(atomic_numbers)) #LINE# #TAB# adjacency += np.diag(atomic_numbers) #LINE# #TAB# return adjacency"
Convert a timestamp as defined in the soap interface to the time format stored in the database  <code> def timestamp_to_ordinal(timestamp): ,"#LINE# #TAB# if timestamp is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# ts_time = get_datetime(timestamp) #LINE# #TAB# ordinal_ts_time = Decimal(ts_time.toordinal()) #LINE# #TAB# total_seconds = (ts_time - datetime(ts_time.year, ts_time.month, #LINE# #TAB# #TAB# ts_time.day, 0, 0, 0)).total_seconds() #LINE# #TAB# fraction = (Decimal(repr(total_seconds)) / Decimal(86400)).quantize(Decimal #LINE# #TAB# #TAB# ('.00000000000000000001'), rounding=ROUND_HALF_UP) #LINE# #TAB# ordinal_ts_time += fraction #LINE# #TAB# log.debug('%s converted to %s', timestamp, ordinal_ts_time) #LINE# #TAB# return ordinal_ts_time"
"Merge ' dict1 ' and ' dict2 ' dicts into ' dict1 ' . ' dict2 ' has precedence over ' dict1 '  <code> def merge_dicts(dict1: Dict, dict2: Dict) ->Dict: ","#LINE# #TAB# for key, val in dict2.items(): #LINE# #TAB# #TAB# if val is not None: #LINE# #TAB# #TAB# #TAB# if isinstance(val, dict) and key in dict1: #LINE# #TAB# #TAB# #TAB# #TAB# dict1[key] = DataTypesUtils.merge_dicts(dict1[key], val) #LINE# #TAB# #TAB# #TAB# elif isinstance(val, list) and key in dict1: #LINE# #TAB# #TAB# #TAB# #TAB# dict1[key] += val #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# dict1[key] = val #LINE# #TAB# return dict1"
Remove None - valued keys from a dictionary recursively  <code> def clean_dict(data): ,"#LINE# #TAB# if is_mapping(data): #LINE# #TAB# #TAB# out = {} #LINE# #TAB# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# #TAB# if v is not None: #LINE# #TAB# #TAB# #TAB# #TAB# out[k] = clean_dict(v) #LINE# #TAB# #TAB# return out #LINE# #TAB# elif is_sequence(data): #LINE# #TAB# #TAB# return [clean_dict(d) for d in data if d is not None] #LINE# #TAB# return data"
Set the encode / decode to our own encrypt_encode / encrypt_decode  <code> def set_backend_funcs(): ,#LINE# #TAB# celery.backends.base.Backend.encode = _encrypt_encode #LINE# #TAB# celery.backends.base.Backend.decode = _decrypt_decode
"from buff and xpoints , returns ypoints = buff[xpoints ] eventually smoothed by moving average over 2*nsmooth+1 positions <code> def get_ypoints(buff, xpoints, nsmooth=0): ","#LINE# #TAB# nsmooth = abs(nsmooth) #LINE# #TAB# xp = np.array(xpoints).astype(int) #LINE# #TAB# y = np.zeros(len(xpoints)) #LINE# #TAB# for i in range(2 * nsmooth + 1): #LINE# #TAB# #TAB# xi = np.minimum(np.maximum(xp - i, 0), len(buff) - 1) #LINE# #TAB# #TAB# y += buff[xi] #LINE# #TAB# y /= 2 * nsmooth + 1 #LINE# #TAB# return y"
"splits the discrete trajectory into conditional sequences by starting state <code> def split_sequences_singletraj(dtraj, nstates, lag): ","#LINE# #TAB# sall = [[] for _ in range(nstates)] #LINE# #TAB# res_states = [] #LINE# #TAB# res_seqs = [] #LINE# #TAB# for t in range(len(dtraj)-lag): #LINE# #TAB# #TAB# sall[dtraj[t]].append(dtraj[t+lag]) #LINE# #TAB# for i in range(nstates): #LINE# #TAB# #TAB# if len(sall[i]) > 0: #LINE# #TAB# #TAB# #TAB# res_states.append(i) #LINE# #TAB# #TAB# #TAB# res_seqs.append(np.array(sall[i])) #LINE# #TAB# return res_states, res_seqs"
See if the user has one of their own local . cfg files for this task such as might be created automatically during the save of a read - only package and return their names  <code> def get_usr_cfg_files_for_py_pkg(pkgName): ,"#LINE# #TAB# thePkg, theFile = findCfgFileForPkg(pkgName, '.cfg') #LINE# #TAB# tname = getEmbeddedKeyVal(theFile, TASK_NAME_KEY) #LINE# #TAB# flist = getCfgFilesInDirForTask(getAppDir(), tname) #LINE# #TAB# return flist"
Split hparams based on key prefixes  <code> def parse_hparams(hparams): ,"#LINE# prefixes = [""agent_"", ""optimizer_"", ""runner_"", ""replay_buffer_""] #LINE# ret = [] #LINE# for prefix in prefixes: #LINE# #TAB# ret_dict = {} #LINE# #TAB# for key in hparams.values(): #LINE# #TAB# if prefix in key: #LINE# #TAB# #TAB# par_name = key[len(prefix):] #LINE# #TAB# #TAB# ret_dict[par_name] = hparams.get(key) #LINE# #TAB# ret.append(ret_dict) #LINE# return ret"
Returns maximum value of a dictionary  <code> def dict_max(dic): ,"#LINE# #TAB# aux = dict(map(lambda item: (item[1],item[0]),dic.items())) #LINE# #TAB# if aux.keys() == []: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# max_value = max(aux.keys()) #LINE# #TAB# return max_value,aux[max_value]"
Return True if a process with the given PID is currently running  <code> def pid_exists(pid): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# os.kill(pid, 0) #LINE# #TAB# except OSError as err: #LINE# #TAB# #TAB# return err.errno == errno.EPERM #LINE# #TAB# else: #LINE# #TAB# #TAB# return True"
Returns the total number of images in gallery <code> def get_gallery_size(datavar): ,"#LINE# #TAB# for line in datavar: #LINE# #TAB# #TAB# if 'images' in line: #LINE# #TAB# #TAB# #TAB# return len(line.split(',')) - 1 #LINE# #TAB# return None"
"Returns an attribute directly associated with this class ( as opposed to subclasses ) , setting default if necessary <code> def class_local(cls, name, default): ","#LINE# #TAB# result = cls.__dict__.get(name, default) #LINE# #TAB# setattr(cls, name, result) #LINE# #TAB# return result"
"I believe this adds zeros to avoid errors e.g. , 30yr loan vs 40yr loan <code> def add_zeros_to_array(array, target_row_count=480): ","#LINE# #TAB# target_row_count = int(target_row_count) #LINE# #TAB# row_count = np.ma.size(array, axis=0) #LINE# #TAB# col_count = np.ma.size(array, axis=1) #LINE# #TAB# additional_rows = target_row_count - row_count #LINE# #TAB# one_row = np.repeat(0, col_count) #LINE# #TAB# empty_rows = np.tile(one_row, (additional_rows, 1)) #LINE# #TAB# new_array = np.append(array, empty_rows, axis=0) #LINE# #TAB# return new_array"
"n - c : carbon - dioxide bisulfate [ HMW84 ]  <code> def lambd_co2_hso4_hmw84(T, P): ","#LINE# #TAB# lambd = -0.003 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return lambd, valid"
"Send HttpResponseRedirect to LOGOUT_URL . ` msg ` is a message displayed on the login page after the logout , to explain the logout reason  <code> def logout_with_message(request, msg, redirect=True, status='success'): ","#LINE# #TAB# logout(request) #LINE# #TAB# if redirect: #LINE# #TAB# #TAB# response = http.HttpResponseRedirect('%s?next=%s' % (settings. #LINE# #TAB# #TAB# #TAB# LOGOUT_URL, request.path)) #LINE# #TAB# else: #LINE# #TAB# #TAB# response = http.HttpResponseRedirect(settings.LOGOUT_URL) #LINE# #TAB# add_logout_reason(request, response, msg, status) #LINE# #TAB# return response"
"Convert a poly model field to an OpenAPI 3 SchemaObject . : param field : the field to be converted : param apistrap : the extension used for adding reusable schema definitions : return : a schema <code> def poly_model_field_to_schema_object(field: PolyModelType, apistrap: ","#LINE# #TAB# Optional[Apistrap]) ->Dict[str, Any]: #LINE# #TAB# return {'anyOf': [schematics_model_to_schema_object(model, apistrap) for #LINE# #TAB# #TAB# model in field.model_classes]}"
"Guess the input type of the parameter based off the default value , if unknown use text <code> def guess_type(val): ","#LINE# #TAB# if isinstance(val, bool): #LINE# #TAB# #TAB# return 'choice' #LINE# #TAB# elif isinstance(val, int): #LINE# #TAB# #TAB# return 'number' #LINE# #TAB# elif isinstance(val, float): #LINE# #TAB# #TAB# return 'number' #LINE# #TAB# elif isinstance(val, str): #LINE# #TAB# #TAB# return 'text' #LINE# #TAB# elif hasattr(val, 'read'): #LINE# #TAB# #TAB# return 'file' #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'text'"
"Returns the Fock state Args : n ( int ) : the occupation number fock_dim ( int ) : the size of the truncated Fock basis Returns : array : the Fock state <code> def fock_state(n, fock_dim=5): ",#LINE# #TAB# ket = np.zeros(fock_dim) #LINE# #TAB# ket[n] = 1.0 #LINE# #TAB# return ket
"- > boolean : True - valid syntax - > boolean : False - invalid syntax <code> def syntax_valid(ds, r=constant.DURATION_REGEX): ",#LINE# #TAB# m = re.compile(r).match(ds) #LINE# #TAB# if m: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
This method generates a dictionary of the query string parameters contained in a given editable URL  <code> def build_editable_options(req): ,"#LINE# #TAB# regexp = re.compile(r""[\?#&](?P<name>[^&=]+)=(?P<value>[^&=]+)"") #LINE# #TAB# matched = regexp.findall(req) #LINE# #TAB# if matched: #LINE# #TAB# #TAB# ret = dict() #LINE# #TAB# #TAB# for option in matched: #LINE# #TAB# #TAB# #TAB# (name, value) = option #LINE# #TAB# #TAB# #TAB# if name in ret: #LINE# #TAB# #TAB# #TAB# #TAB# raise Exception(""%s option already defined"" % name) #LINE# #TAB# #TAB# #TAB# ret[name] = value #LINE# #TAB# #TAB# return ret #LINE# #TAB# return None"
Apply a single ' all ' label to a given bounding box . This bounding box must be as specified by the : map:`bounding_box ` method  <code> def bounding_box_to_bounding_box(bbox): ,"#LINE# #TAB# from menpo.shape import bounding_box #LINE# #TAB# mapping = OrderedDict() #LINE# #TAB# mapping['all'] = np.arange(4) #LINE# #TAB# return bounding_box(bbox.points[0], bbox.points[2]), mapping"
"Make a temporary python file return filename and filehandle  <code> def temp_pyfile(src, ext='.py'): ","#LINE# #TAB# fname = tempfile.mkstemp(ext)[1] #LINE# #TAB# f = open(fname,'w') #LINE# #TAB# f.write(src) #LINE# #TAB# f.flush() #LINE# #TAB# return fname, f"
"Split line into ( numbers , text )  <code> def nums_and_text(line): ","#LINE# #TAB# tokens = line.split() #LINE# #TAB# text = '' #LINE# #TAB# numbers = [] #LINE# #TAB# for tok in tokens: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# numbers.append(tonumber(tok)) #LINE# #TAB# #TAB# except ValueError: #LINE# #TAB# #TAB# #TAB# text += ' ' + tok #LINE# #TAB# return numbers, text"
"getattr for a dot separated path <code> def getattr_path(obj, path): ","#LINE# #TAB# if not path: #LINE# #TAB# #TAB# return None #LINE# #TAB# for attr in path.split('.'): #LINE# #TAB# #TAB# obj = getattr(obj, attr, None) #LINE# #TAB# return obj"
Check if a given object is a record  <code> def is_record(record: object) ->bool: ,"#LINE# #TAB# if not isinstance(record, Record): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not dataclasses.is_dataclass(record): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
returns theta but with angle ( theta[0 ] ) and displacement ( theta[1 ] ) to gradient and intercept <code> def angle_to_gradient(theta): ,#LINE# #TAB# t = np.asarray(theta).copy() #LINE# #TAB# t[0] = np.tan(theta[0]) #LINE# #TAB# t[1] = theta[1] / np.cos(theta[0]) #LINE# #TAB# return t
Import a dotted module path and return the attribute / class designated by the last name in the path . Raise ImportError if the import failed  <code> def import_string(dotted_path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if not isinstance(dotted_path, str): #LINE# #TAB# #TAB# #TAB# return dotted_path #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = importlib.import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""%s"" does not define a ""%s"" attribute/class' % ( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
List LVM volume group associated with a given block device  <code> def list_lvm_volume_group(block_device): ,"#LINE# #TAB# vg = None #LINE# #TAB# pvd = check_output(['pvdisplay', block_device]).splitlines() #LINE# #TAB# for lvm in pvd: #LINE# #TAB# #TAB# lvm = lvm.decode('UTF-8') #LINE# #TAB# #TAB# if lvm.strip().startswith('VG Name'): #LINE# #TAB# #TAB# #TAB# vg = ' '.join(lvm.strip().split()[2:]) #LINE# #TAB# return vg"
gets the pks of an uml class  <code> def get_pks(klass): ,#LINE# #TAB# res = [f for f in klass.filtereditems(IProperty) if f.stereotype( #LINE# #TAB# #TAB# 'sql:primary')] #LINE# #TAB# if not res: #LINE# #TAB# #TAB# inhs = Inheritance(klass) #LINE# #TAB# #TAB# for inh in inhs.values(): #LINE# #TAB# #TAB# #TAB# if inh.context.stereotype('sql:sql_content'): #LINE# #TAB# #TAB# #TAB# #TAB# res = get_pks(inh.context) #LINE# #TAB# #TAB# #TAB# #TAB# if res: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return res #LINE# #TAB# return res
"Utility function for ploting predicted probabilities as bar plots <code> def plot_to_array(x, y, color): ","#LINE# #TAB# fig = plt.figure(figsize=(2, 2)) #LINE# #TAB# fig.add_subplot(111) #LINE# #TAB# plt.barh(x, y, color=color) #LINE# #TAB# fig.canvas.draw() #LINE# #TAB# data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8) #LINE# #TAB# data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,)) #LINE# #TAB# plt.clf() #LINE# #TAB# return data"
"Generates a dictionary with safe keys and values to pass onto Neo4j <code> def extract_params_from_query(query, user_ns): ","#LINE# #TAB# params = {} #LINE# #TAB# for k, v in user_ns.items(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# json.dumps(v) #LINE# #TAB# #TAB# #TAB# params[k] = v #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return params"
"Break a iterable on the item that matches the predicate into lists  <code> def break_iterable(iterable, pred): ",#LINE# #TAB# sublist = [] #LINE# #TAB# for i in iterable: #LINE# #TAB# #TAB# if pred(i): #LINE# #TAB# #TAB# #TAB# yield sublist #LINE# #TAB# #TAB# #TAB# sublist = [] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# sublist.append(i) #LINE# #TAB# yield sublist
"Extracts a file 's contents from a "" file "" input in a CGI form , or None if no such file was uploaded  <code> def get_cgi_parameter_file(form: cgi.FieldStorage, key: str) ->Optional[bytes]: ","#LINE# #TAB# filename, filecontents = get_cgi_parameter_filename_and_file(form, key) #LINE# #TAB# return filecontents"
Iterate through all the single - axis neighbors of a coordinate tuple . Order is deterministic  <code> def iterate_neighbors(pos): ,"#LINE# #TAB# for axis, val in enumerate(pos): #LINE# #TAB# #TAB# for offset in [-1, 1]: #LINE# #TAB# #TAB# #TAB# yield pos[:axis] + (val + offset,) + pos[axis + 1:]"
Calculate the total number of types for each part of speech taggers  <code> def get_pos_counts(tagged_text): ,#LINE# #TAB# pos_counts = {} #LINE# #TAB# for item in tagged_text: #LINE# #TAB# #TAB# if item[1] in pos_counts.keys(): #LINE# #TAB# #TAB# #TAB# pos_counts[item[1]] += 1 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pos_counts.update({item[1]: 1}) #LINE# #TAB# return pos_counts
"Take mask and will create a key to unmask suspected data , then check if the xor'd data matches a regex pattern <code> def deobfuscate_simple(d, r, m): ","#LINE# #TAB# import re #LINE# #TAB# max_mask = m.lower() #LINE# #TAB# for i in range(1, len(max_mask) + 1): #LINE# #TAB# #TAB# t_mask = max_mask[:i] #LINE# #TAB# #TAB# r_mask = xor_simple(d[:i], t_mask) #LINE# #TAB# #TAB# de_enc = xor_simple(d, r_mask) #LINE# #TAB# #TAB# if re.match(r, de_enc): #LINE# #TAB# #TAB# #TAB# return de_enc.strip(), r_mask #LINE# #TAB# return None, None"
"Computes the partisan bias for the given ElectionResults . The partisan bias is defined as the number of districts with above - mean vote share by the first party divided by the total number of districts , minus 1/2  <code> def partisan_bias(election_results): ",#LINE# #TAB# first_party = election_results.election.parties[0] #LINE# #TAB# party_shares = numpy.array(election_results.percents(first_party)) #LINE# #TAB# mean_share = numpy.mean(party_shares) #LINE# #TAB# above_mean_districts = len(party_shares[party_shares > mean_share]) #LINE# #TAB# return above_mean_districts / len(party_shares) - 0.5
"Tests whether T is a transition matrix <code> def is_transition_matrix(T, tol=1e-10): ","#LINE# #TAB# if T.ndim != 2: #LINE# #TAB# #TAB# return False #LINE# #TAB# if T.shape[0] != T.shape[1]: #LINE# #TAB# #TAB# return False #LINE# #TAB# dim = T.shape[0] #LINE# #TAB# X = np.abs(T) - T #LINE# #TAB# x = np.sum(T, axis=1) #LINE# #TAB# return np.abs(x - np.ones(dim)).max() < dim * tol and X.max() < 2.0 * tol"
"Calculate the ISB pelvis anatomic coordinate system axes : x - anterior , y - superior , z - right <code> def create_pelvis_acsisb(lasis, rasis, lpsis, rpsis): ","#LINE# #TAB# oa = (lasis + rasis) / 2.0 #LINE# #TAB# op = (lpsis + lpsis) / 2.0 #LINE# #TAB# z = normaliseVector(rasis - lasis) #LINE# #TAB# n1 = normaliseVector(numpy.cross(rasis - op, lasis - op)) #LINE# #TAB# x = normaliseVector(numpy.cross(n1, z)) #LINE# #TAB# y = normaliseVector(numpy.cross(z, x)) #LINE# #TAB# return oa, x, y, z"
Factory for record bucket generation  <code> def default_bucket_link_factory(pid): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# record = Record.get_record(pid.get_assigned_object()) #LINE# #TAB# #TAB# bucket = record.files.bucket #LINE# #TAB# #TAB# return url_for('invenio_files_rest.bucket_api', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# bucket_id=bucket.id, _external=True) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None"
"Update module_options with any options defined in module path  <code> def load_module_opts_from_file(path, module_options): ","#LINE# #TAB# module_options_file = os.path.join(path, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'runway.module.yml') #LINE# #TAB# if os.path.isfile(module_options_file): #LINE# #TAB# #TAB# with open(module_options_file, 'r') as stream: #LINE# #TAB# #TAB# #TAB# module_options = merge_dicts(module_options, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# yaml.safe_load(stream)) #LINE# #TAB# return module_options"
"Log likelihood for a categorical random variable <code> def log_lik_categorical(ys, ps): ","#LINE# #TAB# ret = 0.0 #LINE# #TAB# for y, p in zip(ys, ps): #LINE# #TAB# #TAB# ret += y * np.log(p + EPS) #LINE# #TAB# return ret"
"radi_ev is the equal volume radius e is the ratio of the diameter to the heigth <code> def rsp_cilinder(x, radi_ev, e): ","#LINE# #TAB# height = radi_ev * (2 / (3 * e ** 2)) ** (1 / 3) #LINE# #TAB# radius = height * e #LINE# #TAB# si = np.sqrt(1 - x ** 2) #LINE# #TAB# cond = si / x > radius / height #LINE# #TAB# rad = np.where(cond, radius / si, -height / x) #LINE# #TAB# r_theta = np.where(cond, radius * x / si ** 2, height * si / x ** 2) #LINE# #TAB# r = rad * 2 #LINE# #TAB# dr = -r_theta / rad #LINE# #TAB# return r, dr"
"r Average daily precipitation intensity <code> def daily_pr_intensity(pr, thresh='1 mm/day', freq='YS'): ","#LINE# #TAB# t = utils.convert_units_to(thresh, pr, 'hydro') #LINE# #TAB# pr_wd = xr.where(pr >= t, pr, 0) #LINE# #TAB# pr_wd.attrs['units'] = pr.units #LINE# #TAB# s = pr_wd.resample(time=freq).sum(dim='time', keep_attrs=True) #LINE# #TAB# sd = utils.pint_multiply(s, 1 * units.day, 'mm') #LINE# #TAB# wd = wetdays(pr, thresh=thresh, freq=freq) #LINE# #TAB# return sd / wd"
"Loads the TensorRec model and TensorFlow session saved in the given directory . : param directory_path : str The path to the directory containing the saved model . : return : <code> def load_model(cls, directory_path): ","#LINE# #TAB# graph_path = os.path.join(directory_path, 'tensorrec_session.cpkt.meta') #LINE# #TAB# saver = tf.train.import_meta_graph(graph_path) #LINE# #TAB# session_path = os.path.join(directory_path, 'tensorrec_session.cpkt') #LINE# #TAB# saver.restore(sess=get_session(), save_path=session_path) #LINE# #TAB# tensorrec_path = os.path.join(directory_path, 'tensorrec.pkl') #LINE# #TAB# with open(tensorrec_path, 'rb') as file: #LINE# #TAB# #TAB# model = pickle.load(file=file) #LINE# #TAB# model._attach_graph_hooks() #LINE# #TAB# return model"
The invariant for all Vector operations . Returns ` True ` when the elements are correctly ordered  <code> def is_ordered(v): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# it = v.iterkeys() #LINE# #TAB# #TAB# x = next(it) #LINE# #TAB# #TAB# while True: #LINE# #TAB# #TAB# #TAB# last, x = x, next(it) #LINE# #TAB# #TAB# #TAB# if x.uid < last.uid: #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# return True"
"merge two lists by overwriting the original with the second one if the key exists : param a1 : : param a2 : : return : <code> def merge_list_no_duplicates(a1, a2): ","#LINE# #TAB# r = [] #LINE# #TAB# if isinstance(a1, list) and isinstance(a2, list): #LINE# #TAB# #TAB# r = a1 #LINE# #TAB# #TAB# for i in a2: #LINE# #TAB# #TAB# #TAB# if i not in a1: #LINE# #TAB# #TAB# #TAB# #TAB# r.append(i) #LINE# #TAB# return r"
"Converts size in characters to size in pixels : param size : size in characters , rows : return : size in pixels , pixels <code> def convert_tkinter_size_to_wx(size): ","#LINE# #TAB# qtsize = size #LINE# #TAB# if size[1] is not None and size[1] < DEFAULT_PIXEL_TO_CHARS_CUTOFF: #LINE# #TAB# #TAB# qtsize = size[0] * DEFAULT_PIXELS_TO_CHARS_SCALING[0], size[1 #LINE# #TAB# #TAB# #TAB# ] * DEFAULT_PIXELS_TO_CHARS_SCALING[1] #LINE# #TAB# return qtsize"
Parse SDFile data part into dict <code> def optional_data(lines): ,"#LINE# #TAB# data = {} #LINE# #TAB# exp = re.compile(r"">.*?<([\w ]+)>"") #LINE# #TAB# for i, line in enumerate(lines): #LINE# #TAB# #TAB# result = exp.match(line) #LINE# #TAB# #TAB# if result: #LINE# #TAB# #TAB# #TAB# data[result.group(1)] = lines[i + 1] #LINE# #TAB# return data"
Return a list of the reserved characters  <code> def list_url_unsafe_chars(name): ,#LINE# #TAB# reserved_chars = '' #LINE# #TAB# for i in name: #LINE# #TAB# #TAB# if i in URL_RESERVED_CHARS: #LINE# #TAB# #TAB# #TAB# reserved_chars += i #LINE# #TAB# return reserved_chars
"saves a list of iterables , commonly they are length 2 and contain floats , but the fmt parameter can be changed for flexibility pickles them to a file named using the string sequence_filename , in the directory that this module is located in <code> def save_sequence(order, sequence, fmt='%.18e', delimiter=', '): ","#LINE# #TAB# check_minimum_length_of_sequence(sequence) #LINE# #TAB# np.savetxt(path_to_sequence_file(order), sequence, fmt=fmt, delimiter= #LINE# #TAB# #TAB# delimiter) #LINE# #TAB# return"
"load yaml file given a file string - like file path . return must be a dictionary . : param yaml_path : : param verbose : : return : <code> def yaml_load(yaml_path: Union[Path, str], verbose=False) ->Dict[str, Any]: ","#LINE# #TAB# assert isinstance(yaml_path, (Path, str, PosixPath)), type(yaml_path) #LINE# #TAB# with open(str(yaml_path), 'r') as stream: #LINE# #TAB# #TAB# data_loaded: dict = yaml.safe_load(stream) #LINE# #TAB# if verbose: #LINE# #TAB# #TAB# print(f'Loaded yaml path:{str(yaml_path)}') #LINE# #TAB# #TAB# pprint(data_loaded) #LINE# #TAB# return data_loaded"
Get a stream 's local address  <code> def get_stream_address(stream): ,#LINE# #TAB# if stream.closed(): #LINE# #TAB# #TAB# return '<closed>' #LINE# #TAB# try: #LINE# #TAB# #TAB# return unparse_host_port(*stream.socket.getsockname()[:2]) #LINE# #TAB# except EnvironmentError: #LINE# #TAB# #TAB# return '<closed>'
Render actions available  <code> def application_actions(context): ,"#LINE# #TAB# return {'roles': context['roles'], 'actions': context['actions'], #LINE# #TAB# #TAB# 'extra': ''}"
"Get a dictionary where k = header name , v = column index . : param infile : CSV file to read from : return : dictionary where k = header name , v = column index <code> def get_headers(infile): ",#LINE# #TAB# headers_map = {} #LINE# #TAB# with open(infile) as csvfile: #LINE# #TAB# #TAB# csvreader = csv.reader(csvfile) #LINE# #TAB# #TAB# for line in csvreader: #LINE# #TAB# #TAB# #TAB# for i in range(len(line)): #LINE# #TAB# #TAB# #TAB# #TAB# headers_map[line[i]] = i #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return headers_map
"converts a numpy array to a GPUArray <code> def to_gpu(ary, allocator=drv.mem_alloc): ","#LINE# #TAB# result = GPUArray(ary.shape, ary.dtype, allocator, strides= #LINE# #TAB# #TAB# _compact_strides(ary)) #LINE# #TAB# result.set(ary) #LINE# #TAB# return result"
Handle resource management within an executable file  <code> def bundle_dir(): ,#LINE# #TAB# if frozen(): #LINE# #TAB# #TAB# directory = sys._MEIPASS #LINE# #TAB# else: #LINE# #TAB# #TAB# directory = os.path.dirname(os.path.abspath(stack()[1][1])) #LINE# #TAB# if os.path.exists(directory): #LINE# #TAB# #TAB# return directory
Suppress ProgressBar 's warning  <code> def get_progressbar(): ,#LINE# #TAB# with suppress_warnings(): #LINE# #TAB# #TAB# from progressbar import ProgressBar #LINE# #TAB# return ProgressBar
import module from python file path and return imported module <code> def get_imported_module_from_file(file_path): ,"#LINE# #TAB# #TAB# if p_compat.is_py3: #LINE# #TAB# #TAB# #TAB# imported_module = importlib.machinery.SourceFileLoader('module_name', file_path).load_module() #LINE# #TAB# #TAB# elif p_compat.is_py2: #LINE# #TAB# #TAB# #TAB# imported_module = imp.load_source('module_name', file_path) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise RuntimeError(""Neither Python 3 nor Python 2."") #LINE# #TAB# #TAB# return imported_module"
Get the select options for the application selector <code> def get_application_choices(): ,"#LINE# #TAB# result = [] #LINE# #TAB# keys = set() #LINE# #TAB# for ct in ContentType.objects.order_by('app_label', 'model'): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if issubclass(ct.model_class(), TranslatableModel) and ct.app_label not in keys: #LINE# #TAB# #TAB# #TAB# #TAB# result.append(('{}'.format(ct.app_label), '{}'.format(ct.app_label.capitalize()))) #LINE# #TAB# #TAB# #TAB# #TAB# keys.add(ct.app_label) #LINE# #TAB# #TAB# except TypeError: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# return result"
"Format a path in utf8 quoting it if necessary  <code> def format_path(p, quote_spaces=False): ","#LINE# #TAB# if b'\n' in p: #LINE# #TAB# #TAB# p = re.sub(b'\n', b'\\n', p) #LINE# #TAB# #TAB# quote = True #LINE# #TAB# else: #LINE# #TAB# #TAB# quote = p[0] == b'""' or (quote_spaces and b' ' in p) #LINE# #TAB# if quote: #LINE# #TAB# #TAB# extra = GIT_FAST_IMPORT_NEEDS_EXTRA_SPACE_AFTER_QUOTE and b' ' or b'' #LINE# #TAB# #TAB# p = b'""' + p + b'""' + extra #LINE# #TAB# return p"
"Resolve dict type based on string Parameters ---------- dict_type : [ ' ndict ' , ' dict ' , ' odict ' ] Dict - type in string format Returns ------- class <code> def resolve_dict_type(dict_type): ","#LINE# #TAB# if dict_type in [dict, col.OrderedDict, NestedDict]: #LINE# #TAB# #TAB# dict_class = dict_type #LINE# #TAB# elif dict_type is None or dict_type == 'ndict': #LINE# #TAB# #TAB# dict_class = NestedDict #LINE# #TAB# elif dict_type == 'dict': #LINE# #TAB# #TAB# dict_class = dict #LINE# #TAB# elif dict_type == 'odict': #LINE# #TAB# #TAB# dict_class = col.OrderedDict #LINE# #TAB# else: #LINE# #TAB# #TAB# raise KeyError(dict_type) #LINE# #TAB# return dict_class"
Load a GOParser object from a pickle file . The function automatically detects whether the file is compressed with gzip . Parameters ---------- fn : str Path of the pickle file . Returns ------- ` GOParser ` The GOParser object stored in the pickle file  <code> def read_pickle(fn): ,"#LINE# #TAB# with misc.open_plain_or_gzip(fn, 'rb') as fh: #LINE# #TAB# #TAB# parser = pickle.load(fh) #LINE# #TAB# return parser"
"Reads first nlines lines of Combosaurus data from filename_in and writes shortened data to filename_out <code> def shorten_dataset(filename_in, filename_out, nlines=100000): ","#LINE# #TAB# with open(filename_in) as fin, open(filename_out, 'w') as fout: #LINE# #TAB# #TAB# for line in fin.readlines(): #LINE# #TAB# #TAB# #TAB# fout.write(line) #LINE# #TAB# #TAB# #TAB# nlines -= 1 #LINE# #TAB# #TAB# #TAB# if nlines == 0: #LINE# #TAB# #TAB# #TAB# #TAB# return"
Return the source of a header in a partially preprocessed state  <code> def get_header(name: str) ->str: ,"#LINE# #TAB# with open(name, 'r') as f: #LINE# #TAB# #TAB# header = f.read() #LINE# #TAB# header = RE_REMOVALS.sub('', header) #LINE# #TAB# header = RE_COMMENT.sub('', header) #LINE# #TAB# header = RE_ENDIAN.sub('\\1' if sys.byteorder == 'little' else '\\2', #LINE# #TAB# #TAB# header) #LINE# #TAB# header = RE_ENDIAN2.sub('\\1' if sys.byteorder != 'little' else '\\2', #LINE# #TAB# #TAB# header) #LINE# #TAB# header = header.replace('typedef int SDL_bool;', '') #LINE# #TAB# return header"
Convert key - value dictionary into CLI argument list  <code> def to_argv(trial: dict) ->List[str]: ,"#LINE# #TAB# argv = [] #LINE# #TAB# for k, v in trial.items(): #LINE# #TAB# #TAB# if v is not None: #LINE# #TAB# #TAB# #TAB# arg = '' #LINE# #TAB# #TAB# #TAB# if isinstance(v, bool): #LINE# #TAB# #TAB# #TAB# #TAB# if v is True: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# arg = '--{}'.format(k) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# arg = '--{}={}'.format(k, v) #LINE# #TAB# #TAB# #TAB# if arg: #LINE# #TAB# #TAB# #TAB# #TAB# argv.append(arg) #LINE# #TAB# return argv"
Set all datetimes to system time . The ` ` utc ` ` parameter of other methods will be used . This can be changed by calling ` ` set_utc ( ) ` ` . .. versionadded : : 0.1.0 <code> def unset_utc(): ,#LINE# #TAB# global _FORCE_UTC #LINE# #TAB# _FORCE_UTC = False
"Determine each polygons node ( x , y ) coordinates from _ extract_all_polygons ( ) <code> def polygon_coords(nodes, mesh_polygons): ","#LINE# #TAB# poly_coords = [] #LINE# #TAB# for p in mesh_polygons: #LINE# #TAB# #TAB# poly_coords.append(nodes[(p - 1), :2]) #LINE# #TAB# return poly_coords"
Checks whether ` chars ` is a control character  <code> def is_control(char): ,#LINE# #TAB# if char == '\t' or char == '\n' or char == '\r': #LINE# #TAB# #TAB# return False #LINE# #TAB# cat = unicodedata.category(char) #LINE# #TAB# if cat.startswith('C'): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Parse "" state "" column from ` qstat ` output for given job_id Returns state for the * first * job matching job_id . Returns ' u ' if ` qstat ` output is empty or job_id is not found  <code> def parse_qstat_state(qstat_out, job_id): ","#LINE# #TAB# if qstat_out.strip() == '': #LINE# #TAB# #TAB# return 'u' #LINE# #TAB# lines = qstat_out.split(b'\n') #LINE# #TAB# while not lines.pop(0).startswith(b'---'): #LINE# #TAB# #TAB# pass #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line: #LINE# #TAB# #TAB# #TAB# job, prior, name, user, state = line.strip().split()[0:5] #LINE# #TAB# #TAB# #TAB# job = job.decode('utf-8') #LINE# #TAB# #TAB# #TAB# job = job.split('.')[0] #LINE# #TAB# #TAB# #TAB# if int(job) == int(job_id): #LINE# #TAB# #TAB# #TAB# #TAB# return state #LINE# #TAB# return 'u'"
Temporarily captures / redirects stdout  <code> def output_capturing(): ,#LINE# #TAB# out = sys.stdout #LINE# #TAB# sys.stdout = StringIO() #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# sys.stdout = out
gets a single products metadata <code> def get_product_metadata_path(product_name): ,"#LINE# #TAB# string_date = product_name.split('_')[-1] #LINE# #TAB# date = datetime.datetime.strptime(string_date, '%Y%m%dT%H%M%S') #LINE# #TAB# path = 'products/{0}/{1}/{2}/{3}'.format(date.year, date.month, date.day, product_name) #LINE# #TAB# return { #LINE# #TAB# #TAB# product_name: { #LINE# #TAB# #TAB# #TAB# 'metadata': '{0}/{1}'.format(path, 'metadata.xml'), #LINE# #TAB# #TAB# #TAB# 'tiles': get_tile_metadata_path('{0}/{1}'.format(path, 'productInfo.json')) #LINE# #TAB# #TAB# } #LINE# #TAB# }"
Generate a ' random ' key of length ' length ' We are using pseudorandom since we are not a bank  <code> def random_key(length): ,#LINE# #TAB# key = '' #LINE# #TAB# for _ in range(length): #LINE# #TAB# #TAB# key += random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ') #LINE# #TAB# return key
"Strips the accents from the defined substring of the given text . Works for text both in unicode NFD and NFC format <code> def strip_accents(text, start_index=0, end_index=-1): ","#LINE# #TAB# if end_index == -1: #LINE# #TAB# #TAB# end_index = len(text) #LINE# #TAB# text_to_consider = text[start_index:end_index] #LINE# #TAB# norm_text = unicodedata.normalize('NFD', text_to_consider) #LINE# #TAB# output = [] #LINE# #TAB# for char in norm_text: #LINE# #TAB# #TAB# cat = unicodedata.category(char) #LINE# #TAB# #TAB# if cat == 'Mn': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# output.append(char) #LINE# #TAB# res_text = text[0:start_index] + ''.join(output) + text[end_index:] #LINE# #TAB# return res_text"
"> > > freq = octave_space(4 , 32 , 1 ) > > > print(freq ) [ 4 . 8 . 16 . 32 . ] <code> def octave_space(lb, ub, step): ","#LINE# #TAB# lbi = round(np.log2(lb) / step) * step #LINE# #TAB# ubi = round(np.log2(ub) / step) * step #LINE# #TAB# x = np.arange(lbi, ubi + step, step) #LINE# #TAB# return 2 ** x"
Create dialect specific actions  <code> def create_dialect_actions(dialect): ,"#LINE# #TAB# CompAction = SCons.Action.Action('$%sCOM ' % dialect, '$%sCOMSTR' % dialect) #LINE# #TAB# CompPPAction = SCons.Action.Action('$%sPPCOM ' % dialect, '$%sPPCOMSTR' % dialect) #LINE# #TAB# ShCompAction = SCons.Action.Action('$SH%sCOM ' % dialect, '$SH%sCOMSTR' % dialect) #LINE# #TAB# ShCompPPAction = SCons.Action.Action('$SH%sPPCOM ' % dialect, '$SH%sPPCOMSTR' % dialect) #LINE# #TAB# return CompAction, CompPPAction, ShCompAction, ShCompPPAction"
"Build foreman - maintain advanced procedure run maintenance - mode - disable <code> def run_disable_maintenance_mode(cls, options=None): ",#LINE# #TAB# cls.command_sub = 'maintenance-mode-disable' #LINE# #TAB# if options is None: #LINE# #TAB# #TAB# options = {} #LINE# #TAB# result = cls._construct_command(options) #LINE# #TAB# return result
"This function calculates the slope and aspect of a DEM map  <code> def calc_gradient(dataset, pixel_spacing): ","#LINE# #TAB# deg2rad = np.pi / 180.0 #LINE# #TAB# rad2deg = 180.0 / np.pi #LINE# #TAB# x, y = np.gradient(dataset, pixel_spacing, pixel_spacing) #LINE# #TAB# hypotenuse_array = np.hypot(x, y) #LINE# #TAB# slope = np.arctan(hypotenuse_array) * rad2deg #LINE# #TAB# aspect = np.arctan2(y / pixel_spacing, -x / pixel_spacing) * rad2deg #LINE# #TAB# aspect = 180 + aspect #LINE# #TAB# return deg2rad, rad2deg, slope, aspect"
Return processor by processing a given sheet of a spreadsheet file  <code> def process_table(fname): ,"#LINE# #TAB# book = openpyxl.load_workbook(fname, read_only=True) #LINE# #TAB# try: #LINE# #TAB# #TAB# rel_sheet = book['Relations'] #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# rel_sheet = book['Causal'] #LINE# #TAB# event_sheet = book['Events'] #LINE# #TAB# entities_sheet = book['Entities'] #LINE# #TAB# sp = SofiaExcelProcessor(rel_sheet.rows, event_sheet.rows, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# entities_sheet.rows) #LINE# #TAB# return sp"
Returns unique items in list_ in the order they were seen  <code> def unique_ordered(list_): ,"#LINE# #TAB# list_ = list(list_) #LINE# #TAB# flag_list = flag_unique_items(list_) #LINE# #TAB# unique_list = compress(list_, flag_list) #LINE# #TAB# return unique_list"
"Pipe sort by name cmd in case sort by coordinates <code> def prepare_bam_file(bam_file, tmp_dir, config): ","#LINE# #TAB# sort_mode = _get_sort_order(bam_file, config) #LINE# #TAB# if sort_mode != ""queryname"": #LINE# #TAB# #TAB# bam_file = sort(bam_file, config, ""queryname"") #LINE# #TAB# return bam_file"
"Logarithmic probability distribution : evaluation : Observed data to compared with simulation data . : type : list : simulation : simulation data to compared with evaluation data : type : list : return : Logarithmic probability distribution : rtype : float <code> def log_p(evaluation, simulation): ",#LINE# #TAB# scale = np.mean(evaluation) / 10 #LINE# #TAB# if scale < 0.01: #LINE# #TAB# #TAB# scale = 0.01 #LINE# #TAB# if len(evaluation) == len(simulation): #LINE# #TAB# #TAB# y = (np.array(evaluation) - np.array(simulation)) / scale #LINE# #TAB# #TAB# normpdf = -y ** 2 / 2 - np.log(np.sqrt(2 * np.pi)) #LINE# #TAB# #TAB# return np.mean(normpdf) #LINE# #TAB# else: #LINE# #TAB# #TAB# logging.warning( #LINE# #TAB# #TAB# #TAB# 'evaluation and simulation lists does not have the same length.') #LINE# #TAB# #TAB# return np.nan
Get meta - information about a file  <code> def get_file_meta(filepath): ,"#LINE# #TAB# meta = {} #LINE# #TAB# meta['filepath'] = os.path.abspath(filepath) #LINE# #TAB# meta['creation_datetime'] = get_creation_datetime(filepath) #LINE# #TAB# meta['last_access_datetime'] = get_access_datetime(filepath) #LINE# #TAB# meta['modification_datetime'] = get_modification_datetime(filepath) #LINE# #TAB# try: #LINE# #TAB# #TAB# import magic #LINE# #TAB# #TAB# f_mime = magic.Magic(mime=True, uncompress=True) #LINE# #TAB# #TAB# f_other = magic.Magic(mime=False, uncompress=True) #LINE# #TAB# #TAB# meta['mime'] = f_mime.from_file(meta['filepath']) #LINE# #TAB# #TAB# meta['magic-type'] = f_other.from_file(meta['filepath']) #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return meta"
Verify whether there is a provider for the URL  <code> def has_provider_for_url(url): ,#LINE# #TAB# registry = get_oembed_providers() #LINE# #TAB# return registry.provider_for_url(url) is not None
Iterator over the tickets of a client <code> def tickets_of_client(client): ,#LINE# #TAB# for project in interfaces.IProjectContainer(client).objects(): #LINE# #TAB# #TAB# for ticket in interfaces.ITickets(project): #LINE# #TAB# #TAB# #TAB# yield ticket
check the first 100 reads to see if a FASTQ file has already been transformed by umis <code> def is_transformed(fastq): ,"#LINE# #TAB# with open_fastq(fastq) as in_handle: #LINE# #TAB# #TAB# for line in islice(in_handle, 400): #LINE# #TAB# #TAB# #TAB# if ""UMI_"" in line: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Returns a tuple ( ahead , behind ) describing how far from the remote ref the local ref is  <code> def stat_ahead_behind(git, local_ref, remote_ref): ","#LINE# #TAB# behind = 0 #LINE# #TAB# ahead = 0 #LINE# #TAB# ret, out = git.call('rev-list', '--left-right', '%s..%s' % (remote_ref, #LINE# #TAB# #TAB# local_ref), raises=False) #LINE# #TAB# if ret == 0: #LINE# #TAB# #TAB# ahead = len(out.split()) #LINE# #TAB# ret, out = git.call('rev-list', '--left-right', '%s..%s' % (local_ref, #LINE# #TAB# #TAB# remote_ref), raises=False) #LINE# #TAB# if ret == 0: #LINE# #TAB# #TAB# behind = len(out.split()) #LINE# #TAB# return ahead, behind"
Find the unique scene labels <code> def unique_scene_labels(scene_list): ,"#LINE# #TAB# if isinstance(scene_list, dcase_util.containers.MetaDataContainer): #LINE# #TAB# #TAB# return scene_list.unique_scene_labels #LINE# #TAB# else: #LINE# #TAB# #TAB# labels = [] #LINE# #TAB# #TAB# for item in scene_list: #LINE# #TAB# #TAB# #TAB# if 'scene_label' in item and item['scene_label'] not in labels: #LINE# #TAB# #TAB# #TAB# #TAB# labels.append(item['scene_label']) #LINE# #TAB# #TAB# labels.sort() #LINE# #TAB# #TAB# return labels"
Accessor for XSD built - in types  <code> def builtin_types(cls): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# builtin_types = cls.meta_schema.maps.namespaces[XSD_NAMESPACE][0].types #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# raise XMLSchemaNotBuiltError(cls.meta_schema, #LINE# #TAB# #TAB# #TAB# 'missing XSD namespace in meta-schema') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# raise XMLSchemaNotBuiltError(cls.meta_schema, #LINE# #TAB# #TAB# #TAB# 'meta-schema unavailable for %r' % cls) #LINE# #TAB# else: #LINE# #TAB# #TAB# if not builtin_types: #LINE# #TAB# #TAB# #TAB# cls.meta_schema.build() #LINE# #TAB# #TAB# return builtin_types"
Add an option ' --log - level ' to set the log level  <code> def ocrd_loglevel(f): ,#LINE# #TAB# loglevel_option(f) #LINE# #TAB# return f
"Create a telstate view by appending ` name ` to all existing namespaces  <code> def relative_view(telstate, name): ","#LINE# #TAB# prefix = telstate.prefixes[-1] #LINE# #TAB# view = telstate.view(prefix + name, exclusive=True) #LINE# #TAB# for prefix in reversed(telstate.prefixes[:-1]): #LINE# #TAB# #TAB# view = view.view(prefix + name) #LINE# #TAB# return view"
Extracts transactions from the html tree structure and returns a list . : param tree : The HTML page <code> def current_transactions(tree): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# rows = tree.xpath(_config['xpath']['transactions_table'])[0].findall( #LINE# #TAB# #TAB# #TAB# 'tr') #LINE# #TAB# #TAB# raw_table_data = list() #LINE# #TAB# #TAB# for row in rows[1:-1]: #LINE# #TAB# #TAB# #TAB# raw_table_data.append([c.text.replace('\n', '') for c in row. #LINE# #TAB# #TAB# #TAB# #TAB# getchildren()]) #LINE# #TAB# #TAB# logging.debug('parsed transactions_table xpath as {}'.format( #LINE# #TAB# #TAB# #TAB# raw_table_data)) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logging.error('def transanctions: {}'.format(str(e))) #LINE# #TAB# #TAB# raise e #LINE# #TAB# return raw_table_data"
convert an LDAP entry dict to a hosts map tuple <code> def hosts_convert(entry): ,"#LINE# #TAB# hostnames = entry['aeFqdn'] #LINE# #TAB# return hostnames[0], hostnames[1:], entry['ipHostNumber']"
"Get clipboard text data . Return a unicode instance , or None  <code> def x_xclip_get(): ","#LINE# #TAB# raw = None #LINE# #TAB# encoding = locale.getpreferredencoding() #LINE# #TAB# for clipboard in X_CLIPBOARDS: #LINE# #TAB# #TAB# process = subprocess.Popen(['xclip', '-selection', clipboard.lower( #LINE# #TAB# #TAB# #TAB# ), '-o'], stdout=subprocess.PIPE) #LINE# #TAB# #TAB# raw, _ = process.communicate() #LINE# #TAB# #TAB# if process.returncode == 0 and raw: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return to_unicode(raw, encoding) if raw else None"
r Return True if at least one GPU is available <code> def check_gpu_existence(): ,"#LINE# #TAB# global _gpu_available #LINE# #TAB# if _gpu_available is None: #LINE# #TAB# #TAB# sess_config = tf.ConfigProto() #LINE# #TAB# #TAB# sess_config.gpu_options.allow_growth = True #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# with tf.Session(config=sess_config): #LINE# #TAB# #TAB# #TAB# #TAB# device_list = device_lib.list_local_devices() #LINE# #TAB# #TAB# #TAB# #TAB# _gpu_available = any(device.device_type == 'GPU' for device in device_list) #LINE# #TAB# #TAB# except AttributeError as e: #LINE# #TAB# #TAB# #TAB# log.warning(f'Got an AttributeError `{e}`, assuming documentation building') #LINE# #TAB# #TAB# #TAB# _gpu_available = False #LINE# #TAB# return _gpu_available"
"Receives raw dataset and adds pre - processed dataset in XML format used in the ACL - RD - TEC-2.0 <code> def preprocess_dataset_acl_xml(raw_dataset, lang='en'): ","#LINE# #TAB# for key in raw_dataset: #LINE# #TAB# #TAB# tags, keyphrases, text_content = parse_acl_xml_content(raw_dataset[ #LINE# #TAB# #TAB# #TAB# key]['raw'], lang=lang) #LINE# #TAB# #TAB# raw_dataset[key]['tags'] = tags #LINE# #TAB# #TAB# raw_dataset[key]['keyphrases'] = keyphrases #LINE# #TAB# #TAB# raw_dataset[key]['raw']['txt'] = text_content"
Test if the data set is an image <code> def is_image(dcm): ,"#LINE# #TAB# if any(hasattr(dcm, attr) for attr in _pix_attrs): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Returns the path where the episode thumbnail should be stored . Defaults to the same path as the episode file but with a .metathumb extension . ep_obj : a TVEpisode instance for which to create the thumbnail <code> def get_episode_thumb_path(ep_obj): ,"#LINE# #TAB# if os.path.isfile(ep_obj.location): #LINE# #TAB# #TAB# tbn_filename = replace_extension(ep_obj.location, 'metathumb') #LINE# #TAB# else: #LINE# #TAB# #TAB# return None #LINE# #TAB# return tbn_filename"
Add our indexes to the catalog . Doing it here instead of in profiles / default / catalog.xml means we do not need to reindex those indexes after every reinstall  <code> def add_indexes(site): ,"#LINE# #TAB# context = site.getSite() #LINE# #TAB# catalog = getToolByName(context, 'portal_catalog') #LINE# #TAB# indexes = catalog.indexes() #LINE# #TAB# for name, meta_type in INDEXES: #LINE# #TAB# #TAB# if name not in indexes: #LINE# #TAB# #TAB# #TAB# catalog.addIndex(name, meta_type) #LINE# #TAB# #TAB# if name in METADATA: #LINE# #TAB# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# #TAB# catalog.manage_addColumn(name) #LINE# #TAB# #TAB# #TAB# except CatalogError: #LINE# #TAB# #TAB# #TAB# #TAB# pass"
"Compute power spectrum density for given window : param window : : param standardize : : return : <code> def get_power_spectrum_density(window, standardize): ","#LINE# #TAB# ft = np.fft.fft2(window, norm='ortho') #LINE# #TAB# ft = np.fft.fftshift(ft) #LINE# #TAB# psd = np.abs(ft) ** 2 #LINE# #TAB# if standardize: #LINE# #TAB# #TAB# psd = psd / np.var(window) #LINE# #TAB# return psd"
"Generate a list of headers to send with the request . Args : config ( glean . Configuration ) : The Glean Configuration object . Returns : headers ( list of ( str , str ) ) : The headers to send  <code> def get_headers_to_send(cls, config: 'Configuration') ->List[Tuple[str, str]]: ","#LINE# #TAB# import glean #LINE# #TAB# headers = [('Content-Type', 'application/json; charset=utf-8'), ( #LINE# #TAB# #TAB# 'User-Agent', config.user_agent), ('Date', cls. #LINE# #TAB# #TAB# _create_date_header_value()), ('X-Client-Type', 'Glean'), ( #LINE# #TAB# #TAB# 'X-Client-Version', glean.__version__)] #LINE# #TAB# if config.ping_tag is not None: #LINE# #TAB# #TAB# headers.append(('X-Debug-ID', config.ping_tag)) #LINE# #TAB# return headers"
"Checks that the given file path exists in the current working directory  <code> def changelog_file_option_validator(ctx, param, value): ","#LINE# #TAB# path = Path(value) #LINE# #TAB# if not path.exists(): #LINE# #TAB# #TAB# filename = click.style(path.name, fg=""blue"", bold=True) #LINE# #TAB# #TAB# ctx.fail( #LINE# #TAB# #TAB# #TAB# ""\n"" #LINE# #TAB# #TAB# #TAB# f"" {x_mark} Unable to find {filename}\n"" #LINE# #TAB# #TAB# #TAB# ' Run ""$ brau init"" to create one' #LINE# #TAB# #TAB# ) #LINE# #TAB# return path"
Dispatch the event from an existing orm obj  <code> def dispatch_orm_event(orm_event): ,"#LINE# #TAB# results = [] #LINE# #TAB# event_obj = loads(orm_event.jsondata) #LINE# #TAB# EventMatch.database_connect() #LINE# #TAB# eventmatch_objs = EventMatch.select().where(EventMatch.deleted >> None & #LINE# #TAB# #TAB# EventMatch.disabled >> None) #LINE# #TAB# EventMatch.database_close() #LINE# #TAB# for eventmatch in eventmatch_objs: #LINE# #TAB# #TAB# jsonpath_expr = parse(eventmatch.jsonpath) #LINE# #TAB# #TAB# if find(jsonpath_expr, event_obj): #LINE# #TAB# #TAB# #TAB# results.append(query_policy.delay(eventmatch.to_hash(), #LINE# #TAB# #TAB# #TAB# #TAB# event_obj, orm_event.uuid)) #LINE# #TAB# return results"
Import and read defaults from given Python entity . See : data:`DEFAULTS_PROVIDERS ` for inner workings  <code> def read_defaults_python(location): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, variable_name = location.rsplit('.', 1) #LINE# #TAB# #TAB# module = import_module(module_path) #LINE# #TAB# #TAB# if not hasattr(module, variable_name): #LINE# #TAB# #TAB# #TAB# log.warning('Unknown entity {}'.format(location)) #LINE# #TAB# #TAB# #TAB# return {} #LINE# #TAB# #TAB# entity = getattr(module, variable_name) #LINE# #TAB# #TAB# if isfunction(entity): #LINE# #TAB# #TAB# #TAB# return entity() #LINE# #TAB# #TAB# return entity #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# log.debug(format_exc()) #LINE# #TAB# #TAB# log.warning('Unable to load entity {}'.format(location)) #LINE# #TAB# return {}"
"Group lhs and rhs into contitional , stable and undefined : param node : ast node : returns : tuple of ( contitional_lhs , stable_lhs),(contitional_rhs , stable_rhs ) , undefined <code> def conditional_symbols(node): ","#LINE# #TAB# gen = ConditionalSymbolVisitor() #LINE# #TAB# gen.visit(node) #LINE# #TAB# lhs = gen.cond_lhs, gen.stable_lhs #LINE# #TAB# rhs = gen.cond_rhs, gen.stable_rhs #LINE# #TAB# undefined = gen.undefined #LINE# #TAB# return lhs, rhs, undefined"
"Get the creationTime and state from current snapshot xml , and create a new xml for snapshot - create with redefine and current flags  <code> def create_redefine_xml(domobj, xmlstr): ",#LINE# #TAB# xmlcur = domobj.snapshotCurrent(0).getXMLDesc(0) #LINE# #TAB# xmltime = xmlcur[xmlcur.find('<creationTime>'):xmlcur.find( #LINE# #TAB# #TAB# '</creationTime>') + 15] #LINE# #TAB# xmlstate = xmlcur[xmlcur.find('<state>'):xmlcur.find('</state>') + 8] #LINE# #TAB# xmlstr = xmlstr[:17] + xmltime + '\n' + xmlstate + xmlstr[17:] #LINE# #TAB# logger.info('Redefine current snapshot using xml: %s' % xmlstr) #LINE# #TAB# return xmlstr
Helper method for catalog based folder contents  <code> def get_obj_position_in_parent(obj): ,"#LINE# #TAB# parent = aq_parent(aq_inner(obj)) #LINE# #TAB# ordered = IOrderedContainer(parent, None) #LINE# #TAB# if ordered is not None: #LINE# #TAB# #TAB# return ordered.getObjectPosition(obj.getId()) #LINE# #TAB# return 0"
"Given a name , validates that the role exists whether it is a proper ID or a name . Returns the role if one was found , else None  <code> def get_role(oneandone_conn, role, full_object=False): ","#LINE# #TAB# for _role in oneandone_conn.list_roles(per_page=1000): #LINE# #TAB# #TAB# if role in (_role['id'], _role['name']): #LINE# #TAB# #TAB# #TAB# if full_object: #LINE# #TAB# #TAB# #TAB# #TAB# return _role #LINE# #TAB# #TAB# #TAB# return _role['id']"
"Retrieves dependencies for the requirement from pipenv . patched . notpip internals  <code> def get_dependencies_from_pip(ireq, sources): ","#LINE# #TAB# extras = ireq.extras or () #LINE# #TAB# try: #LINE# #TAB# #TAB# wheel = build_wheel(ireq, sources) #LINE# #TAB# except WheelBuildError: #LINE# #TAB# #TAB# metadata = read_sdist_metadata(ireq) #LINE# #TAB# #TAB# if not metadata: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# else: #LINE# #TAB# #TAB# metadata = wheel.metadata #LINE# #TAB# requirements = _read_requirements(metadata, extras) #LINE# #TAB# requires_python = _read_requires_python(metadata) #LINE# #TAB# return requirements, requires_python"
"recursively convert dict content into string <code> def to_dict_str(origin_value, encode=None): ","#LINE# #TAB# value = copy.deepcopy(origin_value) #LINE# #TAB# for k, v in value.items(): #LINE# #TAB# #TAB# if isinstance(v, dict): #LINE# #TAB# #TAB# #TAB# value[k] = to_dict_str(v, encode) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if isinstance(v, list): #LINE# #TAB# #TAB# #TAB# value[k] = to_list_str(v, encode) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if encode: #LINE# #TAB# #TAB# #TAB# value[k] = encode(v) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# value[k] = default_encode(v) #LINE# #TAB# return value"
null terminate a string <code> def null_term(str): ,"#LINE# #TAB# if isinstance(str, bytes): #LINE# #TAB# #TAB# str = to_string(str) #LINE# #TAB# idx = str.find('\x00') #LINE# #TAB# if idx != -1: #LINE# #TAB# #TAB# str = str[:idx] #LINE# #TAB# return str"
"Set connection to use as default when requesting ' device ' without connection specification <code> def set_default_connection(io_type, variant): ","#LINE# #TAB# global default_connection #LINE# #TAB# default_connection = {'io_type': io_type, 'variant': variant}"
Retrieve the output prefix <code> def get_prefix(key): ,#LINE# #TAB# global __name_prefix #LINE# #TAB# if key in __name_prefix: #LINE# #TAB# #TAB# return __name_prefix[key] #LINE# #TAB# else: #LINE# #TAB# #TAB# return None
Return the default configuration - file path  <code> def get_default_configfile_path(): ,"#LINE# #TAB# base = homebase.user_config_dir( #LINE# #TAB# #TAB# app_author=CONF_AUTHOR, app_name=CONF_APP, roaming=False, #LINE# #TAB# #TAB# use_virtualenv=False, create=False) #LINE# #TAB# path = os.path.join(base, CONF_FILENAME) #LINE# #TAB# return path"
Turn supported_os into a condition  <code> def convert_supported_osto_conditions(src_object): ,"#LINE# if src_object.supported_os: #LINE# #TAB# conditions = "" OR "".join(""os == '%s'"" % o for o in src_object.supported_os) #LINE# #TAB# return conditions"
Convert all unicode strings occurring in a JSON object to utf-8 encoded string of type str . : param object value : a JSON node : returns object : <code> def unicode_to_str(value): ,"#LINE# #TAB# if isinstance(value, list): #LINE# #TAB# #TAB# return map(unicode_to_str, value) #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# return dict((unicode_to_str(k), unicode_to_str(v)) for k, v in #LINE# #TAB# #TAB# #TAB# value.items()) #LINE# #TAB# if isinstance(value, unicode): #LINE# #TAB# #TAB# return value.encode('utf-8') #LINE# #TAB# return value"
Recursive search for the line 's hydrological level . Parameters ---------- line : a Centerline instance Returns ------- The line 's order <code> def line_order(line): ,#LINE# #TAB# if len(line.inflows) == 0: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# levels = [line_order(s) for s in line.inflows] #LINE# #TAB# #TAB# return np.max(levels) + 1
Calculate the vorticity vector from the vorticity tensor in two dimension <code> def vertical_vorticity2d(tensorW): ,#LINE# #TAB# wz = 2 * tensorW[1][0] #LINE# #TAB# return wz
"If edges given , saves the path provided . Returns whether the path is disconnected or not <code> def check_edges(edges): ","#LINE# #TAB# if edges: #LINE# #TAB# #TAB# path = [edge[0] for edge in edges] #LINE# #TAB# #TAB# path.append(edges[-1][1]) #LINE# #TAB# #TAB# return any(edge[1] not in path for edge in edges), path #LINE# #TAB# else: #LINE# #TAB# #TAB# return None, None"
Gathers the _ is_linux fact using the < file - show/ > RPC on the EVO version file  <code> def get_facts(device): ,"#LINE# #TAB# LINUX_VERSION_PATH = '/usr/share/cevo/cevo_version' #LINE# #TAB# is_linux = None #LINE# #TAB# try: #LINE# #TAB# #TAB# rsp = device.rpc.file_show(normalize=True, filename=LINUX_VERSION_PATH) #LINE# #TAB# #TAB# if rsp.tag == 'file-content': #LINE# #TAB# #TAB# #TAB# is_linux = True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# is_linux = False #LINE# #TAB# except RpcError: #LINE# #TAB# #TAB# is_linux = False #LINE# #TAB# return {'_is_linux': is_linux}"
Called before a view locks for a read . : returns : ` ` accpac . Continue ` ` or ` ` accpac . Abort ` ` : rtype : int <code> def on_before_read_lock(): ,#LINE# #TAB# rvspyTrace('on_before_read_lock') #LINE# #TAB# return Continue
for argument parsing ; restricts float value from 0 to 1 <code> def restricted_float(x): ,"#LINE# #TAB# x = float(x) #LINE# #TAB# if x < 0.0 or x > 1.0: #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError('%r not in range [0.0, 1.0]' % (x,)) #LINE# #TAB# return x"
Create a client for mesh volume APIs  <code> def mesh_volume_create(_): ,#LINE# #TAB# client = create(_) #LINE# #TAB# return client.mesh_volume
"A helper method to read of bytes from a socket to a maximum length <code> def socket_recvall(socket, length, bufsize=4096): ",#LINE# #TAB# data = b'' #LINE# #TAB# while len(data) < length: #LINE# #TAB# #TAB# data += socket.recv(bufsize) #LINE# #TAB# return data
"Compute product of Hankel matrix ( gene_vect ) by vector prod_vect . H is not computed M is the length of the result <code> def fasthankel_prod_mat_vec(gene_vect, prod_vect): ","#LINE# #TAB# L = len(gene_vect) #LINE# #TAB# N = len(prod_vect) #LINE# #TAB# M = L - N + 1 #LINE# #TAB# prod_vect_zero = np.concatenate((np.zeros(M - 1), prod_vect[::-1])) #LINE# #TAB# fft0, fft1 = fft(gene_vect), fft(prod_vect_zero) #LINE# #TAB# prod = fft0 * fft1 #LINE# #TAB# c = ifft(prod) #LINE# #TAB# return np.roll(c, +1)[:M]"
Gets input from user <code> def term_input(prompt_str): ,"#LINE# #TAB# print(prompt_str, end='') #LINE# #TAB# print('\x1b[1m', end='') #LINE# #TAB# out = input('') #LINE# #TAB# print('\x1b[21m', end='') #LINE# #TAB# return out"
Open the log and error filehandles . Args : context ( scriptworker.context . Context ) : the scriptworker context . Yields : log filehandle <code> def get_log_filehandle(context: Any) ->Iterator[IO[str]]: ,"#LINE# #TAB# log_file_name = get_log_filename(context) #LINE# #TAB# makedirs(context.config['task_log_dir']) #LINE# #TAB# with open(log_file_name, 'w', encoding='utf-8') as filehandle: #LINE# #TAB# #TAB# yield filehandle"
Attempts to read a python version string from a runtime.txt file : param path : to source of the string : return : python version : rtype : unicode or None <code> def read_version(path): ,"#LINE# #TAB# version = None #LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# version = open(path, 'r', encoding='utf-8').read().strip() #LINE# #TAB# if version: #LINE# #TAB# #TAB# return re.sub('^python-', '', version) #LINE# #TAB# return version"
Enable serial data logger <code> def enable_logger(filename=None): ,#LINE# #TAB# if SerialPort._logger is None: #LINE# #TAB# #TAB# from larpix.serial_helpers.datalogger import DataLogger #LINE# #TAB# #TAB# SerialPort._logger = DataLogger(filename) #LINE# #TAB# if not SerialPort._logger.is_enabled(): #LINE# #TAB# #TAB# SerialPort._logger.enable() #LINE# #TAB# return
Allocate a buffer on the device with specified number of bytes  <code> def alloc_device(num_bytes): ,"#LINE# #TAB# ptr = c_void_ptr_t(0) #LINE# #TAB# c_num_bytes = c_dim_t(num_bytes) #LINE# #TAB# safe_call(backend.get().af_alloc_device(c_pointer(ptr), c_num_bytes)) #LINE# #TAB# return ptr.value"
"True if any value contains any of the strings in the ' strings ' iterable . Only used by guess_compose  <code> def val_in_str(values, strings): ",#LINE# #TAB# for val in values: #LINE# #TAB# #TAB# if val and any(str(st).lower() == str(val).lower() for st in strings): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
Find artifacts in an edf asci representation Parameters ---------- asc_str : str String with edf asci represenation as returned by read_edf  <code> def find_edf_triggers(asc_str): ,#LINE# #TAB# re_trigger = re.compile('\\bMSG\\t(\\d+)\\tMEG Trigger: (\\d+)') #LINE# #TAB# triggers = re_trigger.findall(asc_str) #LINE# #TAB# return triggers
"a - a ' : chloride hydroxide [ HMW84 ]  <code> def theta_cl_oh_hmw84(T, P): ","#LINE# #TAB# theta = -0.05 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return theta, valid"
Formats datetime object to structured data compatible datetime string  <code> def structured_data_datetime(dt): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# if dt.time(): #LINE# #TAB# #TAB# #TAB# return datetime.strftime(dt, '%Y-%m-%dT%H:%M') #LINE# #TAB# #TAB# return datetime.strftime(dt, '%Y-%m-%d') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return ''"
GET a resource and return the 1st element found <code> def get_resource(intersight): ,"#LINE# #TAB# options = {'http_method': 'get', 'resource_path': intersight.module. #LINE# #TAB# #TAB# params['resource_path'], 'query_params': intersight.module.params[ #LINE# #TAB# #TAB# 'query_params']} #LINE# #TAB# response_dict = intersight.call_api(**options) #LINE# #TAB# if response_dict.get('Results'): #LINE# #TAB# #TAB# response_dict = response_dict['Results'][0] #LINE# #TAB# return response_dict"
Returns highest available version for a package in a list of versions Uses pkg_resources to parse the versions <code> def get_highest_version(versions): ,"#LINE# #TAB# sorted_versions = [] #LINE# #TAB# for ver in versions: #LINE# #TAB# #TAB# sorted_versions.append((pkg_resources.parse_version(ver), ver)) #LINE# #TAB# sorted_versions = sorted(sorted_versions) #LINE# #TAB# sorted_versions.reverse() #LINE# #TAB# return sorted_versions[0][1]"
"Replace all values matching values in - line in a list . Return whether any values were replaced  <code> def list_replace(lst, replace, replacement): ","#LINE# #TAB# replaced = False #LINE# #TAB# for i, v in enumerate(lst): #LINE# #TAB# #TAB# if v == replace: #LINE# #TAB# #TAB# #TAB# lst[i] = replacement #LINE# #TAB# #TAB# #TAB# replaced = True #LINE# #TAB# return replaced"
"Update component settings  <code> def update_settings(component, environment, settings): ","#LINE# #TAB# environment = get_environment_alias(environment) #LINE# #TAB# settings_dict = json_loads(component.settings_json) #LINE# #TAB# if isinstance(settings, str): #LINE# #TAB# #TAB# settings = json_loads(settings) #LINE# #TAB# settings_dict[environment] = settings #LINE# #TAB# component.settings_json = json_dumps(settings_dict) #LINE# #TAB# component.save() #LINE# #TAB# return component"
"get domain current memory inside domain <code> def get_current_memory(guestname, username, password): ","#LINE# #TAB# logger.debug('get the mac address of vm %s' % guestname) #LINE# #TAB# mac = utils.get_dom_mac_addr(guestname) #LINE# #TAB# logger.debug('the mac address of vm %s is %s' % (guestname, mac)) #LINE# #TAB# ip = utils.mac_to_ip(mac, 180) #LINE# #TAB# current = utils.get_remote_memory(ip, username, password, 'MemTotal') #LINE# #TAB# return current"
"Calculate 1 / z efficiently <code> def mpc_reciprocal(z, prec, rnd=round_fast): ","#LINE# #TAB# a, b = z #LINE# #TAB# m = mpf_add(mpf_mul(a, a), mpf_mul(b, b), prec + 10) #LINE# #TAB# re = mpf_div(a, m, prec, rnd) #LINE# #TAB# im = mpf_neg(mpf_div(b, m, prec, rnd)) #LINE# #TAB# return re, im"
"If the bot is the first @mention in the message , then this function returns the stripped message with the bot 's @mention removed . Otherwise , it returns None  <code> def extract_query_without_mention(message: Dict[str, Any], client: ",#LINE# #TAB# ExternalBotHandler) ->Optional[str]: #LINE# #TAB# content = message['content'] #LINE# #TAB# mention = '@**' + client.full_name + '**' #LINE# #TAB# extended_mention_regex = re.compile('^@\\*\\*.*\\|' + str(client. #LINE# #TAB# #TAB# user_id) + '\\*\\*') #LINE# #TAB# extended_mention_match = extended_mention_regex.match(content) #LINE# #TAB# if extended_mention_match: #LINE# #TAB# #TAB# return content[extended_mention_match.end():].lstrip() #LINE# #TAB# if content.startswith(mention): #LINE# #TAB# #TAB# return content[len(mention):].lstrip() #LINE# #TAB# return None
"Retrieves the parsers information . Returns : list[tuple[str , str ] ] : parser names and descriptions  <code> def get_parsers_information(cls): ","#LINE# #TAB# parsers_information = [] #LINE# #TAB# for _, parser_class in cls._GetParsers(): #LINE# #TAB# #TAB# description = getattr(parser_class, 'DESCRIPTION', '') #LINE# #TAB# #TAB# parsers_information.append((parser_class.NAME, description)) #LINE# #TAB# return parsers_information"
Check if file exists with argparse <code> def extant_file(path): ,"#LINE# #TAB# if not os.path.exists(path): #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(""{} does not exist"".format(path)) #LINE# #TAB# return path"
return list of species of a SBML model <code> def get_listofspecies(model): ,#LINE# #TAB# listOfSpecies = None #LINE# #TAB# for e in model: #LINE# #TAB# #TAB# tag = get_sbml_tag(e) #LINE# #TAB# #TAB# if tag == 'listOfSpecies': #LINE# #TAB# #TAB# #TAB# listOfSpecies = e #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return listOfSpecies
get uptime of computer Returns : uptime ( : obj:`datetime.timedelta ` ) <code> def get_uptime(): ,"#LINE# #TAB# with open('/proc/uptime', 'r') as f: #LINE# #TAB# #TAB# uptime_seconds = float(f.readline().split()[0]) #LINE# #TAB# #TAB# uptime = datetime.timedelta(seconds=uptime_seconds) #LINE# #TAB# #TAB# return uptime"
"Implements scalarmult(B , e ) more efficiently  <code> def scalarmult_b(e): ","#LINE# #TAB# e %= L #LINE# #TAB# P = IDENT #LINE# #TAB# for i in range(253): #LINE# #TAB# #TAB# if e & 1: #LINE# #TAB# #TAB# #TAB# P = edwards_add(P, Bpow[i]) #LINE# #TAB# #TAB# e //= 2 #LINE# #TAB# assert e == 0, e #LINE# #TAB# return P"
"Ancient page generator . @param total : Maximum number of pages to retrieve in total @type total : int @param site : Site for generator results . @type site : L{pywikibot.site . BaseSite } <code> def ancient_pages_page_generator(total=100, site=None): ","#LINE# #TAB# if site is None: #LINE# #TAB# #TAB# site = pywikibot.Site() #LINE# #TAB# for page, timestamp in site.ancientpages(total=total): #LINE# #TAB# #TAB# yield page"
"Display an OpenID response . Errors will be displayed directly to the user ; successful responses and other protocol - level messages will be sent using the proper mechanism ( i.e. , direct response , redirection , etc . )  <code> def display_response(request, openid_response): ","#LINE# #TAB# s = getServer(request) #LINE# #TAB# try: #LINE# #TAB# #TAB# webresponse = s.encodeResponse(openid_response) #LINE# #TAB# except EncodingError as why: #LINE# #TAB# #TAB# text = why.response.encodeToKVForm() #LINE# #TAB# #TAB# return render_to_response('server/endpoint.html', {'error': cgi. #LINE# #TAB# #TAB# #TAB# escape(text)}, context_instance=RequestContext(request)) #LINE# #TAB# r = http.HttpResponse(webresponse.body) #LINE# #TAB# r.status_code = webresponse.code #LINE# #TAB# for header, value in webresponse.headers.items(): #LINE# #TAB# #TAB# r[header] = value #LINE# #TAB# return r"
"Returns the ratio of the euclidean distance of each near - neighbor to that of the nearest neighbor for each particle  <code> def normalized_radial_distance(box, positions, neighbors, rmax_guess=2.0): ","#LINE# #TAB# fbox = freud.box.Box.from_box(box) #LINE# #TAB# nlist = _nlist_nn_helper(fbox, positions, neighbors, rmax_guess, True) #LINE# #TAB# rijs = positions[nlist.point_indices] - positions[nlist.query_point_indices #LINE# #TAB# #TAB# ] #LINE# #TAB# rijs = fbox.wrap(rijs) #LINE# #TAB# rs = np.linalg.norm(rijs, axis=-1) #LINE# #TAB# reference_rs = rs[nlist.segments] #LINE# #TAB# normalization = np.repeat(reference_rs, nlist.neighbor_counts) #LINE# #TAB# rs /= normalization #LINE# #TAB# return rs.reshape((positions.shape[0], -1))[:, 1:]"
"Returns true if the left boundary of both boxes is within 2 pts  <code> def bbox_vert_aligned_left(box1: Bbox, box2: Bbox) ->bool: ",#LINE# #TAB# if not (box1 and box2): #LINE# #TAB# #TAB# return False #LINE# #TAB# return abs(box1.left - box2.left) <= 2
"Try to validate a private key . : param private_key_path : the path to the private key . : return : None : raises : an exception if the private key is invalid  <code> def try_validate_fet_private_key_path(private_key_path: str, exit_on_error: ","#LINE# #TAB# bool=True) ->None: #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(private_key_path, 'r') as key: #LINE# #TAB# #TAB# #TAB# data = key.read() #LINE# #TAB# #TAB# #TAB# Entity.from_hex(data) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# logger.error( #LINE# #TAB# #TAB# #TAB# ""This is not a valid private key file: '{}'\n Exception: '{}'"". #LINE# #TAB# #TAB# #TAB# format(private_key_path, e)) #LINE# #TAB# #TAB# if exit_on_error: #LINE# #TAB# #TAB# #TAB# sys.exit(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise"
add an existing user to admins : param name : dom_name : return : success or failure <code> def add_admin(name): ,"#LINE# #TAB# if not is_admin(): #LINE# #TAB# #TAB# return NoContent, 401 #LINE# #TAB# user = db_session.query(User).filter(User.dom_name == name).one_or_none() #LINE# #TAB# if not user: #LINE# #TAB# #TAB# return NoContent, 404 #LINE# #TAB# group = db_session.query(Group).filter(Group.name == 'admins').one() #LINE# #TAB# try: #LINE# #TAB# #TAB# db_session.add(Member(group=group, user=user, admin=True)) #LINE# #TAB# #TAB# db_session.commit() #LINE# #TAB# #TAB# return NoContent, 201 #LINE# #TAB# except SQLAlchemyError: #LINE# #TAB# #TAB# logger.exception('error while adding admin') #LINE# #TAB# #TAB# return NoContent, 500"
"Set the current device for cupy as the device with the lowest weighted average of processor and memory load  <code> def select_device_by_load(wproc=0.5, wmem=0.5): ","#LINE# #TAB# ids = device_by_load(wproc=wproc, wmem=wmem) #LINE# #TAB# cp.cuda.Device(ids[0]).use() #LINE# #TAB# return ids[0]"
"Checks to see if account exists . This procedure does not check it 's status . : param account : Name of the account . : param session : the database session in use . : returns : True if found , otherwise false  <code> def account_exists(account, session=None): ","#LINE# #TAB# query = session.query(models.Account).filter_by(account=account, status #LINE# #TAB# #TAB# =AccountStatus.ACTIVE) #LINE# #TAB# return True if query.first() else False"
"Draws a rectangle with ` xcorners ` and ` ycorners ` . Background is made with ones and the rectangle with zeros . Returns : np.ndarray : 2d binary array  <code> def rectangle_mask(mask, xcorners, ycorners): ","#LINE# #TAB# X_grid, Y_grid = mask.rippletank.X, mask.rippletank.Y #LINE# #TAB# mask = np.ones_like(X_grid) #LINE# #TAB# positions = getPositions(X_grid, Y_grid, xcorners, ycorners) #LINE# #TAB# mask[positions] = 0 #LINE# #TAB# return mask"
"Checks if the CloudFormation stack should be created or updated <code> def deploy_type(stack_name, cfn_client): ","#LINE# #TAB# stacks = get_all_stacks(cfn_cli=cfn_client, token=None, stack_list=[]) #LINE# #TAB# for stack in stacks: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# if stack_name == stack['StackName']: #LINE# #TAB# #TAB# #TAB# #TAB# if stack['StackStatus'] in ('CREATE_COMPLETE', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# 'UPDATE_COMPLETE', 'UPDATE_ROLLBACK_COMPLETE'): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return {'Update': True, 'UpdateStackName': stack[ #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'StackName']} #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return {'Update': False}"
Stop ALL printing of user feedback from package . Args : _ verbose : Boolean indicator whether or not feedback should be printed . Default False . Returns : True <code> def stop_print(_verbose=False) ->bool: ,"#LINE# #TAB# if _verbose: #LINE# #TAB# #TAB# print(""Printing user feedback set to 'False'."") #LINE# #TAB# glob.SILENT = True #LINE# #TAB# return True"
Add a page to the context given its slug <code> def get_page(slug): ,"#LINE# #TAB# for language_code in AVAILABLE_LANGUAGES: #LINE# #TAB# #TAB# slug_field = 'slug_%s' % language_code #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return Page.objects.get(**{slug_field: slug}) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# logger.warning(""Cannot resolve page slug '%s'"" % slug) #LINE# #TAB# return None"
"date : datetime.date eg : week = FromFirstDaysWeek.create_from_date(date(2016 , 9 , 18 ) ) <code> def create_from_date(cls, date_obj): ","#LINE# #TAB# week = int(date_obj.strftime('%W')) #LINE# #TAB# if date(date_obj.year, 1, 1).isoweekday() != cls._FIRST_ISOWEEKDAY: #LINE# #TAB# #TAB# week += 1 #LINE# #TAB# week_obj = cls(year=date_obj.year, week=week) #LINE# #TAB# return week_obj"
"deleted browser options from default : param str option : for example --verbose : return : list[str ] <code> def delete_option(cls, option): ",#LINE# #TAB# cls.default_options.remove(option) #LINE# #TAB# return cls.default_options
"Sanitize flags passed into df <code> def clean_flags(args, caller): ","#LINE# #TAB# flags = '' #LINE# #TAB# if args is None: #LINE# #TAB# #TAB# return flags #LINE# #TAB# allowed = ('a', 'B', 'h', 'H', 'i', 'k', 'l', 'P', 't', 'T', 'x', 'v') #LINE# #TAB# for flag in args: #LINE# #TAB# #TAB# if flag in allowed: #LINE# #TAB# #TAB# #TAB# flags += flag #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise CommandExecutionError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Invalid flag passed to {0}'.format(caller) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return flags"
"Returns True if derivative is out - of - date wrt original , both of which are full file paths  <code> def out_of_date(original, derived): ",#LINE# #TAB# return not os.path.exists(derived) or os.stat(derived).st_mtime < os.stat( #LINE# #TAB# #TAB# original).st_mtime
Get the document fullpath . : param str uuid : The document 's uuid <code> def get_document(uuid): ,"#LINE# #TAB# from .api import Document #LINE# #TAB# from .errors import DocumentNotFound, DeletedDocument #LINE# #TAB# try: #LINE# #TAB# #TAB# document = Document.get_document(uuid) #LINE# #TAB# except (DocumentNotFound, DeletedDocument): #LINE# #TAB# #TAB# path = _simulate_file_not_found() #LINE# #TAB# else: #LINE# #TAB# #TAB# path = document.get('uri', ''), document.is_authorized() #LINE# #TAB# finally: #LINE# #TAB# #TAB# return path"
Return the plugin manager  <code> def get_plugin_manager(): ,#LINE# #TAB# app = apps.get_app_config(PROJECT_NAME) #LINE# #TAB# return app.plugin_manager
"Given a list of files in contents remove all files named ipynb or directories named images and return the result  <code> def remove_ipynb_files(path, contents): ","#LINE# #TAB# contents_return = [] #LINE# #TAB# for entry in contents: #LINE# #TAB# #TAB# if entry.endswith('.ipynb'): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# elif (entry != ""images"") and os.path.isdir(os.path.join(path, entry)): #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# contents_return.append(entry) #LINE# #TAB# return contents_return"
"Get the mimetype from the HTTP response <code> def get_mimetype_from_response(response, strip_params=False): ","#LINE# #TAB# mimetype = response.getheader('Content-type', None) #LINE# #TAB# if not mimetype: #LINE# #TAB# #TAB# raise ValueError('No Content-Type header in response') #LINE# #TAB# if strip_params: #LINE# #TAB# #TAB# return mimetype.split(';', 1)[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# return mimetype"
"Reads the ClassificationScheme of a gdal . Band : param band : gdal . Band : return : ClassificationScheme , None if classes are undefined  <code> def from_raster_band(band: gdal.Band): ","#LINE# #TAB# assert isinstance(band, gdal.Band) #LINE# #TAB# cat = band.GetCategoryNames() #LINE# #TAB# ct = band.GetColorTable() #LINE# #TAB# if cat is None or len(cat) == 0: #LINE# #TAB# #TAB# return None #LINE# #TAB# scheme = ClassificationScheme() #LINE# #TAB# classes = [] #LINE# #TAB# for i, catName in enumerate(cat): #LINE# #TAB# #TAB# cli = ClassInfo(name=catName, label=i) #LINE# #TAB# #TAB# if ct is not None: #LINE# #TAB# #TAB# #TAB# cli.setColor(QColor(*ct.GetColorEntry(i))) #LINE# #TAB# #TAB# classes.append(cli) #LINE# #TAB# scheme.insertClasses(classes) #LINE# #TAB# return scheme"
"Convert the contents of a dataframe into Neo : class:`Events < neo.core . Event > `  <code> def create_neo_events_from_dataframe(dataframe, metadata, file_origin): ","#LINE# #TAB# events_list = [] #LINE# #TAB# if dataframe is not None: #LINE# #TAB# #TAB# for type_name, df in dataframe.groupby('Type'): #LINE# #TAB# #TAB# #TAB# event = neo.Event(name=type_name, file_origin=file_origin, #LINE# #TAB# #TAB# #TAB# #TAB# times=df['Start (s)'].values * pq.s, labels=df['Label'].values) #LINE# #TAB# #TAB# #TAB# events_list.append(event) #LINE# #TAB# return events_list"
Load config file from config_path or test configuration <code> def load_config_file(config_path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# with open(config_path) as json_file: #LINE# #TAB# #TAB# #TAB# return json.load(json_file) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return {'conections': {'default': 'sqlite:///test.db'}, 'queries': { #LINE# #TAB# #TAB# 'default': 'SELECT 1 AS a, 2 AS b'}}"
Get the paths for the XDG config directories  <code> def get_xdg_config_dirs(): ,"#LINE# #TAB# result = [get_xdg_config_home()] #LINE# #TAB# result.extend([x.encode('utf-8') for x in get_env_path( #LINE# #TAB# #TAB# 'XDG_CONFIG_DIRS', default_config_path).decode('utf-8').split(os. #LINE# #TAB# #TAB# pathsep)]) #LINE# #TAB# return result"
Generate singer schema for kafka messages MESSAGE_TIMESTAMP : Automatically will extract from kafka metadata MESSAGE : The original kafka message DYNAMIC_COLUMNS_FOR_PK : optional <code> def generate_schema(primary_keys) ->object: ,"#LINE# #TAB# schema = {'type': 'object', 'properties': {'message_timestamp': {'type': #LINE# #TAB# #TAB# ['integer', 'string', 'null']}, 'message_offset': {'type': [ #LINE# #TAB# #TAB# 'integer', 'null']}, 'message_partition': {'type': ['integer', #LINE# #TAB# #TAB# 'null']}, 'message': {'type': ['object', 'array', 'string', 'null']}}} #LINE# #TAB# for key in primary_keys: #LINE# #TAB# #TAB# schema['properties'][key] = {'type': ['string', 'null']} #LINE# #TAB# return schema"
Validates that the given string is a valid URL <code> def validate_url(url): ,"#LINE# #TAB# urlRegex = re.compile( #LINE# #TAB# #TAB# '^(?:http|ftp)s?://(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|localhost|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/?|[/?]\\S+)$' #LINE# #TAB# #TAB# , re.IGNORECASE) #LINE# #TAB# if urlRegex.match(url): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"r Calculates heat transfer Peclet number or Pe for a specified velocity V characteristic length L and specified properties for the given fluid  <code> def peclet_heat(V, L, rho=None, Cp=None, k=None, alpha=None): ","#LINE# #TAB# if rho and Cp and k: #LINE# #TAB# #TAB# alpha = k/(rho*Cp) #LINE# #TAB# elif not alpha: #LINE# #TAB# #TAB# raise Exception('Either heat capacity and thermal conductivity and\ #LINE# #TAB# #TAB# density, or thermal diffusivity is needed') #LINE# #TAB# return V*L/alpha"
Counts the number of basic changes a ` Change ` will make <code> def count_changes(change): ,"#LINE# #TAB# if isinstance(change, ChangeSet): #LINE# #TAB# #TAB# result = 0 #LINE# #TAB# #TAB# for child in change.changes: #LINE# #TAB# #TAB# #TAB# result += count_changes(child) #LINE# #TAB# #TAB# return result #LINE# #TAB# return 1"
Return random keys of dictionary <code> def shuffle_dictionary(dictionary): ,#LINE# #TAB# keys = dictionary.keys() #LINE# #TAB# shuffle(keys) #LINE# #TAB# return keys
"Load secrets and embed it into the configuration YAML  <code> def secret_yaml(loader, node): ","#LINE# #TAB# fname = os.path.join(os.path.dirname(loader.name), ""secrets.yaml"") #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(fname, encoding=""utf-8"") as secret_file: #LINE# #TAB# #TAB# #TAB# secrets = YAML(typ=""safe"").load(secret_file) #LINE# #TAB# except FileNotFoundError: #LINE# #TAB# #TAB# raise ValueError(""Secrets file {} not found"".format(fname)) from None #LINE# #TAB# try: #LINE# #TAB# #TAB# return secrets[node.value] #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# raise ValueError(""Secret {} not found"".format(node.value)) from None"
Method to register an object class  <code> def register_object_class(objCls): ,#LINE# #TAB# for symbol in objCls.GetSymbols(): #LINE# #TAB# #TAB# _CheckSymbol(symbol) #LINE# #TAB# #TAB# REGISTERED_OBJECT_CLASSES[symbol] = objCls
Check if completion needs confirmation <code> def should_confirm_completion(): ,#LINE# #TAB# return builtins.__xonsh__.env.get('COMPLETIONS_CONFIRM') and get_app( #LINE# #TAB# #TAB# ).current_buffer.complete_state
"Return input converted into a float . If failed then return default  <code> def to_float(s, default=0.0, allow_nan=False): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# f = float(s) #LINE# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# return default #LINE# #TAB# if not allow_nan: #LINE# #TAB# #TAB# if f != f or f in _infs: #LINE# #TAB# #TAB# #TAB# return default #LINE# #TAB# return f"
"Map trace to parameter names  <code> def map_trace_to_names(trace, params): ","#LINE# #TAB# out = {} #LINE# #TAB# allnames = list(params.keys()) + ['prob'] #LINE# #TAB# for name in trace.keys(): #LINE# #TAB# #TAB# tmp_dict = {} #LINE# #TAB# #TAB# tmp = np.array(trace[name]) #LINE# #TAB# #TAB# for para_name, values in zip(allnames, tmp.T): #LINE# #TAB# #TAB# #TAB# tmp_dict[para_name] = values #LINE# #TAB# #TAB# out[name] = tmp_dict #LINE# #TAB# return out"
Yield all the rdefs leading to a composite ( or ` parent ` ) eschema . We must take care of etypes that are composed of themselves  <code> def parent_rdefs(eschema): ,"#LINE# #TAB# for rdef in iterrdefs(eschema, meta=False, final=False): #LINE# #TAB# #TAB# if rdef.composite: #LINE# #TAB# #TAB# #TAB# composite_eschema = composite(rdef) #LINE# #TAB# #TAB# #TAB# if composite_eschema == eschema: #LINE# #TAB# #TAB# #TAB# #TAB# component_eschema = component(rdef) #LINE# #TAB# #TAB# #TAB# #TAB# if component_eschema != eschema: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield rdef"
Returns a list with column names retrieved from the description of a cursor . : param psycopg2.extensions.cursor cursor : The cursor . : return : list <code> def get_column_name(cursor): ,#LINE# #TAB# column_names = [] #LINE# #TAB# for column in cursor.description: #LINE# #TAB# #TAB# column_names.append(column.name) #LINE# #TAB# return column_names
[ A - Z][a - zA - Z_0 - 9 ` ' ] * <code> def t_conid(t): ,"#LINE# #TAB# t.type = keywords.get(t.value, 'CONID') #LINE# #TAB# return t"
"Calculate the number of columns and rows required to divide an image into ` ` n ` ` parts . Return a tuple of integers in the format ( num_columns , num_rows ) <code> def calc_columns_rows(n): ","#LINE# #TAB# num_columns = int(ceil(sqrt(n))) #LINE# #TAB# num_rows = int(ceil(n / float(num_columns))) #LINE# #TAB# return num_columns, num_rows"
"Get annotations for the specified entity <code> def get_annotations(entity_tag, connection): ","#LINE# #TAB# facade = client.AnnotationsFacade.from_connection(connection) #LINE# #TAB# result = (await facade.Get([{""tag"": entity_tag}])).results[0] #LINE# #TAB# if result.error is not None: #LINE# #TAB# #TAB# raise JujuError(result.error) #LINE# #TAB# return result.annotations"
"Very simple patterns . Each pattern has numOnes consecutive bits on . The amount of overlap between consecutive patterns is configurable via the patternOverlap parameter  <code> def get_simple_patterns(numOnes, numPatterns, patternOverlap=0): ","#LINE# assert (patternOverlap < numOnes) #LINE# numNewBitsInEachPattern = numOnes - patternOverlap #LINE# numCols = numNewBitsInEachPattern * numPatterns + patternOverlap #LINE# p = [] #LINE# for i in xrange(numPatterns): #LINE# #TAB# x = numpy.zeros(numCols, dtype='float32') #LINE# #TAB# startBit = i*numNewBitsInEachPattern #LINE# #TAB# nextStartBit = startBit + numOnes #LINE# #TAB# x[startBit:nextStartBit] = 1 #LINE# #TAB# p.append(x) #LINE# return p"
Return the path to an included declaration <code> def declaration_path(name): ,"#LINE# #TAB# from os.path import dirname, join, exists #LINE# #TAB# import metatab.declarations #LINE# #TAB# from metatab.exc import IncludeError #LINE# #TAB# d = dirname(metatab.declarations.__file__) #LINE# #TAB# path = join(d, name) #LINE# #TAB# if not exists(path): #LINE# #TAB# #TAB# path = join(d, name + '.csv') #LINE# #TAB# if not exists(path): #LINE# #TAB# #TAB# raise IncludeError(""No local declaration file for name '{}' "".format(name)) #LINE# #TAB# return path"
Convert strings of authwords to compiled regular expressions . This regexp will later be used to match these words in authors strings  <code> def compile_authwords(authwords): ,"#LINE# #TAB# return {'ignore': authwords.get('ignore', []), 'after': [re.compile( #LINE# #TAB# #TAB# RE_AFTER.format(word)) for word in authwords.get('after', [])], #LINE# #TAB# #TAB# 'separators': [re.compile(RE_SEPARATOR.format(word)) for word in [( #LINE# #TAB# #TAB# ' %s' % word) for word in authwords.get('separators', [])] + [',', #LINE# #TAB# #TAB# ';']]}"
Iterate over zip archive  <code> def iter_zip(arch_f): ,"#LINE# #TAB# with _open_or_pass(arch_f) as fobj: #LINE# #TAB# #TAB# z = zipfile.ZipFile(fobj) #LINE# #TAB# #TAB# for member in z.infolist(): #LINE# #TAB# #TAB# #TAB# extract_file = z.open(member) #LINE# #TAB# #TAB# #TAB# if member.is_dir(): #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# path = _normpath(member.filename) #LINE# #TAB# #TAB# #TAB# if not path: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# yield [path, extract_file]"
Return the indices of all duplicated array elements  <code> def dup_idx(arr): ,"#LINE# #TAB# _, b = np.unique(arr, return_inverse=True) #LINE# #TAB# return np.nonzero(np.logical_or.reduce( #LINE# #TAB# #TAB# b[:, np.newaxis] == np.nonzero(np.bincount(b) > 1), #LINE# #TAB# #TAB# axis=1))[0]"
Returns user Store object  <code> def get_store(cls): ,#LINE# #TAB# store_path = cls.get_store_path() #LINE# #TAB# store = JsonStore(store_path) #LINE# #TAB# return store
"Extract catalog from VO web service  <code> def get_catalog(ra, dec, sr=0.1, fmt='CSV', catalog='GSC241'): ","#LINE# #TAB# serviceType = 'vo/CatalogSearch.aspx' #LINE# #TAB# spec_str = 'RA={}&DEC={}&SR={}&FORMAT={}&CAT={}&MINDET=5' #LINE# #TAB# headers = {'Content-Type': 'text/csv'} #LINE# #TAB# spec = spec_str.format(ra, dec, sr, fmt, catalog) #LINE# #TAB# serviceUrl = '{}/{}?{}'.format(SERVICELOCATION, serviceType, spec) #LINE# #TAB# rawcat = requests.get(serviceUrl, headers=headers) #LINE# #TAB# r_contents = rawcat.content.decode() #LINE# #TAB# rstr = r_contents.split('\r\n') #LINE# #TAB# del rstr[0] #LINE# #TAB# r_csv = csv.DictReader(rstr) #LINE# #TAB# return r_csv"
Template tag to render a feedback form  <code> def feedback_form(context): ,"#LINE# #TAB# user = None #LINE# #TAB# url = None #LINE# #TAB# if context.get('request'): #LINE# #TAB# #TAB# url = context['request'].path #LINE# #TAB# #TAB# if context['request'].user.is_authenticated(): #LINE# #TAB# #TAB# #TAB# user = context['request'].user #LINE# #TAB# return {'form': FeedbackForm(url=url, user=user), 'background_color': #LINE# #TAB# #TAB# FEEDBACK_FORM_COLOR, 'text_color': FEEDBACK_FORM_TEXTCOLOR, 'text': #LINE# #TAB# #TAB# FEEDBACK_FORM_TEXT}"
"Get classifications for a given patent_search object  <code> def get_classifications(patent_search, session): ",#LINE# #TAB# pubs = patent_search.publications #LINE# #TAB# big_list = [] #LINE# #TAB# for pub in pubs: #LINE# #TAB# #TAB# if pub.raw_classification: #LINE# #TAB# #TAB# #TAB# big_list = big_list + process_classification(pub.raw_classification #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# return big_list
Duplicate sys.stdout and sys.stderr to new StringIO  <code> def tee_output_python(): ,"#LINE# #TAB# buffer = StringIO() #LINE# #TAB# out = CapturedStdout(buffer) #LINE# #TAB# orig_stdout, orig_stderr = sys.stdout, sys.stderr #LINE# #TAB# flush() #LINE# #TAB# sys.stdout = TeeingStreamProxy(sys.stdout, buffer) #LINE# #TAB# sys.stderr = TeeingStreamProxy(sys.stderr, buffer) #LINE# #TAB# try: #LINE# #TAB# #TAB# yield out #LINE# #TAB# finally: #LINE# #TAB# #TAB# flush() #LINE# #TAB# #TAB# out.finalize() #LINE# #TAB# #TAB# sys.stdout, sys.stderr = orig_stdout, orig_stderr"
"Parse * dsn_url * into a tuple of ` ( cls , opts ) `  <code> def parse_dsn(dsn_url, scheme_map, default_scheme=None): ","#LINE# #TAB# dsn = urlparse.urlsplit(dsn_url) #LINE# #TAB# scheme = dsn.scheme or default_scheme #LINE# #TAB# if scheme not in scheme_map: #LINE# #TAB# #TAB# raise DSNParseError('scheme must be one of %r, not %r' % ( #LINE# #TAB# #TAB# #TAB# scheme_map.keys(), dsn.scheme)) #LINE# #TAB# return scheme_map[scheme], dsn"
"Copy files only when contents differ  <code> def copy_path_maybe(path_from, path_to, preserve=True, dereference=False): ","#LINE# #TAB# if not os.path.exists(path_to) or os.path.isdir(path_from): #LINE# #TAB# #TAB# copy_path(path_from, path_to, preserve=preserve, dereference= #LINE# #TAB# #TAB# #TAB# dereference) #LINE# #TAB# #TAB# return True #LINE# #TAB# with open(path_from) as file_from, open(path_to) as file_to: #LINE# #TAB# #TAB# lines_from = file_from.readlines() #LINE# #TAB# #TAB# lines_to = file_to.readlines() #LINE# #TAB# if lines_from == lines_to: #LINE# #TAB# #TAB# return False #LINE# #TAB# copy_path(path_from, path_to, preserve=preserve, dereference=dereference) #LINE# #TAB# return True"
Return an numpy array of np.uint8 representing the selection . The array can be smaller than the size of data  <code> def unpack_selection(saved_selection): ,"#LINE# #TAB# if saved_selection is None or len(saved_selection) == 0: #LINE# #TAB# #TAB# return np.array([], dtype=np.uint8) #LINE# #TAB# if isinstance(saved_selection, array.array): #LINE# #TAB# #TAB# return np.array(saved_selection, dtype=np.uint8) #LINE# #TAB# else: #LINE# #TAB# #TAB# a = np.array(saved_selection) #LINE# #TAB# #TAB# maxi = np.max(a[:, (0)]) #LINE# #TAB# #TAB# r = np.zeros(maxi + 1, dtype=np.uint8) #LINE# #TAB# #TAB# r[a[:, (0)]] = a[:, (1)] #LINE# #TAB# #TAB# return r"
Whether the value is or can be converted to a number . > > > is_number('fish ' ) False > > > is_number('20.2 ' ) True > > > is_number(70 ) True <code> def is_number(v): ,#LINE# #TAB# o = as_number(v) #LINE# #TAB# return o is not None
"Make sure any event day we send back for weekday repeating events is not a weekend  <code> def check_weekday(year, month, day, reverse=False): ","#LINE# #TAB# d = date(year, month, day) #LINE# #TAB# while d.weekday() in (5, 6): #LINE# #TAB# #TAB# if reverse: #LINE# #TAB# #TAB# #TAB# d -= timedelta(days=1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# d += timedelta(days=1) #LINE# #TAB# return d.year, d.month, d.day"
Utility function for merging multiple layouts  <code> def merge_layouts(layouts): ,"#LINE# #TAB# layout = layouts[0].clone() #LINE# #TAB# for l in layouts[1:]: #LINE# #TAB# #TAB# layout.files.update(l.files) #LINE# #TAB# #TAB# layout.domains.update(l.domains) #LINE# #TAB# #TAB# for k, v in l.entities.items(): #LINE# #TAB# #TAB# #TAB# if k not in layout.entities: #LINE# #TAB# #TAB# #TAB# #TAB# layout.entities[k] = v #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# layout.entities[k].files.update(v.files) #LINE# #TAB# return layout"
Admin dashboard tag for cartridge to show daily sales totals . Excluding tax and shipping  <code> def sales_today(context): ,"#LINE# #TAB# today_min = localtime(now()).replace(hour=0, minute=0, second=0, #LINE# #TAB# #TAB# microsecond=0) #LINE# #TAB# today_max = localtime(now()).replace(hour=23, minute=59, second=59, #LINE# #TAB# #TAB# microsecond=59) #LINE# #TAB# output = Order.objects.filter(time__range=(today_min, today_max), #LINE# #TAB# #TAB# status__in=settings.SHOP_ORDER_STATUS_COMPLETE).aggregate(Sum( #LINE# #TAB# #TAB# 'item_total')) #LINE# #TAB# context['daily_total'] = output #LINE# #TAB# context['daily_date'] = today_min #LINE# #TAB# return context"
"Get a datetime in the local timezone from date and optionally time <code> def get_local_datetime(date, time, tz=None, timeDefault=dt.time.max): ","#LINE# #TAB# localTZ = timezone.get_current_timezone() #LINE# #TAB# if tz is None or tz == localTZ: #LINE# #TAB# #TAB# localDt = getAwareDatetime(date, time, tz, timeDefault) #LINE# #TAB# else: #LINE# #TAB# #TAB# eventDt = getAwareDatetime(date, time, tz, timeDefault) #LINE# #TAB# #TAB# localDt = eventDt.astimezone(localTZ) #LINE# #TAB# #TAB# if time is None: #LINE# #TAB# #TAB# #TAB# localDt = getAwareDatetime(localDt.date(), None, localTZ, timeDefault) #LINE# #TAB# return localDt"
Returns a list of all FSPS filter names . Filters are sorted by their FSPS index  <code> def list_filters(): ,"#LINE# #TAB# lst = [(name, f.index) for name, f in FILTERS.items()] #LINE# #TAB# lst.sort(key=lambda x: x[1]) #LINE# #TAB# return [l[0] for l in lst]"
Return the current user ID . : return : A unique user ID string or ` ` None ` ` if not available  <code> def fetch_current_user_id(cls): ,#LINE# #TAB# if not _in_flask_context(): #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# from flask_login import current_user #LINE# #TAB# #TAB# return current_user.email if current_user.is_authenticated else None #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return None
Returns the : py : class:`PlannedResources ` of the holder if it does not yet exist one is created <code> def planned_resources_of_holder(holder): ,#LINE# #TAB# if not 'planned_resources' in holder: #LINE# #TAB# #TAB# holder['planned_resources'] = PlannedResources() #LINE# #TAB# return holder['planned_resources']
"Parse the configuration options given to flake8  <code> def parse_options(cls, options): ",#LINE# #TAB# cls.convention = options.docstring_convention #LINE# #TAB# cls.ignore_decorators = re.compile(options.ignore_decorators #LINE# #TAB# #TAB# ) if options.ignore_decorators else None
Define common CLI arguments and options  <code> def base_cli(parser=None): ,#LINE# #TAB# if parser is None: #LINE# #TAB# #TAB# parser = argparse.ArgumentParser(formatter_class=argparse. #LINE# #TAB# #TAB# #TAB# ArgumentDefaultsHelpFormatter) #LINE# #TAB# onefs_cli(parser.add_argument_group('OneFS')) #LINE# #TAB# logging_cli(parser.add_argument_group('Logging')) #LINE# #TAB# return parser
return all the factors of n <code> def get_factors(n): ,"#LINE# #TAB# factors = set() #LINE# #TAB# for i in range(1, int(n ** 0.5) + 1): #LINE# #TAB# #TAB# if not n % i: #LINE# #TAB# #TAB# #TAB# factors.update((i, n // i)) #LINE# #TAB# return factors"
Gets a string that will hopefully help in identifing an element when there is an error  <code> def element_identification_string(element): ,#LINE# #TAB# info_string = 'tag: {}'.format(element.tag) #LINE# #TAB# try: #LINE# #TAB# #TAB# elem_id = element.attrib['id'] #LINE# #TAB# #TAB# info_string += ' id: {}'.format(elem_id) #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return info_string
"Convert the single model into a dict for serialization <code> def to_rest_model(model, includes=None): ","#LINE# #TAB# props = {} #LINE# #TAB# props['data'] = _to_rest(model, includes=includes) #LINE# #TAB# props['included'] = _to_rest_includes(model, includes=includes) #LINE# #TAB# return props"
Find file associated with the package key  <code> def get_associated_file(package_key): ,"#LINE# #TAB# package_key = package_key.replace('-', '_') #LINE# #TAB# for path in flatten([glob.iglob('*.whl'), glob.iglob('*.tar.gz')]): #LINE# #TAB# #TAB# if path.startswith(package_key): #LINE# #TAB# #TAB# #TAB# return path"
Util for safe deepcopy of a dict . Solves the issue with ' TypeError : ca n't pickle dict_items objects ' of the default ' copy.deepcopy '  <code> def safe_deepcopy_dict(d): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# new_d = copy.deepcopy(d) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# new_d = dict() #LINE# #TAB# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# #TAB# new_d[k] = copy.deepcopy(list(v.items())) #LINE# #TAB# return new_d"
"Hard thresholding function on Tensor A with sparsity s <code> def hard_threshold(A, s): ","#LINE# #TAB# A_ = np.copy(A) #LINE# #TAB# A_ = A_.ravel() #LINE# #TAB# if len(A_) > 0: #LINE# #TAB# #TAB# th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher') #LINE# #TAB# #TAB# A_[np.abs(A_) < th] = 0.0 #LINE# #TAB# A_ = A_.reshape(A.shape) #LINE# #TAB# return A_"
Internal helper that returns the default endpoint for a given function . This always is the function name  <code> def endpoint_from_view_func(view_func): ,"#LINE# #TAB# assert view_func is not None, 'expected view func if endpoint is not provided.' #LINE# #TAB# return view_func.__name__"
"return a shortest table name <code> def model_name(constraint, table): ",#LINE# #TAB# name = table.name.split('_') #LINE# #TAB# if len(name) == 1: #LINE# #TAB# #TAB# return name[0] #LINE# #TAB# return ''.join(x[0] for x in name[:-1]) + '_' + name[-1]
Parses the provided arguments and exits on an error . Use the option -h on the command line to get an overview of the required and optional arguments  <code> def parse_the_arguments(argv): ,"#LINE# #TAB# parser = argparse.ArgumentParser() #LINE# #TAB# parser.add_argument('-p', '--project', required=True, action='store', #LINE# #TAB# #TAB# dest='project_name', help= #LINE# #TAB# #TAB# 'Project name in which the folder needs to be created.') #LINE# #TAB# parser.add_argument('-f', '--folder', required=True, action='store', #LINE# #TAB# #TAB# dest='folder_name', help='Name for the folder to be created.') #LINE# #TAB# args = parser.parse_args() #LINE# #TAB# return args"
"Expects iterable to be ( key , value ) tuples  <code> def group_to_dict(iterable): ","#LINE# #TAB# d = defaultdict(list) #LINE# #TAB# for key, value in iterable: #LINE# #TAB# #TAB# d[key].append(value) #LINE# #TAB# return d"
"Return a list of at most ` n ` elements of an iterable , or all the elements of the iterable if there are less than ` n ` <code> def take_at_most(iterable, n): ",#LINE# #TAB# iterator = iter(iterable) #LINE# #TAB# result = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# for _ in range(n): #LINE# #TAB# #TAB# #TAB# result.append(next(iterator)) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# pass #LINE# #TAB# return result
"adds state abbreviations to a dataframe , based on state codes <code> def convert_state_codes(dataframe): ","#LINE# #TAB# state_codes = pandas.DataFrame(np.array([i for i in STATE_CODES.items() #LINE# #TAB# #TAB# ], dtype=np.dtype([('state', '|U2'), ('code', int)]))) #LINE# #TAB# merged = pandas.merge(dataframe, state_codes, left_on='state_code', #LINE# #TAB# #TAB# right_on='code', how='left') #LINE# #TAB# column_names = dataframe.columns.tolist() #LINE# #TAB# column_names.remove('state_code') #LINE# #TAB# column_names.insert(0, 'state') #LINE# #TAB# return merged[column_names]"
"Get information about this current console window  <code> def get_console_info(kernel32, handle): ","#LINE# #TAB# csbi = ConsoleScreenBufferInfo() #LINE# #TAB# lpcsbi = ctypes.byref(csbi) #LINE# #TAB# dword = ctypes.c_ulong() #LINE# #TAB# lpdword = ctypes.byref(dword) #LINE# #TAB# if not kernel32.GetConsoleScreenBufferInfo(handle, lpcsbi) or not kernel32.GetConsoleMode(handle, lpdword): #LINE# #TAB# #TAB# raise ctypes.WinError() #LINE# #TAB# fg_color = csbi.wAttributes % 16 #LINE# #TAB# bg_color = csbi.wAttributes & 240 #LINE# #TAB# native_ansi = bool(dword.value & ENABLE_VIRTUAL_TERMINAL_PROCESSING) #LINE# #TAB# return fg_color, bg_color, native_ansi"
Yield filenames for all loaded Python modules  <code> def get_loaded_modules(): ,"#LINE# #TAB# for module in sys.modules.values(): #LINE# #TAB# #TAB# filename = getattr(module, '__file__', None) #LINE# #TAB# #TAB# if filename: #LINE# #TAB# #TAB# #TAB# uncompiled = re.sub('py[co]$', 'py', filename) #LINE# #TAB# #TAB# #TAB# yield uncompiled"
"X : rows = observations , cols = variables varcol : variable for which different colors should be assigned N : number of colors that can be used <code> def pretty_coloring(X, varcol=0, N=100): ","#LINE# #TAB# coloridx = colorindex(X[:, (varcol)], N) #LINE# #TAB# colors = np.array(get_colors(N)) #LINE# #TAB# return colors[(coloridx), :]"
"Asserts that ` topic = = expected `  <code> def to_equal(topic, expected): ",#LINE# #TAB# topic = utils.fix_string(topic) #LINE# #TAB# expected = utils.fix_string(expected) #LINE# #TAB# try: #LINE# #TAB# #TAB# return expected == topic #LINE# #TAB# except: #LINE# #TAB# #TAB# return False
Returns Instagram user ID for the given handler <code> def get_id(handler): ,"#LINE# #TAB# req = requests.get(BASE_URL.format(handler=handler)) #LINE# #TAB# try: #LINE# #TAB# #TAB# return re.search(PATTERN_PUBLIC, req.text).group(1) #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return re.search(PATTERN_PRIVATE, req.text).group(1) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# return None"
Get a list of all devices on pci slots using xhci drivers <code> def get_udev_xhci_devices(udev_client): ,#LINE# #TAB# enumerator = GUdev.Enumerator(client=udev_client) #LINE# #TAB# enumerator.add_match_subsystem('pci') #LINE# #TAB# devices = [device for device in enumerator.execute() if device. #LINE# #TAB# #TAB# get_driver() == 'xhci_hcd'] #LINE# #TAB# devices.sort(key=lambda device: device.get_property('PCI_SLOT_NAME')) #LINE# #TAB# return devices
"Manage response for request VoteAboutDraw . : param context : request context : param response : response received : return : None : type context : RequestFutureContext <code> def on_vote(context, response): ",#LINE# #TAB# request = context.request #LINE# #TAB# vote = request.vote #LINE# #TAB# assert Game.is_player_game(context.game) #LINE# #TAB# assert context.game.power.name == context.request.game_role #LINE# #TAB# context.game.power.vote = vote
"Recursively traverses from root_path to find all .mat files Returns a list of .mat files , with full path Parameters ----------------------- root_path : string The absolute path to start searching from <code> def get_matlab_filepaths(root_path): ","#LINE# #TAB# matlab_filepaths = [] #LINE# #TAB# for root, dirs, files in os.walk(root_path): #LINE# #TAB# #TAB# mat_files = [f for f in files if f[-4:] == '.mat'] #LINE# #TAB# #TAB# for f in mat_files: #LINE# #TAB# #TAB# #TAB# matlab_filepaths.append(os.path.join(root, f)) #LINE# #TAB# return matlab_filepaths"
"Read a date out of a catalog brain  <code> def brain_date(brain, key): ","#LINE# #TAB# if key in brain: #LINE# #TAB# #TAB# value = brain[key] #LINE# #TAB# #TAB# if value not in (Missing.Value, None): #LINE# #TAB# #TAB# #TAB# return value.HTML4() #LINE# #TAB# return ''"
"Convert an index among allowed speeds to its corresponding speed : param index : the speed index [ ] : return : the corresponding speed [ m / s ] <code> def index_to_speed(cls, index): ",#LINE# #TAB# if cls.SPEED_COUNT > 1: #LINE# #TAB# #TAB# return cls.SPEED_MIN + index * (cls.SPEED_MAX - cls.SPEED_MIN) / (cls #LINE# #TAB# #TAB# #TAB# .SPEED_COUNT - 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return cls.SPEED_MIN
Parse stdout and return the energy of the reference wavefunction  <code> def parse_reference_energy(stdout: str): ,"#LINE# #TAB# energy_dict = PreservingDict() #LINE# #TAB# total_energy_re = re.compile('total energy\\s+=\\s+([\\d\\-\\.]+)') #LINE# #TAB# mobj = total_energy_re.search(stdout) #LINE# #TAB# total_energy = Decimal(mobj[1]) #LINE# #TAB# energy_key = 'HF TOTAL ENERGY' #LINE# #TAB# dft_mobj = re.search('density functional', stdout) #LINE# #TAB# if dft_mobj: #LINE# #TAB# #TAB# energy_key = 'DFT TOTAL ENERGY' #LINE# #TAB# energy_dict[energy_key] = total_energy #LINE# #TAB# current_energy = total_energy #LINE# #TAB# energy_dict['CURRENT ENERGY'] = current_energy #LINE# #TAB# return energy_dict"
Convert a table row to a dictionary  <code> def row_to_dict(row): ,"#LINE# #TAB# o = {} #LINE# #TAB# for colname in row.colnames: #LINE# #TAB# #TAB# if isinstance(row[colname], np.string_) and row[colname].dtype.kind in ['S', 'U']: #LINE# #TAB# #TAB# #TAB# o[colname] = str(row[colname]) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# o[colname] = row[colname] #LINE# #TAB# return o"
"Required by the ` ` check ` ` management command , this returns a list of messages from all the relevant check functions for this version of Django  <code> def run_checks(): ","#LINE# #TAB# checks = [check_test_runner(), check_boolean_field_default_value()] #LINE# #TAB# return [output for output in checks if output]"
Return a map containing the settle modes as provided by the remote . Skip any default value  <code> def get_remote_settle_modes(pn_link): ,#LINE# #TAB# modes = {} #LINE# #TAB# snd = pn_link.remote_snd_settle_mode #LINE# #TAB# if snd == proton.Link.SND_UNSETTLED: #LINE# #TAB# #TAB# modes['snd-settle-mode'] = 'unsettled' #LINE# #TAB# elif snd == proton.Link.SND_SETTLED: #LINE# #TAB# #TAB# modes['snd-settle-mode'] = 'settled' #LINE# #TAB# if pn_link.remote_rcv_settle_mode == proton.Link.RCV_SECOND: #LINE# #TAB# #TAB# modes['rcv-settle-mode'] = 'second' #LINE# #TAB# return modes
"Unlink a file , but do n't complain if it does n't exist  <code> def soft_unlink(file): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# os.unlink(file) #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# pass
Checks the dekad of a date and returns the dekad date . Parameters ---------- date : datetime Date to check . Returns ------- new_date : datetime Date of the dekad  <code> def check_dekad(date): ,"#LINE# #TAB# if date.day < 11: #LINE# #TAB# #TAB# dekad = 10 #LINE# #TAB# elif date.day > 10 and date.day < 21: #LINE# #TAB# #TAB# dekad = 20 #LINE# #TAB# else: #LINE# #TAB# #TAB# dekad = calendar.monthrange(date.year, date.month)[1] #LINE# #TAB# new_date = datetime(date.year, date.month, dekad) #LINE# #TAB# return new_date"
"Normalize the key name to title case  <code> def normalize_name(name, overrides=None): ","#LINE# #TAB# normalized_name = name.title() #LINE# #TAB# if overrides: #LINE# #TAB# #TAB# override_map = dict([(name.title(), name) for name in overrides]) #LINE# #TAB# #TAB# return override_map.get(normalized_name, normalized_name) #LINE# #TAB# else: #LINE# #TAB# #TAB# return normalized_name"
"Convert a string to a boolean value  <code> def to_bool(value, do_raise=True): ","#LINE# #TAB# #TAB# value = value.lower() #LINE# #TAB# #TAB# if value.isdigit(): #LINE# #TAB# #TAB# #TAB# return bool(int(value)) #LINE# #TAB# #TAB# if value in _str_true: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif value in _str_false: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# if do_raise: #LINE# #TAB# #TAB# #TAB# raise ValueError(""invalid literal for to_bool(): %r"" % value) #LINE# #TAB# #TAB# return False"
"Typedef generator for event statistics records  <code> def typedef_event_stat(flavour, list_flavour, addon=None): ","#LINE# #TAB# tdef = {'_id': {'type': flavour['String'], 'required': True}, 'ts': { #LINE# #TAB# #TAB# 'type': flavour['Datetime'], 'required': True, 'default': time.time #LINE# #TAB# #TAB# }, 'ts_from': {'type': flavour['Datetime'], 'required': True, #LINE# #TAB# #TAB# 'default': time.time}, 'ts_to': {'type': flavour['Datetime'], #LINE# #TAB# #TAB# 'required': True, 'default': time.time}, 'count': {'type': flavour[ #LINE# #TAB# #TAB# 'Integer'], 'required': True}, 'internal': {'required': True}, #LINE# #TAB# #TAB# 'external': {'required': True}, 'overall': {'required': True}} #LINE# #TAB# if addon is not None: #LINE# #TAB# #TAB# tdef.update(addon) #LINE# #TAB# return tdef"
Normalize attribute names for shorthand and work arounds for limitations in Python 's syntax <code> def clean_attribute(attribute): ,"#LINE# #TAB# attribute = {'cls': 'class', 'className': 'class', 'class_name': #LINE# #TAB# #TAB# 'class', 'fr': 'for', 'html_for': 'for', 'htmlFor': 'for'}.get( #LINE# #TAB# #TAB# attribute, attribute) #LINE# #TAB# if attribute[0] == '_': #LINE# #TAB# #TAB# attribute = attribute[1:] #LINE# #TAB# if attribute in set(['http_equiv']) or attribute.startswith('data_'): #LINE# #TAB# #TAB# attribute = attribute.replace('_', '-').lower() #LINE# #TAB# if attribute.split('_')[0] in ('xlink', 'xml', 'xmlns'): #LINE# #TAB# #TAB# attribute = attribute.replace('_', ':', 1).lower() #LINE# #TAB# return attribute"
"MEV distribution function , to minimize numerically for computing quantiles <code> def mev_fun(y, pr, N, C, W): ",#LINE# #TAB# nyears = N.size #LINE# #TAB# mev0f = np.sum((1 - np.exp(-(y / C) ** W)) ** N) - nyears * pr #LINE# #TAB# return mev0f
"Loops PSMs and outputs dictionaries passed to writer . Dictionaries contain the PSMs and info to which split pool the respective PSM belongs <code> def generate_psms_split(fn, oldheader, bioset, splitcol): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# splitcolnr = get_splitcolnr(oldheader, bioset, splitcol) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# raise RuntimeError('Cannot find bioset header column in ' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'input file {}, though --bioset has ' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'been passed'.format(fn)) #LINE# #TAB# for psm in tsvreader.generate_tsv_psms(fn, oldheader): #LINE# #TAB# #TAB# yield {'psm': psm, #LINE# #TAB# #TAB# #TAB# 'split_pool': psm[oldheader[splitcolnr]] #LINE# #TAB# #TAB# #TAB# }"
Transcrypt the sketch python code to javascript . : param sketch_name : name for new sketch : type sketch_name : string : return : file names : rtype : list of strings <code> def transcrypt_sketch(sketch_name): ,#LINE# #TAB# sketch_files = SketchFiles(sketch_name) #LINE# #TAB# sketch_files.validate_name() #LINE# #TAB# if not sketch_files.sketch_exists: #LINE# #TAB# #TAB# raise PythonSketchDoesNotExist(sketch_files.sketch_py.resolve()) #LINE# #TAB# compile_sketch_js(sketch_files) #LINE# #TAB# return sketch_files
Topic name of this dialog service Returns ------- topic_name : str Topic name of this dialog service <code> def topic_name(cls): ,#LINE# #TAB# cls_name = cls.__name__.lower() #LINE# #TAB# if cls_name.endswith('dialogservice'): #LINE# #TAB# #TAB# cls_name = cls_name[:-13] #LINE# #TAB# elif cls_name.endswith('dialog'): #LINE# #TAB# #TAB# cls_name = cls_name[:-6] #LINE# #TAB# return cls_name
"c - a - a ' : lithium chloride nitrate [ PK74 ]  <code> def psi_li_cl_no3_pk74(T, P): ","#LINE# #TAB# psi = -0.003 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
Gets the storage instance that is to have the compute attached  <code> def get_vol_to_attach(link): ,#LINE# #TAB# if link.target.kind == infrastructure.STORAGE: #LINE# #TAB# #TAB# uid = link.target.attributes['occi.core.id'] #LINE# #TAB# elif link.source.kind == infrastructure.STORAGE: #LINE# #TAB# #TAB# uid = link.source.attributes['occi.core.id'] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise AttributeError('Id of the Volume not found!') #LINE# #TAB# return uid
"Convert ' value ' to int <code> def parse_int(value, default=0): ","#LINE# #TAB# if not value: #LINE# #TAB# #TAB# return default #LINE# #TAB# try: #LINE# #TAB# #TAB# return int(value) #LINE# #TAB# except (ValueError, TypeError): #LINE# #TAB# #TAB# return default"
"Helper that returns the format for struct.pack , given a size and a signedness > > > struct_fmt(4 , False ) ' I ' > > > struct_fmt(8 , True ) ' q ' <code> def struct_fmt(size, signed=False): ","#LINE# #TAB# formats = {(1): 'b', (2): 'h', (4): 'i', (8): 'q'} #LINE# #TAB# assert size in formats, 'unknown size' #LINE# #TAB# if not signed: #LINE# #TAB# #TAB# return formats[size].upper() #LINE# #TAB# else: #LINE# #TAB# #TAB# return formats[size]"
Check if path exists and is a file <code> def exists_file(fname): ,#LINE# #TAB# if fname and os.path.exists(fname) and os.path.isfile(fname): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Query the SQLite3 DB for the drive offset  <code> def drive_db_query(drivestr): ,"#LINE# #TAB# db_path = os.path.join(get_path(), 'resources', 'drives.db') #LINE# #TAB# conn = sqlite3.connect(db_path) #LINE# #TAB# cursor = conn.cursor() #LINE# #TAB# cursor.execute('SELECT Offset FROM Drives WHERE Name LIKE ""%' + #LINE# #TAB# #TAB# drivestr + '%""') #LINE# #TAB# results = cursor.fetchall() #LINE# #TAB# conn.close() #LINE# #TAB# return results"
"Setup persistence diagrams by reading off distances <code> def process_distances(pairs, ordered_simplices): ","#LINE# #TAB# dgms = {} #LINE# #TAB# posneg = np.zeros(len(ordered_simplices)) #LINE# #TAB# for [bi, di] in pairs: #LINE# #TAB# #TAB# bidxs, bd = ordered_simplices[bi] #LINE# #TAB# #TAB# didxs, dd = ordered_simplices[di] #LINE# #TAB# #TAB# assert posneg[bi] == 0 and posneg[di] == 0 #LINE# #TAB# #TAB# posneg[bi], posneg[di] = 1, -1 #LINE# #TAB# #TAB# assert dd >= bd #LINE# #TAB# #TAB# p = len(bidxs) - 1 #LINE# #TAB# #TAB# if bd != dd: #LINE# #TAB# #TAB# #TAB# dgms.setdefault(p, []).append([bd, dd]) #LINE# #TAB# return dgms"
"Return a Numpy array from a Pandas dataframe . Iterating over a DataFrame has weird side effects , such as the first row being the column names . Converting to Numpy is more safe  <code> def if_pandas_df_convert_to_numpy(obj): ","#LINE# #TAB# if pd is not None and isinstance(obj, pd.DataFrame): #LINE# #TAB# #TAB# return obj.values #LINE# #TAB# else: #LINE# #TAB# #TAB# return obj"
Get instance metadata  <code> def get_metadata(instance): ,"#LINE# #TAB# profile_name = Container.config.discovery.profile_name() #LINE# #TAB# region = instance['Placement']['AvailabilityZone'][:-1] #LINE# #TAB# metadata = copy.copy(instance) #LINE# #TAB# metadata['Tags'] = dict([i.get('Key'), i.get('Value')] for i in #LINE# #TAB# #TAB# metadata.get('Tags', [])) #LINE# #TAB# metadata['region'] = region #LINE# #TAB# metadata['profile'] = profile_name #LINE# #TAB# metadata.pop('BlockDeviceMappings', None) #LINE# #TAB# metadata.pop('LaunchTime', None) #LINE# #TAB# metadata.pop('NetworkInterfaces', None) #LINE# #TAB# return metadata"
"Converts P , Pdot to F , Fdot ( or vice versa ) Convert period , period derivative and period second derivative to the equivalent frequency counterparts . Will also convert from f to p <code> def p_to_f(p, pd, pdd=None): ","#LINE# #TAB# f = 1.0 / p #LINE# #TAB# fd = -pd / (p * p) #LINE# #TAB# if pdd is None: #LINE# #TAB# #TAB# return [f, fd] #LINE# #TAB# else: #LINE# #TAB# #TAB# if pdd == 0.0: #LINE# #TAB# #TAB# #TAB# fdd = 0.0 #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# fdd = 2.0 * pd * pd / p ** 3.0 - pdd / (p * p) #LINE# #TAB# #TAB# return [f, fd, fdd]"
"Get transformer connected to given buses  <code> def connected_transformer(network, busids): ",#LINE# #TAB# mask = (network.transformers.bus0.isin(busids)) #LINE# #TAB# return network.transformers[mask]
"Create intent token dictionary <code> def create_intent_token_dict(intents, intent_split_symbol): ","#LINE# #TAB# distinct_tokens = set([token for intent in intents for token in intent. #LINE# #TAB# #TAB# split(intent_split_symbol)]) #LINE# #TAB# return {token: idx for idx, token in enumerate(sorted(distinct_tokens))}"
Generates a raw URI - M based on the archive it belongs to . Supported URI patterns are found in ` archive_mappings `  <code> def generate_raw_urim(urim): ,"#LINE# #TAB# raw_urim = urim #LINE# #TAB# for domainname in archive_mappings: #LINE# #TAB# #TAB# if domainname in urim: #LINE# #TAB# #TAB# #TAB# search_pattern = archive_mappings[domainname][0] #LINE# #TAB# #TAB# #TAB# replacement_pattern = archive_mappings[domainname][1] #LINE# #TAB# #TAB# #TAB# if replacement_pattern not in urim: #LINE# #TAB# #TAB# #TAB# #TAB# raw_urim = urim.replace(search_pattern, replacement_pattern) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return raw_urim"
Load and return the person dataset  <code> def load_person_dataset(): ,"#LINE# #TAB# base_dir = join(dirname(__file__), 'data') #LINE# #TAB# table_A = pd.read_csv(join(base_dir, 'person_table_A.csv')) #LINE# #TAB# table_B = pd.read_csv(join(base_dir, 'person_table_B.csv')) #LINE# #TAB# return table_A, table_B"
"Return a list of meta_type who implements the given interface  <code> def meta_types_for_interface(interface, excepts=[]): ","#LINE# #TAB# addables = Products.meta_types #LINE# #TAB# if interface.extends(ISilvaObject): #LINE# #TAB# #TAB# addables = extensionRegistry.get_addables() #LINE# #TAB# return [addable['name'] for addable in filter_types_for_interfaces( #LINE# #TAB# #TAB# addables, [interface], excepts)]"
"Take an integer and interpret it as a set of enum values  <code> def parse_bitfield(cls, the_int): ","#LINE# #TAB# if not isinstance(the_int, int): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'Argument should be an int, we received {the_int} fo type {type(the_int)}' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return {cls(b) for b in cls._bits(the_int)}"
"Parse a string containing a sequence of PEM Tacks . Raise a SyntaxError if input is malformed  <code> def create_from_pem_list(cls, data): ",#LINE# #TAB# tacks = [] #LINE# #TAB# bList = PEMDecoder(data).decodeList('TACK') #LINE# #TAB# for b in bList: #LINE# #TAB# #TAB# tacks.append(Tack(b)) #LINE# #TAB# return tacks
Return a clean - up vars dict . All private variables are removed and magni configgers are converted to dictionaries . Parameters ---------- vars_dict : dict The dictionary of variables to clean . Returns ------- cleaned_vars : dict The cleaned dictionary of variables  <code> def clean_vars(vars_dict): ,"#LINE# #TAB# cleaned_vars = dict() #LINE# #TAB# for key, val in vars_dict.items(): #LINE# #TAB# #TAB# if not key.startswith('_'): #LINE# #TAB# #TAB# #TAB# if isinstance(val, _Configger): #LINE# #TAB# #TAB# #TAB# #TAB# cleaned_vars[key] = dict(val) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# cleaned_vars[key] = val #LINE# #TAB# return cleaned_vars"
"Registers a decoder for a specific encoding method  <code> def RegisterDecoder(cls, decoder): ",#LINE# #TAB# encoding_method = decoder.ENCODING_METHOD.lower() #LINE# #TAB# if encoding_method in cls._decoders: #LINE# #TAB# raise KeyError( #LINE# #TAB# #TAB# 'Decoder for encoding method: {0:s} already set.'.format( #LINE# #TAB# #TAB# #TAB# decoder.ENCODING_METHOD)) #LINE# #TAB# cls._decoders[encoding_method] = decoder
Split extension off the path info <code> def split_extension(event): ,"#LINE# #TAB# request = event.request #LINE# #TAB# path, ext = splitext(request.path_info) #LINE# #TAB# if not path.startswith('/static'): #LINE# #TAB# #TAB# request.original_path_info = request.path_info #LINE# #TAB# #TAB# request.path_info = path #LINE# #TAB# request.path_extension = ext"
In many cases this is equivalent to cs.vertcat  <code> def seq_to_sx_matrix(seq): ,"#LINE# #TAB# n = len(seq) #LINE# #TAB# e0 = SX(seq[0]) #LINE# #TAB# if e0.shape == (1, 1): #LINE# #TAB# #TAB# res = SX(n, 1) #LINE# #TAB# #TAB# for i, elt in enumerate(seq): #LINE# #TAB# #TAB# #TAB# res[i, 0] = elt #LINE# #TAB# #TAB# return res #LINE# #TAB# else: #LINE# #TAB# #TAB# n1, n2 = e0.shape #LINE# #TAB# #TAB# res = SX(n1, n2 * n) #LINE# #TAB# #TAB# for i, elt in enumerate(seq): #LINE# #TAB# #TAB# #TAB# res[:, (i)] = elt #LINE# #TAB# #TAB# return res"
"Cut unicode string from right to fit a given width  <code> def unicode_right(s, width): ",#LINE# #TAB# i = len(s) #LINE# #TAB# j = 0 #LINE# #TAB# for ch in reversed(s): #LINE# #TAB# #TAB# j += __unicode_width_mapping[east_asian_width(ch)] #LINE# #TAB# #TAB# if width < j: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# i -= 1 #LINE# #TAB# return s[i:]
"Parses provider and term from given identifiers annotation uri . Parameters ---------- uri : str uri ( identifiers.org url ) Returns ------- ( provider , identifier ) if resolvable , None otherwise <code> def parse_annotation_info(uri): ","#LINE# #TAB# match = URL_IDENTIFIERS_PATTERN.match(uri) #LINE# #TAB# if match: #LINE# #TAB# #TAB# provider, identifier = match.group(1), match.group(2) #LINE# #TAB# #TAB# if provider.isupper(): #LINE# #TAB# #TAB# #TAB# identifier = '%s:%s' % (provider, identifier) #LINE# #TAB# #TAB# #TAB# provider = provider.lower() #LINE# #TAB# else: #LINE# #TAB# #TAB# LOGGER.warning( #LINE# #TAB# #TAB# #TAB# ""%s does not conform to 'http(s)://identifiers.org/collection/id' or'http(s)://identifiers.org/COLLECTION:id"" #LINE# #TAB# #TAB# #TAB# , uri) #LINE# #TAB# #TAB# return None #LINE# #TAB# return provider, identifier"
"If text contains lower case characters , return True  <code> def contains_lower(text: str) ->bool: ",#LINE# #TAB# for character in text: #LINE# #TAB# #TAB# if character.islower(): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
"Check whether a job exists and return a bool . Parameters ---------- status_dir : str Directory containing json status file . job_name : str Unique job name identification . Returns ------- exists : bool True if the job exists in the status json  <code> def job_exists(cls, status_dir, job_name): ","#LINE# #TAB# if job_name.endswith('.h5'): #LINE# #TAB# #TAB# job_name = job_name.replace('.h5', '') #LINE# #TAB# obj = cls(status_dir) #LINE# #TAB# exists = False #LINE# #TAB# if obj.data: #LINE# #TAB# #TAB# for jobs in obj.data.values(): #LINE# #TAB# #TAB# #TAB# if jobs: #LINE# #TAB# #TAB# #TAB# #TAB# for name in jobs.keys(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if name == job_name: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# exists = True #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return exists"
Turn a list of metadata into a list of printable representations <code> def metadata_repr_as_list(metadata_list): ,"#LINE# #TAB# output = [] #LINE# #TAB# for metadata_dict in metadata_list: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# output.append('%s - %s' % (MetadataType.objects.get(pk= #LINE# #TAB# #TAB# #TAB# #TAB# metadata_dict['id']), metadata_dict.get('value', ''))) #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return output"
Checks that * skl2onnx * converter is available  <code> def skl2onnx_installed(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# import skl2onnx #LINE# #TAB# #TAB# return True #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False
Set config defaults  <code> def load_defaults(config): ,"#LINE# #TAB# config_defaults = config['Config'].get('Defaults', {}) #LINE# #TAB# defaults = {} #LINE# #TAB# defaults['type'] = config_defaults.get('Type', 'list') #LINE# #TAB# defaults['source'] = config_defaults.get('Source', None) #LINE# #TAB# defaults['source_loader'] = config_defaults.get('SourceLoader', None) #LINE# #TAB# defaults['source_type'] = config_defaults.get('SourceType', None) #LINE# #TAB# defaults['randomizer'] = config_defaults.get('Randomizer', None) #LINE# #TAB# defaults['use_name_skp'] = config_defaults.get('UseNameForSourceKeyPath', #LINE# #TAB# #TAB# None) #LINE# #TAB# return defaults"
reset the parameter to default value <code> def reset_params(): ,#LINE# #TAB# GlobalParams._num_of_blocks_per_dataset = 2 #LINE# #TAB# GlobalParams._num_of_files_per_block = 5 #LINE# #TAB# GlobalParams._num_of_runs_per_file = 1 #LINE# #TAB# GlobalParams._num_of_lumis_per_block = 2 #LINE# #TAB# GlobalParams._num_of_events_per_file = 1000 #LINE# #TAB# GlobalParams._size_of_file = 20000000 #LINE# #TAB# GlobalParams._blocks_open_for_writing = False
Uses the Windows Management Instrumentation to identify serial devices  <code> def _detect_available_configs(serial_matcher=None): ,"#LINE# #TAB# #TAB# if serial_matcher: #LINE# #TAB# #TAB# #TAB# channels = find_serial_devices(serial_matcher) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# channels = find_serial_devices() #LINE# #TAB# #TAB# return [{'interface': 'usb2can', 'channel': c} for c in channels]"
"Checks if a file is compressed ( gzip , bzip2 or xz ) <code> def is_compressed(filepath): ","#LINE# #TAB# with open(filepath, 'rb') as fin: #LINE# #TAB# #TAB# signature = fin.peek(8)[:8] #LINE# #TAB# #TAB# if tuple(signature[:2]) == (31, 139): #LINE# #TAB# #TAB# #TAB# return Compression.gzip #LINE# #TAB# #TAB# elif tuple(signature[:3]) == (66, 90, 104): #LINE# #TAB# #TAB# #TAB# return Compression.bzip2 #LINE# #TAB# #TAB# elif tuple(signature[:7]) == (253, 55, 122, 88, 90, 0, 0): #LINE# #TAB# #TAB# #TAB# return Compression.xz #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return Compression.noncompressed"
Test if current environment is MPI or not <code> def is_mpi_env(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# import mpi4py #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# import mpi4py.MPI #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# return False #LINE# #TAB# if mpi4py.MPI.COMM_WORLD.size == 1 and mpi4py.MPI.COMM_WORLD.rank == 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"Compute the histogram ( pdf ) of the data <code> def build_pdf(x, density=True): ","#LINE# #TAB# hist, edges = numpy.histogram(x, bins=build_support(x), density=density) #LINE# #TAB# return hist, edges"
"Finds all of the tiles that a object could intersect with Returns the norms and corners of any of those that are drivable <code> def find_candidate_tiles(obj_corners, tile_size): ","#LINE# #TAB# minx, miny = np.floor(np.amin(obj_corners, axis=0) / tile_size).astype(int) #LINE# #TAB# maxx, maxy = np.floor(np.amax(obj_corners, axis=0) / tile_size).astype(int) #LINE# #TAB# xr = list(range(minx, maxx + 1)) #LINE# #TAB# yr = list(range(miny, maxy + 1)) #LINE# #TAB# possible_tiles = np.array([(x, y) for x in xr for y in yr]) #LINE# #TAB# return possible_tiles"
Parses the given vyper source code and returns a list of python AST objects for all statements in the source . Performs pre - processing of source code before parsing as well as post - processing of the resulting AST  <code> def parse_to_ast(source_code: str) -> List[ast.stmt]: ,"#LINE# #TAB# class_types, reformatted_code = pre_parse(source_code) #LINE# #TAB# if '\x00' in reformatted_code: #LINE# #TAB# #TAB# raise ParserException('No null bytes (\\x00) allowed in the source code.') #LINE# #TAB# parsed_ast = ast.parse(reformatted_code) #LINE# #TAB# annotate_and_optimize_ast(parsed_ast, reformatted_code, class_types) #LINE# #TAB# return parsed_ast.body"
Mimic numpy 's casting for np.sum <code> def get_sum_dtype(dtype): ,"#LINE# #TAB# if np.issubdtype(dtype, np.float_): #LINE# #TAB# #TAB# return np.float_ #LINE# #TAB# if dtype.kind == 'u' and np.can_cast(dtype, np.uint): #LINE# #TAB# #TAB# return np.uint #LINE# #TAB# if np.can_cast(dtype, np.int_): #LINE# #TAB# #TAB# return np.int_ #LINE# #TAB# return dtype"
"Check if a string is a valid duration accepted by DuckDuckGo . A valid duration is d ( past day ) , w ( past week ) , m ( past month ) or a ( any time )  <code> def is_duration(arg): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# if arg not in ('d', 'w', 'm', 'a'): #LINE# #TAB# #TAB# #TAB# raise ValueError #LINE# #TAB# except (TypeError, IndexError, ValueError): #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError('%s is not a valid duration' % arg) #LINE# #TAB# return arg"
Returns sets of elements recognisable as booleans <code> def get_boolean_elements(): ,"#LINE# #TAB# true_set = {'True', 'true', 'TRUE', 'tru', 1, 't', 'T', '1', float(1), #LINE# #TAB# #TAB# True, 'yes', 'Yes', 'YES', 'Y', 'y'} #LINE# #TAB# false_set = {'False', 'false', 'FALSE', 'fals', 0, 'f', 'F', '0', float #LINE# #TAB# #TAB# (0), False, 'no', 'No', 'NO', 'N', 'n'} #LINE# #TAB# return true_set, false_set"
Validate if each element in a list is a registered scope  <code> def validate_scopes(value_list): ,#LINE# #TAB# for value in value_list: #LINE# #TAB# #TAB# if value not in current_oauth2server.scopes: #LINE# #TAB# #TAB# #TAB# raise ScopeDoesNotExists(value) #LINE# #TAB# return True
"Parses the JSON file containing the datasets configuration . Moreover , it validates the loaded object against the defined schema . Returns : config ( dict ) : dictionary of ( dataset_id ) - > configuration  <code> def load_datasets_config(): ","#LINE# #TAB# obj = None #LINE# #TAB# with open(CONFIG_FILE, 'rt') as f: #LINE# #TAB# #TAB# obj = json.load(f) #LINE# #TAB# assert obj is not None #LINE# #TAB# schema = None #LINE# #TAB# with open(CONFIG_SCHEMA_FILE, 'rt') as f: #LINE# #TAB# #TAB# schema = json.load(f) #LINE# #TAB# assert schema is not None #LINE# #TAB# validate(instance=obj, schema=schema) #LINE# #TAB# config = {} #LINE# #TAB# for entry in obj: #LINE# #TAB# #TAB# dataset = entry['dataset'] or '' #LINE# #TAB# #TAB# assert dataset not in config #LINE# #TAB# #TAB# config[dataset] = entry['config'] #LINE# #TAB# return config"
str - > int Bitcoin uses tx version 2 for nSequence signaling . Zcash uses tx version 2 for joinsplits  <code> def guess_version(redeem_script): ,#LINE# #TAB# n = riemann.get_current_network_name() #LINE# #TAB# if 'sprout' in n: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# if 'overwinter' in n: #LINE# #TAB# #TAB# return 3 #LINE# #TAB# if 'sapling' in n: #LINE# #TAB# #TAB# return 4 #LINE# #TAB# try: #LINE# #TAB# #TAB# script_array = redeem_script.split() #LINE# #TAB# #TAB# script_array.index('OP_CHECKSEQUENCEVERIFY') #LINE# #TAB# #TAB# return 2 #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return 1
"Convert token to float . Args : token ( str ) : a string token to convert to float Returns : float if the token can be converted , otherwise the token <code> def convert_to_float(token): ","#LINE# #TAB# if token == '': #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# float(token.replace(',', '')) #LINE# #TAB# #TAB# return float(token.replace(',', '')) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return token"
"Return a ` dict ` of group name to repo names  <code> def get_groups() ->Dict[str, str]: ","#LINE# #TAB# fname = get_config_fname('groups.yml') #LINE# #TAB# groups = {} #LINE# #TAB# if os.path.isfile(fname) and os.stat(fname).st_size > 0: #LINE# #TAB# #TAB# with open(fname, 'r') as f: #LINE# #TAB# #TAB# #TAB# groups = yaml.load(f, Loader=yaml.FullLoader) #LINE# #TAB# return groups"
"Connect to host and execute SELECT 1 to make sure it 's up and running . : return : True if server is ready for connections : rtype : bool <code> def server_ready(host, user='root', password='', port=3306): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# with _connect(host, user=user, password=password, port=port) as conn: #LINE# #TAB# #TAB# #TAB# cursor = conn.cursor() #LINE# #TAB# #TAB# #TAB# query = 'SELECT 1' #LINE# #TAB# #TAB# #TAB# cursor.execute(query) #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# except MySQLError as err: #LINE# #TAB# #TAB# log.error(err) #LINE# #TAB# #TAB# return False"
Return the length of the longest benchmark name in a given list of benchmark JSON objects <code> def find_longest_name(benchmark_list): ,#LINE# #TAB# longest_name = 1 #LINE# #TAB# for bc in benchmark_list: #LINE# #TAB# #TAB# if len(bc['name']) > longest_name: #LINE# #TAB# #TAB# #TAB# longest_name = len(bc['name']) #LINE# #TAB# return longest_name
Extract view name : param view : : return : <code> def extract_view_name(view): ,"#LINE# #TAB# if hasattr(view, '__name__'): #LINE# #TAB# #TAB# return view.__name__ #LINE# #TAB# else: #LINE# #TAB# #TAB# return view.__class__.__name__"
Returns Extended Resource for service type management  <code> def get_resources(cls): ,"#LINE# #TAB# attr_map = apidef.RESOURCE_ATTRIBUTE_MAP[apidef.COLLECTION_NAME] #LINE# #TAB# collection_name = apidef.COLLECTION_NAME.replace('_', '-') #LINE# #TAB# controller = base.create_resource(collection_name, apidef.RESOURCE_NAME, #LINE# #TAB# #TAB# servicetype_db.ServiceTypeManager.get_instance(), attr_map) #LINE# #TAB# return [extensions.ResourceExtension(collection_name, controller, #LINE# #TAB# #TAB# attr_map=attr_map)]"
"Return ( euid , egid ) of process @pid . Raises an error if @pid does not exist <code> def get_process_uid_and_gid(pid): ","#LINE# #TAB# pid_dir = os.path.join('/proc', pid) #LINE# #TAB# if not os.path.isdir(pid_dir): #LINE# #TAB# #TAB# raise KeyError('No such PID %d' % pid) #LINE# #TAB# st = os.stat(pid_dir) #LINE# #TAB# return st.st_uid, st.st_gid"
Helper function to extract plain text from .doc files : param doc_path : path to .doc file to be extracted : return : string of extracted text <code> def extract_text_from_doc(doc_path): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# import textract #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# return ' ' #LINE# #TAB# #TAB# text = textract.process(doc_path).decode('utf-8') #LINE# #TAB# #TAB# return text #LINE# #TAB# except KeyError: #LINE# #TAB# #TAB# return ' '
Return multiple edges between 2 nodes : return : list of edge IDs that connect the same nodes <code> def multi_edges(graph): ,"#LINE# #TAB# multiple_edges = [] #LINE# #TAB# for node in graph.nodes(): #LINE# #TAB# #TAB# for neighbor in graph.neighbors(node): #LINE# #TAB# #TAB# #TAB# if graph.number_of_edges(node, neighbor) > 1: #LINE# #TAB# #TAB# #TAB# #TAB# edge_id = [i for i in graph[node][neighbor]] #LINE# #TAB# #TAB# #TAB# #TAB# if edge_id not in multiple_edges: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# multiple_edges.append(edge_id) #LINE# #TAB# return multiple_edges"
"c - a - a ' : hydrogen chloride sulfate [ HMW84 ]  <code> def psi_h_cl_so4_hmw84(T, P): ","#LINE# #TAB# psi = 0.0 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return psi, valid"
Returns transcript format  <code> def get_transcript_format(transcript_content): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# sjson_obj = json.loads(transcript_content) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# srt_subs = SubRipFile.from_string(transcript_content, error_handling=SubRipFile.ERROR_RAISE) #LINE# #TAB# #TAB# if len(srt_subs) > 0: #LINE# #TAB# #TAB# #TAB# return TranscriptFormat.SRT #LINE# #TAB# return TranscriptFormat.SJSON"
"Calculate lambda in smooth conditions  <code> def lamb_smooth(params, lamb): ",#LINE# #TAB# re = params[0] #LINE# #TAB# return 2 * np.log10(re * lamb ** 0.5) - 0.8 - 1 / lamb ** 0.5
True if the object is a h5py . Group - like object . A file is a group . : param obj : An object <code> def is_group(obj): ,"#LINE# #TAB# t = get_h5_class(obj) #LINE# #TAB# return t in [H5Type.GROUP, H5Type.FILE]"
Return a fixed - length random string : rtype : str <code> def get_random_string(nbytes=8): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# val = binascii.hexlify(os.urandom(nbytes)).decode('utf-8') #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# print('ERROR, perhaps urandom is not supported: %s' % e) #LINE# #TAB# #TAB# val = u''.join(u'%02x' % random.randrange(256) for i in range(nbytes)) #LINE# #TAB# return val"
Monkey - patch urllib3 with SecureTransport - backed SSL - support  <code> def inject_into_urllib3(): ,#LINE# #TAB# util.SSLContext = SecureTransportContext #LINE# #TAB# util.ssl_.SSLContext = SecureTransportContext #LINE# #TAB# util.HAS_SNI = HAS_SNI #LINE# #TAB# util.ssl_.HAS_SNI = HAS_SNI #LINE# #TAB# util.IS_SECURETRANSPORT = True #LINE# #TAB# util.ssl_.IS_SECURETRANSPORT = True
"Computes the radial distance for the given boundary points , according to Xu et al 2012 , "" A comprehensive descriptor of shape "" <code> def compute_radial_distance(points): ","#LINE# #TAB# dist_center = compute_dist_to_center(points) #LINE# #TAB# rad_dist = np.sqrt(dist_center[:, (0)] ** 2 + dist_center[:, (1)] ** 2) #LINE# #TAB# rad_dist_norm = rad_dist / np.amax(rad_dist) #LINE# #TAB# return rad_dist, rad_dist_norm"
Return all instances that compute ` feature_name ` <code> def get_instances(feature_name): ,#LINE# #TAB# feats = [] #LINE# #TAB# for ft in AncillaryFeature.features: #LINE# #TAB# #TAB# if ft.feature_name == feature_name: #LINE# #TAB# #TAB# #TAB# feats.append(ft) #LINE# #TAB# return feats
Get keys specified in arguments <code> def find_keys(args): ,"#LINE# #TAB# key = args['--key'] #LINE# #TAB# if key: #LINE# #TAB# #TAB# return [key] #LINE# #TAB# keyfile = args['--apikeys'] #LINE# #TAB# if keyfile: #LINE# #TAB# #TAB# return read_keyfile(keyfile) #LINE# #TAB# envkey = os.environ.get('TINYPNG_API_KEY', None) #LINE# #TAB# if envkey: #LINE# #TAB# #TAB# return [envkey] #LINE# #TAB# local_keys = join(abspath("".""), ""tinypng.keys"") #LINE# #TAB# if isfile(local_keys): #LINE# #TAB# #TAB# return read_keyfile(local_keys) #LINE# #TAB# home_keys = join(expanduser(""~/.tinypng.keys"")) #LINE# #TAB# if isfile(home_keys): #LINE# #TAB# #TAB# return read_keyfile(home_keys) #LINE# #TAB# return []"
"Change image format from others to jpg . Args : jpeg_dir : str , the dir only have image  <code> def check_image_format(jpeg_dir): ","#LINE# #TAB# image_path_list = glob.glob(os.path.join(jpeg_dir, '*.*')) #LINE# #TAB# for jpeg_path in image_path_list: #LINE# #TAB# #TAB# suffix = os.path.basename(jpeg_path).split('.')[-1] #LINE# #TAB# #TAB# if suffix != 'jpg' and verify_image(jpeg_path): #LINE# #TAB# #TAB# #TAB# image = Image.open(jpeg_path) #LINE# #TAB# #TAB# #TAB# save_path = jpeg_path.replace('.{}'.format(suffix), '.jpg') #LINE# #TAB# #TAB# #TAB# image.save(save_path) #LINE# #TAB# #TAB# #TAB# os.remove(jpeg_path) #LINE# #TAB# #TAB# #TAB# print('Image from {} to {}'.format(jpeg_path, save_path)) #LINE# #TAB# return True"
"Check if x is distinct from each solution in seta . : param seta : a list : param x : a Solution object : return : True or False <code> def is_distinct(sol_list, sol): ",#LINE# #TAB# for ins in sol_list: #LINE# #TAB# #TAB# if sol.is_equal(ins): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"Return a modified dictionary with keys prefixed by prefix  <code> def add_prefix_to_dict(dictionary, prefix): ","#LINE# #TAB# return {'{}_{}'.format(prefix, key): value for key, value in dictionary #LINE# #TAB# #TAB# .items()}"
"Search a peak and its position in arrays xdata ad ydata . Return three integer : - peak position - peak value - index of peak position in array xdata This result may accelerate the fwhm search  <code> def search_peak(xdata, ydata): ","#LINE# #TAB# ymax = max(ydata) #LINE# #TAB# idx = __give_index(ymax, ydata) #LINE# #TAB# return xdata[idx], ymax, idx"
Check the user provided ` values ` . Parameters ---------- values : numpy.array A numpy array containing values . Return ------ values : numpy array <code> def check_values(values): ,"#LINE# #TAB# if not isinstance(values, np.ndarray): #LINE# #TAB# #TAB# raise ValueError('values has to be a numpy.ndarray!') #LINE# #TAB# return values"
"Returns a label if all the elements of an overlay agree on a consistent label otherwise returns the default label  <code> def get_overlay_label(cls, overlay, default_label=''): ",#LINE# #TAB# #TAB# if all(el.label==overlay.get(0).label for el in overlay): #LINE# #TAB# #TAB# #TAB# return overlay.get(0).label #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return default_label
"c - a - a ' : sodium chloride hydroxide [ PP87ii ]  <code> def psi_na_cl_oh_pp87ii(T, P): ","#LINE# #TAB# psi = 0.0273 - 9.93 / T #LINE# #TAB# valid = logical_and(T >= 298.15, T <= 523.25) #LINE# #TAB# return psi, valid"
"Returns the zero - inflated negative binomial log - likelihood of the data  <code> def zinb_ll(data, P, R, Z): ","#LINE# #TAB# lls = nb_ll(data, P, R) #LINE# #TAB# clusters = P.shape[1] #LINE# #TAB# for c in range(clusters): #LINE# #TAB# #TAB# pass #LINE# #TAB# return lls"
Computes the Hilbert signals . Parameters ---------- filtered_signals : numpy.ndarray Prefiltered signals Returns ------- numpy.ndarray The Hilbert transformed signals <code> def get_hilbert_signals(filtered_signals): ,"#LINE# #TAB# hilbert_signals = [None] * len(filtered_signals) #LINE# #TAB# for i in range(len(filtered_signals)): #LINE# #TAB# #TAB# hilbert_signals[i] = np.empty(filtered_signals[i].shape) #LINE# #TAB# #TAB# for channel in range(len(filtered_signals[i])): #LINE# #TAB# #TAB# #TAB# hilbert_signals[i][(channel), :] = np.imag(scipy.signal.hilbert #LINE# #TAB# #TAB# #TAB# #TAB# (filtered_signals[i][(channel), :])) #LINE# #TAB# return hilbert_signals"
"Get home directory / base directory for borg : - BORG_BASE_DIR , if set - HOME , if set - ~$USER , if USER is set - ~ <code> def get_base_dir(): ","#LINE# #TAB# base_dir = os.environ.get('BORG_BASE_DIR') or os.environ.get('HOME') #LINE# #TAB# if not base_dir: #LINE# #TAB# #TAB# base_dir = os.path.expanduser('~%s' % os.environ.get('USER', '')) #LINE# #TAB# return base_dir"
Get categories from database and create the ones that do n't exist . : param genres_names : : return : A dict with Genres mapped to their name <code> def get_genres(genres_names): ,#LINE# #TAB# genres = {genre.name: genre for genre in Genre.objects.filter(name__in= #LINE# #TAB# #TAB# genres_names)} #LINE# #TAB# if len(genres) != len(genres_names): #LINE# #TAB# #TAB# new_genres = {} #LINE# #TAB# #TAB# for genre_name in genres_names: #LINE# #TAB# #TAB# #TAB# if genre_name not in genres: #LINE# #TAB# #TAB# #TAB# #TAB# new_genres[genre_name] = Genre(name=genre_name) #LINE# #TAB# #TAB# Genre.objects.bulk_create(new_genres.values()) #LINE# #TAB# #TAB# for genre in Genre.objects.filter(name__in=new_genres): #LINE# #TAB# #TAB# #TAB# genres[genre.name] = genre #LINE# #TAB# return genres
"Filtering only the VGPU allocations from a list of allocations . : param allocations : Information about resources allocated to the instance via placement , of the form returned by SchedulerReportClient.get_allocations_for_consumer  <code> def vgpu_allocations(allocations): ",#LINE# #TAB# if not allocations: #LINE# #TAB# #TAB# return {} #LINE# #TAB# RC_VGPU = orc.VGPU #LINE# #TAB# vgpu_allocations = {} #LINE# #TAB# for rp in allocations: #LINE# #TAB# #TAB# res = allocations[rp]['resources'] #LINE# #TAB# #TAB# if RC_VGPU in res and res[RC_VGPU] > 0: #LINE# #TAB# #TAB# #TAB# vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}} #LINE# #TAB# return vgpu_allocations
"Return a new list of namedtuples by combining dicts of namedtuples or objects  <code> def get_list_w_id2nts(ids, id2nts, flds, dflt_null=""""): ","#LINE# #TAB# combined_nt_list = [] #LINE# #TAB# ntobj = cx.namedtuple(""Nt"", "" "".join(flds)) #LINE# #TAB# for item_id in ids: #LINE# #TAB# #TAB# nts = [id2nt.get(item_id) for id2nt in id2nts] #LINE# #TAB# #TAB# vals = _combine_nt_vals(nts, flds, dflt_null) #LINE# #TAB# #TAB# combined_nt_list.append(ntobj._make(vals)) #LINE# #TAB# return combined_nt_list"
"skewedPV mocking the realdoniach signature  <code> def donny_pv(x, amplitude=1.0, center=0.0, sigma=1.0, gamma=0.5): ","#LINE# #TAB# E = center #LINE# #TAB# F = sigma #LINE# #TAB# g = 1 - gamma #LINE# #TAB# val = my_GL(x, 1, E, F, alpha=1) #LINE# #TAB# val += (1 - val) * np.exp(-g * (x - E) / F) * np.heaviside(x - E, 1) #LINE# #TAB# return amplitude * val"
"Order the slices span in ascending order . When we are slicing a pbcarray we might be rolling and padding the array so it 's probably a good idea to make the array as small as possible early on  <code> def order_slices(dim, slices): ","#LINE# #TAB# sizes = [] #LINE# #TAB# for idim, sli in slices: #LINE# #TAB# #TAB# step = sli.step or 1 #LINE# #TAB# #TAB# start = sli.start or (0 if step > 0 else shape_[idim]) #LINE# #TAB# #TAB# stop = sli.stop or (shape_[idim] if step > 0 else 0) #LINE# #TAB# #TAB# size = abs((max(start, stop) - min(start, stop)) // step) #LINE# #TAB# #TAB# sizes.append(size) #LINE# #TAB# sizes, slices = zip(*sorted(zip(sizes, slices))) #LINE# #TAB# return slices"
Validate answers are 1 through 4 and only numbers <code> def validate_answers(answers): ,"#LINE# #TAB# invalid_msg = 'Answers can only be numbers from 1 through 4: ' #LINE# #TAB# for index, value in enumerate(answers): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# seq_num = int(value) #LINE# #TAB# #TAB# #TAB# if seq_num < 0 or seq_num > 4: #LINE# #TAB# #TAB# #TAB# #TAB# input_error_message(invalid_msg, index, answers) #LINE# #TAB# #TAB# #TAB# #TAB# return index, False #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# input_error_message(invalid_msg, index, answers) #LINE# #TAB# #TAB# #TAB# return index, False #LINE# #TAB# return None, True"
Replace illegal characters with underscore <code> def sanitize_column_names(data): ,"#LINE# #TAB# new_names = {} #LINE# #TAB# for name in data.columns: #LINE# #TAB# #TAB# new_names[name] = _ILLEGAL_CHARACTER_PAT.sub('_', name) #LINE# #TAB# return new_names"
"Invert a displacement field using one voxel of padding in the computation . Parameters ---------- field : sitk . Image Input displacement field . Returns ------- sitk . Image The inverse of the displacement , computed under padding  <code> def invert_displacement_padded(field: sitk.Image) ->sitk.Image: ","#LINE# #TAB# inverse = sitk.InvertDisplacementField(field_zero_padding(field)) #LINE# #TAB# inverse = sitk.Crop(inverse, (1, 1, 1), (1, 1, 1)) #LINE# #TAB# inverse.CopyInformation(field) #LINE# #TAB# return inverse"
"Checks the type of connection and tries to create it <code> def create_new_connection(from_port, to_port): ","#LINE# #TAB# from rafcon.gui.mygaphas.items.ports import ScopedVariablePortView, LogicPortView, DataPortView #LINE# #TAB# if isinstance(from_port, LogicPortView) and isinstance(to_port, LogicPortView): #LINE# #TAB# #TAB# return add_transition_to_state(from_port, to_port) #LINE# #TAB# elif isinstance(from_port, (DataPortView, ScopedVariablePortView)) and \ #LINE# #TAB# #TAB# #TAB# isinstance(to_port, (DataPortView, ScopedVariablePortView)): #LINE# #TAB# #TAB# return add_data_flow_to_state(from_port, to_port) #LINE# #TAB# elif from_port and to_port: #LINE# #TAB# #TAB# logger.error(""Connection of non-compatible ports: {0} and {1}"".format(type(from_port), type(to_port))) #LINE# #TAB# return False"
"use processors to compute file metrics  <code> def compute_file_metrics(processors, language, key, token_list): ","#LINE# #TAB# tli = itertools.tee(token_list, len(processors)) #LINE# #TAB# metrics = OrderedDict() #LINE# #TAB# for p in processors: #LINE# #TAB# #TAB# p.reset() #LINE# #TAB# for p, tl in zip(processors, tli): #LINE# #TAB# #TAB# p.process_file(language, key, tl) #LINE# #TAB# for p in processors: #LINE# #TAB# #TAB# metrics.update(p.metrics) #LINE# #TAB# return metrics"
Convert an IPv6 address from binary form into text representation used when socket . inet_pton is not available  <code> def inet6_ntop(addr): ,"#LINE# #TAB# if len(addr) != 16: #LINE# #TAB# #TAB# raise ValueError(""invalid length of packed IP address string"") #LINE# #TAB# address = "":"".join(plain_str(bytes_hex(addr[idx:idx + 2])).lstrip('0') or '0' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# for idx in range(0, 16, 2)) #LINE# #TAB# try: #LINE# #TAB# #TAB# match = max(_IP6_ZEROS.finditer(address), #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# key=lambda m: m.end(1) - m.start(1)) #LINE# #TAB# #TAB# return '{}::{}'.format(address[:match.start()], address[match.end():]) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return address"
Convert given raw attribute name into a valid React HTML tag attribute name . : param name : attribute to convert : return : valid attribute : type name : str : rtype : str <code> def safe_react_attribute_name(name): ,"#LINE# #TAB# if name == 'class': #LINE# #TAB# #TAB# return 'className' #LINE# #TAB# if '-' in name: #LINE# #TAB# #TAB# input_pieces = name.split('-') #LINE# #TAB# #TAB# output_pieces = [input_pieces[0]] #LINE# #TAB# #TAB# for piece in input_pieces[1:]: #LINE# #TAB# #TAB# #TAB# output_pieces.append('%s%s' % (piece[0].upper(), piece[1:])) #LINE# #TAB# #TAB# return ''.join(output_pieces) #LINE# #TAB# if name == 'xlink:href': #LINE# #TAB# #TAB# return 'href' #LINE# #TAB# return name"
"Handles additional link dependencies <code> def set_target_additional_dependencies(context, flag_name, ad_libs, node): ","#LINE# #TAB# del flag_name, node #LINE# #TAB# if ad_libs: #LINE# #TAB# #TAB# add_libs = [] #LINE# #TAB# #TAB# for d in re.split('[; ]', ad_libs): #LINE# #TAB# #TAB# #TAB# if d != '%(AdditionalDependencies)': #LINE# #TAB# #TAB# #TAB# #TAB# if os.path.splitext(d)[1] == '.lib': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# add_libs.append(d.replace('.lib', '')) #LINE# #TAB# #TAB# context.add_lib_deps = True #LINE# #TAB# #TAB# message(context, 'Additional Dependencies = {}'.format(add_libs), '') #LINE# #TAB# #TAB# context.settings[context.current_setting]['add_lib_deps'] = add_libs"
Returns a list of valid primary currency codes . These are the digital currencies which can be traded on Independent Reserve . This method does not take any parameters . : return : list <code> def get_valid_primary_currency_codes(): ,#LINE# #TAB# response = requests.get(PublicMethods.api_url + #LINE# #TAB# #TAB# '/Public/GetValidPrimaryCurrencyCodes') #LINE# #TAB# return response
Return the absolute filepath to the XML schema file that can be used to parse the given XML . : param xml : the pre - parsed XML object : return : the XSD absolute filepath <code> def get_schema_filepath(xml): ,"#LINE# #TAB# schema_directory = os.path.join(os.path.dirname(os.path.abspath( #LINE# #TAB# #TAB# __file__)), 'schemas') #LINE# #TAB# schema_filename = get_schema_filename(xml) #LINE# #TAB# schema_filepath = os.path.join(schema_directory, schema_filename) #LINE# #TAB# return schema_filepath"
"Yield successive n - sized chunks from a list . Args : list : The list to chunk . chunk_size : The max chunk size . Returns : List of lists  <code> def split_chunk(list, chunk_size): ","#LINE# #TAB# for i in range(0, len(list), chunk_size): #LINE# #TAB# #TAB# yield list[i:i + chunk_size]"
Change underscore ' _ ' to CamelCase string in string param keyword : string which has to be modified as above return : string . Example one_two changes to OneTwo <code> def underscore_to_camelcase(keyword): ,#LINE# #TAB# parts = keyword.split('_') #LINE# #TAB# keyword = '' #LINE# #TAB# for part in parts: #LINE# #TAB# #TAB# if part == 'db': #LINE# #TAB# #TAB# #TAB# part = 'DB' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# part = part.capitalize() #LINE# #TAB# #TAB# keyword += part #LINE# #TAB# return keyword
Get a list of all IPv4 interfaces found in this computer <code> def get_all_ipv4_addresses(): ,"#LINE# #TAB# proto = netifaces.AF_INET #LINE# #TAB# return [addr['addr'] for iface in netifaces.interfaces() for #LINE# #TAB# #TAB# iface_proto, addrs in netifaces.ifaddresses(iface).items() if proto == #LINE# #TAB# #TAB# iface_proto for addr in addrs if 'addr' in addr]"
"Performs ` ` nexus3 list ` ` <code> def cmd_list(nexus_client, repository_path): ","#LINE# #TAB# artefact_list = nexus_client.list(repository_path) #LINE# #TAB# if isinstance(artefact_list, (list, types.GeneratorType)): #LINE# #TAB# #TAB# for artefact in iter(artefact_list): #LINE# #TAB# #TAB# #TAB# print(artefact) #LINE# #TAB# #TAB# return exception.CliReturnCode.SUCCESS.value #LINE# #TAB# else: #LINE# #TAB# #TAB# return exception.CliReturnCode.UNKNOWN_ERROR.value"
"Take a word and build it into the dict map . Modify the words dicts in place and then return the int that represents the given word <code> def word_map(hub, word, words, r_words): ",#LINE# #TAB# if word in words: #LINE# #TAB# #TAB# return words[word] #LINE# #TAB# num = len(words) + 1 #LINE# #TAB# words[word] = num #LINE# #TAB# r_words[num] = word #LINE# #TAB# return num
"Plots residuals from a regression . : param y_true : True values : param y_pred : Models predicted value : param title : Plot title : param ax : Pass your own ax : return : matplotlib . Axes <code> def plot_residuals(y_true: DataType, y_pred: DataType, title: str=None, ax: ","#LINE# #TAB# Axes=None) ->Axes: #LINE# #TAB# title = f'Residual Plot' if title is None else title #LINE# #TAB# residuals = y_pred - y_true #LINE# #TAB# r2 = r2_score(y_true, y_pred) #LINE# #TAB# if ax is None: #LINE# #TAB# #TAB# fig, ax = plt.subplots() #LINE# #TAB# ax.scatter(y_pred, residuals, label=f'$R^2 = {r2:0.3f}$') #LINE# #TAB# ax.axhline(y=0, color='grey', linestyle='--') #LINE# #TAB# ax.set_ylabel('Residuals') #LINE# #TAB# ax.set_xlabel('Predicted Value') #LINE# #TAB# ax.set_title(title) #LINE# #TAB# ax.legend(loc='best') #LINE# #TAB# return ax"
"Counts the indent levels on the front . It is assumed that one tab equals 4 spaces  <code> def indent_level(line, spacesPerTab=4): ",#LINE# #TAB# x = 0 #LINE# #TAB# nextTab = 4 #LINE# #TAB# for ch in line: #LINE# #TAB# #TAB# if ch == ' ': #LINE# #TAB# #TAB# #TAB# x = x + 1 #LINE# #TAB# #TAB# elif ch == '\t': #LINE# #TAB# #TAB# #TAB# x = nextTab #LINE# #TAB# #TAB# #TAB# nextTab = x + spacesPerTab #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return x
"the cx_Oracle module implicitly imports datetime ; make sure this happens  <code> def load_cx_oracle(finder, module): ",#LINE# #TAB# finder.IncludeModule('datetime') #LINE# #TAB# try: #LINE# #TAB# #TAB# finder.IncludeModule('decimal') #LINE# #TAB# except ImportError: #LINE# #TAB# #TAB# pass
"a generator that returns all the keys in a set of nested Mapping instances . The keys take the form X.Y.Z <code> def iteritems_breadth_first(a_mapping, include_dicts=False): ","#LINE# #TAB# subordinate_mappings = [] #LINE# #TAB# for key, value in six.iteritems(a_mapping): #LINE# #TAB# #TAB# if isinstance(value, collections.Mapping): #LINE# #TAB# #TAB# #TAB# subordinate_mappings.append((key, value)) #LINE# #TAB# #TAB# #TAB# if include_dicts: #LINE# #TAB# #TAB# #TAB# #TAB# yield key, value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield key, value #LINE# #TAB# for key, a_map in subordinate_mappings: #LINE# #TAB# #TAB# for sub_key, value in iteritems_breadth_first(a_map, include_dicts): #LINE# #TAB# #TAB# #TAB# yield '%s.%s' % (key, sub_key), value"
"Marshal the sub - entity . : param sub_entity : either embedded link or embedded representation . : param marshaler : marshaller for the Siren entities . : returns : dictionary with sub - entity data . : raises : : class : ValueError  <code> def marshal_sub_entity(sub_entity, marshaler): ","#LINE# #TAB# logger = logging.getLogger(__name__) #LINE# #TAB# if hasattr(sub_entity, 'target'): #LINE# #TAB# #TAB# logger.debug('Marshal sub-entity as an embedded link') #LINE# #TAB# #TAB# marshaled_sub_entity = marshaler.marshal_embedded_link(sub_entity) #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.debug('Marshal sub-entity as an embedded representation') #LINE# #TAB# #TAB# marshaled_sub_entity = marshaler.marshal_embedded_representation( #LINE# #TAB# #TAB# #TAB# sub_entity) #LINE# #TAB# return marshaled_sub_entity"
Get all parent item IDs for each item in dict keys  <code> def get_id2children(objs): ,"#LINE# #TAB# id2children = {} #LINE# #TAB# for obj in objs: #LINE# #TAB# #TAB# _get_id2children(id2children, obj.dbid, obj) #LINE# #TAB# return id2children"
Create directory . Args : directory : path to directory Returns : Absolute path to created directory str <code> def create_dir(directory: str) ->str: ,#LINE# #TAB# path = real_path(directory) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# if e.errno != errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return path
"Adds indicators to a ohlc df using ' finta . TA ' <code> def add_indicators(df, conf={}): ","#LINE# #TAB# avail = dir(TA) #LINE# #TAB# for ind in conf: #LINE# #TAB# #TAB# if ind in avail: #LINE# #TAB# #TAB# #TAB# df = pd.concat([getattr(TA, ind)(ohlc=df, **conf[ind]), df], axis=1 #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# return df"
Transforms the 4-dimensional output tensor into a suitable image format  <code> def process_output(output_tensor): ,"#LINE# #TAB# sr_img = output_tensor.clip(0, 1) * 255 #LINE# #TAB# sr_img = np.uint8(sr_img) #LINE# #TAB# return sr_img"
"Returns 1 if sequence has more items than number and 0 if not  <code> def more_than(sequence, number): ",#LINE# #TAB# i = 0 #LINE# #TAB# for item in sequence: #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# #TAB# if i > number: #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return 0
Set up the otter - api service  <code> def make_service(config): ,"#LINE# #TAB# s = MultiService() #LINE# #TAB# if config['realtime']: #LINE# #TAB# #TAB# from twisted.internet import reactor as clock #LINE# #TAB# else: #LINE# #TAB# #TAB# clock = Clock() #LINE# #TAB# core = MimicCore.fromPlugins(clock) #LINE# #TAB# root = MimicRoot(core, clock) #LINE# #TAB# site = get_site(root.app.resource(), logging=bool(config['verbose'])) #LINE# #TAB# service(config['listen'], site).setServiceParent(s) #LINE# #TAB# return s"
Enumerate Duco module tree  <code> def enumerate_node_tree(): ,"#LINE# #TAB# node_id = 1 #LINE# #TAB# node_found = True #LINE# #TAB# node_list = [] #LINE# #TAB# while node_found: #LINE# #TAB# #TAB# node_type = probe_node_id(node_id) #LINE# #TAB# #TAB# if node_type is False: #LINE# #TAB# #TAB# #TAB# node_found = False #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# node_list.append(Node.factory(node_id, node_type)) #LINE# #TAB# #TAB# node_id = node_id + 1 #LINE# #TAB# return node_list"
A built - in check to see if connecting to the configured default database backend succeeds  <code> def check_database_connected(db): ,"#LINE# #TAB# from sqlalchemy.exc import DBAPIError, SQLAlchemyError #LINE# #TAB# errors = [] #LINE# #TAB# try: #LINE# #TAB# #TAB# with db.engine.connect() as connection: #LINE# #TAB# #TAB# #TAB# connection.execute('SELECT 1;') #LINE# #TAB# except DBAPIError as e: #LINE# #TAB# #TAB# msg = 'DB-API error: {!s}'.format(e) #LINE# #TAB# #TAB# errors.append(Error(msg, id=health.ERROR_DB_API_EXCEPTION)) #LINE# #TAB# except SQLAlchemyError as e: #LINE# #TAB# #TAB# msg = 'Database misconfigured: ""{!s}""'.format(e) #LINE# #TAB# #TAB# errors.append(Error(msg, id=health.ERROR_SQLALCHEMY_EXCEPTION)) #LINE# #TAB# return errors"
"Find all good quartets and removes the middle edges  <code> def remove_all_p4(RBMG, BMG, P4_list=None): ","#LINE# #TAB# GP4 = RBMG.copy() #LINE# #TAB# if P4_list is None: #LINE# #TAB# #TAB# P4_list = find_all_P4(GP4) #LINE# #TAB# for path in P4_list: #LINE# #TAB# #TAB# if is_good_quartet(path, BMG) and GP4.has_edge(path[1], path[2]): #LINE# #TAB# #TAB# #TAB# GP4.remove_edge(path[1], path[2]) #LINE# #TAB# #TAB# #TAB# print(path[1], '-|-', path[2]) #LINE# #TAB# return GP4"
"Override environment variable value temporarily <code> def override_env(env: str, value: Any) ->Generator[None, None, None]: ",#LINE# #TAB# orig_value = os.environ.get(env) #LINE# #TAB# os.environ[env] = value #LINE# #TAB# try: #LINE# #TAB# #TAB# yield #LINE# #TAB# finally: #LINE# #TAB# #TAB# if orig_value is not None: #LINE# #TAB# #TAB# #TAB# os.environ[env] = orig_value #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# del os.environ[env]
and_expression : or_p_expression OP_AND and_expression | or_p_expression <code> def p_and_expression(tok): ,"#LINE# #TAB# if len(tok) == 4: #LINE# #TAB# #TAB# tok[0] = LogicalBinOpRule(tok[2], tok[1], tok[3]) #LINE# #TAB# else: #LINE# #TAB# #TAB# tok[0] = tok[1]"
"walk a directory tree , using a generator <code> def do_recursive(directory): ","#LINE# #TAB# for name in os.listdir(directory): #LINE# #TAB# #TAB# fullpath = os.path.join(directory, name) #LINE# #TAB# #TAB# if os.path.isdir(fullpath) and not os.path.islink(fullpath): #LINE# #TAB# #TAB# #TAB# for name in do_recursive(fullpath): #LINE# #TAB# #TAB# #TAB# #TAB# yield name #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield fullpath"
Checks if an nginx parsed entry is an ' ssl on ' directive . : param list entry : the parsed entry : returns : Whether it 's an ' ssl on ' directive : rtype : bool <code> def is_ssl_on_directive(entry): ,"#LINE# #TAB# return isinstance(entry, list) and len(entry) == 2 and entry[0 #LINE# #TAB# #TAB# ] == 'ssl' and entry[1] == 'on'"
"complete missing attributes with those in fallbackModule <code> def stitch_modules(module, fallbackModule): ","#LINE# #TAB# for name, attr in fallbackModule.__dict__.items(): #LINE# #TAB# #TAB# if name not in module.__dict__: #LINE# #TAB# #TAB# #TAB# module.__dict__[name] = attr"
Replacement for os.path.split ( )  <code> def path_split(path): ,"#LINE# #TAB# if len(path_separators) == 1: #LINE# #TAB# #TAB# front, _, tail = path.rpartition(path_sep) #LINE# #TAB# #TAB# return front, tail #LINE# #TAB# for x in reversed(path): #LINE# #TAB# #TAB# if x in path_separators: #LINE# #TAB# #TAB# #TAB# front, tail = path.rsplit(x, maxsplit=1) #LINE# #TAB# #TAB# #TAB# return front, tail #LINE# #TAB# return '', path"
Use moving average to get better readings  <code> def get_average(temp_base): ,"#LINE# #TAB# if not hasattr(get_average, 'temp'): #LINE# #TAB# #TAB# get_average.temp = [temp_base, temp_base, temp_base] #LINE# #TAB# get_average.temp[2] = get_average.temp[1] #LINE# #TAB# get_average.temp[1] = get_average.temp[0] #LINE# #TAB# get_average.temp[0] = temp_base #LINE# #TAB# temp_avg = (get_average.temp[0] + get_average.temp[1] + get_average.temp[2] #LINE# #TAB# #TAB# ) / 3 #LINE# #TAB# return temp_avg"
"Convert a raw Double value to an integer . Returns 1 or 2 ( unknown values are treated as 1 )  <code> def interpret_double(bb, context=None): ",#LINE# #TAB# if bb.strip() == b'2': #LINE# #TAB# #TAB# return 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1
Undo what ` install_figura_importer ` _ did  <code> def uninstall_figura_importer(suffix): ,"#LINE# #TAB# with _implock: #LINE# #TAB# #TAB# for i, (sfx, loader) in enumerate(_INSTALLED_LOADERS): #LINE# #TAB# #TAB# #TAB# if suffix == sfx: #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# del _INSTALLED_LOADERS[i]"
"Create a key from a word and sense , e.g. "" usage_example|NOUN "" . word ( unicode ) : The word . sense ( unicode ) : The sense . RETURNS ( unicode ) : The key  <code> def make_key(word: str, sense: str) ->str: ","#LINE# #TAB# text = re.sub('\\s', '_', word) #LINE# #TAB# return text + '|' + sense"
Parses title and description out of a docstring  <code> def parse_docstring(doc): ,"#LINE# #TAB# doc_split = cleandoc(doc).split('\n') #LINE# #TAB# if len(doc_split) == 1: #LINE# #TAB# #TAB# return doc_split[0], '' #LINE# #TAB# elif doc_split[1] != '': #LINE# #TAB# #TAB# return '', '\n'.join(doc_split) #LINE# #TAB# else: #LINE# #TAB# #TAB# title = doc_split[0] #LINE# #TAB# #TAB# description = '\n'.join(doc_split[2:]) #LINE# #TAB# #TAB# return title, description"
"u Parse a list of arguments into a dict <code> def parse_args(arglist, scheme): ","#LINE# #TAB# arglist = [i for i in arglist if i.type != token.COMMA] #LINE# #TAB# ret_mapping = dict([(k, None) for k in scheme]) #LINE# #TAB# for i, arg in enumerate(arglist): #LINE# #TAB# #TAB# if arg.type == syms.argument and arg.children[1].type == token.EQUAL: #LINE# #TAB# #TAB# #TAB# slot = arg.children[0].value #LINE# #TAB# #TAB# #TAB# ret_mapping[slot] = arg.children[2] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# slot = scheme[i] #LINE# #TAB# #TAB# #TAB# ret_mapping[slot] = arg #LINE# #TAB# return ret_mapping"
Convert an object holding raw analysis results into a dictionary . This is meant for JSON dumping  <code> def raw_to_dict(obj): ,"#LINE# #TAB# result = {} #LINE# #TAB# for a in obj._fields: #LINE# #TAB# #TAB# v = getattr(obj, a, None) #LINE# #TAB# #TAB# if v is not None: #LINE# #TAB# #TAB# #TAB# result[a] = v #LINE# #TAB# return result"
Gets a JSON serializable dictionary representation . : param bbn : BBN . : return : Dictionary  <code> def to_dict(bbn): ,"#LINE# #TAB# return {'nodes': {n.id: n.to_dict() for n in bbn.get_nodes()}, 'edges': #LINE# #TAB# #TAB# [{'pa': edge.i.id, 'ch': edge.j.id} for _, edge in bbn.edges.items()]}"
"Validate that the supplied uri is handled by this class . Returns : True if the URI is valid for this class , else False  <code> def valid_uri(uri): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# cmd = Command('hg identify ' + uri) #LINE# #TAB# #TAB# cmd.wait() #LINE# #TAB# #TAB# return cmd.rcode == 0 #LINE# #TAB# except PakitError: #LINE# #TAB# #TAB# return False
"Dimensional density of electrolyte [ kg.m-3 ] , from thermodynamics . c_k in [ mol.m-3 ]  <code> def rho_dimensional(c_e, c_ox=0, c_hy=0): ",#LINE# #TAB# return M_w / V_w + (M_e - V_e * M_w / V_w) * c_e + (M_ox - V_ox * M_w / V_w #LINE# #TAB# #TAB# ) * c_ox + (M_hy - V_hy * M_w / V_w) * c_hy
"Create a fresh temporary directory based on the fiven prefix . Returns the new path  <code> def create_working_dir(config, prefix): ","#LINE# #TAB# basepath = config.get(""Execution"", ""directory"") #LINE# #TAB# if not prefix: #LINE# #TAB# #TAB# prefix = 'opensubmit' #LINE# #TAB# finalpath = tempfile.mkdtemp(prefix=prefix + '_', dir=basepath) #LINE# #TAB# if not finalpath.endswith(os.sep): #LINE# #TAB# #TAB# finalpath += os.sep #LINE# #TAB# logger.debug(""Created fresh working directory at {0}."".format(finalpath)) #LINE# #TAB# return finalpath"
"Construct feasible initial guess for transformation matrix A  <code> def fill_a(A, right_eigenvectors): ","#LINE# #TAB# num_micro, num_eigen = right_eigenvectors.shape #LINE# #TAB# A = A.copy() #LINE# #TAB# A[1:, 0] = -1 * A[1:, 1:].sum(1) #LINE# #TAB# A[0] = -1 * dot(right_eigenvectors[:, 1:].real, A[1:]).min(0) #LINE# #TAB# A /= A[0].sum() #LINE# #TAB# return A"
"Compare before and after results from various salt functions returning a dict describing the changes that were made <code> def compare_lists(old=None, new=None): ",#LINE# #TAB# ret = dict() #LINE# #TAB# for item in new: #LINE# #TAB# #TAB# if item not in old: #LINE# #TAB# #TAB# #TAB# ret['new'] = item #LINE# #TAB# for item in old: #LINE# #TAB# #TAB# if item not in new: #LINE# #TAB# #TAB# #TAB# ret['old'] = item #LINE# #TAB# return ret
Convert local filesystem path to sqlite uri  <code> def path_to_local_sqlite_uri(path): ,#LINE# #TAB# path = posixpath.abspath(pathname2url(os.path.abspath(path))) #LINE# #TAB# prefix = 'sqlite://' if sys.platform == 'win32' else 'sqlite:///' #LINE# #TAB# return prefix + path
Convert base64 string to an image array . The function can handle raw string or bytes encoded sting . Arguments : ---------- base64_str : bytes or raw string of image in base64 format Returns : -------- img : standard image array <code> def base64_str_to_img(base64_str): ,#LINE# #TAB# import base64 #LINE# #TAB# import io #LINE# #TAB# import cv2 #LINE# #TAB# import imageio #LINE# #TAB# if type(base64_str) == type(b''): #LINE# #TAB# #TAB# base64_str = base64_str.decode() #LINE# #TAB# img = imageio.imread(io.BytesIO(base64.b64decode(base64_str))) #LINE# #TAB# return img
"Check if a key exists in a map and if it 's not None : param dict : map to look for key : param key : key to find : return : true if key is in dict and not None <code> def not_in_dict_or_none(dict, key): ",#LINE# #TAB# if key not in dict or dict[key] is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Create a WebhookMetadata from a comment added to an issue  <code> def build_from_issue_comment(gh_token, body): ","#LINE# #TAB# if body[""action""] in [""created"", ""edited""]: #LINE# #TAB# #TAB# github_con = Github(gh_token) #LINE# #TAB# #TAB# repo = github_con.get_repo(body['repository']['full_name']) #LINE# #TAB# #TAB# issue = repo.get_issue(body['issue']['number']) #LINE# #TAB# #TAB# text = body['comment']['body'] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# comment = issue.get_comment(body['comment']['id']) #LINE# #TAB# #TAB# except UnknownObjectException: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return WebhookMetadata(repo, issue, text, comment) #LINE# #TAB# return None"
"insert_jobs function inserts data into Brain . Jobs table jobs must be in Job format : param jobs : < list > of Jobs : return : < dict > rethinkdb insert response value <code> def insert_jobs(jobs, verify_jobs=True, conn=None): ","#LINE# #TAB# assert isinstance(jobs, list) #LINE# #TAB# if verify_jobs and not verify({'Jobs': jobs}, Jobs()): #LINE# #TAB# #TAB# raise ValueError('Invalid Jobs') #LINE# #TAB# inserted = RBJ.insert(jobs).run(conn) #LINE# #TAB# return inserted"
"Gives the order of the type for the dictionary Args : t ( type ) : The type to compare Returns : 1 for ( dict , OrderedDict ) , 0 otherwise <code> def compare_type(t): ","#LINE# #TAB# if t in [dict, OrderedDict]: #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
Find knee of a decreading curve using the kneedle algorithm . Parameters ---------- s : array_like Decreasing curve . Returns ------- int Knee as an index  <code> def find_knee(s): ,"#LINE# #TAB# yn = s / np.max(s, axis=-1)[..., np.newaxis] #LINE# #TAB# xn = np.linspace(0, 1, yn.shape[-1]) #LINE# #TAB# dn = 1 - yn - xn #LINE# #TAB# knee = np.argmax(dn, axis=-1) #LINE# #TAB# return knee"
"Returns a list of SPARQL Update Algebra expressions <code> def translate_update(q, base=None, initNs=None): ","#LINE# #TAB# res = [] #LINE# #TAB# prologue = None #LINE# #TAB# if not q.request: #LINE# #TAB# #TAB# return res #LINE# #TAB# for p, u in zip(q.prologue, q.request): #LINE# #TAB# #TAB# prologue = translatePrologue(p, base, initNs, prologue) #LINE# #TAB# #TAB# u = traverse(u, visitPost=functools.partial(translatePName, #LINE# #TAB# #TAB# #TAB# prologue=prologue)) #LINE# #TAB# #TAB# u = _traverse(u, _simplifyFilters) #LINE# #TAB# #TAB# u = traverse(u, visitPost=translatePath) #LINE# #TAB# #TAB# res.append(translate_update1(u, prologue)) #LINE# #TAB# return res"
"Resolve service_info from service  <code> def get_info_from_service(service, zconf): ","#LINE# #TAB# service_info = None #LINE# #TAB# try: #LINE# #TAB# #TAB# service_info = zconf.get_service_info('_googlecast._tcp.local.', #LINE# #TAB# #TAB# #TAB# service) #LINE# #TAB# #TAB# if service_info: #LINE# #TAB# #TAB# #TAB# _LOGGER.debug( #LINE# #TAB# #TAB# #TAB# #TAB# 'get_info_from_service resolved service %s to service_info %s', #LINE# #TAB# #TAB# #TAB# #TAB# service, service_info) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return service_info"
"Check that the stepszie in t is an integer x dt <code> def check_integrate_dt(t,dt): ",#LINE# #TAB# if dt is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# mult= round((t[1]-t[0])/dt) #LINE# #TAB# if nu.fabs(mult*dt-t[1]+t[0]) < 10.**-10.: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False
"Add gradient clipping of clip during training  <code> def clip_grad(learn:Learner, clip:float=0.1)->Learner: ","#LINE# #TAB# ""Add gradient clipping of `clip` during training."" #LINE# #TAB# learn.callback_fns.append(partial(GradientClipping, clip=clip)) #LINE# #TAB# return learn"
"A function to get the pmin value for the aplpy chart , as a function of the instrument . Args : fc_params : the parameters of the finding charts ( a dictionary ) Returns : pmin ( float ) : the pmin value <code> def get_pmin(survey): ","#LINE# #TAB# if survey in ['2MASS-J', '2MASS-H', '2MASS-K']: #LINE# #TAB# #TAB# return 30.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return 10.0"
"Set + image+ anchor points to centre of image <code> def centre_image(image: Union[TextureRegion, Sequence[TextureRegion]]): ","#LINE# #TAB# if not isinstance(image, collections.abc.Sequence): #LINE# #TAB# #TAB# image = [image] #LINE# #TAB# for img in image: #LINE# #TAB# #TAB# img.anchor_x = img.width // 2 #LINE# #TAB# #TAB# img.anchor_y = img.height // 2"
Extract the Latin Etymology ID tags from the table of contents  <code> def get_tags(request: requests.Response) ->List[str]: ,#LINE# #TAB# tags = [] #LINE# #TAB# for a_element in request.html.xpath(_TOC_ETYMOLOGY_XPATH_SELECTOR): #LINE# #TAB# #TAB# tag = a_element.attrs['href'].lstrip('#') #LINE# #TAB# #TAB# tags.append(tag) #LINE# #TAB# if not tags: #LINE# #TAB# #TAB# tags = ['Latin'] #LINE# #TAB# return tags
"False discovery rate of Benjamini - Yekutieli <code> def fdr_by(alpha, pvals, I): ","#LINE# #TAB# L = I * (I - 1) / 2.0 #LINE# #TAB# pvals_rank = rank.rankdata(pvals, reverse=True) #LINE# #TAB# denom = 1 / np.arange(1, L + 1) #LINE# #TAB# fdr = pvals_rank * alpha / denom #LINE# #TAB# return fdr"
"Fetches new tracks ( as of the given date ) released by the current Spotify user 's followed artists <code> def fetch_new_releases(spotify, as_of_date=datetime.today().date()): ","#LINE# #TAB# followed_artist_ids = _get_followed_artists(spotify) #LINE# #TAB# all_albums = [] #LINE# #TAB# for artist_id in followed_artist_ids: #LINE# #TAB# #TAB# artist_albums = _get_releases(spotify, artist_id) #LINE# #TAB# #TAB# all_albums += artist_albums #LINE# #TAB# new_albums = _filter_new_releases(all_albums, as_of_date) #LINE# #TAB# all_tracks = [] #LINE# #TAB# for album in new_albums: #LINE# #TAB# #TAB# all_tracks += _get_album_tracks(spotify, album) #LINE# #TAB# new_tracks = parse_tracks(all_tracks) #LINE# #TAB# logging.info('Fetched new tracks ' + str(new_tracks)) #LINE# #TAB# return new_tracks"
"We can not use a basic ` ` reverse('product_default_start ' ) ` ` here because * organization * and ` ` get_broker ` ` might be different  <code> def product_url(provider, subscriber=None, request=None): ","#LINE# #TAB# location = '/app/' #LINE# #TAB# if subscriber: #LINE# #TAB# #TAB# location += '%s/' % subscriber #LINE# #TAB# if settings.BUILD_ABSOLUTE_URI_CALLABLE: #LINE# #TAB# #TAB# return build_absolute_uri(request, location=location, provider=provider #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# if not is_broker(provider): #LINE# #TAB# #TAB# location = '/%s' % provider + location #LINE# #TAB# return location"
Provide marker via index @param i : index <code> def load_marker(i): ,"#LINE# #TAB# markers = ['o', 'v', 'D', 's', '*', 'd', '^', 'x', '+'] #LINE# #TAB# return markers[i % len(markers)]"
Get AWS keys from default S3fs location if available . Returns ------- ACCESS_KEY : str SECRET_KEY : str Notes ----- Reads ~/.passwd - s3fs to get ACCESSKEY and SECRET KEY <code> def get_key_from_s3fs(): ,"#LINE# #TAB# key_path = os.path.expanduser('~/.passwd-s3fs') #LINE# #TAB# if os.path.exists(key_path): #LINE# #TAB# #TAB# with open(key_path, 'r') as kfl: #LINE# #TAB# #TAB# #TAB# content = kfl.readline() #LINE# #TAB# #TAB# #TAB# ACCESS_KEY, SECRET_KEY = content.strip().split(':') #LINE# #TAB# #TAB# #TAB# return ACCESS_KEY, SECRET_KEY"
Helper function . Needed for easier testing <code> def get_application(): ,#LINE# #TAB# from .models import APPLICATION #LINE# #TAB# return APPLICATION
Returns a list containing i d and names of all styles  <code> def get_styles_list(): ,"#LINE# #TAB# view = get_view('_design/styles', 'by-category') #LINE# #TAB# styles = [['', '']] #LINE# #TAB# for row in view(include_docs=False)['rows']: #LINE# #TAB# #TAB# styles.append([row['id'], '{}{} {}'.format(row['key'][0], row['key' #LINE# #TAB# #TAB# #TAB# ][1], row['value'])]) #LINE# #TAB# return styles"
"create a generator listing all files with a particular extension in a folder arborescence The recursivity is broken when at least 1 file with a particular extenssion is found  <code> def recursive_file_gen(dir, ext): ","#LINE# #TAB# if path.isdir(dir): #LINE# #TAB# #TAB# file_found = False #LINE# #TAB# #TAB# for fn in iglob(path.join(dir, '*.' + ext)): #LINE# #TAB# #TAB# #TAB# yield fn #LINE# #TAB# #TAB# #TAB# file_found = True #LINE# #TAB# #TAB# if not file_found: #LINE# #TAB# #TAB# #TAB# for item in listdir(dir): #LINE# #TAB# #TAB# #TAB# #TAB# for fn in recursive_file_gen(path.join(dir, item), ext): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield fn"
Attempts folder creation Tries to create a folder . Raises an exception if one exists already  <code> def make_folder(path): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path) #LINE# #TAB# except OSError as exception: #LINE# #TAB# #TAB# if exception.errno != errno.EEXIST: #LINE# #TAB# #TAB# #TAB# raise
Return the default location for the project directory depending of the operating system <code> def get_default_project_directory(): ,"#LINE# #TAB# server_config = Config.instance().get_section_config(""Server"") #LINE# #TAB# path = os.path.expanduser(server_config.get(""projects_path"", ""~/GNS3/projects"")) #LINE# #TAB# path = os.path.normpath(path) #LINE# #TAB# try: #LINE# #TAB# #TAB# os.makedirs(path, exist_ok=True) #LINE# #TAB# except OSError as e: #LINE# #TAB# #TAB# raise aiohttp.web.HTTPInternalServerError(text=""Could not create project directory: {}"".format(e)) #LINE# #TAB# return path"
"Often images are provided without an extension , so we try to find a suitable one using glob  <code> def get_real_image_from_path(image_path): ","#LINE# #TAB# formats = 'png', 'jpg', 'svg', 'gif', 'jpeg', 'bmp' #LINE# #TAB# image = image_path #LINE# #TAB# if not os.path.exists(image): #LINE# #TAB# #TAB# for f in glob(image + '*'): #LINE# #TAB# #TAB# #TAB# if f.lower().endswith(formats): #LINE# #TAB# #TAB# #TAB# #TAB# image = f #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return image"
"Strips out preceding path and file extension from a filename . : param filename : Filename , possibly including path and extension . : return : Filename without path and without extension  <code> def get_base_filename(filename): ",#LINE# #TAB# base_file = os.path.basename(filename) #LINE# #TAB# return os.path.splitext(base_file)[0]
"Scale multiple vectors by a given factor , assuming they lie in the XY plane Parameters ---------- a : array XY(Z ) components of the vectors . factor : float Scale factor . Returns ------- array Scaled vectors ( XY )  <code> def scale_vectors_xy_numba(a, factor): ","#LINE# #TAB# m = a.shape[0] #LINE# #TAB# b = zeros((m, 3)) #LINE# #TAB# for i in range(m): #LINE# #TAB# #TAB# b[(i), :] = scale_vector_xy_numba(a[(i), :], factor) #LINE# #TAB# return b"
Adjust the structure electron density based on structure name  <code> def adjust_red_by_structure_name(dicom_dataset): ,"#LINE# #TAB# structure_names = [structure_set.ROIName for structure_set in #LINE# #TAB# #TAB# dicom_dataset.StructureSetROISequence] #LINE# #TAB# adjustment_map = RED_adjustment_map_from_structure_names(structure_names) #LINE# #TAB# adjusted_dicom_dataset = adjust_rel_elec_density(dicom_dataset, #LINE# #TAB# #TAB# adjustment_map) #LINE# #TAB# return adjusted_dicom_dataset"
"Returns the first group that matches the rgx_pattern in the string_to_search  <code> def find_expression(string_to_search: str, rgx_pattern: str) ->Optional[str]: ",#LINE# #TAB# if string_to_search: #LINE# #TAB# #TAB# pattern = re.compile(rgx_pattern) #LINE# #TAB# #TAB# match = pattern.search(string_to_search) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# return match.group() #LINE# #TAB# return None
"Splits a database cross reference string in the format db : accession : version into its constituents <code> def split_dbxref(dbxref: str) ->(str, str, str): ","#LINE# #TAB# split_id = dbxref.split(':') #LINE# #TAB# if len(split_id) < 2 or len(split_id) > 3: #LINE# #TAB# #TAB# raise AttributeError( #LINE# #TAB# #TAB# #TAB# 'dbxref must consist of 2 or 3 elements, separated by semicolon') #LINE# #TAB# db = split_id[0] #LINE# #TAB# accession = split_id[1] #LINE# #TAB# version = '' #LINE# #TAB# if len(split_id) == 3: #LINE# #TAB# #TAB# version = split_id[2] #LINE# #TAB# return db, accession, version"
return 2 . 7 from backport - sha - 2 . 7 <code> def get_base_branch(cherry_pick_branch): ,"#LINE# #TAB# prefix, sha, base_branch = cherry_pick_branch.split(""-"", 2) #LINE# #TAB# if prefix != ""backport"": #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'branch name is not prefixed with ""backport-"". Is this a cherry_picker branch?' #LINE# #TAB# #TAB# ) #LINE# #TAB# if not re.match(""[0-9a-f]{7,40}"", sha): #LINE# #TAB# #TAB# raise ValueError(f""branch name has an invalid sha: {sha}"") #LINE# #TAB# validate_sha(sha) #LINE# #TAB# version_from_branch(base_branch) #LINE# #TAB# return base_branch"
Returns true if the fork was split into two parts and false if not . Parameters ---------- mp : dictionary The measured data . Returns ------- forkIsSplit : boolean <code> def is_fork_split(mp): ,#LINE# #TAB# forkIsSplit = False #LINE# #TAB# for key in mp.keys(): #LINE# #TAB# #TAB# if key[:1] == 'S' or key[1:2] == 'S': #LINE# #TAB# #TAB# #TAB# forkIsSplit = True #LINE# #TAB# return forkIsSplit
"Format a report as per InfluxDB line protocol <code> def line_protocol(name, tags: dict = None, fields: dict = None, timestamp: float = None) -> str: ","#LINE# #TAB# output_str = name #LINE# #TAB# if tags: #LINE# #TAB# #TAB# output_str += ',' #LINE# #TAB# #TAB# output_str += ','.join('%s=%s' % (key, value) for key, value in sorted(tags.items())) #LINE# #TAB# output_str += ' ' #LINE# #TAB# output_str += ','.join(('%s=%r' % (key, value)).replace(""'"", '""') for key, value in sorted(fields.items())) #LINE# #TAB# if timestamp is not None: #LINE# #TAB# #TAB# output_str += ' %d' % (timestamp * 1E9) #LINE# #TAB# return output_str + '\n'"
Convert the given object to a commit if possible and return it <code> def to_commit(obj): ,"#LINE# #TAB# if obj.type == 'tag': #LINE# #TAB# #TAB# obj = deref_tag(obj) #LINE# #TAB# if obj.type != ""commit"": #LINE# #TAB# #TAB# raise ValueError(""Cannot convert object %r to type commit"" % obj) #LINE# #TAB# return obj"
Will return the current time adjusted using the input timezone offset  <code> def get_offset_time(utc_offset): ,#LINE# #TAB# if utc_offset is not None: #LINE# #TAB# #TAB# minutes = _offset_to_min(utc_offset) #LINE# #TAB# #TAB# offset = timedelta(minutes=minutes) #LINE# #TAB# #TAB# offset_time = datetime.utcnow() + offset #LINE# #TAB# #TAB# offset_time = offset_time.replace(tzinfo=_FixedOffset(minutes)) #LINE# #TAB# else: #LINE# #TAB# #TAB# offset_time = datetime.now() #LINE# #TAB# return offset_time
"Generate a coherent tag given previous tag and current label  <code> def get_coherent_next_tag(prev_label: str, cur_label: str) -> str: ","#LINE# #TAB# if cur_label == ""O"": #LINE# #TAB# #TAB# return ""O"" #LINE# #TAB# if prev_label == cur_label: #LINE# #TAB# #TAB# return f""I-{cur_label}"" #LINE# #TAB# else: #LINE# #TAB# #TAB# return f""B-{cur_label}"""
"Gets a random , available local port <code> def get_available_local_port(): ","#LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# s.bind(('', 0)) #LINE# #TAB# s.listen(1) #LINE# #TAB# port = s.getsockname()[1] #LINE# #TAB# s.close() #LINE# #TAB# return port"
Return an slb connection <code> def slb_connect(module): ,"#LINE# #TAB# region, slb_params = get_acs_connection_info(module) #LINE# #TAB# if region: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# slb = connect_to_acs(footmark.slb, region, **slb_params) #LINE# #TAB# #TAB# except AnsibleACSError as e: #LINE# #TAB# #TAB# #TAB# module.fail_json(msg=str(e)) #LINE# #TAB# return slb"
construct context variables needed by the chooser JS <code> def get_chooser_js_data(): ,"#LINE# #TAB# return {'step': 'chooser', 'error_label': _('Server Error'), #LINE# #TAB# #TAB# 'error_message': _( #LINE# #TAB# #TAB# 'Report this error to your webmaster with the following information:')}"
look for dependency that setuptools can not check or that are too painful to install with setuptools <code> def find_missing_modules(): ,#LINE# #TAB# missing_modules = [] #LINE# #TAB# for module in MODULES: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# __import__(module[1]) #LINE# #TAB# #TAB# except ImportError: #LINE# #TAB# #TAB# #TAB# missing_modules.append(module) #LINE# #TAB# return missing_modules
"gets all allocators instances from the dump , order them by size  <code> def make_size_caches(dumpname): ","#LINE# #TAB# from haystack.reverse import context #LINE# #TAB# log.debug('\t[-] Loading the context for a dumpname.') #LINE# #TAB# ctx = context.get_context(dumpname) #LINE# #TAB# log.debug('\t[-] Make the size dictionnaries.') #LINE# #TAB# sizeCache = StructureSizeCache(ctx) #LINE# #TAB# sizeCache.cacheSizes() #LINE# #TAB# return ctx, sizeCache"
Check if PID is running for Windows systems <code> def is_pid_running_on_windows(pid): ,"#LINE# #TAB# import ctypes.wintypes #LINE# #TAB# kernel32 = ctypes.windll.kernel32 #LINE# #TAB# handle = kernel32.OpenProcess(1, 0, pid) #LINE# #TAB# if handle == 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# exit_code = ctypes.wintypes.DWORD() #LINE# #TAB# ret = kernel32.GetExitCodeProcess(handle, ctypes.byref(exit_code)) #LINE# #TAB# is_alive = ret == 0 or exit_code.value == _STILL_ALIVE #LINE# #TAB# kernel32.CloseHandle(handle) #LINE# #TAB# return is_alive"
class decorator to Register mf make <code> def register_make(cls): ,#LINE# #TAB# assert cls.nxm_headers is not None #LINE# #TAB# assert cls.nxm_headers is not [] #LINE# #TAB# for nxm_header in cls.nxm_headers: #LINE# #TAB# #TAB# assert nxm_header not in _MF_FIELDS #LINE# #TAB# #TAB# _MF_FIELDS[nxm_header] = cls.make #LINE# #TAB# return cls
"set Lizard login credentials <code> def set_headers(username, password): ",#LINE# #TAB# REQUESTS_HEADERS['username'] = username #LINE# #TAB# REQUESTS_HEADERS['password'] = password #LINE# #TAB# REQUESTS_HEADERS['Content-Type'] = 'application/json'
"Check if a file exists  <code> def file_exists(db, user_id, path): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# get_file( #LINE# #TAB# #TAB# #TAB# db, #LINE# #TAB# #TAB# #TAB# user_id, #LINE# #TAB# #TAB# #TAB# path, #LINE# #TAB# #TAB# #TAB# include_content=False, #LINE# #TAB# #TAB# #TAB# decrypt_func=unused_decrypt_func, #LINE# #TAB# #TAB# ) #LINE# #TAB# #TAB# return True #LINE# #TAB# except NoSuchFile: #LINE# #TAB# #TAB# return False"
"Return a function which gets width and height of console ( linux , osx , windows , cygwin )  <code> def environ_cols_wrapper(): ",#LINE# #TAB# _environ_cols = None #LINE# #TAB# if IS_WIN: #LINE# #TAB# #TAB# _environ_cols = _environ_cols_windows #LINE# #TAB# #TAB# if _environ_cols is None: #LINE# #TAB# #TAB# #TAB# _environ_cols = _environ_cols_tput #LINE# #TAB# if IS_NIX: #LINE# #TAB# #TAB# _environ_cols = _environ_cols_linux #LINE# #TAB# return _environ_cols
Predict the category of a company from its c_vector : param c_vector : vector of size 512 as a string : return : category ( S or P ) <code> def predict_category(c_vector): ,#LINE# #TAB# model = load_model() #LINE# #TAB# vector_description = preprocess_vector(c_vector) #LINE# #TAB# return model.predict(vector_description)[0]
Determine True / False from argument <code> def process_bool_arg(arg): ,"#LINE# #TAB# if isinstance(arg, bool): #LINE# #TAB# #TAB# return arg #LINE# #TAB# elif isinstance(arg, str): #LINE# #TAB# #TAB# if arg.lower() in ['true', '1']: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# elif arg.lower() in ['false', '0']: #LINE# #TAB# #TAB# #TAB# return False"
Check if the version of vJoyInterface.dll and the vJoy Driver match <code> def driver_match(): ,#LINE# #TAB# result = _vj.driver_match() #LINE# #TAB# if result == 0: #LINE# #TAB# #TAB# raise vJoyDriverMismatch() #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
Returns True if value is convertible to float <code> def is_convertible_to_float(value): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# float(value) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
"Gets rid of the existing engine . Useful for unittesting , use with care . Do not call this function if there are multiple threads accessing the engine . Only do that in single - threaded test environments or console sessions  <code> def refresh_engine(): ",#LINE# #TAB# global _engine #LINE# #TAB# with _engine_lock: #LINE# #TAB# #TAB# session.remove() #LINE# #TAB# #TAB# if _engine is not None: #LINE# #TAB# #TAB# #TAB# _engine.dispose() #LINE# #TAB# #TAB# _engine = None
[ a - zA - Z][a - zA - Z0 - 9_$ ] * <code> def t_id(t): ,"#LINE# #TAB# t.type = reserved.get(t.value, 'ID') #LINE# #TAB# return t"
"Get the contours for a 2D grid of data at the specified levels as a fraction the integral over the whole grid  <code> def grid_hpd_contours(data, levels): ","#LINE# #TAB# N = 1000.0 #LINE# #TAB# thresholds = np.linspace(0, data.max(), N) #LINE# #TAB# integral = ((data >= thresholds[:, (None), (None)]) * data).sum(axis=(1, 2) #LINE# #TAB# #TAB# ) #LINE# #TAB# norm = integral[0] #LINE# #TAB# function = interpolate.interp1d(integral, thresholds) #LINE# #TAB# contours = function(np.array(levels) * norm) #LINE# #TAB# return contours"
"Creates new observations from formset . Error handling is performed by upper layers  <code> def create_new_observations(formset, user): ","#LINE# #TAB# new_observations = [] #LINE# #TAB# for observation_data in formset.cleaned_data: #LINE# #TAB# #TAB# transmitter_uuid = observation_data['transmitter_uuid'] #LINE# #TAB# #TAB# transmitter = formset.transmitters[transmitter_uuid] #LINE# #TAB# #TAB# observation = create_new_observation(station=observation_data[ #LINE# #TAB# #TAB# #TAB# 'ground_station'], transmitter=transmitter, start= #LINE# #TAB# #TAB# #TAB# observation_data['start'], end=observation_data['end'], author=user #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# new_observations.append(observation) #LINE# #TAB# for observation in new_observations: #LINE# #TAB# #TAB# observation.save() #LINE# #TAB# return new_observations"
"Add tid to the relevant map  <code> def add_tid(xs, x_map, tid): ",#LINE# #TAB# for x in xs: #LINE# #TAB# #TAB# if x in x_map: #LINE# #TAB# #TAB# #TAB# x_map[x].add(tid) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# x_map[x] = {tid}
Minify script in a very insecure way  <code> def min_script(js): ,"#LINE# #TAB# if js: #LINE# #TAB# #TAB# return js.replace('\n', ' ') #LINE# #TAB# return ''"
Get a logging handle  <code> def get_logger(name=None): ,#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# if len(logger.handlers) == 0: #LINE# #TAB# #TAB# logger = add_stream_handler(logger) #LINE# #TAB# return logger
"Helper method that translates Windows and Unix filepaths <code> def transform_path(files, search_for: str, replace_with: str, win_to_unix: ","#LINE# #TAB# bool=False, unix_to_win: bool=False): #LINE# #TAB# for file in files: #LINE# #TAB# #TAB# file = file.replace(search_for, replace_with) #LINE# #TAB# #TAB# if win_to_unix: #LINE# #TAB# #TAB# #TAB# file = file.replace('\\', '/') #LINE# #TAB# #TAB# elif unix_to_win: #LINE# #TAB# #TAB# #TAB# file = file.replace('/', '\\') #LINE# #TAB# return files"
"Removes the /topics/ prefix from the topic name , if present  <code> def sanitize_topic_name(cls, topic): ","#LINE# #TAB# if not topic: #LINE# #TAB# #TAB# return None #LINE# #TAB# prefix = '/topics/' #LINE# #TAB# if topic.startswith(prefix): #LINE# #TAB# #TAB# topic = topic[len(prefix):] #LINE# #TAB# if not re.match('^[a-zA-Z0-9-_\\.~%]+$', topic): #LINE# #TAB# #TAB# raise ValueError('Malformed topic name.') #LINE# #TAB# return topic"
"Must be called at the very beginning of a script  <code> def use_project(project_dir, postfix=None): ","#LINE# #TAB# P = ProjectManager(given_directory=project_dir, postfix=postfix) #LINE# #TAB# LogSingleton(P.root) #LINE# #TAB# return P"
Transforms a Markdown text into HTML  <code> def to_markdown(value: str) ->str: ,"#LINE# #TAB# classes_dict = {} #LINE# #TAB# markdowner = md.Markdown(extras={'footnotes': None, 'header-ids': None, #LINE# #TAB# #TAB# 'fenced-code-blocks': None, 'html-classes': classes_dict}) #LINE# #TAB# html = mark_safe(markdowner.convert(value)) #LINE# #TAB# return html"
Get the furthest ancestor repository that is not archived  <code> def get_parent_repo(reponame): ,#LINE# #TAB# api = get_api() #LINE# #TAB# user_org = api.get_user().login #LINE# #TAB# repo = api.get_repo(f'{user_org}/{reponame}') #LINE# #TAB# while repo.parent and not repo.parent.archived: #LINE# #TAB# #TAB# repo = repo.parent #LINE# #TAB# return repo
"Drude1D model function derivatives  <code> def fit_deriv(x, amplitude, x_0, fwhm): ","#LINE# #TAB# d_amplitude = (fwhm / x_0) ** 2 / ((x / x_0 - x_0 / x) ** 2 + (fwhm / #LINE# #TAB# #TAB# x_0) ** 2) #LINE# #TAB# d_x_0 = -2 * amplitude * d_amplitude * (1 / x_0 + d_amplitude * (x_0 ** #LINE# #TAB# #TAB# 2 / fwhm ** 2) * ((-x / x_0 - 1 / x) * (x / x_0 - x_0 / x) - 2 * #LINE# #TAB# #TAB# fwhm ** 2 / x_0 ** 3)) #LINE# #TAB# d_fwhm = 2 * amplitude * d_amplitude / fwhm * (1 - d_amplitude) #LINE# #TAB# return [d_amplitude, d_x_0, d_fwhm]"
"Filters the dict recursively , removing all $ -entries . Not the best performance - solution right now  <code> def filter_config_dict_recursive_key(final_dict): ","#LINE# #TAB# if not isinstance(final_dict, dict): #LINE# #TAB# #TAB# return final_dict #LINE# #TAB# filtered = {k: v for k, v in final_dict.items() if not k.startswith('$')} #LINE# #TAB# return {k: filter_config_dict_recursive_key(v) for k, v in filtered. #LINE# #TAB# #TAB# items()}"
Returns the mutation context acording to CHASM  <code> def get_chasm_context(tri_nuc): ,"#LINE# #TAB# if len(tri_nuc) != 3: #LINE# #TAB# #TAB# raise ValueError('Chasm context requires a three nucleotide string ' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# '(Provided: ""{0}"")'.format(tri_nuc)) #LINE# #TAB# if tri_nuc[1:] == 'CG': #LINE# #TAB# #TAB# return 'C*pG' #LINE# #TAB# elif tri_nuc[:2] == 'CG': #LINE# #TAB# #TAB# return 'CpG*' #LINE# #TAB# elif tri_nuc[:2] == 'TC': #LINE# #TAB# #TAB# return 'TpC*' #LINE# #TAB# elif tri_nuc[1:] == 'GA': #LINE# #TAB# #TAB# return 'G*pA' #LINE# #TAB# else: #LINE# #TAB# #TAB# return tri_nuc[1]"
Breaks a package string in module and class  <code> def get_config_from_package(package): ,#LINE# #TAB# package_x = package.split('.') #LINE# #TAB# package_conf = {} #LINE# #TAB# package_conf['class'] = package_x[-1] #LINE# #TAB# package_conf['module'] = '.'.join(package_x[:-1][:]) #LINE# #TAB# return package_conf
Get taxon name without suffix  <code> def canonical_taxon(taxon): ,"#LINE# #TAB# if taxon.startswith('s__'): #LINE# #TAB# #TAB# generic, specific = taxon.split() #LINE# #TAB# #TAB# if '_' in specific: #LINE# #TAB# #TAB# #TAB# canonical_specific = specific.rsplit('_', 1)[0] #LINE# #TAB# #TAB# #TAB# return '{} {}'.format(generic, canonical_specific) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return taxon #LINE# #TAB# rank_prefix = '' #LINE# #TAB# if taxon[1:3] == '__': #LINE# #TAB# #TAB# rank_prefix = taxon[0:3] #LINE# #TAB# #TAB# taxon = taxon[3:] #LINE# #TAB# if '_' in taxon: #LINE# #TAB# #TAB# taxon = taxon.rsplit('_', 1)[0] #LINE# #TAB# return rank_prefix + taxon"
"Initialize a new file that starts out with some data . Pass data as a list , dict , or JSON string  <code> def with_data(path, data): ","#LINE# #TAB# if isinstance(data, str): #LINE# #TAB# #TAB# data = json.loads(data) #LINE# #TAB# if os.path.exists(path): #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# ""File exists, not overwriting data. Set the 'data' attribute on a normally-initialized 'livejson.File' instance if you really want to do this."" #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# else: #LINE# #TAB# #TAB# f = File(path) #LINE# #TAB# #TAB# f.data = data #LINE# #TAB# #TAB# return f"
"Discovers the paths to BWA and Samtools  <code> def which_path(bwa_path, samtools_path, method): ","#LINE# #TAB# if bwa_path: #LINE# #TAB# #TAB# use_bwa = bwa_path #LINE# #TAB# elif method == 'mCtoT': #LINE# #TAB# #TAB# use_bwa = find_executable('bwa') #LINE# #TAB# elif method == 'CtoT': #LINE# #TAB# #TAB# use_bwa = find_executable('bwameth.py') #LINE# #TAB# if samtools_path: #LINE# #TAB# #TAB# use_samtools = samtools_path #LINE# #TAB# else: #LINE# #TAB# #TAB# use_samtools = find_executable('samtools') #LINE# #TAB# return use_bwa, use_samtools"
"Helper function to choose an individual channel from a cube Arguments : predictions : RAMONVolume containing a numpy array ( x , y , z ) Returns : pixel_out : The raw trained classifier <code> def choose_channel_4d_3d(vol, channel): ","#LINE# #TAB# prob_channel = vol[:, :, :, (channel)] #LINE# #TAB# return prob_channel"
Encode every value of a dict to UTF-8 . Useful for POSTing requests on the ' data ' parameter of urlencode  <code> def encoded_dict(in_dict): ,"#LINE# #TAB# out_dict = {} #LINE# #TAB# for k, v in in_dict.items(): #LINE# #TAB# #TAB# if isinstance(v, unicode): #LINE# #TAB# #TAB# #TAB# if sys.version_info < (3, 0): #LINE# #TAB# #TAB# #TAB# #TAB# v = v.encode('utf8') #LINE# #TAB# #TAB# elif isinstance(v, str): #LINE# #TAB# #TAB# #TAB# if sys.version_info < (3, 0): #LINE# #TAB# #TAB# #TAB# #TAB# v.decode('utf8') #LINE# #TAB# #TAB# out_dict[k] = v #LINE# #TAB# return out_dict"
"Smooths an input set of coordinates by applying Chaikins  <code> def chaikins_corner_cutting(coords, refinements=1): ","#LINE# #TAB# coords = np.array(coords) #LINE# #TAB# for _ in range(refinements): #LINE# #TAB# #TAB# L = coords.repeat(2, axis=0) #LINE# #TAB# #TAB# R = np.empty_like(L) #LINE# #TAB# #TAB# R[0] = L[0] #LINE# #TAB# #TAB# R[2::2] = L[1:-1:2] #LINE# #TAB# #TAB# R[1:-1:2] = L[2::2] #LINE# #TAB# #TAB# R[-1] = L[-1] #LINE# #TAB# #TAB# coords = L * 0.75 + R * 0.25 #LINE# #TAB# return coords"
Compute softmax probability over raw logits  <code> def compute_softmax(scores): ,#LINE# #TAB# if not scores: #LINE# #TAB# #TAB# return [] #LINE# #TAB# max_score = None #LINE# #TAB# for score in scores: #LINE# #TAB# #TAB# if max_score is None or score > max_score: #LINE# #TAB# #TAB# #TAB# max_score = score #LINE# #TAB# exp_scores = [] #LINE# #TAB# total_sum = 0.0 #LINE# #TAB# for score in scores: #LINE# #TAB# #TAB# x = math.exp(score - max_score) #LINE# #TAB# #TAB# exp_scores.append(x) #LINE# #TAB# #TAB# total_sum += x #LINE# #TAB# probs = [] #LINE# #TAB# for score in exp_scores: #LINE# #TAB# #TAB# probs.append(score / total_sum) #LINE# #TAB# return probs
"Fit a parsimonious language model to terms in docs  <code> def parsimonious_wordcloud(docs, w=0.5, k=10): ","#LINE# #TAB# from weighwords import ParsimoniousLM #LINE# #TAB# model = ParsimoniousLM(docs, w=w) #LINE# #TAB# return [model.top(k, d) for d in docs]"
"Check if the preceding words increase , decrease , or negate / nullify the valence <code> def scalar_inc_dec(word, valence, is_cap_diff): ",#LINE# #TAB# scalar = 0.0 #LINE# #TAB# word_lower = word.lower() #LINE# #TAB# if word_lower in BOOSTER_DICT: #LINE# #TAB# #TAB# scalar = BOOSTER_DICT[word_lower] #LINE# #TAB# #TAB# if valence < 0: #LINE# #TAB# #TAB# #TAB# scalar *= -1 #LINE# #TAB# #TAB# if word.isupper() and is_cap_diff: #LINE# #TAB# #TAB# #TAB# if valence > 0: #LINE# #TAB# #TAB# #TAB# #TAB# scalar += C_INCR #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# scalar -= C_INCR #LINE# #TAB# return scalar
Additional check for the dimension model to ensure that attributes given as the key and label attribute on the dimension exist  <code> def check_attribute_exists(instance): ,"#LINE# #TAB# attributes = instance.get('attributes', {}).keys() #LINE# #TAB# if instance.get('key_attribute') not in attributes: #LINE# #TAB# #TAB# return False #LINE# #TAB# label_attr = instance.get('label_attribute') #LINE# #TAB# if label_attr and label_attr not in attributes: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
Split and validate path for an account . : param req : a swob request : returns : a tuple of path parts as strings <code> def get_account_name_and_placement(req): ,"#LINE# #TAB# drive, part, account = split_and_validate_path(req, 3) #LINE# #TAB# validate_internal_account(account) #LINE# #TAB# return drive, part, account"
"Ipset seems to add entries in a non - deterministic order when doing atomic replace . This will cause the differ to output changes even when there are none . To fix this , ensure the entries for each ipset is sorted before being diffed  <code> def diff_filter(diff): ",#LINE# #TAB# entries = [] #LINE# #TAB# for i in diff: #LINE# #TAB# #TAB# if i.startswith('add '): #LINE# #TAB# #TAB# #TAB# entries.append(i) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for entry in sorted(entries): #LINE# #TAB# #TAB# #TAB# yield entry #LINE# #TAB# #TAB# entries = [] #LINE# #TAB# #TAB# yield i #LINE# #TAB# for entry in sorted(entries): #LINE# #TAB# #TAB# yield entry
Return the resolution from the number of bits  <code> def get_resolution_from_bits(resolution_bits): ,"#LINE# #TAB# if resolution_bits in [8, 12, 14, 15, 16]: #LINE# #TAB# #TAB# def_name = f'PS5000A_DR_{resolution_bits}BIT' #LINE# #TAB# else: #LINE# #TAB# #TAB# raise InvalidParameterError( #LINE# #TAB# #TAB# #TAB# f'A resolution of {resolution_bits}-bits is not supported') #LINE# #TAB# return ps.PS5000A_DEVICE_RESOLUTION[def_name]"
"* * Dirname * * ( i.e. , parent directory ) of the passed path if this path has a dirname * or * raise an exception otherwise  <code> def get_dirname(pathname: str) ->str: ","#LINE# #TAB# die_if_basename(pathname) #LINE# #TAB# dirname = get_dirname_or_empty(pathname) #LINE# #TAB# assert len(dirname), 'Pathname ""{}"" dirname empty.'.format(pathname) #LINE# #TAB# return dirname"
Add reporting arguments to an argument parser . Parameters ---------- parser : ` argparse . ArgumentParser ` Returns ------- ` argparse . ArgumentGroup ` The argument group created  <code> def add_reporting_args(parser): ,"#LINE# #TAB# g = parser.add_argument_group('Reporting options') #LINE# #TAB# g.add_argument('-l', '--log-file', type=str, metavar=file_mv, default= #LINE# #TAB# #TAB# None, help=textwrap.dedent( #LINE# #TAB# #TAB# )) #LINE# #TAB# g.add_argument('-q', '--quiet', action='store_true', help= #LINE# #TAB# #TAB# 'Only output errors and warnings.') #LINE# #TAB# g.add_argument('-v', '--verbose', action='store_true', help= #LINE# #TAB# #TAB# 'Enable verbose output. Ignored if --quiet is specified.') #LINE# #TAB# return parser"
Returns object description from LLDB value . : param lldb . SBValue obj : LLDB value object . : return : Object description from LLDB value . : rtype : str | None <code> def get_description_value(obj): ,#LINE# #TAB# desc = None if obj is None else obj.GetObjectDescription() #LINE# #TAB# if desc == '<nil>': #LINE# #TAB# #TAB# desc = None #LINE# #TAB# return desc
"Based on given location coordinates and radius in kilometers returns coordinates of the bounding box  <code> def get_bounding_box(location, radius): ","#LINE# #TAB# equator_len = 111 #LINE# #TAB# current_latitude_km_length = math.cos(location[0] * math.pi / 180 #LINE# #TAB# #TAB# ) * equator_len #LINE# #TAB# return {'lat_min': location[0] - radius / equator_len, 'lat_max': #LINE# #TAB# #TAB# location[0] + radius / equator_len, 'lon_min': location[1] - radius / #LINE# #TAB# #TAB# current_latitude_km_length, 'lon_max': location[1] + radius / #LINE# #TAB# #TAB# current_latitude_km_length}"
Concatenate list of columns into a string  <code> def cols_str(columns): ,"#LINE# #TAB# cols = """" #LINE# #TAB# for c in columns: #LINE# #TAB# #TAB# cols = cols + wrap(c) + ', ' #LINE# #TAB# return cols[:-2]"
"Convert an object to either a scalar or a row or column vector  <code> def vector_or_scalar(x, type='row'): ","#LINE# #TAB# if isinstance(x, (list, tuple)): #LINE# #TAB# #TAB# x = np.array(x) #LINE# #TAB# if isinstance(x, np.ndarray): #LINE# #TAB# #TAB# assert x.ndim == 1 #LINE# #TAB# #TAB# if type == 'column': #LINE# #TAB# #TAB# #TAB# x = x[:, (None)] #LINE# #TAB# return x"
"Returns the affine coordinates of a given point on the provided ec_group  <code> def get_affine_coords_via_ec_point(ec_point, curve: 'Curve'): ","#LINE# #TAB# affine_x = _get_new_BN() #LINE# #TAB# affine_y = _get_new_BN() #LINE# #TAB# with backend._tmp_bn_ctx() as bn_ctx: #LINE# #TAB# #TAB# res = backend._lib.EC_POINT_get_affine_coordinates_GFp(curve. #LINE# #TAB# #TAB# #TAB# ec_group, ec_point, affine_x, affine_y, bn_ctx) #LINE# #TAB# #TAB# backend.openssl_assert(res == 1) #LINE# #TAB# return affine_x, affine_y"
Adds sane block data to a transaction_sequence <code> def add_block_data(transaction_sequence: Dict): ,#LINE# #TAB# for step in transaction_sequence['steps']: #LINE# #TAB# #TAB# step['gasLimit'] = '0x7d000' #LINE# #TAB# #TAB# step['gasPrice'] = '0x773594000' #LINE# #TAB# #TAB# step['blockCoinbase'] = '0xcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcb' #LINE# #TAB# #TAB# step['blockDifficulty'] = '0xa7d7343662e26' #LINE# #TAB# #TAB# step['blockGasLimit'] = '0x7d0000' #LINE# #TAB# #TAB# step['blockNumber'] = '0x66e393' #LINE# #TAB# #TAB# step['blockTime'] = '0x5bfa4639' #LINE# #TAB# return transaction_sequence
"Default handling for incoming lines  <code> def on_line(client, line): ","#LINE# #TAB# if line.startswith(""PING""): #LINE# #TAB# #TAB# client.send(""PONG"" + line[4:]) #LINE# #TAB# #TAB# return True #LINE# #TAB# if line.startswith("":""): #LINE# #TAB# #TAB# actor, _, line = line[1:].partition("" "") #LINE# #TAB# else: #LINE# #TAB# #TAB# actor = None #LINE# #TAB# command, _, args = line.partition("" "") #LINE# #TAB# command = NUMERIC_EVENTS.get(command, command) #LINE# #TAB# parser = PARSERS.get(command, False) #LINE# #TAB# if parser: #LINE# #TAB# #TAB# parser(client, command, actor, args) #LINE# #TAB# #TAB# return True #LINE# #TAB# elif parser is False: #LINE# #TAB# #TAB# return True"
"Load configuration from a YAML file . Raise FileNotFoundError if path does not exist . Raise yaml . YAMLError if YAML file is invalid . Raise ValueError if YAML file does not contain a root object . Raise ValueError if YAML file is missing "" bucket "" value  <code> def load_config(path): ","#LINE# #TAB# with open(path, 'r') as yaml_file: #LINE# #TAB# #TAB# config = yaml.safe_load(yaml_file) #LINE# #TAB# #TAB# validate_config(config) #LINE# #TAB# #TAB# return config"
"Given a dictionary of command options , return the name of the task : param options : Options passed to the handle method of the management command : return : The task name ( str ) <code> def get_task_name(options): ",#LINE# #TAB# options_dict = dict(options) #LINE# #TAB# return [task for task in [key for key in options_dict] if str( #LINE# #TAB# #TAB# options_dict[task]) == 'True'][0]
"Parse the nyquist_vel parameter , extract from the radar if needed  <code> def parse_nyquist_vel(nyquist_vel, radar, check_uniform): ","#LINE# #TAB# if nyquist_vel is None: #LINE# #TAB# #TAB# nyquist_vel = [radar.get_nyquist_vel(i, check_uniform) for i in #LINE# #TAB# #TAB# #TAB# range(radar.nsweeps)] #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# len(nyquist_vel) #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# nyquist_vel = [nyquist_vel for i in range(radar.nsweeps)] #LINE# #TAB# return nyquist_vel"
"convert lng / lat to Google - style Mercator tile coordinate ( x , y ) at the given zoom level <code> def get_tile_pos_from_lng_lat(lng, lat, zoom): ","#LINE# #TAB# lat_rad = lat * math.pi / 180.0 #LINE# #TAB# lat_rad = inv_gudermannian(lat_rad) #LINE# #TAB# x = 2 ** zoom * (lng + 180.0) / 360.0 #LINE# #TAB# y = 2 ** zoom * (math.pi - lat_rad) / (2 * math.pi) #LINE# #TAB# return x, y"
"Convert contacts into numpy matrix . @param contacts contact list as obtained from "" parse "" @param seq_len sequence length @return np.array((seq_len , seq_len ) , score ) <code> def get_numpy_cmap(contacts, seq_len=-1, min_dist=0): ","#LINE# #TAB# max_i = max(contacts, key=lambda x: x[1])[1] #LINE# #TAB# max_j = max(contacts, key=lambda x: x[2])[2] #LINE# #TAB# n = int(max(seq_len, max_i, max_j)) #LINE# #TAB# cmap = np.zeros((n, n)) #LINE# #TAB# for c in contacts: #LINE# #TAB# #TAB# i = c[1] - 1 #LINE# #TAB# #TAB# j = c[2] - 1 #LINE# #TAB# #TAB# if abs(i - j) >= min_dist: #LINE# #TAB# #TAB# #TAB# cmap[i, j] = c[0] #LINE# #TAB# #TAB# #TAB# cmap[j, i] = c[0] #LINE# #TAB# return cmap"
"Get a previously - registered convex polyhedron by its name <code> def convex_polyhedron_shapedef(cls, name, params): ","#LINE# #TAB# shape_info = cls.convex_polyhedron_functions[name](**params) #LINE# #TAB# vertices = np.array(shape_info.vertices, dtype=np.float32).tolist() #LINE# #TAB# result = dict(type='ConvexPolyhedron', vertices=vertices, rounding_radius=0 #LINE# #TAB# #TAB# ) #LINE# #TAB# rmax = np.max(np.linalg.norm(vertices, axis=-1)) #LINE# #TAB# result['circumsphere_radius'] = rmax #LINE# #TAB# result['rounding_volume_polynomial' #LINE# #TAB# #TAB# ] = shape_info.rounding_volume_polynomial #LINE# #TAB# return result"
"Generate Veo wallet : return : address , private_key , passphrase <code> def generate_wallet(): ","#LINE# #TAB# private_key_raw, public_key = keys.gen_keypair(curve.secp256k1) #LINE# #TAB# private_key = keys.hexlify(int_to_string(private_key_raw)) #LINE# #TAB# address = base64.b64encode(b'\x04' + int_to_string(public_key.x) + #LINE# #TAB# #TAB# int_to_string(public_key.y)) #LINE# #TAB# private_key = private_key.decode('utf-8') #LINE# #TAB# passphrase = private_key_raw #LINE# #TAB# address = address.decode('utf-8') #LINE# #TAB# return address, private_key, passphrase"
Convert bigraphemes to single Cyrilic  <code> def convert_to_monographeme(text): ,"#LINE# #TAB# for i, j in bi2mono.iteritems(): #LINE# #TAB# #TAB# text = text.replace(i, j) #LINE# #TAB# return text"
Truncate lists to maximum length  <code> def trunc_list(s: List) ->List: ,#LINE# #TAB# if len(s) > max_list_size: #LINE# #TAB# #TAB# i = max_list_size // 2 #LINE# #TAB# #TAB# j = i - 1 #LINE# #TAB# #TAB# s = s[:i] + [ELLIPSIS] + s[-j:] #LINE# #TAB# return s
Broadcast a state - by - node TPM so that singleton dimensions are expanded over the full network  <code> def expand_tpm(tpm): ,#LINE# #TAB# unconstrained = np.ones([2] * (tpm.ndim - 1) + [tpm.shape[-1]]) #LINE# #TAB# return tpm * unconstrained
Raise TypeError if x is a str or if x is an iterable which contains a str  <code> def to_utf8_optional_iterator(x): ,"#LINE# #TAB# if isinstance(x, STRING_TYPES): #LINE# #TAB# #TAB# return to_utf8(x) #LINE# #TAB# try: #LINE# #TAB# #TAB# l = list(x) #LINE# #TAB# except TypeError as e: #LINE# #TAB# #TAB# assert 'is not iterable' in str(e) #LINE# #TAB# #TAB# return x #LINE# #TAB# else: #LINE# #TAB# #TAB# return [ to_utf8_if_string(e) for e in l ]"
"Computes eq . 26 for a radius r , slope t , axis ratio q , and coordinates z <code> def hyp2f1_series(t, q, r, z, max_terms=20): ","#LINE# #TAB# q_ = (1 - q ** 2) / q ** 2 #LINE# #TAB# u = 0.5 * (1 - np.sqrt(1 - q_ * (r / z) ** 2)) #LINE# #TAB# a_n = 1.0 #LINE# #TAB# F = np.zeros_like(z, dtype='complex64') #LINE# #TAB# for n in range(max_terms): #LINE# #TAB# #TAB# F += a_n * u ** n #LINE# #TAB# #TAB# a_n *= (2 * n + 4 - 2 * t) / (2 * n + 4 - t) #LINE# #TAB# return F"
"returning to data from Q and QstarH Based on FastHankel_prod_mat_vec  <code> def fast_hankel2dt(Q, QH): ","#LINE# #TAB# M, K = Q.shape #LINE# #TAB# K, N = QH.shape #LINE# #TAB# L = M + N - 1 #LINE# #TAB# vec_sum = np.zeros((L,), dtype=complex) #LINE# #TAB# for k in range(K): #LINE# #TAB# #TAB# prod_vect = QH[(k), :] #LINE# #TAB# #TAB# gene_vect = np.concatenate((np.zeros(N - 1), Q[:, (k)], np.zeros(N - #LINE# #TAB# #TAB# #TAB# 1))) #LINE# #TAB# #TAB# vec_k = FastHankel_prod_mat_vec(gene_vect, prod_vect[::-1]) #LINE# #TAB# #TAB# vec_sum += vec_k #LINE# #TAB# datadenoised = vec_sum * vec_mean(M, L) #LINE# #TAB# return datadenoised"
"Given a file name loads as a pandas data frame <code> def load_tabular_file(fname, return_meta=False, header=True, index_col=True): ","#LINE# #TAB# if index_col: #LINE# #TAB# #TAB# index_col = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# index_col = None #LINE# #TAB# if header: #LINE# #TAB# #TAB# header = 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# header = None #LINE# #TAB# df = pd.read_csv(fname, header=header, index_col=index_col, sep='\t') #LINE# #TAB# if return_meta: #LINE# #TAB# #TAB# json_fname = fname.replace('tsv', 'json') #LINE# #TAB# #TAB# meta = pd.read_json(json_fname) #LINE# #TAB# #TAB# return df, meta #LINE# #TAB# else: #LINE# #TAB# #TAB# return df"
"Returns a future for a given object <code> def get_future(obj, loop=None): ",#LINE# #TAB# framework = get_framework(obj) #LINE# #TAB# if loop is None: #LINE# #TAB# #TAB# loop = framework.get_event_loop() #LINE# #TAB# future = framework.get_future(loop) #LINE# #TAB# return future
Return the contents of javascript resource file  <code> def copy_javascript(name): ,"#LINE# #TAB# folder = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'js') #LINE# #TAB# with open(os.path.join(folder, name + '.js')) as fobj: #LINE# #TAB# #TAB# content = fobj.read() #LINE# #TAB# return content"
Find migrations for this project  <code> def find_migrations() ->List[str]: ,"#LINE# #TAB# configuration: Dict[str, Union[List[str], int, float]] = load_config() #LINE# #TAB# migrations_directory = join(CONFIG_DIRECTORY, 'migrations') #LINE# #TAB# return [join(migrations_directory, migration) for migration in #LINE# #TAB# #TAB# configuration.get('migrations', [])]"
Configuration flags for executing g++ on MacOS <code> def gxx_modifier_darwin(conf): ,#LINE# #TAB# v = conf.env #LINE# #TAB# v.CXXFLAGS_cxxshlib = ['-fPIC'] #LINE# #TAB# v.LINKFLAGS_cxxshlib = ['-dynamiclib'] #LINE# #TAB# v.cxxshlib_PATTERN = 'lib%s.dylib' #LINE# #TAB# v.FRAMEWORKPATH_ST = '-F%s' #LINE# #TAB# v.FRAMEWORK_ST = ['-framework'] #LINE# #TAB# v.ARCH_ST = ['-arch'] #LINE# #TAB# v.LINKFLAGS_cxxstlib = [] #LINE# #TAB# v.SHLIB_MARKER = [] #LINE# #TAB# v.STLIB_MARKER = [] #LINE# #TAB# v.SONAME_ST = []
Parse a return statement . Ex : return 5 ; <code> def parse_return(index): ,"#LINE# #TAB# index = match_token(index, token_kinds.return_kw, ParserError.GOT) #LINE# #TAB# if token_is(index, token_kinds.semicolon): #LINE# #TAB# #TAB# return nodes.Return(None), index #LINE# #TAB# node, index = parse_expression(index) #LINE# #TAB# index = match_token(index, token_kinds.semicolon, ParserError.AFTER) #LINE# #TAB# return nodes.Return(node), index"
"Get the device that a given path exists on  <code> def get_device(path, as_root=False): ","#LINE# #TAB# stdout = _execute_shell_cmd('df', [], path, as_root=as_root) #LINE# #TAB# return stdout.splitlines()[1].split()[0]"
NotImplemented : cufflinks has some strict plotly limits so could n't be reliably installed <code> def plotly_timeseries(df): ,"#LINE# #TAB# fig = df.iplot([{'x': df.index, 'y': df[col], 'name': col} for col in #LINE# #TAB# #TAB# df.columns], filename='cufflinks/simple-line') #LINE# #TAB# return fig"
Set the headers used in the Cloud Files Request . : return headers : <code> def set_headers(headers): ,#LINE# #TAB# if ARGS.get('base_headers'): #LINE# #TAB# #TAB# headers.update(ARGS.get('base_headers')) #LINE# #TAB# return headers
"r Determine if one surface is in the other  <code> def no_intersections(nodes1, degree1, nodes2, degree2): ","#LINE# #TAB# from bezier import _surface_intersection #LINE# #TAB# located = _surface_intersection.locate_point( #LINE# #TAB# #TAB# nodes2, degree2, nodes1[0, 0], nodes1[1, 0] #LINE# #TAB# ) #LINE# #TAB# if located is not None: #LINE# #TAB# #TAB# return None, True #LINE# #TAB# located = _surface_intersection.locate_point( #LINE# #TAB# #TAB# nodes1, degree1, nodes2[0, 0], nodes2[1, 0] #LINE# #TAB# ) #LINE# #TAB# if located is not None: #LINE# #TAB# #TAB# return None, False #LINE# #TAB# return [], None"
"Parse a configuration value , splitting by whitespace and/or commas and taking quoting into account , etc . , yielding a list of strings  <code> def parse_list(value): ","#LINE# #TAB# if value is None: #LINE# #TAB# #TAB# return [] #LINE# #TAB# if six.PY2 and isinstance(value, six.text_type): #LINE# #TAB# #TAB# value = value.encode('utf-8') #LINE# #TAB# parser = shlex.shlex(value) #LINE# #TAB# parser.whitespace += ',' #LINE# #TAB# parser.whitespace_split = True #LINE# #TAB# values = list(parser) #LINE# #TAB# for i, value in enumerate(values): #LINE# #TAB# #TAB# if value.startswith('""') and value.endswith('""'): #LINE# #TAB# #TAB# #TAB# values[i] = value[1:-1] #LINE# #TAB# #TAB# elif value.startswith(""'"") and value.endswith(""'""): #LINE# #TAB# #TAB# #TAB# values[i] = value[1:-1] #LINE# #TAB# return values"
"Skip questions depending on the type answer  <code> def post_type(configurator, question, answer): ","#LINE# #TAB# value = validate_choices(configurator, question, answer) #LINE# #TAB# if value != u'Dexterity': #LINE# #TAB# #TAB# configurator.variables['package.dexterity_type_name'] = '' #LINE# #TAB# #TAB# configurator.variables['package.dexterity_type_name_lower'] = '' #LINE# #TAB# return value"
"Read everything up to one of the chars in endchars . This is outside the formal grammar . The InvalidMailbox TokenList that is returned acts like a Mailbox , but the data attributes are None  <code> def get_invalid_mailbox(value, endchars): ","#LINE# #TAB# invalid_mailbox = InvalidMailbox() #LINE# #TAB# while value and value[0] not in endchars: #LINE# #TAB# #TAB# if value[0] in PHRASE_ENDS: #LINE# #TAB# #TAB# #TAB# invalid_mailbox.append(ValueTerminal(value[0], 'misplaced-special') #LINE# #TAB# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# #TAB# value = value[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# token, value = get_phrase(value) #LINE# #TAB# #TAB# #TAB# invalid_mailbox.append(token) #LINE# #TAB# return invalid_mailbox, value"
Parse HS_DESC_CONTENT response events for descriptor content Update the HS instance object with the data from the new descriptor  <code> def new_desc_content(desc_content_event): ,"#LINE# #TAB# logger.debug('Received new HS_DESC_CONTENT event for %s.onion', #LINE# #TAB# #TAB# desc_content_event.address) #LINE# #TAB# descriptor_text = str(desc_content_event.descriptor).encode('utf-8') #LINE# #TAB# if len(descriptor_text) < 5: #LINE# #TAB# #TAB# logger.debug('Empty descriptor received for %s.onion', #LINE# #TAB# #TAB# #TAB# desc_content_event.address) #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# descriptor.descriptor_received(descriptor_text) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# logger.exception( #LINE# #TAB# #TAB# #TAB# 'An unexpected exception occured in the new descriptor callback.') #LINE# #TAB# return None"
"Convert an integer to one- , four- or eight - unit graph6 sequence . This function is undefined if ` n ` is not in ` ` range(2 * * 36 ) ` `  <code> def n_to_data(n): ","#LINE# #TAB# if n <= 62: #LINE# #TAB# #TAB# return [n] #LINE# #TAB# elif n <= 258047: #LINE# #TAB# #TAB# return [63, n >> 12 & 63, n >> 6 & 63, n & 63] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [63, 63, n >> 30 & 63, n >> 24 & 63, n >> 18 & 63, n >> 12 & #LINE# #TAB# #TAB# #TAB# 63, n >> 6 & 63, n & 63]"
"Get the given peer s info <code> def atlas_get_peer( peer_hostport, peer_table=None ): ","#LINE# #TAB# ret = None #LINE# #TAB# with AtlasPeerTableLocked(peer_table) as ptbl: #LINE# #TAB# #TAB# ret = ptbl.get(peer_hostport, None) #LINE# #TAB# return ret"
compare brightness values to see if a color - scheme is light or dark <code> def is_dark_theme(color_list): ,"#LINE# #TAB# fg_brightness = util.get_hls_val(color_list[7], 'light') #LINE# #TAB# bg_brightness = util.get_hls_val(color_list[0], 'light') #LINE# #TAB# return fg_brightness > bg_brightness"
"@param url : page to get the title from @return : the page title in utf-8 or None in case that any kind of exception occured e.g. connection error , URL not known <code> def get_title_of_page(url): ",#LINE# #TAB# if BEAUTIFUL_SOUP_IMPORTED: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# opener = make_invenio_opener('UrlUtils') #LINE# #TAB# #TAB# #TAB# soup = BeautifulSoup.BeautifulSoup(opener.open(url)) #LINE# #TAB# #TAB# #TAB# return soup.title.string.encode('utf-8') #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# return 'Title not available'
"n - n ' : magnesium - hydrogen - phosphate sodium [ HFM89 ]  <code> def lambd_mghpo4_na_hfm89(T, P): ","#LINE# #TAB# lambd = -0.124 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return lambd, valid"
"Edit the current state Args : args ( object ) : Cli args <code> def cli_edit_state(fire: object, args: list): ",#LINE# #TAB# current_index = fire._current_index #LINE# #TAB# hold = editor.edit(contents=str(fire.states[current_index])).decode() #LINE# #TAB# args[current_index] = hold
"Needed to evaluate nested fields - e.g. collateral fields <code> def get_fieldname(field, obj): ","#LINE# #TAB# if isinstance(field, tuple): #LINE# #TAB# #TAB# if len(field) == 1: #LINE# #TAB# #TAB# #TAB# return [element.get(field[0]) for element in obj if element.get #LINE# #TAB# #TAB# #TAB# #TAB# (field[0])] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return get_fieldname(field[1:], obj[field[0]] if obj.get(field[ #LINE# #TAB# #TAB# #TAB# #TAB# 0]) else {} if len(field) > 2 else [{}]) #LINE# #TAB# else: #LINE# #TAB# #TAB# return [obj[field]] if obj.get(field) else []"
Returns a list of routes to configure the Local API Service based on the APIs configured in the template  <code> def _make_routing_list(api_provider): ,"#LINE# #TAB# #TAB# routes = [] #LINE# #TAB# #TAB# for api in api_provider.get_all(): #LINE# #TAB# #TAB# #TAB# route = Route(methods=[api.method], function_name=api.function_name, path=api.path, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# binary_types=api.binary_media_types) #LINE# #TAB# #TAB# #TAB# routes.append(route) #LINE# #TAB# #TAB# return routes"
"Check if the Dict passed in POST is of valid format or not . ( if there 's an "" @type "" key in the dict ) : param object _ - Object to be checked <code> def valid_object(object_: Dict[str, Any]) ->bool: ",#LINE# #TAB# if '@type' in object_: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Predict ' input ' vector which categories it belongs to  <code> def bnn_predict(device, input, length): ","#LINE# #TAB# result = SUCCESS #LINE# #TAB# cat = ctypes.c_ushort(0) #LINE# #TAB# result = _lib.xq_bnn_predict(ctypes.byref(device), input.ctypes.data_as #LINE# #TAB# #TAB# (ctypes.POINTER(ctypes.c_ubyte)), ctypes.c_uint32(length), ctypes. #LINE# #TAB# #TAB# byref(cat)) #LINE# #TAB# return result, cat.value"
Returns ( nat ) log of number of binary trees with ` size ` internal nodes  <code> def log_number_binary_trees(size): ,"#LINE# #TAB# assert isinstance(size, int) #LINE# #TAB# assert size >= 0 #LINE# #TAB# log = 0.0 #LINE# #TAB# for k in range(2, size + 1): #LINE# #TAB# #TAB# log += math.log(size + k) - math.log(k) #LINE# #TAB# return log"
Convert pil image to binary string  <code> def pil_img_to_img_bin(pil_img): ,"#LINE# #TAB# output = io.BytesIO() #LINE# #TAB# pil_img.save(output, format='JPEG') #LINE# #TAB# img_bin = output.getvalue() #LINE# #TAB# return img_bin"
"Read a run - length encoded run from the given fo with the given header and bit_width  <code> def read_rle(file_obj, header, bit_width, debug_logging): ","#LINE# #TAB# count = header >> 1 #LINE# #TAB# zero_data = b""\x00\x00\x00\x00"" #LINE# #TAB# width = (bit_width + 7) // 8 #LINE# #TAB# data = file_obj.read(width) #LINE# #TAB# data = data + zero_data[len(data):] #LINE# #TAB# value = struct.unpack(b""<i"", data)[0] #LINE# #TAB# if debug_logging: #LINE# #TAB# #TAB# logger.debug(""Read RLE group with value %s of byte-width %s and count %s"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# value, width, count) #LINE# #TAB# for _ in range(count): #LINE# #TAB# #TAB# yield value"
Find an unused local port . returns : int : an unused port number <code> def get_open_port(): ,"#LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #LINE# #TAB# s.bind(('', 0)) #LINE# #TAB# s.listen(1) #LINE# #TAB# port = s.getsockname()[1] #LINE# #TAB# s.close() #LINE# #TAB# return port"
Converts duckling output to the correct format  <code> def to_entities(js): ,"#LINE# #TAB# entities = {} #LINE# #TAB# for entity in js: #LINE# #TAB# #TAB# key, value = entity['dim'], entity['value'] #LINE# #TAB# #TAB# if key == 'time': #LINE# #TAB# #TAB# #TAB# key = 'datetime' #LINE# #TAB# #TAB# if key not in entities: #LINE# #TAB# #TAB# #TAB# entities[key] = [] #LINE# #TAB# #TAB# entities[key].append(value) #LINE# #TAB# return entities"
Get project settings only <code> def get_project_settings_only(project): ,#LINE# #TAB# path = project_settings_path(project) #LINE# #TAB# project_settings = {} #LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# project_settings = _read_json_with_comments(path) #LINE# #TAB# return project_settings
"Pad the middle dimension of a 3D tensor with "" padding "" zeros left and right . Apologies for the inane API , but Theano makes this really hard  <code> def temporal_padding(x, padding=(1, 1)): ","#LINE# #TAB# assert len(padding) == 2 #LINE# #TAB# input_shape = x.shape #LINE# #TAB# output_shape = input_shape[0], input_shape[1] + padding[0] + padding[1 #LINE# #TAB# #TAB# ], input_shape[2] #LINE# #TAB# output = T.zeros(output_shape) #LINE# #TAB# result = T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[0], #LINE# #TAB# #TAB# :], x) #LINE# #TAB# if hasattr(x, '_keras_shape'): #LINE# #TAB# #TAB# result._keras_shape = x._keras_shape[0], x._keras_shape[1] + py_sum( #LINE# #TAB# #TAB# #TAB# padding), x._keras_shape[2] #LINE# #TAB# return result"
"Implements r=1 / sqrt(sum((1 / S)*(q.v)^2 ) per note from Alexander Brady <code> def ellipse_size(H, Sij, GB): ","#LINE# #TAB# HX = np.inner(H.T, GB) #LINE# #TAB# lenHX = np.sqrt(np.sum(HX ** 2)) #LINE# #TAB# Esize, Rsize = nl.eigh(G2lat.U6toUij(Sij)) #LINE# #TAB# R = np.inner(HX / lenHX, Rsize) ** 2 * Esize #LINE# #TAB# lenR = 1.0 / np.sqrt(np.sum(R)) #LINE# #TAB# return lenR"
Create a pipe with FDs set CLOEXEC  <code> def pipe_cloexec(): ,"#LINE# #TAB# r, w = os.pipe() #LINE# #TAB# _set_cloexec_flag(r) #LINE# #TAB# _set_cloexec_flag(w) #LINE# #TAB# return r, w"
Get an entry 's URL field ( url or ee )  <code> def main_url(entry): ,"#LINE# #TAB# urlfields = 'url', 'ee' #LINE# #TAB# for f in urlfields: #LINE# #TAB# #TAB# if f in entry.fields: #LINE# #TAB# #TAB# #TAB# return entry.fields[f] #LINE# #TAB# return None"
Convert an tornado . HTTPResponse object to a requests . Response object <code> def new_response(response): ,#LINE# #TAB# new = Response() #LINE# #TAB# new._content = response.body #LINE# #TAB# new.status_code = response.code #LINE# #TAB# new.headers = dict(response.headers.get_all()) #LINE# #TAB# return new
Returns a FileDescriptorSet proto to be used by tf.io.decode_proto  <code> def get_descriptor_set(): ,"#LINE# #TAB# proto = pb.FileDescriptorSet() #LINE# #TAB# file_proto = proto.file.add(name=_FILE_NAME, package=_PACKAGE, syntax= #LINE# #TAB# #TAB# 'proto3') #LINE# #TAB# message_proto = file_proto.message_type.add(name=_MESSAGE_NAME) #LINE# #TAB# message_proto.field.add(name=_EXAMPLES_FIELD_NAME, number=1, type=pb. #LINE# #TAB# #TAB# FieldDescriptorProto.TYPE_BYTES, label=pb.FieldDescriptorProto. #LINE# #TAB# #TAB# LABEL_REPEATED) #LINE# #TAB# message_proto.field.add(name=_CONTEXT_FIELD_NAME, number=2, type=pb. #LINE# #TAB# #TAB# FieldDescriptorProto.TYPE_BYTES) #LINE# #TAB# return proto"
Get byte size of file - like object : param data_buffer : file - like object : return : total size in bytes <code> def get_buffer_size(data_buffer): ,"#LINE# #TAB# data_buffer.seek(0, os.SEEK_END) #LINE# #TAB# size = data_buffer.tell() #LINE# #TAB# data_buffer.seek(0) #LINE# #TAB# return size"
"Returns a funnel chart for the metrics specified in the GET variables  <code> def geckoboard_funnel(request, frequency=settings.STATISTIC_FREQUENCY_DAILY): ","#LINE# #TAB# params = get_gecko_params(request, cumulative=True) #LINE# #TAB# metrics = Metric.objects.filter(uid__in=params['uids']) #LINE# #TAB# items = [(metric.latest_count(frequency=params['frequency'], count=not params['cumulative'], #LINE# #TAB# #TAB# cumulative=params['cumulative']), metric.title) for metric in metrics] #LINE# #TAB# return { #LINE# #TAB# #TAB# 'items'#TAB# : items, #LINE# #TAB# #TAB# 'type'#TAB# : params['type'], #LINE# #TAB# #TAB# 'percentage': params['percentage'], #LINE# #TAB# #TAB# 'sort'#TAB# : params['sort'], #LINE# #TAB# }"
calculates the component a view is in <code> def calc_component_name(module): ,#LINE# #TAB# parts = module.split('.') #LINE# #TAB# if len(parts) == 2: #LINE# #TAB# #TAB# if parts[0] in listapps(): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return settings.component_packages[parts[0]] #LINE# #TAB# return parts[2]
"Open a tunneled connection from a 0MQ url  <code> def open_tunnel(addr, server, keyfile=None, password=None, paramiko=None, timeout=60): ","#LINE# #TAB# lport = select_random_ports(1)[0] #LINE# #TAB# transport, addr = addr.split('://') #LINE# #TAB# ip,rport = addr.split(':') #LINE# #TAB# rport = int(rport) #LINE# #TAB# if paramiko is None: #LINE# #TAB# #TAB# paramiko = sys.platform == 'win32' #LINE# #TAB# if paramiko: #LINE# #TAB# #TAB# tunnelf = paramiko_tunnel #LINE# #TAB# else: #LINE# #TAB# #TAB# tunnelf = openssh_tunnel #LINE# #TAB# tunnel = tunnelf(lport, rport, server, remoteip=ip, keyfile=keyfile, password=password, timeout=timeout) #LINE# #TAB# return 'tcp://127.0.0.1:%i'%lport, tunnel"
Enable the validation of the minimal slice count of 4 slices again ( DEFAULT ENABLED ) <code> def enable_validate_slicecount(): ,#LINE# #TAB# global validate_slicecount #LINE# #TAB# validate_slicecount = True
Given a filename walk its ast and determine if it declares a namespace package  <code> def declares_namespace_package(filename): ,"#LINE# import ast #LINE# with open(filename) as fp: #LINE# #TAB# init_py = ast.parse(fp.read(), filename) #LINE# calls = [node for node in ast.walk(init_py) if isinstance(node, ast.Call)] #LINE# for call in calls: #LINE# #TAB# if len(call.args) != 1: #LINE# #TAB# continue #LINE# #TAB# if isinstance(call.func, ast.Attribute) and call.func.attr != 'declare_namespace': #LINE# #TAB# continue #LINE# #TAB# if isinstance(call.func, ast.Name) and call.func.id != 'declare_namespace': #LINE# #TAB# continue #LINE# #TAB# if isinstance(call.args[0], ast.Name) and call.args[0].id == '__name__': #LINE# #TAB# return True #LINE# return False"
"Raise an exception if there are different amount of specified occurrences in src  <code> def assert_occurrence(probe, target, amount=1): ","#LINE# #TAB# occ = len(probe) #LINE# #TAB# if occ > amount: #LINE# #TAB# #TAB# msg = 'more than' #LINE# #TAB# elif occ < amount: #LINE# #TAB# #TAB# msg = 'less than' #LINE# #TAB# elif not occ: #LINE# #TAB# #TAB# msg = 'no' #LINE# #TAB# else: #LINE# #TAB# #TAB# msg = None #LINE# #TAB# if msg: #LINE# #TAB# #TAB# raise CommandExecutionError('Found {0} expected occurrences in ""{1}"" expression'.format(msg, target)) #LINE# #TAB# return occ"
"returns the argmax ( key ) and max ( value ) from a dictionary if pick_random_best is True , then decide ties randomly  <code> def argmax_vmax_dict(d, pick_random_best=True): ","#LINE# #TAB# max_key = None #LINE# #TAB# max_val = float('-inf') #LINE# #TAB# maxL = [] #LINE# #TAB# for k, v in d.items(): #LINE# #TAB# #TAB# if v > max_val: #LINE# #TAB# #TAB# #TAB# max_val = v #LINE# #TAB# #TAB# #TAB# max_key = k #LINE# #TAB# #TAB# #TAB# maxL = [(k, v)] #LINE# #TAB# #TAB# elif v == max_val: #LINE# #TAB# #TAB# #TAB# maxL.append((k, v)) #LINE# #TAB# if pick_random_best: #LINE# #TAB# #TAB# return random.choice(maxL) #LINE# #TAB# else: #LINE# #TAB# #TAB# return max_key, max_val"
! @brief Convert a 64-bit int to an IEEE754 float <code> def u64_to_float64(data): ,"#LINE# #TAB# d = struct.pack('>Q', data) #LINE# #TAB# return struct.unpack('>d', d)[0]"
Get a list of architectures given our dockerfiles <code> def get_platforms(path: str = get_dockerfiles_path()) -> List[str]: ,"#LINE# #TAB# dockerfiles = glob.glob(os.path.join(path, ""Dockerfile.*"")) #LINE# #TAB# dockerfiles = list(filter(lambda x: x[-1] != '~', dockerfiles)) #LINE# #TAB# files = list(map(lambda x: re.sub(r""Dockerfile.(.*)"", r""\1"", x), dockerfiles)) #LINE# #TAB# platforms = list(map(lambda x: os.path.split(x)[1], sorted(files))) #LINE# #TAB# return platforms"
Run discovery mode for every stream in the tap configuration : param config : connection and streams configuration : return : list of information about every stream <code> def discover_streams(config: Dict) ->List[Dict]: ,"#LINE# #TAB# streams = [] #LINE# #TAB# for table_spec in config['tables']: #LINE# #TAB# #TAB# schema = discover_schema(config, table_spec) #LINE# #TAB# #TAB# streams.append({'stream': table_spec['table_name'], 'tap_stream_id': #LINE# #TAB# #TAB# #TAB# table_spec['table_name'], 'schema': schema, 'metadata': #LINE# #TAB# #TAB# #TAB# load_metadata(table_spec, schema)}) #LINE# #TAB# return streams"
try to get socket type from srv record : param proto : record protocol according to service.proto.<domain > template : return : socket type or -1 if unknown protocol <code> def guess_socket_type(proto: Optional[str]) ->int: ,"#LINE# #TAB# if proto in ('tcp', '_tcp'): #LINE# #TAB# #TAB# return socket.SOCK_STREAM #LINE# #TAB# elif proto in ('udp', '_udp'): #LINE# #TAB# #TAB# return socket.SOCK_DGRAM #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1"
"Read all lines from access_log_path starting at start_position and append them to an empty buffer . Return that buffer  <code> def access_log_to_buffer(access_log_path, start_position=0, chunk_size=None): ","#LINE# #TAB# log_buffer = StringIO() #LINE# #TAB# with open(access_log_path) as f: #LINE# #TAB# #TAB# line_count = 0 #LINE# #TAB# #TAB# for position, line in enumerate(f): #LINE# #TAB# #TAB# #TAB# if position > start_position: #LINE# #TAB# #TAB# #TAB# #TAB# log_buffer.write(line) #LINE# #TAB# #TAB# #TAB# #TAB# line_count += 1 #LINE# #TAB# #TAB# #TAB# #TAB# if chunk_size is not None and line_count == chunk_size: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# log_buffer.seek(0) #LINE# #TAB# return log_buffer"
Return True if version_string is in v1.0.0 form <code> def version_needs_aka(version_string): ,#LINE# #TAB# if not version_string: #LINE# #TAB# #TAB# return False #LINE# #TAB# matches = VERSION_WITH_LEADING_V_MATCH_RE.match(version_string) #LINE# #TAB# if not matches: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Encode a number as a unicode character  <code> def encode_number(number): ,"#LINE# #TAB# number = number << 1 #LINE# #TAB# if number < 0: #LINE# #TAB# #TAB# number = ~number #LINE# #TAB# encoded = '' #LINE# #TAB# while number >= _SIXTH_BIT_MASK: #LINE# #TAB# #TAB# code_point = (_SIXTH_BIT_MASK | number & _FIVE_BIT_MASK #LINE# #TAB# #TAB# #TAB# ) + _ENCODING_OFFSET_MINUS_ONE #LINE# #TAB# #TAB# encoded += chr(code_point) #LINE# #TAB# #TAB# number = _urshift32(number, _BIT_SHIFT) #LINE# #TAB# encoded += chr(number + _ENCODING_OFFSET_MINUS_ONE) #LINE# #TAB# return encoded"
Convert ISO 3166 ` code ` to localized country name  <code> def code_to_name(code): ,"#LINE# #TAB# if not _countries: #LINE# #TAB# #TAB# _init_countries() #LINE# #TAB# with aeidon.util.silent(LookupError): #LINE# #TAB# #TAB# return d_('iso_3166', _countries[code]) #LINE# #TAB# return code"
"leaves related articles and wikitables in place <code> def templates_collector(text, open, close): ","#LINE# #TAB# others = [] #LINE# #TAB# spans = [i for i in findBalanced(text, open, close)] #LINE# #TAB# spanscopy = copy(spans) #LINE# #TAB# for i in range(len(spans)): #LINE# #TAB# #TAB# start, end = spans[i] #LINE# #TAB# #TAB# o = text[start:end] #LINE# #TAB# #TAB# ol = o.lower() #LINE# #TAB# #TAB# if 'vaata|' in ol or 'wikitable' in ol: #LINE# #TAB# #TAB# #TAB# spanscopy.remove(spans[i]) #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# others.append(o) #LINE# #TAB# text = dropSpans(spanscopy, text) #LINE# #TAB# return text, others"
The entire non - stiff DETEST suite of problems plus the problems from Shampine - Baca paper . More problems can be added <code> def detest_suite_plus(): ,"#LINE# #TAB# detestkeys = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', #LINE# #TAB# #TAB# 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5', #LINE# #TAB# #TAB# 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'SB1', #LINE# #TAB# #TAB# 'SB2', 'SB3'] #LINE# #TAB# return [detest(dtkey) for dtkey in detestkeys]"
Returns True if the given operand ( string ) contains an integer number <code> def is_int(op): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# int(op) #LINE# #TAB# #TAB# return True #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return False
Return True if the ` node ` has a an xsi : type attribute  <code> def is_typed(node): ,"#LINE# #TAB# if hasattr(node, 'attrib'): #LINE# #TAB# #TAB# return TAG_XSI_TYPE in node.attrib #LINE# #TAB# return False"
"computes minimum distance from a point and a polyline i.e. browsing consecutive segments of the polyline . Returns the distance , and curvilinear position from extremities <code> def dist_to_poly(xn, yn, nb, xpoly, ypoly): ","#LINE# #TAB# x1, y1 = xpoly[0], ypoly[0] #LINE# #TAB# dmin = _dist(xn, yn, x1, y1) #LINE# #TAB# smin, totlen = 0.0, 0.0 #LINE# #TAB# for x2, y2 in izip(xpoly[1:], ypoly[1:]): #LINE# #TAB# #TAB# d, s, slen = dist_p_seg(xn, yn, x1, y1, x2, y2) #LINE# #TAB# #TAB# if d < dmin: #LINE# #TAB# #TAB# #TAB# smin = totlen + s #LINE# #TAB# #TAB# #TAB# dmin = d #LINE# #TAB# #TAB# totlen += slen #LINE# #TAB# #TAB# x1, y1 = x2, y2 #LINE# #TAB# return dmin, smin, totlen - smin"
"Gets a file name for the generated doc files . : param name : Name of a folder or file . : param is_step : If name is related to step file or not . : return : The file name for the doc file  <code> def get_file_name(name: str, is_step: bool) ->str: ","#LINE# #TAB# file_name = name.replace('_', '-') #LINE# #TAB# if is_step: #LINE# #TAB# #TAB# file_name = f'{file_name}-steps.rst' #LINE# #TAB# else: #LINE# #TAB# #TAB# file_name = f'{file_name}.rst' #LINE# #TAB# return file_name"
"Query the folder i d of the directory  <code> def get_folder_id(path, c): ","#LINE# #TAB# sql = """""" #LINE# #TAB# #TAB# SELECT folder_id #LINE# #TAB# #TAB# FROM folders #LINE# #TAB# #TAB# WHERE path = ? #LINE# #TAB# #TAB# """""" #LINE# #TAB# c.execute(sql, (path,)) #LINE# #TAB# return c.fetchone()[0]"
Return value converted from Tcl object to Python object  <code> def tclobj_to_py(val): ,"#LINE# #TAB# if val and hasattr(val, '__len__') and not isinstance(val, str): #LINE# #TAB# #TAB# if getattr(val[0], 'typename', None) == 'StateSpec': #LINE# #TAB# #TAB# #TAB# val = _list_from_statespec(val) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# val = list(map(_convert_stringval, val)) #LINE# #TAB# elif hasattr(val, 'typename'): #LINE# #TAB# #TAB# val = _convert_stringval(val) #LINE# #TAB# return val"
get the corresponding collection of the given i d <code> def get_id_col(id): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# col = collection.IDS[id] #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# raise IndexError #LINE# #TAB# else: #LINE# #TAB# #TAB# return col
"Get a : py : class:` . CMakeBuilder ` object from the command line  <code> def get_cmake_builder(args, default_dep_types=None): ","#LINE# #TAB# build_worktree = get_build_worktree(args) #LINE# #TAB# build_projects = get_build_projects(build_worktree, args, solve_deps=False) #LINE# #TAB# cmake_builder = qibuild.cmake_builder.CMakeBuilder(build_worktree, #LINE# #TAB# #TAB# build_projects) #LINE# #TAB# cmake_builder.dep_types = get_dep_types(args, default=default_dep_types) #LINE# #TAB# return cmake_builder"
"Parse track from YouTube  <code> def go_pafy(raw_song, meta_tags=None): ","#LINE# #TAB# if internals.is_youtube(raw_song): #LINE# #TAB# #TAB# track_info = pafy.new(raw_song) #LINE# #TAB# else: #LINE# #TAB# #TAB# track_url = generate_youtube_url(raw_song, meta_tags) #LINE# #TAB# #TAB# if track_url: #LINE# #TAB# #TAB# #TAB# track_info = pafy.new(track_url) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# track_info = None #LINE# #TAB# return track_info"
"Common code between all 3 t - test functions  <code> def ttest_finish(df, t): ","#LINE# #TAB# prob = distributions.t.sf(np.abs(t), df) * 2 #LINE# #TAB# if t.ndim == 0: #LINE# #TAB# #TAB# t = t[()] #LINE# #TAB# return t, prob"
Returns the variable for latitude <code> def get_lat_variable(nc): ,"#LINE# #TAB# if 'latitude' in nc.variables: #LINE# #TAB# #TAB# return 'latitude' #LINE# #TAB# latitudes = nc.get_variables_by_attributes(standard_name=""latitude"") #LINE# #TAB# if latitudes: #LINE# #TAB# #TAB# return latitudes[0].name #LINE# #TAB# return None"
"Returns the target namespace used in the schema Args : xsd_tree : namespaces : Returns : <code> def get_target_namespace(xsd_tree, namespaces): ","#LINE# #TAB# root_attributes = xsd_tree.getroot().attrib #LINE# #TAB# target_namespace = root_attributes['targetNamespace' #LINE# #TAB# #TAB# ] if 'targetNamespace' in root_attributes else None #LINE# #TAB# target_namespace_prefix = '' #LINE# #TAB# if target_namespace is not None: #LINE# #TAB# #TAB# for prefix, url in list(namespaces.items()): #LINE# #TAB# #TAB# #TAB# if url == target_namespace: #LINE# #TAB# #TAB# #TAB# #TAB# target_namespace_prefix = prefix #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# return target_namespace, target_namespace_prefix"
Parses boolean values represented as ' yes ' or ' no ' ( case insensitive )  <code> def parse_yes_no(value): ,"#LINE# #TAB# result = {normalize_text('yes'): True, normalize_text('no'): False}.get( #LINE# #TAB# #TAB# normalize_text(value)) #LINE# #TAB# if result is None: #LINE# #TAB# #TAB# raise SpreadsheetImportError([u'must be ""yes"" or ""no""']) #LINE# #TAB# return result"
"Get a list of entitlements on a repository  <code> def list_entitlements(owner, repo, page, page_size, show_tokens): ","#LINE# #TAB# client = get_entitlements_api() #LINE# #TAB# with catch_raise_api_exception(): #LINE# #TAB# #TAB# data, _, headers = client.entitlements_list_with_http_info( #LINE# #TAB# #TAB# #TAB# owner=owner, #LINE# #TAB# #TAB# #TAB# repo=repo, #LINE# #TAB# #TAB# #TAB# page=page, #LINE# #TAB# #TAB# #TAB# page_size=page_size, #LINE# #TAB# #TAB# #TAB# show_tokens=show_tokens, #LINE# #TAB# #TAB# ) #LINE# #TAB# ratelimits.maybe_rate_limit(client, headers) #LINE# #TAB# page_info = PageInfo.from_headers(headers) #LINE# #TAB# entitlements = [ent.to_dict() for ent in data] #LINE# #TAB# return entitlements, page_info"
"Convert a document into a list of tokens . This lowercases , tokenizes , de - accents ( optional ) . -- the output are final tokens = unicode strings , that wo n't be processed any further  <code> def simple_preprocess(doc, deacc=False, min_len=2, max_len=30, lower=False): ","#LINE# #TAB# tokens = [token for token in tokenize(doc, lower=lower, deacc=deacc, #LINE# #TAB# #TAB# errors='ignore') if min_len <= len(token) <= max_len and not token. #LINE# #TAB# #TAB# startswith('_')] #LINE# #TAB# return tokens"
Initializing users by hardcoding password . Another use case is to read usernames from an external file ( like /etc / passwd )  <code> def init_users(): ,"#LINE# #TAB# admin = AuthUser(username='admin') #LINE# #TAB# admin.set_and_encrypt_password('password', salt='123') #LINE# #TAB# g.users = {'admin': admin}"
"Convert a raw Color value to a gomill colour . Returns ' b ' or ' w '  <code> def interpret_colour(s, context=None): ","#LINE# #TAB# colour = s.decode('ascii').lower() #LINE# #TAB# if colour not in ('b', 'w'): #LINE# #TAB# #TAB# raise ValueError #LINE# #TAB# return colour"
"Set signal to zero up to certain threshold . Parameters ---------- A : ndarray threshold : float Percentage of maximal amplitude value Returns ---------- A : ndarray Background subtracted array <code> def background_subtract_treshold(A, threshold): ","#LINE# #TAB# for i in range(np.shape(A)[0]): #LINE# #TAB# #TAB# for j in range(np.shape(A)[1]): #LINE# #TAB# #TAB# #TAB# if A[i, j] < np.max(A) * threshold: #LINE# #TAB# #TAB# #TAB# #TAB# A[i, j] = 0 #LINE# #TAB# return A"
Return dictionary of styles defined in * style_dir *  <code> def read_style_directory(style_dir): ,"#LINE# #TAB# styles = dict() #LINE# #TAB# for path in Path(style_dir).glob(f'*.{STYLE_EXTENSION}'): #LINE# #TAB# #TAB# with warnings.catch_warnings(record=True) as warns: #LINE# #TAB# #TAB# #TAB# styles[path.stem] = rc_params_from_file(path, #LINE# #TAB# #TAB# #TAB# #TAB# use_default_template=False) #LINE# #TAB# #TAB# for w in warns: #LINE# #TAB# #TAB# #TAB# _log.warning('In %s: %s', path, w.message) #LINE# #TAB# return styles"
"There is a bug in simple ITK for Z axis in 3D images . This is a fix  <code> def _fix_sitk_bug(path, metadata): ","#LINE# #TAB# #TAB# ds = dicom.read_file(path) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# metadata[""voxelsize_mm""][0] = ds.SpacingBetweenSlices #LINE# #TAB# #TAB# except Exception as e: #LINE# #TAB# #TAB# #TAB# logger.warning(""Read dicom 'SpacingBetweenSlices' failed: "", e) #LINE# #TAB# #TAB# return metadata"
"Runs through the given list of file extensions and returns the first file with the given base path and extension combination that actually exists  <code> def find_first_file_with_ext(base_paths, prefix, exts): ","#LINE# #TAB# for base_path in base_paths: #LINE# #TAB# #TAB# for ext in exts: #LINE# #TAB# #TAB# #TAB# filename = os.path.join(base_path, ""%s%s"" % (prefix, ext)) #LINE# #TAB# #TAB# #TAB# if os.path.exists(filename) and os.path.isfile(filename): #LINE# #TAB# #TAB# #TAB# #TAB# logger.debug(""Found first file with relevant extension: %s"", filename) #LINE# #TAB# #TAB# #TAB# #TAB# return base_path, ext #LINE# #TAB# logger.debug(""No files found for prefix %s, extensions %s"", prefix, "", "".join(exts)) #LINE# #TAB# return None, None"
"Get settings value  <code> def settings_val(name, default=None, allow_none=False): ","#LINE# #TAB# value = getattr(settings, name, None) #LINE# #TAB# if value is None and not allow_none: #LINE# #TAB# #TAB# value = default #LINE# #TAB# return value"
Formats the file size into a human readable format  <code> def format_filesize(size): ,"#LINE# #TAB# for suffix in (""bytes"", ""KB"", ""MB"", ""GB"", ""TB""): #LINE# #TAB# #TAB# if size < 1024.0: #LINE# #TAB# #TAB# #TAB# if suffix in (""GB"", ""TB""): #LINE# #TAB# #TAB# #TAB# #TAB# return ""{0:3.2f} {1}"".format(size, suffix) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# return ""{0:3.1f} {1}"".format(size, suffix) #LINE# #TAB# #TAB# size /= 1024.0"
"Calculates the request payload size <code> def calculate_size(name, txn_id, thread_id, key, value, ttl): ",#LINE# #TAB# data_size = 0 #LINE# #TAB# data_size += calculate_size_str(name) #LINE# #TAB# data_size += calculate_size_str(txn_id) #LINE# #TAB# data_size += LONG_SIZE_IN_BYTES #LINE# #TAB# data_size += calculate_size_data(key) #LINE# #TAB# data_size += calculate_size_data(value) #LINE# #TAB# data_size += LONG_SIZE_IN_BYTES #LINE# #TAB# return data_size
"Convert datetime . time to int timestamp with microseconds <code> def prepare_time_micros(data, schema): ","#LINE# #TAB# if isinstance(data, datetime.time): #LINE# #TAB# #TAB# return long(data.hour * MCS_PER_HOUR + data.minute * MCS_PER_MINUTE #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# + data.second * MCS_PER_SECOND + data.microsecond) #LINE# #TAB# else: #LINE# #TAB# #TAB# return data"
"Add some helpful attributes to node  <code> def format_function_node(node,path,stack): ",#LINE# #TAB# node.weight = calcFnWeight(node) #LINE# #TAB# node.path = path #LINE# #TAB# node.pclass = getCurrentClass(stack) #LINE# #TAB# return node
Create a system from restart information . Parameters ---------- restart : dict A dictionary with restart information . Returns ------- system : object like : py : class:` . System ` The system object we create here  <code> def create_system_from_restart(restart): ,"#LINE# #TAB# settings = restart['system'] #LINE# #TAB# box = box_from_restart(settings) #LINE# #TAB# system = System(temperature=settings['temperature']['set'], units= #LINE# #TAB# #TAB# settings['units'], box=box) #LINE# #TAB# system.particles = particles_from_restart(settings) #LINE# #TAB# return system"
STOP DTMF ACKNOWLEDGE Section 9 . 3 . 30 <code> def stop_dtmf_acknowledge(): ,#LINE# #TAB# a = TpPd(pd=0x3) #LINE# #TAB# b = MessageType(mesType=0x32) #LINE# #TAB# packet = a / b #LINE# #TAB# return packet
Given a layer of estntltk objects yields pairwise intersecting elements . Breaks when the layer is changed or deleted after initializing the iterator  <code> def iterate_intersecting_pairs(layer): ,"#LINE# #TAB# yielded = set() #LINE# #TAB# ri = layer[:] #LINE# #TAB# for i1, elem1 in enumerate(ri): #LINE# #TAB# #TAB# for i2, elem2 in enumerate(ri): #LINE# #TAB# #TAB# #TAB# if i1 != i2 and elem1['start'] <= elem2['start'] < elem1['end']: #LINE# #TAB# #TAB# #TAB# #TAB# inds = (i1, i2) if i1 < i2 else (i2, i1) #LINE# #TAB# #TAB# #TAB# #TAB# if inds not in yielded and in_by_identity(layer, elem1) and in_by_identity(layer, elem2): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yielded.add(inds) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield elem1, elem2"
"Given a center and width in energy units get back a width in nm  <code> def nm_width(center, width, units=""wn"") -> float: ","#LINE# #TAB# red = wt_units.converter(center - width / 2., units, ""nm"") #LINE# #TAB# blue = wt_units.converter(center + width / 2., units, ""nm"") #LINE# #TAB# return red - blue"
"Get a dict of the skipped tests per arch parsing the bs4 instance  <code> def get_skipped_dict(arch, soup): ","#LINE# #TAB# re_arch = re.compile(arch + '_') #LINE# #TAB# arch_findings = soup.find_all('td', id=re_arch) #LINE# #TAB# results = SortedDict() #LINE# #TAB# for arch_item in arch_findings: #LINE# #TAB# #TAB# match = arch_item.find(title='cancelled') #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# module_name = arch_item.find_previous(class_='name').get_text( #LINE# #TAB# #TAB# #TAB# #TAB# ).strip('\n') #LINE# #TAB# #TAB# #TAB# test_link = arch_item.a['href'] #LINE# #TAB# #TAB# #TAB# results.update({module_name: test_link}) #LINE# #TAB# return results"
Creates an enum instance from the corresponding value  <code> def from_value(value): ,#LINE# #TAB# for e in CharacterAttribute: #LINE# #TAB# #TAB# if e.value == value: #LINE# #TAB# #TAB# #TAB# return e #LINE# #TAB# return CharacterAttribute.default
"Converts a protocol version number , such as "" 1.0 "" to a tuple ( 1 , 0 ) . If the version number is bad , ( 0 , ) indicating version 0 is returned  <code> def protocol_tuple(s): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# return tuple(int(part) for part in s.split('.')) #LINE# #TAB# except (TypeError, ValueError, AttributeError): #LINE# #TAB# #TAB# return 0,"
"Return highest , not occupied position in the children of ` ` node ` ` . : param node : : type node : Node or uuid4 <code> def find_highest_position(cur, node): ","#LINE# #TAB# if node is not None: #LINE# #TAB# #TAB# id = str(node) #LINE# #TAB# else: #LINE# #TAB# #TAB# id = None #LINE# #TAB# sql = """""" #LINE# #TAB# SELECT #LINE# #TAB# #TAB# MAX(position) #LINE# #TAB# FROM #LINE# #TAB# #TAB# nodes #LINE# #TAB# WHERE #LINE# #TAB# #TAB# parent=%s; #LINE# #TAB# """""" #LINE# #TAB# cur.execute(sql, (id,)) #LINE# #TAB# result = cur.fetchone()['max'] #LINE# #TAB# if result is not None: #LINE# #TAB# #TAB# return result #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1"
"Normalize each channel in image : param image : NumPy array with shape ( ... , n_channels ) : return : Same image normalized over each channel <code> def normalize_channels(image: np.ndarray) ->np.ndarray: ","#LINE# #TAB# normalized = np.zeros(image.shape, np.uint8) #LINE# #TAB# for i in range(image.shape[2]): #LINE# #TAB# #TAB# normalized[..., i] = cv2.normalize(image[..., i], None, 0, 255, cv2 #LINE# #TAB# #TAB# #TAB# .NORM_MINMAX, 8) #LINE# #TAB# return normalized"
Enable the validation whether the volume is orthogonal again ( DEFAULT ENABLED ) <code> def enable_validate_orthogonal(): ,#LINE# #TAB# global validate_orthogonal #LINE# #TAB# validate_orthogonal = True
"Return the half - top of the given array  <code> def set_ht(A, a): ","#LINE# #TAB# nx, _ = A.shape #LINE# #TAB# li = int(nx / 2) #LINE# #TAB# A[:li, :] = a"
Getting and testing response from the /products endpoint <code> def get_products(response) ->List[Product]: ,"#LINE# #TAB# if type(response) is not list: #LINE# #TAB# #TAB# raise FailedTest('The response should be a JSON Array') #LINE# #TAB# try: #LINE# #TAB# #TAB# products = [dacite.from_dict(data_class=Product, data=product) for #LINE# #TAB# #TAB# #TAB# product in response] #LINE# #TAB# except (dacite.exceptions.WrongTypeError, dacite.exceptions. #LINE# #TAB# #TAB# MissingValueError, dacite.exceptions.UnexpectedDataError): #LINE# #TAB# #TAB# raise FailedTest( #LINE# #TAB# #TAB# #TAB# 'Incorrect JSON format in response from the /products endpoint') #LINE# #TAB# return products"
Extract IDs from documents . : param documents : Dict of documents grouped by ` ` _ group_by_type ` ` . : returns : Dict of the same format with documents replaced by their ids  <code> def extract_ids(documents): ,"#LINE# #TAB# document_ids = {} #LINE# #TAB# for model, documents in documents.items(): #LINE# #TAB# #TAB# pk_field = model.pk_field() #LINE# #TAB# #TAB# document_ids[model] = [getattr(doc, pk_field) for doc in documents] #LINE# #TAB# return document_ids"
Check if the provided ipv4 or ipv6 address is a valid CIDR address  <code> def is_valid_cidr(address): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# netaddr.IPNetwork(address) #LINE# #TAB# except netaddr.core.AddrFormatError: #LINE# #TAB# #TAB# return False #LINE# #TAB# except UnboundLocalError: #LINE# #TAB# #TAB# return False #LINE# #TAB# ip_segment = address.split('/') #LINE# #TAB# if len(ip_segment) <= 1 or ip_segment[1] == '': #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Return version of the Quantum DataSet installed Returns None if no data is installed <code> def check_quantum_dataset_installation(location: str) ->Optional[str]: ,"#LINE# #TAB# qdfile = os.path.join(location, 'quantumdataset.txt') #LINE# #TAB# if not os.path.exists(qdfile): #LINE# #TAB# #TAB# return None #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(os.path.join(location, 'quantumdataset.txt'), 'rt') as fid: #LINE# #TAB# #TAB# #TAB# version = fid.readline().strip() #LINE# #TAB# except Exception as ex: #LINE# #TAB# #TAB# raise Exception( #LINE# #TAB# #TAB# #TAB# 'could not correct data for QuantumDataset at location %s' % #LINE# #TAB# #TAB# #TAB# location) from ex #LINE# #TAB# return version"
"Retrieves data vote places ( bureaux de vote in French ) with geocodes . @param folder where to download @param as_df return as a dataframe @param fLOG logging function @return list of dataframe <code> def villes_geo(folder='.', as_df=False, fLOG=noLOG): ","#LINE# #TAB# this = os.path.abspath(os.path.dirname(__file__)) #LINE# #TAB# data = os.path.join(this, 'data_elections', 'villesgeo.zip') #LINE# #TAB# geo = unzip_files(data, where_to=folder) #LINE# #TAB# if isinstance(geo, list): #LINE# #TAB# #TAB# res = geo[0] #LINE# #TAB# else: #LINE# #TAB# #TAB# res = geo #LINE# #TAB# if as_df: #LINE# #TAB# #TAB# return pandas.read_csv(res, encoding='utf-8', sep='\t') #LINE# #TAB# else: #LINE# #TAB# #TAB# return res"
No idea how th f to get this to work <code> def run_command_orig(cmd): ,"#LINE# #TAB# process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) #LINE# #TAB# stdout, stderr = process.communicate() #LINE# #TAB# if process.returncode == 0: #LINE# #TAB# #TAB# os.killpg(os.getpgid(pro.pid), signal.SIGTERM) #LINE# #TAB# else: #LINE# #TAB# #TAB# raise BadRCError(""Bad rc (%s) for cmd '%s': %s"" % (process.returncode, cmd, stdout + stderr)) #LINE# #TAB# return stdout"
Return checker executable in the form of a list of arguments for subprocess . Popen <code> def get_checker_executable(name): ,"#LINE# #TAB# if programs.is_program_installed(name): #LINE# #TAB# #TAB# return [name] #LINE# #TAB# else: #LINE# #TAB# #TAB# path1 = programs.python_script_exists(package=None, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# module=name+'_script') #LINE# #TAB# #TAB# path2 = programs.python_script_exists(package=None, module=name) #LINE# #TAB# #TAB# if path1 is not None: #LINE# #TAB# #TAB# #TAB# return [sys.executable, path1] #LINE# #TAB# #TAB# elif path2 is not None: #LINE# #TAB# #TAB# #TAB# return [sys.executable, path2]"
"a = b = c , alpha = beta = gamma <code> def trigonal_p(cp): ","#LINE# #TAB# a, b, c, al, be, ga = cp #LINE# #TAB# anew = (a + b + c) / 3.0 #LINE# #TAB# alnew = (al + be + ga) / 3.0 #LINE# #TAB# return [anew, anew, anew, alnew, alnew, alnew]"
Turn a n - D tensor into a 2D tensor where the first dimension is conserved  <code> def batch_flatten(x): ,"#LINE# #TAB# y = T.reshape(x, (x.shape[0], T.prod(x.shape[1:]))) #LINE# #TAB# if hasattr(x, '_keras_shape'): #LINE# #TAB# #TAB# if None in x._keras_shape[1:]: #LINE# #TAB# #TAB# #TAB# y._keras_shape = x._keras_shape[0], None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# y._keras_shape = x._keras_shape[0], np.prod(x._keras_shape[1:]) #LINE# #TAB# return y"
Get the univariate time series data . : return : numpy array <code> def get_data(): ,#LINE# #TAB# dat = get_dataframe() #LINE# #TAB# dat = np.array(dat['avg']) #LINE# #TAB# return dat
"Replaces the URL 's file extension  <code> def change_extension(url: str, ext: str) ->str: ","#LINE# #TAB# base_url, _ = os.path.splitext(url) #LINE# #TAB# return f'{base_url}.{ext}'"
"Takes array of connections and returns a path . Connections is array of lists with 1 or 2 elements . These elements are indices of teh vertices , connected to this vertex Guarantees that first index < last index <code> def restore_path(connections): ","#LINE# #TAB# start, end = [idx for idx, conn in enumerate(connections) if len(conn) == 1 #LINE# #TAB# #TAB# ] #LINE# #TAB# path = [start] #LINE# #TAB# prev_point = None #LINE# #TAB# cur_point = start #LINE# #TAB# while True: #LINE# #TAB# #TAB# next_points = [pnt for pnt in connections[cur_point] if pnt != #LINE# #TAB# #TAB# #TAB# prev_point] #LINE# #TAB# #TAB# if not next_points: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# next_point = next_points[0] #LINE# #TAB# #TAB# path.append(next_point) #LINE# #TAB# #TAB# prev_point, cur_point = cur_point, next_point #LINE# #TAB# return path"
"Return the component referred to by the provided reference , regardless of whether it is a normal or anonymous reference  <code> def from_pickled(cls, object_reference): ","#LINE# #TAB# if not isinstance(object_reference, _PickledAnonymousReference): #LINE# #TAB# #TAB# assert isinstance(object_reference, tuple) #LINE# #TAB# #TAB# object_reference = pyxb.namespace.ExpandedName(object_reference) #LINE# #TAB# return object_reference"
Return the set of all child classes of ` cls ` . Parameters ---------- cls : Type Returns ------- frozenset[Type ] <code> def flatten_subclass_tree(cls): ,"#LINE# #TAB# subclasses = frozenset(cls.__subclasses__()) #LINE# #TAB# children = frozenset(toolz.concat(map(flatten_subclass_tree, subclasses))) #LINE# #TAB# return frozenset({cls}) | subclasses | children"
"Prompt the user for the user , password and receiver values for the config . Returns : str , str , str : user e - mail , user password and receiver e - mail ( or whatever the user enters when prompted for these )  <code> def prompt_for_config_values(): ","#LINE# #TAB# print( #LINE# #TAB# #TAB# ) #LINE# #TAB# user = input(""Please enter the sender's e-mail address: "") #LINE# #TAB# password = input( #LINE# #TAB# #TAB# ""Please enter the password for the sender's e-mail address: "") #LINE# #TAB# receiver = input(""Please enter the receiver's email address: "") #LINE# #TAB# if not user or not password or not receiver: #LINE# #TAB# #TAB# print( #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# print('Everything looks spiffy, thank you!') #LINE# #TAB# return user, password, receiver"
Read the source of a Django template returning the Unicode text  <code> def read_template_source(filename): ,"#LINE# #TAB# from django.conf import settings #LINE# #TAB# if not settings.configured: #LINE# #TAB# #TAB# settings.configure() #LINE# #TAB# with open(filename, ""rb"") as f: #LINE# #TAB# #TAB# text = f.read().decode(settings.FILE_CHARSET) #LINE# #TAB# return text"
"Decodes the ` ` POST``ed signed data <code> def decode_signed_user(encoded_sig, encoded_data): ","#LINE# #TAB# decoded_sig = _decode(encoded_sig) #LINE# #TAB# decoded_data = loads(_decode(encoded_data)) #LINE# #TAB# if decoded_sig != hmac.new(app.config['CANVAS_CLIENT_SECRET'], #LINE# #TAB# #TAB# encoded_data, sha256).digest(): #LINE# #TAB# #TAB# raise ValueError(""sig doesn't match hash"") #LINE# #TAB# return decoded_sig, decoded_data"
"Return monitor config dictionaries for all the monitors , including the built it ones  <code> def get_all_monitor_configs(configuration, platform_controller): ","#LINE# #TAB# all_monitors = [] #LINE# #TAB# for monitor in configuration.monitor_configs: #LINE# #TAB# #TAB# all_monitors.append(monitor.copy()) #LINE# #TAB# for monitor in platform_controller.get_default_monitors(configuration): #LINE# #TAB# #TAB# all_monitors.append(configuration.parse_monitor_config(monitor, #LINE# #TAB# #TAB# #TAB# 'monitor with module name ""%s"" requested by platform' % monitor #LINE# #TAB# #TAB# #TAB# ['module']).copy()) #LINE# #TAB# return all_monitors"
"Verify that a font contains a specific set of characters . Args : filepath : Path to fsontfile alphabet : A string of characters to check for  <code> def font_supports_alphabet(filepath, alphabet): ",#LINE# #TAB# if alphabet == '': #LINE# #TAB# #TAB# return True #LINE# #TAB# font = fontTools.ttLib.TTFont(filepath) #LINE# #TAB# if not all(any(ord(c) in table.cmap.keys() for table in font['cmap']. #LINE# #TAB# #TAB# tables) for c in alphabet): #LINE# #TAB# #TAB# return False #LINE# #TAB# font = PIL.ImageFont.truetype(filepath) #LINE# #TAB# try: #LINE# #TAB# #TAB# for character in alphabet: #LINE# #TAB# #TAB# #TAB# font.getsize(character) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
"Uses the extended Euclidean algorithm to find a modular quotient  <code> def divide_mod(numerator, denominator, modulo): ","#LINE# #TAB# _, solution = _dM(numerator, denominator, modulo) #LINE# #TAB# return solution % modulo"
Utility function to determine whether the supplied href attribute is an email link  <code> def is_email_link(href=None): ,#LINE# #TAB# print('email_link()') #LINE# #TAB# return href and 'mailto:' in href
Build reverse complement of ' s '  <code> def reverse_complement(s): ,"#LINE# #TAB# s = s.upper() #LINE# #TAB# assert is_DNA(s), 'Your sequence must be DNA!' #LINE# #TAB# r = reverse(s) #LINE# #TAB# rc = complement(r) #LINE# #TAB# return rc"
Return whether a string has open quotes  <code> def unmatched_quotes_in_line(text): ,"#LINE# #TAB# text = text.replace(""\\'"", """") #LINE# #TAB# text = text.replace('\\""', '') #LINE# #TAB# if text.count('""') % 2: #LINE# #TAB# #TAB# return '""' #LINE# #TAB# elif text.count(""'"") % 2: #LINE# #TAB# #TAB# return ""'"" #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
"Creates a list of generators for PYPOWER  <code> def make_gen_list(generators, bus_ids): ","#LINE# #TAB# gen_list = np.zeros((len(generators), 21), dtype=np.float64) #LINE# #TAB# for i, gen in enumerate(generators): #LINE# #TAB# #TAB# _fill_gen_array(gen_list[i], gen, bus_ids) #LINE# #TAB# return gen_list"
"Yields line number ranges and top - level elements of the syntax tree for a CMakeLists file , given a generator of tokens from the file . toks must really be a generator , not a list , for this to work  <code> def parse_file(toks): ","#LINE# #TAB# prev_type = 'newline' #LINE# #TAB# for line_num, (typ, tok_contents) in toks: #LINE# #TAB# #TAB# if typ == 'comment': #LINE# #TAB# #TAB# #TAB# yield [line_num], Comment(tok_contents) #LINE# #TAB# #TAB# elif typ == 'newline' and prev_type == 'newline': #LINE# #TAB# #TAB# #TAB# yield [line_num], BlankLine() #LINE# #TAB# #TAB# elif typ == 'word': #LINE# #TAB# #TAB# #TAB# line_nums, cmd = parse_command(line_num, tok_contents, toks) #LINE# #TAB# #TAB# #TAB# yield line_nums, cmd #LINE# #TAB# #TAB# prev_type = typ"
Set hardware clock to value of software clock  <code> def swclock_to_hwclock(): ,"#LINE# #TAB# res = __salt__['cmd.run_all'](['hwclock', '--systohc'], python_shell=False) #LINE# #TAB# if res['retcode'] != 0: #LINE# #TAB# #TAB# msg = 'hwclock failed to set hardware clock from software clock: {0}'.format(res['stderr']) #LINE# #TAB# #TAB# raise CommandExecutionError(msg) #LINE# #TAB# return True"
Returns the filter of the file or None if no file matches  <code> def get_filter_for_char(char): ,#LINE# #TAB# if char == 'f': #LINE# #TAB# #TAB# char = '-' #LINE# #TAB# for tp in _filemode_table[0]: #LINE# #TAB# #TAB# if char == tp[1]: #LINE# #TAB# #TAB# #TAB# return tp[0] #LINE# #TAB# return None
"Get the common ancestor for all the members : param members : The types ( schema entities ) in this union : return : The common ancestor , if any <code> def get_common_ancestor(members): ","#LINE# #TAB# ancestries = [] #LINE# #TAB# for member in members: #LINE# #TAB# #TAB# ancestries.append(GraphQLSchemaTypes._get_ancestry_of(member)) #LINE# #TAB# min_length = min([len(ancestry) for ancestry in ancestries]) #LINE# #TAB# result = None #LINE# #TAB# for i in range(min_length): #LINE# #TAB# #TAB# first = ancestries[0][i] #LINE# #TAB# #TAB# for j in range(1, len(ancestries)): #LINE# #TAB# #TAB# #TAB# if ancestries[j][i] != first: #LINE# #TAB# #TAB# #TAB# #TAB# return result #LINE# #TAB# #TAB# result = first #LINE# #TAB# return result"
"Decide how to set strict mode . If a value was provided -- always use it . Otherwise decide based on the existence of config_file  <code> def strict_mode_strategy(strict, config_file): ",#LINE# #TAB# if strict is not None: #LINE# #TAB# #TAB# return strict #LINE# #TAB# return config_file is not None
Generator for analyzing all the muon events Parameters ---------- source : ctapipe.io . EventSource input event source Returns ------- analyzed_muon : container A ctapipe event container ( MuonParameter ) with muon information <code> def analyze_muon_source(source): ,#LINE# #TAB# log.info(f'[FUNCTION] {__name__}') #LINE# #TAB# if geom_dict is None: #LINE# #TAB# #TAB# geom_dict = {} #LINE# #TAB# numev = 0 #LINE# #TAB# for event in source: #LINE# #TAB# #TAB# numev += 1 #LINE# #TAB# #TAB# analyzed_muon = analyze_muon_event(event) #LINE# #TAB# #TAB# yield analyzed_muon
Return an auto incremented value for the given counter <code> def auto_inc(counter='app'): ,"#LINE# #TAB# doc = mongoframes.Frame.get_db().Counter.find_one_and_update({'_id': #LINE# #TAB# #TAB# counter}, update={'$inc': {'count': 1}, '$set': {'_id': counter}}, #LINE# #TAB# #TAB# upsert=True, return_document=ReturnDocument.AFTER) #LINE# #TAB# return doc['count']"
returns the base class of module that has the same name as module <code> def get_base_module_class(module): ,#LINE# #TAB# base_module_class_name = get_class_name_from_module_name(module.name) #LINE# #TAB# for base_module_class in type(module).__mro__: #LINE# #TAB# #TAB# if base_module_class.__name__ == base_module_class_name: #LINE# #TAB# #TAB# #TAB# return base_module_class
"Converts a string to its lexicographical order  <code> def str_to_ord(content, weights): ","#LINE# ordinal = 0 #LINE# for i, c in enumerate(content): #LINE# #TAB# ordinal += weights[i] * _ALPHABET.index(c) + 1 #LINE# return ordinal"
"Return Ma , where M is a 3x3 transformation matrix , for each pixel <code> def threeddot_simple(M, a): ","#LINE# #TAB# result = np.empty(a.shape, dtype=a.dtype) #LINE# #TAB# for i in range(a.shape[0]): #LINE# #TAB# #TAB# for j in range(a.shape[1]): #LINE# #TAB# #TAB# #TAB# A = np.array([a[i, j, 0], a[i, j, 1], a[i, j, 2]]).reshape((3, 1)) #LINE# #TAB# #TAB# #TAB# L = np.dot(M, A) #LINE# #TAB# #TAB# #TAB# result[i, j, 0] = L[0] #LINE# #TAB# #TAB# #TAB# result[i, j, 1] = L[1] #LINE# #TAB# #TAB# #TAB# result[i, j, 2] = L[2] #LINE# #TAB# return result"
Group objects by their models Args : objects ( : obj:`list ` of : obj:`Model ` ) : list of model objects Returns : : obj:`dict ` : dictionary with object grouped by their class <code> def group_objects_by_model(objects): ,#LINE# #TAB# grouped_objects = {} #LINE# #TAB# for obj in objects: #LINE# #TAB# #TAB# if obj.__class__ not in grouped_objects: #LINE# #TAB# #TAB# #TAB# grouped_objects[obj.__class__] = [] #LINE# #TAB# #TAB# if obj not in grouped_objects[obj.__class__]: #LINE# #TAB# #TAB# #TAB# grouped_objects[obj.__class__].append(obj) #LINE# #TAB# return grouped_objects
A predicate which matches a type string with an array dimension list  <code> def has_arrlist(type_str): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# abi_type = grammar.parse(type_str) #LINE# #TAB# except exceptions.ParseError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return abi_type.arrlist is not None
"Plot each particle 's mass versus size  <code> def mass_size(f, ax=None): ","#LINE# #TAB# ax.plot(f['mass'], f['size'], 'ko', alpha=0.1) #LINE# #TAB# ax.set_xlabel('mass') #LINE# #TAB# ax.set_ylabel('size') #LINE# #TAB# return ax"
"Return image scaled to scaled_size . scaled_size should be a sequence with the same length as image  <code> def scale_multidimensional(image, scaled_size): ","#LINE# #TAB# slices = [slice(0, x - 1, y * 1.0j) for x, y in zip(image.shape, #LINE# #TAB# #TAB# scaled_size)] #LINE# #TAB# coords = [numpy.rint(x).astype(numpy.int) for x in numpy.ogrid[slices]] #LINE# #TAB# return image[coords]"
"Returns the time of the latest notification . : returns : Returns a dictionary of key / value pairs . But there is only one key , ' last_viewed_at ' <code> def get_latest_notification(): ",#LINE# #TAB# url = urls.notifications(True) #LINE# #TAB# data = helper.request_get(url) #LINE# #TAB# return data
"Simple ROI indicator . : param dataframe : : param decimals : : return : <code> def return_on_investment(dataframe, decimals=2) ->DataFrame: ","#LINE# #TAB# close = np.array(dataframe['close']) #LINE# #TAB# buy = np.array(dataframe['buy']) #LINE# #TAB# buy_idx = np.where(buy == 1)[0] #LINE# #TAB# roi = np.zeros(len(close)) #LINE# #TAB# if len(buy_idx) > 0: #LINE# #TAB# #TAB# buy_chunks = np.split(close, buy_idx)[1:] #LINE# #TAB# #TAB# for idx, chunk in zip(buy_idx, buy_chunks): #LINE# #TAB# #TAB# #TAB# chunk_roi = np.round(100.0 * (chunk / chunk[0] - 1.0), decimals) #LINE# #TAB# #TAB# #TAB# roi[idx:idx + len(chunk)] = chunk_roi #LINE# #TAB# dataframe['roi'] = roi #LINE# #TAB# return dataframe"
"Returns 1 if there are symbols between the entities , 0 if not  <code> def symbols_in_between(datapoint): ","#LINE# #TAB# i, j = in_between_offsets(datapoint) #LINE# #TAB# tokens = datapoint.segment.tokens[i:j] #LINE# #TAB# for tkn in tokens: #LINE# #TAB# #TAB# if punct_set.intersection(tkn): #LINE# #TAB# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
Check if the tx hash is present and matches the terms . : param tx_message : the transaction message : return : whether the transaction hash is valid <code> def is_valid_message(tx_message: TransactionMessage) ->bool: ,"#LINE# #TAB# tx_hash = tx_message.signing_payload.get('tx_hash') #LINE# #TAB# is_valid = isinstance(tx_hash, bytes) #LINE# #TAB# return is_valid"
"Parse the query param looking for sparse fields params <code> def parse_param(key, val): ","#LINE# #TAB# regex = re.compile(r'fields\[([A-Za-z]+)\]') #LINE# #TAB# match = regex.match(key) #LINE# #TAB# if match: #LINE# #TAB# #TAB# if not isinstance(val, list): #LINE# #TAB# #TAB# #TAB# val = val.split(',') #LINE# #TAB# #TAB# fields = [field.lower() for field in val] #LINE# #TAB# #TAB# rtype = match.groups()[0].lower() #LINE# #TAB# #TAB# return rtype, fields"
Returns the list of custom color definitions for the TikZ file  <code> def get_color_definitions(data): ,"#LINE# #TAB# definitions = [] #LINE# #TAB# ff = data['float format'] #LINE# #TAB# for name, rgb in data['custom colors'].items(): #LINE# #TAB# #TAB# definitions.append( #LINE# #TAB# #TAB# #TAB# f'\\definecolor{{{name}}}{{rgb}}{{{rgb[0]:{ff}},{rgb[1]:{ff}},{rgb[2]:{ff}}}}' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return definitions"
"Generates raw secrets by re - scanning the line with the specified plugin <code> def raw_secret_generator(plugin, secret_line, filetype): ","#LINE# #TAB# for raw_secret in plugin.secret_generator(secret_line, filetype=filetype): #LINE# #TAB# #TAB# yield raw_secret #LINE# #TAB# if issubclass(plugin.__class__, HighEntropyStringsPlugin): #LINE# #TAB# #TAB# with plugin.non_quoted_string_regex(strict=False): #LINE# #TAB# #TAB# #TAB# for raw_secret in plugin.secret_generator(secret_line): #LINE# #TAB# #TAB# #TAB# #TAB# yield raw_secret"
Get user docker configuration <code> def get_config(): ,#LINE# #TAB# cfg = os.path.expanduser('~/.dockercfg') #LINE# #TAB# try: #LINE# #TAB# #TAB# fic = open(cfg) #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# config = json.loads(fic.read()) #LINE# #TAB# #TAB# finally: #LINE# #TAB# #TAB# #TAB# fic.close() #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# config = {'rootPath': '/dev/null'} #LINE# #TAB# if not 'Configs' in config: #LINE# #TAB# #TAB# config['Configs'] = {} #LINE# #TAB# return config
return output of 822 - date command <code> def get_date_822(): ,"#LINE# #TAB# cmd = '/bin/date' #LINE# #TAB# if not os.path.exists(cmd): #LINE# #TAB# #TAB# raise ValueError('%s command does not exist.'%cmd) #LINE# #TAB# args = [cmd,'-R'] #LINE# #TAB# result = get_cmd_stdout(args).strip() #LINE# #TAB# result = normstr(result) #LINE# #TAB# return result"
Build swagger spec section for pagination <code> def get_swagger_pagination_def(resultsPerPage): ,"#LINE# #TAB# return {'name': 'page', 'type': 'int', 'in': 'query', 'description': #LINE# #TAB# #TAB# 'The page number for this paginated query ({} results per page)'. #LINE# #TAB# #TAB# format(resultsPerPage)}"
"Sorting function by default <code> def sbst_simple_comparison(v1, v2): ",#LINE# #TAB# if v1 == v2: #LINE# #TAB# #TAB# return 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -1 if v1 < v2 else 1
Parses a scale expression and returns the scale and a list of ranges  <code> def parse_scale(scale_exp): ,"#LINE# #TAB# m = re.search(""(\w+?)\{(.*?)\}"", scale_exp) #LINE# #TAB# if m is None: #LINE# #TAB# #TAB# raise InvalidFormat('Unable to parse the given time period.') #LINE# #TAB# scale = m.group(1) #LINE# #TAB# range = m.group(2) #LINE# #TAB# if scale not in SCALES: #LINE# #TAB# #TAB# raise InvalidFormat('%s is not a valid scale.' % scale) #LINE# #TAB# ranges = re.split(""\s"", range) #LINE# #TAB# return scale, ranges"
Close this thread 's table <code> def table_close(table_name): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# wrapper = _thread_local.wrapper #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# pass #LINE# #TAB# else: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# wrapper.close() #LINE# #TAB# #TAB# except Exception: #LINE# #TAB# #TAB# #TAB# log.exception('Error closing table') #LINE# #TAB# #TAB# del _thread_local.wrapper #LINE# #TAB# return True
"A helper function to look up an object by key <code> def find_by_key(cls, groupkey, key, raises=False): ","#LINE# #TAB# ob = None #LINE# #TAB# try: #LINE# #TAB# #TAB# ob = keyedcache.cache_get(groupkey, key) #LINE# #TAB# except keyedcache.NotCachedError as e: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# ob = cls.objects.get(key__exact=key) #LINE# #TAB# #TAB# #TAB# keyedcache.cache_set(e.key, value=ob) #LINE# #TAB# #TAB# except cls.DoesNotExist: #LINE# #TAB# #TAB# #TAB# log.debug('No such %s: %s', groupkey, key) #LINE# #TAB# #TAB# #TAB# if raises: #LINE# #TAB# #TAB# #TAB# #TAB# raise #LINE# #TAB# return ob"
Teager Kaiser Energy Parameters ---------- ts : array of size [ 1 x samples ] Returns ------- <code> def teager_kaiser_energy(ts): ,"#LINE# #TAB# ts = ts.ravel() #LINE# #TAB# l = len(ts) #LINE# #TAB# ts = np.hstack([0.0, ts, 0.0]) #LINE# #TAB# new_ts = np.zeros(len(ts)) #LINE# #TAB# for i in range(1, l): #LINE# #TAB# #TAB# new_ts[i] = np.power(ts[i + 1], 2.0) - ts[i + 1] * ts[i - 1] #LINE# #TAB# return ts"
"Eagerly strip comments , because the serializer ca n't output them right now anyway , and they trigger some funky errors  <code> def strip_comments(lines): ","#LINE# #TAB# output = [] #LINE# #TAB# inComment = False #LINE# #TAB# wholeCommentLines = 0 #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# text, inComment = strip_commentsFromLine(line.text, inComment) #LINE# #TAB# #TAB# if line.text != text and text.strip() == '' or line.text.strip( #LINE# #TAB# #TAB# #TAB# ) == '' and inComment: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# if line.text.endswith('\n') and not text.endswith('\n'): #LINE# #TAB# #TAB# #TAB# #TAB# text += '\n' #LINE# #TAB# #TAB# #TAB# line.text = text #LINE# #TAB# #TAB# #TAB# output.append(line) #LINE# #TAB# return output"
"Get rendered file from template file path  <code> def get_rendered_file(file_path_template, template_kwargs): ","#LINE# #TAB# assert os.path.isfile(file_path_template #LINE# #TAB# #TAB# ), 'Template file {file_path_template} must be a FILE but is not.'.format( #LINE# #TAB# #TAB# file_path_template=file_path_template) #LINE# #TAB# rendered_file = render_template(file_path_template, **template_kwargs) #LINE# #TAB# return rendered_file"
"Get package file path , which is "" package_dir / relative_path ""  <code> def get_file_path_package(package_dir, relative_path, replace_dict=None): ","#LINE# #TAB# replace_dict = replace_dict if replace_dict is not None else {} #LINE# #TAB# for replace_key, replace_val in replace_dict.items(): #LINE# #TAB# #TAB# relative_path = relative_path.replace(replace_key, replace_val) #LINE# #TAB# file_path_package = os.path.abspath(os.path.join(package_dir, #LINE# #TAB# #TAB# relative_path)) #LINE# #TAB# return file_path_package"
"Takes a native tweet ( dict ) . Returns list of all mentioned users in tuple form ( user id_str , user screen name ) , or empty list if none  <code> def get_users_mentioned(tweet): ","#LINE# #TAB# if not contains_mention(tweet): #LINE# #TAB# #TAB# return [] #LINE# #TAB# users = [] #LINE# #TAB# for m in tweet['entities']['user_mentions']: #LINE# #TAB# #TAB# users.append((m['id_str'], m['screen_name'])) #LINE# #TAB# return users"
Calls each element of sequence to invoke the side effect  <code> def call_each(seq): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# reduce(lambda _, y: y(), seq) #LINE# #TAB# except TypeError as e: #LINE# #TAB# #TAB# if text_type(e) != ""reduce() of empty sequence with no initial value"": #LINE# #TAB# #TAB# #TAB# raise"
Convert a vtkImageData to numpy array . .. seealso : : : meth:`vtkimagedata_from_array ` for the reverse  <code> def vtkimagedata_to_array(image_data): ,"#LINE# #TAB# points = vtk_to_numpy(image_data.GetPointData().GetScalars()) #LINE# #TAB# shape = image_data.GetDimensions() #LINE# #TAB# shape = shape[1], shape[0], image_data.GetNumberOfScalarComponents() #LINE# #TAB# return points.reshape(shape)[::-1]"
Unpickle a pstats . Stats object <code> def unpickle_stats(stats): ,#LINE# #TAB# stats = cPickle.loads(stats) #LINE# #TAB# stats.stream = True #LINE# #TAB# return stats
"Given a list of directories , return a list of files to watch for modification and subsequent server reload  <code> def build_reload_files_list(extra_dirs): ","#LINE# #TAB# extra_files = extra_dirs[:] #LINE# #TAB# for extra_dir in extra_dirs: #LINE# #TAB# #TAB# for dirname, dirs, files in os.walk(extra_dir): #LINE# #TAB# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# #TAB# filename = os.path.join(dirname, filename) #LINE# #TAB# #TAB# #TAB# #TAB# if os.path.isfile(filename): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# extra_files.append(filename) #LINE# #TAB# return extra_files"
""" c - a : lithium nitrate [ PM73 ]  <code> def bc_li_no3_pm73(T, P): ","#LINE# #TAB# b0 = 0.142 #LINE# #TAB# b1 = 0.278 #LINE# #TAB# b2 = 0 #LINE# #TAB# Cphi = -0.00551 #LINE# #TAB# C0 = Cphi / (2 * sqrt(np_abs(i2c['Li'] * i2c['NO3']))) #LINE# #TAB# C1 = 0 #LINE# #TAB# alph1 = 2 #LINE# #TAB# alph2 = -9 #LINE# #TAB# omega = -9 #LINE# #TAB# valid = T == 298.15 #LINE# #TAB# return b0, b1, b2, C0, C1, alph1, alph2, omega, valid"
Guess the location of home directories on a system <code> def get_homedir_location(): ,"#LINE# #TAB# location = '/home' #LINE# #TAB# with settings(hide('warnings', 'stdout', 'stderr'), warn_only=True): #LINE# #TAB# #TAB# if test('-d /Users'): #LINE# #TAB# #TAB# #TAB# location = '/Users' #LINE# #TAB# #TAB# elif test('-d /export/home'): #LINE# #TAB# #TAB# #TAB# location = '/export/home' #LINE# #TAB# return location"
"Get the primitive real space vectors for the unit cell and lattice type . Note that the resulting matrix will need to be scaled by a factor equal to the wavelength in Angstroms  <code> def get_real_space_primitive_matrix(lattice, matrix, wd=None): ","#LINE# #TAB# cell, a, u = parse_matrix(matrix) #LINE# #TAB# o = _Othercell() #LINE# #TAB# if wd: #LINE# #TAB# #TAB# o.set_working_directory(wd) #LINE# #TAB# #TAB# auto_logfiler(o) #LINE# #TAB# o.set_cell(cell) #LINE# #TAB# o.set_lattice(lattice) #LINE# #TAB# o.generate() #LINE# #TAB# op = symop_to_mat(o.get_reindex_op('aP')) #LINE# #TAB# primitive_a = matmul(invert(op), a) #LINE# #TAB# real_a = invert(primitive_a) #LINE# #TAB# return real_a[0:3], real_a[3:6], real_a[6:9]"
"Remove rows with a localization probability below 0 . 75 <code> def filter_localization_probability(df, threshold=0.75): ","#LINE# #TAB# df = df.copy() #LINE# #TAB# localization_probability_mask = df['Localization prob'].values >= threshold #LINE# #TAB# return df.iloc[localization_probability_mask, :]"
"The bi - square weight function calculated over values of xx <code> def bi_square(xx, idx=None): ",#LINE# #TAB# ans = np.zeros(xx.shape) #LINE# #TAB# ans[idx] = (1-xx[idx]**2)**2 #LINE# #TAB# return ans
Grab the data stored in a .json file <code> def grab_data_from_json_file(path): ,"#LINE# #TAB# parameters, samples = Read._grab_params_and_samples_from_json_file(path) #LINE# #TAB# injection = {i: float('nan') for i in parameters} #LINE# #TAB# return {'parameters': parameters, 'samples': samples, 'injection': #LINE# #TAB# #TAB# injection}"
"Returns the lowered method . Capitalize headers prepend HTTP_ and change - to _  <code> def pre_process_method_headers(method, headers): ","#LINE# #TAB# method = method.lower() #LINE# #TAB# _wsgi_headers = [""content_length"", ""content_type"", ""query_string"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ""remote_addr"", ""remote_host"", ""remote_user"", #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# ""request_method"", ""server_name"", ""server_port""] #LINE# #TAB# _transformed_headers = {} #LINE# #TAB# for header, value in headers.items(): #LINE# #TAB# #TAB# header = header.replace(""-"", ""_"") #LINE# #TAB# #TAB# header = ""http_{header}"".format( #LINE# #TAB# #TAB# #TAB# header=header) if header.lower() not in _wsgi_headers else header #LINE# #TAB# #TAB# _transformed_headers.update({header.upper(): value}) #LINE# #TAB# return method, _transformed_headers"
"Check whether a value is a positive integer <code> def check_value(value, length=1): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# value = int(value) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError(""invalid integer value: '{}'"". #LINE# #TAB# #TAB# #TAB# format(value)) #LINE# #TAB# try: #LINE# #TAB# #TAB# assert value >= length #LINE# #TAB# except AssertionError as err: #LINE# #TAB# #TAB# raise argparse.ArgumentTypeError('invalid positive integer value: {}' #LINE# #TAB# #TAB# #TAB# .format(value)) #LINE# #TAB# return value"
"Redirect stdout to the given file descriptor . If not file descriptor is given , creates a StringIO ( )  <code> def redirect_stdout(fd=None): ",#LINE# #TAB# fd = io.StringIO() if fd is None else fd #LINE# #TAB# with contextlib.redirect_stdout(fd): #LINE# #TAB# #TAB# yield fd
"Returns whether two NetworkX graphs ( directed or undirected ) are equal  <code> def graphs_equal(graph1, graph2): ","#LINE# #TAB# if not graph1.order() == graph2.order() or not graph1.size( #LINE# #TAB# #TAB# ) == graph2.size(): #LINE# #TAB# #TAB# return False #LINE# #TAB# for u, v in graph1.edges(): #LINE# #TAB# #TAB# if not graph2.has_edge(u, v): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
Routine providing a geometry for testing Returns ------- poly_spitzbergen : OGRGeometry 4-corner polygon over high latitudes ( is much curved on the globe ) <code> def setup_test_geom_spitzbergen(): ,"#LINE# #TAB# osr_spref = get_geog_spatial_ref() #LINE# #TAB# points = [(8.391827331539572, 77.35762113396143), (16.87007957357446, #LINE# #TAB# #TAB# 81.59290885863483), (40.5011949830408, 79.73786853853339), ( #LINE# #TAB# #TAB# 25.43098663332705, 75.61353436967198)] #LINE# #TAB# poly_spitzbergen = create_polygon_geometry(points, osr_spref, segment=None) #LINE# #TAB# return poly_spitzbergen"
"Remove ~/.onecodex file , returning True if successul or False if the file did n't exist  <code> def remove_creds(creds_file=None): ","#LINE# #TAB# if creds_file is None: #LINE# #TAB# #TAB# creds_file = os.path.expanduser('~/.onecodex') #LINE# #TAB# try: #LINE# #TAB# #TAB# os.remove(creds_file) #LINE# #TAB# except Exception as e: #LINE# #TAB# #TAB# if e.errno == errno.ENOENT: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# #TAB# elif e.errno == errno.EACCES: #LINE# #TAB# #TAB# #TAB# click.echo('Please check the permissions on {}'.format( #LINE# #TAB# #TAB# #TAB# #TAB# collapse_user(creds_file)), err=True) #LINE# #TAB# #TAB# #TAB# sys.exit(1) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise #LINE# #TAB# return True"
"assert helper that quickly validates default value . designed to get out of the way and reduce overhead when asserts are stripped  <code> def validate_default_value(handler, default, norm, param='value'): ","#LINE# #TAB# assert default is not None, '%s lacks default %s' % (handler.name, param) #LINE# #TAB# assert norm(default) == default, '%s: invalid default %s: %r' % (handler #LINE# #TAB# #TAB# .name, param, default) #LINE# #TAB# return True"
get a random exposure time Returns : - ` ` randomExptime ` ` -- a random exptime from a set of discrete values <code> def random_exptime(): ,"#LINE# #TAB# randomNum = random.randint(1, 7) #LINE# #TAB# if randomNum == 1: #LINE# #TAB# #TAB# randomExptime = '200' #LINE# #TAB# if randomNum == 2: #LINE# #TAB# #TAB# randomExptime = '100' #LINE# #TAB# if randomNum == 3: #LINE# #TAB# #TAB# randomExptime = '50' #LINE# #TAB# if randomNum == 4: #LINE# #TAB# #TAB# randomExptime = '250' #LINE# #TAB# if randomNum == 5: #LINE# #TAB# #TAB# randomExptime = '360' #LINE# #TAB# if randomNum == 6: #LINE# #TAB# #TAB# randomExptime = '720' #LINE# #TAB# if randomNum == 7: #LINE# #TAB# #TAB# randomExptime = '240' #LINE# #TAB# return randomExptime"
Clean regexes with groups occuring multiple times <code> def clean_regex(regex): ,"#LINE# #TAB# args = re.findall('\\<([^\\)]+)\\>', regex) #LINE# #TAB# for arg in set(args): #LINE# #TAB# #TAB# new_regex = '' #LINE# #TAB# #TAB# for counter, chunk in enumerate(regex.split('<' + arg + '>')): #LINE# #TAB# #TAB# #TAB# if counter == 0: #LINE# #TAB# #TAB# #TAB# #TAB# new_regex += chunk #LINE# #TAB# #TAB# #TAB# elif counter == 1: #LINE# #TAB# #TAB# #TAB# #TAB# new_regex += '<' + arg + '>' + chunk #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# new_regex += '<{}_occurence_{}>{}'.format(arg, counter, chunk) #LINE# #TAB# #TAB# regex = new_regex #LINE# #TAB# return regex"
"Translates cardinal points to xy points in the form of bounds . Useful for converting to the format required for WFS from REST style queries  <code> def nsew_2_bounds(cardinal, order=['minx', 'miny', 'maxx', 'maxy']): ","#LINE# #TAB# tnsltr = {xy: c for xy, c in zip(['minx', 'miny', 'maxx', 'maxy'], [ #LINE# #TAB# #TAB# 'west', 'south', 'east', 'north'])} #LINE# #TAB# bnds = [cardinal.get(tnsltr[o]) for o in order] #LINE# #TAB# return bnds"
Add python types decoding . See JsonEncoder <code> def date_decoder(dic): ,"#LINE# #TAB# if '__date__' in dic: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# d = datetime.date(**{c: v for c, v in dic.items() if not c == ""__date__""}) #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# raise json.JSONDecodeError(""Corrupted date format !"", str(dic), 1) #LINE# #TAB# elif '__datetime__' in dic: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# d = datetime.datetime(**{c: v for c, v in dic.items() if not c == ""__datetime__""}) #LINE# #TAB# #TAB# except (TypeError, ValueError): #LINE# #TAB# #TAB# #TAB# raise json.JSONDecodeError(""Corrupted datetime format !"", str(dic), 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return dic #LINE# #TAB# return d"
"Add a card , return the i d of card  <code> def add_card(card: Card) ->int: ","#LINE# #TAB# card.id = _db.insert(card.to_dict()) #LINE# #TAB# _db.update(card.to_dict(), doc_ids=[card.id]) #LINE# #TAB# return card.id"
Create a dictionary with the priority information of the exception generated : param exception : : return : <code> def format_exception(exception): ,"#LINE# #TAB# if not exception: #LINE# #TAB# #TAB# return None #LINE# #TAB# type_ = type(exception).__name__ #LINE# #TAB# traceback_ = traceback.format_exc() #LINE# #TAB# return {'type': type_, 'traceback': traceback_}"
"Return new stat file object , record as active stat object <code> def init_statfileobj(): ","#LINE# #TAB# global _active_statfileobj #LINE# #TAB# assert not _active_statfileobj, _active_statfileobj #LINE# #TAB# _active_statfileobj = StatFileObj() #LINE# #TAB# return _active_statfileobj"
"Add a new service <code> def new_service(name, restart=True): ","#LINE# #TAB# out = __mgmt(name, 'service', 'new') #LINE# #TAB# if restart: #LINE# #TAB# #TAB# if out == 'success': #LINE# #TAB# #TAB# #TAB# return __firewall_cmd('--reload') #LINE# #TAB# return out"
gets the path to the default config - file <code> def get_default_config_file_path(init_filename=None): ,"#LINE# #TAB# prm_dir = get_package_prm_dir() #LINE# #TAB# if not init_filename: #LINE# #TAB# #TAB# init_filename = DEFAULT_FILENAME #LINE# #TAB# src = os.path.join(prm_dir, init_filename) #LINE# #TAB# return src"
Finds the lowest scale where x < = scale  <code> def get_scale(x): ,"#LINE# #TAB# scales = [20, 50, 100, 200, 400, 600, 800, 1000] #LINE# #TAB# for scale in scales: #LINE# #TAB# #TAB# if x <= scale: #LINE# #TAB# #TAB# #TAB# return scale #LINE# #TAB# return x"
Return a new worker stats counter instance . : rtype : dict <code> def worker_stats_counter(): ,"#LINE# #TAB# return {worker.Process.BATCHES: 0, worker.Process.ERROR: 0, worker. #LINE# #TAB# #TAB# Process.PROCESSED: 0, worker.Process.PENDING: 0}"
"Generate google news for a given location . Parameters ---------- city : string city name state : string state name <code> def get_location_googlenews(city, state): ","#LINE# #TAB# url = get_location_url(city, state) #LINE# #TAB# r = requests.get(url) #LINE# #TAB# if r.status_code != 200: #LINE# #TAB# #TAB# raise RuntimeError('requests status_code={}'.format(status_code)) #LINE# #TAB# d = feedparser.parse(r.text) #LINE# #TAB# result = [_gen_item(item) for item in d.entries] #LINE# #TAB# return result"
"Check if a custom page path is valid or not , to prevent malicious requests . : param path : custom page path ( url path ) : return : valid or not <code> def validate_custom_page_path(path): ",#LINE# #TAB# sp = path.split('/') #LINE# #TAB# if '.' in sp or '..' in sp: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
if the filter statement ( after where ) is fully specified eg ` where FOR = ' 123 ' ` or ` where doi!='123 ' ` <code> def line_filter_is_complete(line): ,#LINE# #TAB# l = line.split('where') #LINE# #TAB# if len(l) > 1 and l[-1].strip(): #LINE# #TAB# #TAB# if 'is empty' in l[-1] or 'is not empty' in l[-1]: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# for x in G.lang_filter_operators(): #LINE# #TAB# #TAB# #TAB# #TAB# if x in l[-1]: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# after_filter = l[-1].split(x) #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# if len(after_filter) > 1 and after_filter[-1].strip(): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# return True
"This function is used to train model according to Ordinary Least Squares(linear model )  <code> def ols_train(family, prop): ","#LINE# #TAB# train, test = df_prediction(family, prop) #LINE# #TAB# OLS = linear_model.LinearRegression() #LINE# #TAB# train_X = train[train.columns[4:]] #LINE# #TAB# OLS.fit(train_X, train[prop]) #LINE# #TAB# return OLS"
Get the default group type for migrating cgsnapshots . Get the default group type for migrating consistencygroups to groups and cgsnapshots to group_snapshots  <code> def get_default_cgsnapshot_type(): ,"#LINE# #TAB# grp_type = {} #LINE# #TAB# ctxt = context.get_admin_context() #LINE# #TAB# try: #LINE# #TAB# #TAB# grp_type = get_group_type_by_name(ctxt, DEFAULT_CGSNAPSHOT_TYPE) #LINE# #TAB# except exception.GroupTypeNotFoundByName: #LINE# #TAB# #TAB# LOG.exception('Default cgsnapshot type %s is not found.', #LINE# #TAB# #TAB# #TAB# DEFAULT_CGSNAPSHOT_TYPE) #LINE# #TAB# return grp_type"
The NCX file parsing <code> def parser_toc(obj): ,"#LINE# #TAB# result = [] #LINE# #TAB# nav_points = obj.content.getElementsByTagName('navPoint') #LINE# #TAB# for np in nav_points: #LINE# #TAB# #TAB# order = np.getAttribute('playOrder') #LINE# #TAB# #TAB# chapter = np.getElementsByTagName('navLabel').item(0 #LINE# #TAB# #TAB# #TAB# ).getElementsByTagName('text').item(0).firstChild.nodeValue #LINE# #TAB# #TAB# result.append({'order': order, 'chapter': chapter}) #LINE# #TAB# return result"
"Return the verify parameter . The value of verify can be either True / False or a cacert . : param cacert None or path to CA cert file : param insecure truthy value to switch on SSL verification <code> def verification_needed(cacert, insecure): ",#LINE# #TAB# if insecure is False or insecure is None: #LINE# #TAB# #TAB# verify = cacert or True #LINE# #TAB# else: #LINE# #TAB# #TAB# verify = False #LINE# #TAB# return verify
Iterator over the first letter of each word <code> def first_letters(text): ,#LINE# #TAB# for match in first_letters.pattern.finditer(text): #LINE# #TAB# #TAB# yield text[match.start()]
Decode bytes from Bitcoin base58 string and test checksum <code> def check_decode(enc): ,"#LINE# #TAB# dec = decode(enc) #LINE# #TAB# raw, chk = dec[:-4], dec[-4:] #LINE# #TAB# if chk != sha256d_hash(raw)[:4]: #LINE# #TAB# #TAB# raise ValueError('base58 decoding checksum error dec={}'.format(dec)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return raw"
Checks if the user is logged in  <code> def is_logged_in(user): ,#LINE# #TAB# if current_app.config.get('LOGIN_DISABLED' #LINE# #TAB# #TAB# ) or user.is_authenticated and user.is_active: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Function used to update KWinner modules boost strength after each epoch  <code> def update_boost_strength(m): ,"#LINE# if isinstance(m, KWinnersBase): #LINE# #TAB# if m.training: #LINE# #TAB# m.boostStrength = m.boostStrength * m.boostStrengthFactor"
"Returns the ClassInfo object of the type . If not exists , it will be created  <code> def get_classinfo(cls) ->_ClassInfo: ",#LINE# #TAB# if '_class_info' not in cls.__dict__: #LINE# #TAB# #TAB# cls._class_info = _ClassInfo() #LINE# #TAB# return cls._class_info
Calculate bit size of the nonnegative integer n <code> def gmpy_bitcount(n): ,#LINE# #TAB# if n: #LINE# #TAB# #TAB# return MPZ(n).numdigits(2) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 0
"Insert single kafka message into the internal store <code> def consume_kafka_message(message, topic, primary_keys, local_store): ","#LINE# #TAB# singer_record = kafka_message_to_singer_record(message, topic, primary_keys #LINE# #TAB# #TAB# ) #LINE# #TAB# insert_ts = local_store.insert(singer.format_message(singer. #LINE# #TAB# #TAB# RecordMessage(stream=topic, record=singer_record, time_extracted= #LINE# #TAB# #TAB# utils.now()))) #LINE# #TAB# return insert_ts"
"Given that CVS directory exists , parse and process <code> def handle_cvs(path): ","#LINE# #TAB# cvsDir = os.path.join(path, 'CVS') #LINE# #TAB# if os.path.isdir(cvsDir): #LINE# #TAB# #TAB# for file in ('Entries', 'Repository', 'Root'): #LINE# #TAB# #TAB# #TAB# if not os.path.isfile(os.path.join(cvsDir, file)): #LINE# #TAB# #TAB# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# root = handle_cvsRoot(cvsDir) #LINE# #TAB# except IOError as err: #LINE# #TAB# #TAB# log.error('IOError: %s', err) #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# entries = handle_cvsEntries(cvsDir) #LINE# #TAB# #TAB# repository = handle_cvsRepository(cvsDir) #LINE# #TAB# return root, repository, entries"
"Uniform prior distribution  <code> def uniform_prior(value, umin, umax): ",#LINE# #TAB# if umin <= value <= umax: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return -np.inf
"Create comms for communicating within a node  <code> def create_node_comm(comm, comm_dir): ","#LINE# #TAB# node_ranks = discover_path_access_ranks(comm, comm_dir) #LINE# #TAB# assert len(node_ranks) >= 1 #LINE# #TAB# node_root = min(node_ranks) #LINE# #TAB# node_comm = comm.Split(node_root, node_ranks.index(comm.rank)) #LINE# #TAB# return node_comm, node_root"
"Up move and down move initialize up move and down move : param df : data <code> def get_um_dm(cls, df): ",#LINE# #TAB# hd = df['high_delta'] #LINE# #TAB# df['um'] = (hd + hd.abs()) / 2 #LINE# #TAB# ld = -df['low_delta'] #LINE# #TAB# df['dm'] = (ld + ld.abs()) / 2
"Compute the lenght of a list of values encoded using an optimal code Parameters ---------- x : array - like , shape ( n_samples ) The values to be encoded . Returns ------- Return the length of the encoded dataset ( float ) <code> def optimal_code_length(x): ","#LINE# #TAB# new_x = discretize(x) #LINE# #TAB# unique, count = np.unique(new_x, return_counts=True) #LINE# #TAB# ldm = np.sum(count * -np.log2(count / len(new_x))) #LINE# #TAB# return ldm"
Parse directive from doc <code> def parse_directive(doc): ,"#LINE# #TAB# desc, data = _parse_doc(doc, RE_DIRECTIVE) #LINE# #TAB# directives = OrderedDict() #LINE# #TAB# for k, v in data.items(): #LINE# #TAB# #TAB# if k.startswith('$'): #LINE# #TAB# #TAB# #TAB# directives[k[1:]] = v #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# raise ValueError('Invalid directive {}'.format(k)) #LINE# #TAB# return desc, directives"
Return the battery capacity after last full charge ( measured in mAh ) <code> def battery_charge_last(): ,"#LINE# #TAB# p = '/sys/class/power_supply/BAT0/charge_full' #LINE# #TAB# if os.path.exists(p): #LINE# #TAB# #TAB# with open(p, 'r') as f: #LINE# #TAB# #TAB# #TAB# mAh = 1000 #LINE# #TAB# #TAB# #TAB# charge_last = int(f.readline().split()[0]) / mAh #LINE# #TAB# #TAB# return charge_last #LINE# #TAB# else: #LINE# #TAB# #TAB# return NF"
"Loads an SSL context from a certificate and private key file  <code> def load_ssl_context(cert_file, pkey_file): ",#LINE# #TAB# from OpenSSL import SSL #LINE# #TAB# ctx = SSL.Context(SSL.SSLv23_METHOD) #LINE# #TAB# ctx.use_certificate_file(cert_file) #LINE# #TAB# ctx.use_privatekey_file(pkey_file) #LINE# #TAB# return ctx
"Decode a segwit address  <code> def segwit_addr_decode(hrp, addr): ","#LINE# #TAB# hrpgot, data = bech32_decode(addr) #LINE# #TAB# if hrpgot != hrp: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# decoded = convertbits(data[1:], 5, 8, False) #LINE# #TAB# if decoded is None or len(decoded) < 2 or len(decoded) > 40: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# if data[0] > 16: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# return data[0], decoded"
Returns ' true ' if the provided value is empty . Args : value ( object ) : the value Result : boolean : the result <code> def true_if_empty_filter(value): ,"#LINE# #TAB# if not isinstance(value, bool) and not value: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
Convert container exit code to signal number . : param code : container exit code : type code : int : return : signal number or None ( if * code * does not represent a signal ) : rtype : int or NoneType <code> def get_signal_number_from_exit_code(code): ,#LINE# #TAB# if code >= 128: #LINE# #TAB# #TAB# return code - 128 #LINE# #TAB# return None
Return the spatial reference of a feature path Parameters ---------- feature_path : str File path to the OGR supported feature . Returns ------- osr . SpatialReference : : class:`osr . SpatialReference ` of the input feature file path <code> def feature_path_osr(feature_path): ,#LINE# #TAB# feature_ds = ogr.Open(feature_path) #LINE# #TAB# feature_osr = feature_ds_osr(feature_ds) #LINE# #TAB# feature_ds = None #LINE# #TAB# return feature_osr
Create an iterator of tensors from node attributes of an ONNX model  <code> def get_attribute_tensors(onnx_model_proto): ,#LINE# #TAB# for node in onnx_model_proto.graph.node: #LINE# #TAB# #TAB# for attribute in node.attribute: #LINE# #TAB# #TAB# #TAB# if attribute.HasField('t'): #LINE# #TAB# #TAB# #TAB# #TAB# yield attribute.t #LINE# #TAB# #TAB# #TAB# for tensor in attribute.tensors: #LINE# #TAB# #TAB# #TAB# #TAB# yield tensor
"Remove Padding with zeroes + last byte equal to the number of padding bytes <code> def remove_zero_len_padding(str, blocksize=AES_blocksize): ","#LINE# 'Remove Padding with zeroes + last byte equal to the number of padding bytes' #LINE# try: #LINE# #TAB# pad_len = ord(str[-1]) #LINE# except TypeError: #LINE# #TAB# pad_len = str[-1] #LINE# assert pad_len < blocksize, 'padding error' #LINE# assert pad_len < len(str), 'padding error' #LINE# return str[:-pad_len]"
write some content to a tmp file <code> def write_to_tmpfile(content): ,"#LINE# #TAB# path = get_tmpfile() #LINE# #TAB# with open(path, 'wb') as f: #LINE# #TAB# #TAB# f.write(content) #LINE# #TAB# return path"
"return the list of modules in the program module <code> def get_modules(parent, subparsers, progs): ","#LINE# #TAB# mods = package_modules(parent) #LINE# #TAB# for mod in sorted(mods): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# __import__(mod) #LINE# #TAB# #TAB# #TAB# mod_name = mod.split('.')[-1] #LINE# #TAB# #TAB# #TAB# m = getattr(parent, mod_name) #LINE# #TAB# #TAB# #TAB# m.add_parser(subparsers, mod_name) #LINE# #TAB# #TAB# #TAB# progs[mod_name] = getattr(m, mod_name) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return progs"
Do a simple evaluation of the score  <code> def process_result(result): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# score = float(result[0].split('/')[0]) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# return None, None #LINE# #TAB# try: #LINE# #TAB# #TAB# delta = score - float(result[1].split('/')[0]) #LINE# #TAB# except IndexError: #LINE# #TAB# #TAB# delta = None #LINE# #TAB# return score, delta"
argument_list : argument | argument COMMA argument_list <code> def p_argument_list(p): ,"#LINE# #TAB# if len(p) == 2: #LINE# #TAB# #TAB# p[0] = p[1] #LINE# #TAB# else: #LINE# #TAB# #TAB# p[0] = p[1][0] + p[3][0], {**p[1][1], **p[3][1]}"
"Recursively apply config s variables values to its property <code> def parse_config_property(item: _T, variables: Dict[str, Union[str, Path, float, bool, None]]) -> _T: ","#LINE# #TAB# if isinstance(item, str): #LINE# #TAB# #TAB# return item.format(**variables) #LINE# #TAB# elif isinstance(item, list): #LINE# #TAB# #TAB# return [parse_config_property(item, variables) for item in item] #LINE# #TAB# elif isinstance(item, dict): #LINE# #TAB# #TAB# return {k: parse_config_property(v, variables) for k, v in item.items()} #LINE# #TAB# else: #LINE# #TAB# #TAB# return item"
"Return description from a single row . We only return the name , type ( inferred from the data ) and if the values can be NULL . String columns in Druid are NULLable . Numeric columns are NOT NULL  <code> def get_description_from_payload(payload): ","#LINE# #TAB# return [(col['label'], Type[col['type'].upper()], None, None, None, #LINE# #TAB# #TAB# None, True) for col in payload['table']['cols']]"
"Institute trend graph for machine category  <code> def get_institute_trend_graph_url(institute, start, end): ","#LINE# #TAB# filename = get_institute_trend_graph_filename(institute, start, end) #LINE# #TAB# urls = { #LINE# #TAB# #TAB# 'graph_url': urlparse.urljoin(GRAPH_URL, filename + "".png""), #LINE# #TAB# #TAB# 'data_url': urlparse.urljoin(GRAPH_URL, filename + "".csv""), #LINE# #TAB# } #LINE# #TAB# return urls"
"count the number of reads present in the file bam_file = pysam . AlignmentFile(file , "" rb "" ) Needs the bam file and bam index file to work <code> def count_bam(file): ","#LINE# #TAB# tot_records = int(pysam.view('-c', file)) #LINE# #TAB# return tot_records"
"Extract a set of fields into a new dict for indexing by resource server . Allow for these fields to be ` None ` when absent : - "" refresh_token "" - "" token_type "" <code> def convert_token_info_dict(source_dict): ","#LINE# #TAB# expires_in = source_dict.get('expires_in', 0) #LINE# #TAB# return {'scope': source_dict['scope'], 'access_token': source_dict[ #LINE# #TAB# #TAB# 'access_token'], 'refresh_token': source_dict.get('refresh_token'), #LINE# #TAB# #TAB# 'token_type': source_dict.get('token_type'), 'expires_at_seconds': #LINE# #TAB# #TAB# int(time.time() + expires_in), 'resource_server': source_dict[ #LINE# #TAB# #TAB# 'resource_server']}"
"Return the ssh_usernames . Defaults to a built - in list of users for trying  <code> def ssh_usernames(vm_, opts, default_users=None): ","#LINE# #TAB# if default_users is None: #LINE# #TAB# #TAB# default_users = ['root'] #LINE# #TAB# usernames = salt.config.get_cloud_config_value( #LINE# #TAB# #TAB# 'ssh_username', vm_, opts #LINE# #TAB# ) #LINE# #TAB# if not isinstance(usernames, list): #LINE# #TAB# #TAB# usernames = [usernames] #LINE# #TAB# usernames = [x for x in usernames if x] #LINE# #TAB# initial = usernames[:] #LINE# #TAB# for name in default_users: #LINE# #TAB# #TAB# if name not in usernames: #LINE# #TAB# #TAB# #TAB# usernames.append(name) #LINE# #TAB# usernames.extend(initial) #LINE# #TAB# return usernames"
Create a dictionary out of the YAML file received Parameters ---------- path : str Path of the YAML file  <code> def read_config(path): ,"#LINE# #TAB# with open(path, 'r') as stream: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# config = yaml.safe_load(stream) #LINE# #TAB# #TAB# except yaml.YAMLError as exc: #LINE# #TAB# #TAB# #TAB# print(exc) #LINE# #TAB# return config"
Get synthetic classification data with n_samples samples  <code> def get_data(n_samples=100): ,"#LINE# #TAB# X, y = make_classification( #LINE# #TAB# #TAB# n_samples=n_samples, #LINE# #TAB# #TAB# n_features=N_FEATURES, #LINE# #TAB# #TAB# n_classes=N_CLASSES, #LINE# #TAB# #TAB# random_state=0, #LINE# #TAB# ) #LINE# #TAB# X = X.astype(np.float32) #LINE# #TAB# return X, y"
"Returns a list of tuples ( name , i d ) of assignments which have at least one ungraded submission <code> def get_assignments_needing_grading(course_id): ","#LINE# #TAB# assigns = get_all_course_assignments(course_id) #LINE# #TAB# to_grade = [a for a in assigns if a['needs_grading_count'] > 0] #LINE# #TAB# to_grade = [(g['name'].strip(), g['id']) for g in to_grade] #LINE# #TAB# return to_grade"
"Shorthand to working with templates : param template : string template : param template_name : template filename : return : <code> def get_template(cls, template=None, template_name=None): ",#LINE# #TAB# if template: #LINE# #TAB# #TAB# return Template(template) #LINE# #TAB# elif template_name: #LINE# #TAB# #TAB# return get_template(template_name) #LINE# #TAB# return None
Checks if l is a numpy array of integers <code> def is_int_vector(l): ,"#LINE# #TAB# if isinstance(l, np.ndarray): #LINE# #TAB# #TAB# if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Converts a position vector in internal coordinates to absolute coordinates in Angstrom  <code> def rel_to_abs(vector, cell): ","#LINE# #TAB# if len(vector) == 3: #LINE# #TAB# #TAB# cell_np = np.array(cell) #LINE# #TAB# #TAB# postionR = np.array(vector) #LINE# #TAB# #TAB# new_abs_post = np.matmul(postionR, cell_np) #LINE# #TAB# #TAB# new_abs_pos = [i for i in new_abs_post] #LINE# #TAB# #TAB# return new_abs_pos #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"Compute dissimilarity between rows of ` a ` and ` b ` as : math:`||a - b||_2 ^ 2 `  <code> def l2_dissimilarity(a, b): ","#LINE# #TAB# assert len(a.shape) == len(b.shape) #LINE# #TAB# return (a - b).norm(p=2, dim=-1) ** 2"
"Get payloads from previous layer  <code> def add_payloads(prev_layer, input_spikes): ","#LINE# #TAB# payloads = tf.where(k.equal(input_spikes, 0.0), k.zeros_like( #LINE# #TAB# #TAB# input_spikes), prev_layer.payloads) #LINE# #TAB# print('Using spikes with payloads from layer {}'.format(prev_layer.name)) #LINE# #TAB# return input_spikes + payloads"
Create a device - object to manipulate a specific device with  <code> def device_create_from_str(device_str): ,"#LINE# #TAB# _LOGGER.debug('Creating device-entity for device [%s].' % device_str) #LINE# #TAB# try: #LINE# #TAB# #TAB# device = CFUNC_hdhomerun_device_create_from_str(ascii_bytes( #LINE# #TAB# #TAB# #TAB# device_str), None) #LINE# #TAB# except: #LINE# #TAB# #TAB# _LOGGER.exception('Library call to create device entity failed.') #LINE# #TAB# #TAB# raise #LINE# #TAB# if not device: #LINE# #TAB# #TAB# message = 'Could not build device entity.' #LINE# #TAB# #TAB# _LOGGER.exception(message) #LINE# #TAB# #TAB# raise Exception(message) #LINE# #TAB# return device.contents"
Return client calls for a chalice app . This is similar to ` ` get_client_calls ` ` except it will automatically traverse into chalice views with the assumption that they will be called  <code> def get_client_calls_for_app(source_code): ,#LINE# #TAB# parsed = parse_code(source_code) #LINE# #TAB# parsed.parsed_ast = AppViewTransformer().visit(parsed.parsed_ast) #LINE# #TAB# ast.fix_missing_locations(parsed.parsed_ast) #LINE# #TAB# t = SymbolTableTypeInfer(parsed) #LINE# #TAB# binder = t.bind_types() #LINE# #TAB# collector = APICallCollector(binder) #LINE# #TAB# api_calls = collector.collect_api_calls(parsed.parsed_ast) #LINE# #TAB# return api_calls
"Returns unique elements , preserving order  <code> def unique_everseen(iterable): ","#LINE# #TAB# seen = set() #LINE# #TAB# seen_add = seen.add #LINE# #TAB# iterable = filterfalse(seen.__contains__, iterable) #LINE# #TAB# for element in iterable: #LINE# #TAB# #TAB# seen_add(element) #LINE# #TAB# #TAB# yield element"
"Create an Analysis object from a directory  <code> def from_dir(cls, data_dir, prefix='result', key=None): ","#LINE# #TAB# if key is None: #LINE# #TAB# #TAB# key = os.path.basename(os.path.normpath(data_dir)) #LINE# #TAB# states = states_from_dir(data_dir, prefix=prefix) #LINE# #TAB# analysis_inst = cls(states, key=key) #LINE# #TAB# if analysis_inst.empty: #LINE# #TAB# #TAB# return None #LINE# #TAB# analysis_inst.data_dir = data_dir #LINE# #TAB# return analysis_inst"
Returns class that should be used to instance an ew dockable DCC window : return : class <code> def get_dockable_window_class(): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# from tpQtLib.widgets import window #LINE# #TAB# #TAB# return window.MainWindow #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# pass #LINE# #TAB# return None
This method will return the cumulative number of bytes occupied by the files on disk in the directory and its subdirectories  <code> def get_dir_size_recursively(dirPath): ,"#LINE# #TAB# return int(subprocess.check_output(['du', '-s', dirPath], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# env=dict(os.environ, BLOCKSIZE='512')).decode('utf-8').split()[0]) * 512"
RETURN THE FIRST STEP IN PATH ALONG WITH THE REMAINING TAIL <code> def tail_field(field): ,"#LINE# #TAB# if field == ""."" or field==None: #LINE# #TAB# #TAB# return ""."", ""."" #LINE# #TAB# elif ""."" in field: #LINE# #TAB# #TAB# if ""\\."" in field: #LINE# #TAB# #TAB# #TAB# return tuple(k.replace(""\a"", ""."") for k in field.replace(""\\."", ""\a"").split(""."", 1)) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return field.split(""."", 1) #LINE# #TAB# else: #LINE# #TAB# #TAB# return field, ""."""
Add arguments to parser for this script <code> def construct_parser(): ,"#LINE# #TAB# parser = ArgumentParser() #LINE# #TAB# script_helpers.setup_parser_help(parser, __doc__) #LINE# #TAB# file_help = 'File with list of files in which keywords are to be changed' #LINE# #TAB# parser.add_argument('--file-list', help=file_help) #LINE# #TAB# parser.add_argument('files', nargs='*', help= #LINE# #TAB# #TAB# 'Files in which to add/change keywords') #LINE# #TAB# key_group = parser.add_mutually_exclusive_group(required=True) #LINE# #TAB# key_group.add_argument('--key-file', help= #LINE# #TAB# #TAB# 'File with keywords and values to be set') #LINE# #TAB# key_group.add_argument('--key-value', nargs=2, help='Keyword to add/change' #LINE# #TAB# #TAB# ) #LINE# #TAB# return parser"
Load a config file from ` ` filepath ` `  <code> def load_config(filepath): ,"#LINE# #TAB# from snntoolbox.utils.utils import import_configparser #LINE# #TAB# configparser = import_configparser() #LINE# #TAB# assert os.path.isfile(filepath #LINE# #TAB# #TAB# ), 'Configuration file not found at {}.'.format(filepath) #LINE# #TAB# config = configparser.ConfigParser() #LINE# #TAB# config.optionxform = str #LINE# #TAB# config.read(filepath) #LINE# #TAB# return config"
Iterate through programs to collect all their labels  <code> def collect_labels(programs: Programs) ->LabelInfos: ,#LINE# #TAB# result: LabelInfos = defaultdict(list) #LINE# #TAB# for program in programs: #LINE# #TAB# #TAB# for label in program.labels: #LINE# #TAB# #TAB# #TAB# result[label.name].append(program.name) #LINE# #TAB# return result
Lowercase parmList / exemplar keys Parameters ---------- parmlist : list parmList or exemplar reflection information Returns ------- list Same list with lower - cased name keys <code> def lower_parmlist_keys(parmlist): ,#LINE# #TAB# for parm in parmlist: #LINE# #TAB# #TAB# parm['name'] = parm['name'].lower() #LINE# #TAB# #TAB# if 'parmList' in parm: #LINE# #TAB# #TAB# #TAB# parm['parmList'] = lower_parmlist_keys(parm['parmList']) #LINE# #TAB# #TAB# if 'exemplar' in parm: #LINE# #TAB# #TAB# #TAB# parm['exemplar'] = lower_parmlist_keys(parm['exemplar']) #LINE# #TAB# return parmlist
"Takes a complicated raw_material like the run command and return something simple . Example : ' java HelloWorld -n 3 ' = > ' java ' <code> def create_default_name(bundle_type, raw_material): ","#LINE# #TAB# if bundle_type == 'run': #LINE# #TAB# #TAB# raw_material = raw_material.split(' ')[0] #LINE# #TAB# name = (bundle_type + '-' if bundle_type else '' #LINE# #TAB# #TAB# ) + NOT_NAME_CHAR_REGEX.sub('-', raw_material) #LINE# #TAB# name = re.compile('\\-+').sub('-', name) #LINE# #TAB# if not re.match(BEGIN_NAME_STR, name): #LINE# #TAB# #TAB# name = '_' + name #LINE# #TAB# name = shorten_name(name) #LINE# #TAB# return name"
Returns string p_s whose non - number chars have been removed <code> def keep_digits(s): ,#LINE# #TAB# if s is None: #LINE# #TAB# #TAB# return s #LINE# #TAB# res = '' #LINE# #TAB# for c in s: #LINE# #TAB# #TAB# if c.isdigit(): #LINE# #TAB# #TAB# #TAB# res += c #LINE# #TAB# return res
"Scores the geolocation of a candidate . A distance of ` radius ` or more will result in a 0.0 score  <code> def score_geolocation(wanted: LocationReferencePoint, actual: Line, radius: ","#LINE# #TAB# float, is_last_lrp: bool) ->float: #LINE# #TAB# if is_last_lrp: #LINE# #TAB# #TAB# actual_point = actual.end_node.coordinates #LINE# #TAB# else: #LINE# #TAB# #TAB# actual_point = actual.start_node.coordinates #LINE# #TAB# dist = distance(coords(wanted), actual_point) #LINE# #TAB# if dist < radius: #LINE# #TAB# #TAB# return 1.0 - dist / radius #LINE# #TAB# return 0.0"
"take AWS tag body , parse into start , end times <code> def parse_offhours(cls, body): ","#LINE# #TAB# range_dict = cls._parse_csvbody(body) #LINE# #TAB# start = cls._parse_time(range_dict['start']) #LINE# #TAB# end = cls._parse_time(range_dict['end']) #LINE# #TAB# return start, end"
"adds the constant harmonic contribution to the energy and the heat capacity <code> def add_harmonic_contribution(input_dict, E_sampling, Cv_sampling): ",#LINE# #TAB# input_dict['E'] += E_sampling #LINE# #TAB# input_dict['Cv'] += Cv_sampling #LINE# #TAB# return
"Retrieve the full path of used specified organisation logo  <code> def organisation_logo_path(feature, parent): ","#LINE# #TAB# _ = feature, parent #LINE# #TAB# organisation_logo_file = setting( #LINE# #TAB# #TAB# inasafe_organisation_logo_path['setting_key']) #LINE# #TAB# if os.path.exists(organisation_logo_file): #LINE# #TAB# #TAB# return organisation_logo_file #LINE# #TAB# else: #LINE# #TAB# #TAB# LOGGER.info( #LINE# #TAB# #TAB# #TAB# 'The custom organisation logo is not found in {logo_path}. ' #LINE# #TAB# #TAB# #TAB# 'Default organisation logo will be used.').format( #LINE# #TAB# #TAB# #TAB# logo_path=organisation_logo_file) #LINE# #TAB# #TAB# return inasafe_default_settings['organisation_logo_path']"
Maps a single transaction row to a dictionary . Parameters ---------- txn : pd . DataFrame A single transaction object to convert to a dictionary . Returns ------- dict Mapped transaction  <code> def map_transaction(txn): ,"#LINE# #TAB# if isinstance(txn['sid'], dict): #LINE# #TAB# #TAB# sid = txn['sid']['sid'] #LINE# #TAB# #TAB# symbol = txn['sid']['symbol'] #LINE# #TAB# else: #LINE# #TAB# #TAB# sid = txn['sid'] #LINE# #TAB# #TAB# symbol = txn['sid'] #LINE# #TAB# return {'sid': sid, 'symbol': symbol, 'price': txn['price'], 'order_id': #LINE# #TAB# #TAB# txn['order_id'], 'amount': txn['amount'], 'commission': txn[ #LINE# #TAB# #TAB# 'commission'], 'dt': txn['dt']}"
"Find path of file in directory containing the search string <code> def find_file(path_dir, search_str, file_ext): ","#LINE# #TAB# import os #LINE# #TAB# file_path = None #LINE# #TAB# for file_name in os.listdir(path_dir): #LINE# #TAB# #TAB# if (search_str in file_name) and (file_name.endswith(file_ext)): #LINE# #TAB# #TAB# #TAB# file_path = os.path.join(path_dir, file_name) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# if file_path == None: #LINE# #TAB# #TAB# raise SystemError('No file found containing string: ' #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# '{}.'.format(search_str)) #LINE# #TAB# return file_path"
Connect to AWS CloudFormation : returns : boto.cloudformation.connection <code> def connect_cloudformation(): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return cloudformation.connect_to_region(config. #LINE# #TAB# #TAB# #TAB# get_environment_option('region'), aws_access_key_id=config. #LINE# #TAB# #TAB# #TAB# get_environment_option('access-key-id'), aws_secret_access_key= #LINE# #TAB# #TAB# #TAB# config.get_environment_option('secret-access-key')) #LINE# #TAB# except Exception as err: #LINE# #TAB# #TAB# logger.error('A problem occurred connecting to AWS CloudFormation: {}' #LINE# #TAB# #TAB# #TAB# .format(err)) #LINE# #TAB# #TAB# raise"
Create the necessary number of lists to compute colors feature extraction . # Arguments : nbBins : Int . The number of bins wanted for color histogram . # Outputs : lists : A list of 2 * 3*`nbBins ` empty lists  <code> def list_creator(nbBins): ,#LINE# #TAB# nbLists = 2 * 3 * nbBins #LINE# #TAB# return [[] for _ in range(nbLists)]
"Determine if : raml_resource : is a singular subresource . : param raml_resource : Instance of ramlfications.raml . ResourceNode . : param route_name : Name of the : raml_resource :  <code> def singular_subresource(raml_resource, route_name): ","#LINE# #TAB# static_parent = get_static_parent(raml_resource, method='POST') #LINE# #TAB# if static_parent is None: #LINE# #TAB# #TAB# return False #LINE# #TAB# schema = resource_schema(static_parent) or {} #LINE# #TAB# properties = schema.get('properties', {}) #LINE# #TAB# if route_name not in properties: #LINE# #TAB# #TAB# return False #LINE# #TAB# db_settings = properties[route_name].get('_db_settings', {}) #LINE# #TAB# is_obj = db_settings.get('type') == 'relationship' #LINE# #TAB# single_obj = not db_settings.get('uselist', True) #LINE# #TAB# return is_obj and single_obj"
"When polysh quits , we kill all the remote shells we started <code> def kill_all() ->None: ","#LINE# #TAB# for i in dispatchers.all_instances(): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# os.kill(-i.pid, signal.SIGKILL) #LINE# #TAB# #TAB# except OSError: #LINE# #TAB# #TAB# #TAB# pass"
Convert a metric name into the operator and metric names  <code> def metric_parts(metric): ,"#LINE# #TAB# operator, met = resolve_metric_as_tuple(metric) #LINE# #TAB# return operator.name, met.name"
"Returns accuracy score evaluation result . - overall accuracy - mean accuracy - mean IU - fwavacc <code> def get_scores(confusion_matrix, n_classes): ","#LINE# #TAB# hist = confusion_matrix #LINE# #TAB# acc = np.diag(hist).sum() / hist.sum() #LINE# #TAB# acc_cls = np.diag(hist) / hist.sum(axis=1) #LINE# #TAB# acc_cls = np.nanmean(acc_cls) #LINE# #TAB# iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)) #LINE# #TAB# mean_iu = np.nanmean(iu) #LINE# #TAB# freq = hist.sum(axis=1) / hist.sum() #LINE# #TAB# fwavacc = (freq[freq > 0] * iu[freq > 0]).sum() #LINE# #TAB# cls_iu = dict(zip(range(n_classes), iu)) #LINE# #TAB# return acc, acc_cls, fwavacc, mean_iu, cls_iu"
"Run git config --get remote.<remote>.url in path . : param path : Path where git is to be run : param remote : Remote name : return : str or None <code> def get_remote_url(path, remote='origin'): ","#LINE# #TAB# path = get_path(path) #LINE# #TAB# cmd = ['config', '--get', 'remote.%s.url' % remote] #LINE# #TAB# return __run_git(cmd, path)[0]"
"Generate a new column name that is guaranteed not to conflict with an existing set of column names  <code> def robust_column_name(base_name, column_names): ",#LINE# #TAB# robust_name = base_name #LINE# #TAB# i = 1 #LINE# #TAB# while robust_name in column_names: #LINE# #TAB# #TAB# robust_name = base_name + '.{}'.format(i) #LINE# #TAB# #TAB# i += 1 #LINE# #TAB# return robust_name
"Fetch dictionary content after storing it , from gridfs or json . : param doc <code> def fetch_dict(doc): ",#LINE# #TAB# if doc and doc.get(FILE_ID): #LINE# #TAB# #TAB# content_file = app.storage.get(doc[FILE_ID]) #LINE# #TAB# #TAB# content = json.loads(content_file.read()) #LINE# #TAB# #TAB# return content #LINE# #TAB# if doc and doc.get('content'): #LINE# #TAB# #TAB# return decode_dict(doc['content']) #LINE# #TAB# return {}
"definition : func(b1 , b2 ) { ... } objective : returns you the midsegment of a trapezoid on its two bases 1/2 ( base 1 + base 2 ) <code> def trapezoid_midsegment(b1, b2): ",#LINE# #TAB# midsegment = 1 / 2 * (b1 + b2) #LINE# #TAB# return midsegment
"Down sampling tick data to a tick based target index  <code> def down_sample_tick_to_tick(df, target_index, resample_func): ","#LINE# #TAB# start = target_index[0].asfreq(df.index.freq, how='start') #LINE# #TAB# end = target_index[-1].asfreq(df.index.freq, how='end') #LINE# #TAB# new_df = df[start:end] #LINE# #TAB# new_df = new_df.resample(target_index.freq).agg(resample_func) #LINE# #TAB# return new_df"
"Input validation for strings  <code> def input_validate_str(string, name, max_len=None, exact_len=None): ","#LINE# #TAB# if type(string) is not str: #LINE# #TAB# #TAB# raise pyhsm.exception.YHSM_WrongInputType(name, str, type(string)) #LINE# #TAB# if max_len != None and len(string) > max_len: #LINE# #TAB# #TAB# raise pyhsm.exception.YHSM_InputTooLong(name, max_len, len(string)) #LINE# #TAB# if exact_len != None and len(string) != exact_len: #LINE# #TAB# #TAB# raise pyhsm.exception.YHSM_WrongInputSize(name, exact_len, len(string)) #LINE# #TAB# return string"
Count execution time for marker ` ` t ` `  <code> def time_pop(t): ,#LINE# #TAB# global _TimeTotalDict #LINE# #TAB# global _TimeDeltaDict #LINE# #TAB# global _TimeCountsDict #LINE# #TAB# tm = time.time() #LINE# #TAB# if t not in _TimeTotalDict: #LINE# #TAB# #TAB# return #LINE# #TAB# dt = tm - _TimeDeltaDict[t] #LINE# #TAB# _TimeTotalDict[t] += dt #LINE# #TAB# _TimeCountsDict[t] += 1
Return whether the file exists and print a message if it exists  <code> def file_exists(filename): ,"#LINE# #TAB# if os.path.exists(filename): #LINE# #TAB# #TAB# pywikibot.output(u""'%s' already exists."" % filename) #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Return True if the qualifiers contain a NOT <code> def _has_not_qual(ntd): ,#LINE# #TAB# #TAB# for qual in ntd.Qualifier: #LINE# #TAB# #TAB# #TAB# if 'not' in qual: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# #TAB# if 'NOT' in qual: #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# return False
"Uses regex to attempt to extract claims from a large string of text . : param text : Large string for claimset : type text : str : return : list of tuples ( claim_number , claim_text ) <code> def regex_extract_claims(text): ","#LINE# #TAB# claim_r = '((\\d+)\\s*\\.[ |\\t])?([A-Z].*?[\\.])\\s*\\n' #LINE# #TAB# matches = re.finditer(claim_r, text, re.DOTALL) #LINE# #TAB# claimset_list = [] #LINE# #TAB# for match_num, match in enumerate(matches): #LINE# #TAB# #TAB# match_num = match_num + 1 #LINE# #TAB# #TAB# claim_text = match.group(3) #LINE# #TAB# #TAB# if match.group(2): #LINE# #TAB# #TAB# #TAB# number = match.group(2) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# number = match_num #LINE# #TAB# #TAB# claimset_list.append((number, claim_text)) #LINE# #TAB# return claimset_list"
"Compute a normalized sigmoid function with specified range . Args : n ( int ) : number of samples to used r ( float ) : X range [ -r , r ] where to compute the sigmoid <code> def norm_sigmoid(n, r): ","#LINE# #TAB# y = sigmoid(np.linspace(-r, r, n)) #LINE# #TAB# y = y - y.min() #LINE# #TAB# y = y / y.max() #LINE# #TAB# return y"
Given data in chunks yield lines of text <code> def lines_from_stream(chunks): ,#LINE# #TAB# buf = buffer.DecodingLineBuffer() #LINE# #TAB# for chunk in chunks: #LINE# #TAB# #TAB# buf.feed(chunk) #LINE# #TAB# #TAB# for _ in buf: #LINE# #TAB# #TAB# #TAB# yield _
"Iterate through fields as ` ( attribute_name , field_instance ) `  <code> def iterate_over_fields(cls): ","#LINE# #TAB# for attr in dir(cls): #LINE# #TAB# #TAB# clsattr = getattr(cls, attr) #LINE# #TAB# #TAB# if isinstance(clsattr, BaseField): #LINE# #TAB# #TAB# #TAB# yield attr, clsattr"
"Deprecated . Use MediaRatios.reel instead . Acceptable min , max values of with / height ratios for a story upload : return : tuple of ( min . ratio , max . ratio ) <code> def reel_ratios(): ","#LINE# #TAB# warnings.warn( #LINE# #TAB# #TAB# 'Client.reel_ratios() is deprecated. Please use MediaRatios.reel instead.' #LINE# #TAB# #TAB# , ClientDeprecationWarning) #LINE# #TAB# return MediaRatios.reel"
"Attaches data to beacon <code> def attach_data_to_beacon(beacon_details, credentials): ","#LINE# #TAB# request_body = beacon_details.attachment_request_body() #LINE# #TAB# header = Header(credentials.access_token) #LINE# #TAB# delete_url = url_builder.batch_delete_url(beacon_details) #LINE# #TAB# status = requests.post(delete_url, headers=header.get_header_body()) #LINE# #TAB# url = url_builder.beacon_attachment_url(beacon_details) #LINE# #TAB# if status.status_code is 200: #LINE# #TAB# #TAB# response = requests.post(url, data=json.dumps(request_body), #LINE# #TAB# #TAB# #TAB# headers=header.get_header_body()) #LINE# #TAB# return response.content"
Get the previously active : class:`.plotter ` instance or default one  <code> def get_active_plotter() ->Plotter: ,"#LINE# #TAB# plotter = _active_plotter.get() #LINE# #TAB# assert isinstance(plotter, Plotter), plotter #LINE# #TAB# return plotter"
"Checks if the given filename has one of the given extensions <code> def has_extension(filename, extensions): ",#LINE# #TAB# for ext in extensions: #LINE# #TAB# #TAB# if filename.endswith('.' + ext): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False
returns the relative path of the current directory to the git repository . This path will be added the filename path to find the file . It current_dir is the git root this function returns an empty string  <code> def delta_dir(): ,"#LINE# #TAB# repo = Repo() #LINE# #TAB# current_dir = os.getcwd() #LINE# #TAB# repo_dir = repo.tree().abspath #LINE# #TAB# delta_dir = current_dir.replace(repo_dir, '') #LINE# #TAB# if delta_dir: #LINE# #TAB# #TAB# return delta_dir + '/' #LINE# #TAB# else: #LINE# #TAB# #TAB# return ''"
"Enable type profile . * * EXPERIMENTAL * * <code> def start_type_profile() ->typing.Generator[T_JSON_DICT, T_JSON_DICT, None]: ",#LINE# #TAB# cmd_dict: T_JSON_DICT = {'method': 'Profiler.startTypeProfile'} #LINE# #TAB# json = yield cmd_dict
Get speaker i d from a BAS partitur file Parameters ---------- path : str a path to the file Returns ------- str or None the speaker i d <code> def parse_speaker(path): ,"#LINE# #TAB# speaker = '' #LINE# #TAB# with open(path, 'r', encoding='utf8') as f: #LINE# #TAB# #TAB# lines = f.readlines() #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# splitline = re.split('\\s', line) #LINE# #TAB# #TAB# if splitline[0] == 'SPN:': #LINE# #TAB# #TAB# #TAB# return splitline[1].strip() #LINE# #TAB# return None"
Register widget in dictionary  <code> def register_widget(widget): ,#LINE# #TAB# WIDGETS[widget.name] = widget #LINE# #TAB# return widget
"Return trace with total energy of three component stream <code> def observed_energy(stream, rho): ","#LINE# #TAB# data = [energy1c(tr.data, rho) for tr in stream] #LINE# #TAB# data = np.sum(data, axis=0) #LINE# #TAB# tr = obspy.core.Trace(data=data, header=stream[0].stats) #LINE# #TAB# tr.stats.channel = tr.stats.channel[:2] + 'X' #LINE# #TAB# return tr"
"Unique elements , preserving order  <code> def unique_everseen(iterable, filterfalse_=itertools.filterfalse): ","#LINE# #TAB# seen = set() #LINE# #TAB# seen_add = seen.add #LINE# #TAB# for element in filterfalse_(seen.__contains__, iterable): #LINE# #TAB# #TAB# seen_add(element) #LINE# #TAB# #TAB# yield element"
"this function will downalod the image given url <code> def downlaod_file_images(img_url, oufilename): ","#LINE# #TAB# print(image_basepath + img_url) #LINE# #TAB# print(oufilename) #LINE# #TAB# r = requests.get(image_basepath + img_url, stream=True) #LINE# #TAB# with open(oufilename + '.jpg', 'wb') as f: #LINE# #TAB# #TAB# f.write(r.content) #LINE# #TAB# #TAB# return"
"Recursively merges two dicts . When keys exist in both the value of ' y ' is used . Args : x ( dict ) : First dict y ( dict ) : Second dict Returns : dict : Merged dict containing values of x and y <code> def merge_dicts(x, y): ","#LINE# #TAB# if x is None and y is None: #LINE# #TAB# #TAB# return dict() #LINE# #TAB# if x is None: #LINE# #TAB# #TAB# return y #LINE# #TAB# if y is None: #LINE# #TAB# #TAB# return x #LINE# #TAB# merged = dict(x, **y) #LINE# #TAB# xkeys = x.keys() #LINE# #TAB# for key in xkeys: #LINE# #TAB# #TAB# if type(x[key]) is dict and key in y: #LINE# #TAB# #TAB# #TAB# merged[key] = merge_dicts(x[key], y[key]) #LINE# #TAB# return merged"
"Show the data type information gui Parameters ---------- tmc : TmcFile , str , pathlib . Path The tmc file to show <code> def create_types_gui(tmc): ","#LINE# #TAB# if isinstance(tmc, (str, pathlib.Path)): #LINE# #TAB# #TAB# tmc = pytmc.parser.parse(tmc) #LINE# #TAB# interface = TmcTypes(tmc) #LINE# #TAB# interface.setMinimumSize(600, 400) #LINE# #TAB# return interface"
"start < datetime > end < datetime > returns < str > human readable elapsed time <code> def ts_delta_str(start, end): ","#LINE# #TAB# delta = end - start #LINE# #TAB# ts = int(delta.total_seconds()) #LINE# #TAB# hours, remainder = divmod(ts, 3600) #LINE# #TAB# minutes, seconds = divmod(remainder, 60) #LINE# #TAB# if hours > 0: #LINE# #TAB# #TAB# delta_str = ( #LINE# #TAB# #TAB# #TAB# f'{hours} hour(s), {minutes} minute(s), {seconds} second(s)') #LINE# #TAB# elif minutes > 0: #LINE# #TAB# #TAB# delta_str = f'{minutes} minute(s), {seconds} second(s)' #LINE# #TAB# else: #LINE# #TAB# #TAB# delta_str = f'{seconds} second(s)' #LINE# #TAB# return delta_str"
"create a T1File instance from PFB font file of given name <code> def from_pfb_filename(cls, filename): ","#LINE# #TAB# with open(filename, 'rb') as file: #LINE# #TAB# #TAB# t1file = cls.from_PFB_bytes(file.read()) #LINE# #TAB# return t1file"
Get mappings from the square array A to the flat vector of parameters alpha  <code> def get_maps(A): ,"#LINE# #TAB# N = A.shape[0] #LINE# #TAB# flat_map = [] #LINE# #TAB# for i in range(1, N): #LINE# #TAB# #TAB# for j in range(1, N): #LINE# #TAB# #TAB# #TAB# flat_map.append([i, j]) #LINE# #TAB# flat_map = np.array(flat_map) #LINE# #TAB# square_map = np.zeros(A.shape, 'int') #LINE# #TAB# for k in range((N - 1) ** 2): #LINE# #TAB# #TAB# i, j = flat_map[k] #LINE# #TAB# #TAB# square_map[i, j] = k #LINE# #TAB# return flat_map, square_map"
Return a yaml file s contents as a dictionary <code> def get_yaml_document(filePath): ,#LINE# #TAB# with open(filePath) as stream: #LINE# #TAB# #TAB# doc = yaml.load(stream) #LINE# #TAB# #TAB# return doc
"Make the columns have better names , and ordered in a better order  <code> def cleanup_frame(frame): ","#LINE# #TAB# frame = frame.rename(columns={'Non- Hispanic white': 'White'}) #LINE# #TAB# frame = frame.reindex(columns=['Asian', 'White', 'Hispanic', 'Black']) #LINE# #TAB# return frame"
"Perform a simple DNS lookup , return results in a dictionary <code> def dns_lookup(input, timeout=3, server=''): ","#LINE# #TAB# resolver = Resolver() #LINE# #TAB# resolver.timeout = float(timeout) #LINE# #TAB# resolver.lifetime = float(timeout) #LINE# #TAB# if server: #LINE# #TAB# #TAB# resolver.nameservers = [server] #LINE# #TAB# try: #LINE# #TAB# #TAB# records = resolver.query(input, 'A') #LINE# #TAB# #TAB# return {'addrs': [ii.address for ii in records], 'error': '', #LINE# #TAB# #TAB# #TAB# 'name': input} #LINE# #TAB# except DNSException as e: #LINE# #TAB# #TAB# return {'addrs': [], 'error': repr(e), 'name': input}"
Remove metabolites that are not involved in any reactions and returns pruned model <code> def prune_unused_metabolites(cobra_model): ,"#LINE# #TAB# output_model = cobra_model.copy() #LINE# #TAB# inactive_metabolites = [m for m in output_model.metabolites #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# if len(m.reactions) == 0] #LINE# #TAB# output_model.remove_metabolites(inactive_metabolites) #LINE# #TAB# return output_model, inactive_metabolites"
Add PyroAdapter to the list of available in Pyro adapters and handle new supported protocols This function should be called after reimport of Pyro module to enable PYROS:// again  <code> def setup_pskadapter(): ,#LINE# #TAB# Pyro.protocol.getProtocolAdapter = getProtocolAdapter #LINE# #TAB# Pyro.core.processStringURI = processStringURI
Perform a lexsort and return the sort indices and shape as a tuple  <code> def flds_firstsort(d): ,"#LINE# #TAB# shape = [ len( np.unique(d[l]) ) #LINE# #TAB# #TAB# #TAB# for l in ['xs', 'ys', 'zs'] ]; #LINE# #TAB# si = np.lexsort((d['z'],d['y'],d['x'])); #LINE# #TAB# return si,shape;"
"Yield write_script ( ) argument tuples for a distribution 's console_scripts and gui_scripts entry points  <code> def get_args(cls, dist, header=None): ","#LINE# #TAB# if header is None: #LINE# #TAB# #TAB# header = cls.get_header() #LINE# #TAB# spec = str(dist.as_requirement()) #LINE# #TAB# for type_ in ('console', 'gui'): #LINE# #TAB# #TAB# group = type_ + '_scripts' #LINE# #TAB# #TAB# for name, ep in dist.get_entry_map(group).items(): #LINE# #TAB# #TAB# #TAB# cls._ensure_safe_name(name) #LINE# #TAB# #TAB# #TAB# script_text = cls.template % locals() #LINE# #TAB# #TAB# #TAB# args = cls._get_script_args(type_, name, header, script_text) #LINE# #TAB# #TAB# #TAB# for res in args: #LINE# #TAB# #TAB# #TAB# #TAB# yield res"
Returns a generator of data read from the socket when the tty setting is not enabled  <code> def frames_iter_no_tty(socket): ,"#LINE# #TAB# while True: #LINE# #TAB# #TAB# stream, n = next_frame_header(socket) #LINE# #TAB# #TAB# if n < 0: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# while n > 0: #LINE# #TAB# #TAB# #TAB# result = read(socket, n) #LINE# #TAB# #TAB# #TAB# if result is None: #LINE# #TAB# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# #TAB# data_length = len(result) #LINE# #TAB# #TAB# #TAB# if data_length == 0: #LINE# #TAB# #TAB# #TAB# #TAB# return #LINE# #TAB# #TAB# #TAB# n -= data_length #LINE# #TAB# #TAB# #TAB# yield stream, result"
"Consume digits of pi and compute 1 digit freq . counts  <code> def one_digit_freqs(digits, normalize=False): ","#LINE# #TAB# freqs = np.zeros(10, dtype='i4') #LINE# #TAB# for d in digits: #LINE# #TAB# #TAB# freqs[int(d)] += 1 #LINE# #TAB# if normalize: #LINE# #TAB# #TAB# freqs = freqs/freqs.sum() #LINE# #TAB# return freqs"
Add constraints to a namespace that are LinearExpressions of basic constraints  <code> def add_symbolic_contents_constraints(namespace): ,#LINE# #TAB# left = namespace.contents_left #LINE# #TAB# right = namespace.contents_right #LINE# #TAB# top = namespace.contents_top #LINE# #TAB# bottom = namespace.contents_bottom #LINE# #TAB# namespace.contents_width = right - left #LINE# #TAB# namespace.contents_height = top - bottom #LINE# #TAB# namespace.contents_v_center = bottom + namespace.contents_height / 2.0 #LINE# #TAB# namespace.contents_h_center = left + namespace.contents_width / 2.0
"Creates two lists derived from the supplied iterable : duplicate values and unique values Returns : unique , duplicate <code> def unique_duplicate(iterable): ","#LINE# #TAB# unique = [] #LINE# #TAB# duplicate = [] #LINE# #TAB# for x in iterable: #LINE# #TAB# #TAB# if x in unique: #LINE# #TAB# #TAB# #TAB# if x not in duplicate: #LINE# #TAB# #TAB# #TAB# #TAB# duplicate.append(x) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# unique.append(x) #LINE# #TAB# return unique, duplicate"
"Return names of all load balancers associated with an account <code> def list_elbs(region=None, key=None, keyid=None, profile=None): ","#LINE# #TAB# return [e.name for e in get_all_elbs(region=region, key=key, keyid=keyid, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# profile=profile)]"
Utility function which takes a list l and returns the minimum for the alphabetical order among all cyclic permutations of the list  <code> def cycle_sort(l): ,"#LINE# #TAB# s = l #LINE# #TAB# for i in range(0, len(l)): #LINE# #TAB# #TAB# temp = l[i:] + l[0:i] #LINE# #TAB# #TAB# if temp < s: #LINE# #TAB# #TAB# #TAB# s = temp #LINE# #TAB# return s"
"Returns a single ( or list of ) float(s ) with the V - band absolute magnitude(s ) Parameters ---------- spec_type : str , list The single or list of spectral types Returns ------- Mv : float , list <code> def get_stellar_mv(spec_type): ","#LINE# #TAB# props = _get_stellar_properties(spec_type) #LINE# #TAB# if isinstance(props, (list, tuple)): #LINE# #TAB# #TAB# return [prop['Mv'] for prop in props] #LINE# #TAB# else: #LINE# #TAB# #TAB# return props['Mv']"
"Try multiple times to run throw_random <code> def throw_random( lengths, mask ): ","#LINE# #TAB# saved = None #LINE# #TAB# for i in range( maxtries ): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return throw_random_bits( lengths, mask ) #LINE# #TAB# #TAB# except MaxtriesException as e: #LINE# #TAB# #TAB# #TAB# saved = e #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# raise e"
Takes a python - native tweet obect ( a dict ) . Returns True if a tweet contains a mention <code> def contains_mention(tweet): ,#LINE# #TAB# if 'entities' not in tweet: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True if len(tweet['entities']['user_mentions']) > 0 else False
"input : string , name of a file containing a profile description output : asp . TermSet , with atoms matching the contents of the input file Parses a profile description , and returns a TermSet object  <code> def read_profile(filename): ","#LINE# #TAB# p = profile_parser.Parser() #LINE# #TAB# accu = TermSet() #LINE# #TAB# file = open(filename, 'r') #LINE# #TAB# s = file.readline() #LINE# #TAB# while s != '': #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# accu = p.parse(s, filename) #LINE# #TAB# #TAB# except EOFError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# #TAB# s = file.readline() #LINE# #TAB# return accu"
Get Information about user that will be passed into The ' User - Agent ' header with requests <code> def get_user_info(): ,"#LINE# #TAB# platform = sys.platform #LINE# #TAB# python_version = '{}.{}.{}'.format(sys.version_info.major, sys. #LINE# #TAB# #TAB# version_info.minor, sys.version_info.micro) #LINE# #TAB# bonsai_cli_version = __version__ #LINE# #TAB# user_info = 'bonsai-cli/{} (python {}; {})'.format(bonsai_cli_version, #LINE# #TAB# #TAB# python_version, platform) #LINE# #TAB# return user_info"
Determine whether or not the path matches one or more paths in the ENABLED_PATHS setting . : param path : A string describing the path to be matched  <code> def is_enabled_path(path): ,"#LINE# #TAB# for enabled_path in ENABLED_PATHS: #LINE# #TAB# #TAB# match = re.search(enabled_path, path[1:]) #LINE# #TAB# #TAB# if match: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Performs a simple query returning the count of each value of the field ' fieldname ' in the QuerySet , returning the results as a SortedDict of value : count <code> def value_counts(qs, fieldname): ","#LINE# #TAB# values_counts = qs.values_list(fieldname).order_by(fieldname).annotate( #LINE# #TAB# #TAB# models.Count(fieldname)) #LINE# #TAB# count_dict = SortedDict() #LINE# #TAB# for val, count in values_counts: #LINE# #TAB# #TAB# count_dict[val] = count #LINE# #TAB# return count_dict"
"Pauses the execution if response status code is > 500 . : param api : Api instance . : param response : requests . Response object : param sleep : Time to sleep in between the requests  <code> def general_error_sleeper(api, response, sleep=300): ","#LINE# #TAB# while response.status_code >= 500: #LINE# #TAB# #TAB# logger.warning('Caught [%s] status code! Waiting for [%s]s', #LINE# #TAB# #TAB# #TAB# response.status_code, sleep) #LINE# #TAB# #TAB# time.sleep(sleep) #LINE# #TAB# #TAB# response = api.session.send(response.request) #LINE# #TAB# return response"
bivariateIntegral - Analytic integral over the specified bivariate Gaussian . Parameters ---------- params - list of floats - Values of parameters for the Gaussian . Returns ------- contInteg - float - Integral over the bivariate GAussian <code> def bivariate_integral(params): ,"#LINE# #TAB# mux, sigmax, muy, sigmay, A, rho = params #LINE# #TAB# contInteg = 2 * np.pi * A * np.abs(sigmax * sigmay) * np.sqrt(1 - rho ** 2) #LINE# #TAB# return contInteg"
return True if the current distribution is running on debian like OS  <code> def is_archlinux(): ,"#LINE# #TAB# if platform.system().lower() == 'linux': #LINE# #TAB# #TAB# if platform.linux_distribution() == ('', '', ''): #LINE# #TAB# #TAB# #TAB# if os.path.exists('/etc/arch-release'): #LINE# #TAB# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
Returns a list of sets found in the scene ( outliner ) : return : list < str > <code> def get_sets(): ,"#LINE# #TAB# sets = maya.cmds.ls(type='objectSet') #LINE# #TAB# top_sets = list() #LINE# #TAB# for obj_set in sets: #LINE# #TAB# #TAB# if obj_set == 'defaultObjectSet': #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# outputs = maya.cmds.listConnections(obj_set, plugs=False, #LINE# #TAB# #TAB# #TAB# connections=False, destination=True, source=False, #LINE# #TAB# #TAB# #TAB# skipConversionNodes=True) #LINE# #TAB# #TAB# if not outputs: #LINE# #TAB# #TAB# #TAB# top_sets.append(obj_set) #LINE# #TAB# return top_sets"
Function for turning distance into astronomical perspective  <code> def astro_dist(mass): ,"#LINE# #TAB# value = G.value * mass / c.value ** 2 #LINE# #TAB# astro_dist = u.def_unit('astro-m(GM/c2)', u.m / value) #LINE# #TAB# return astro_dist"
"Creates the object using the information from a message  <code> def from_msg(cls, msg): ","#LINE# #TAB# kwargs = dict(node_id=uuid.UUID(msg.node), generation=msg.generation, #LINE# #TAB# #TAB# free_bytes=msg.free_bytes) #LINE# #TAB# result = cls(**kwargs) #LINE# #TAB# return result"
Find size of a list object even if it is a one element non - list : param query_list : List to be sized <code> def list_size(query_list): ,"#LINE# #TAB# if isinstance(query_list, list): #LINE# #TAB# #TAB# return len(query_list) #LINE# #TAB# else: #LINE# #TAB# #TAB# return 1"
Ensure that the color scale is formatted in rgb strings . If the colorscale is a hex string then convert to rgb  <code> def colors_to_rgb(colorscale): ,"#LINE# #TAB# if colorscale[0][1][0] == ""#"": #LINE# #TAB# #TAB# plotly_colors = np.array(colorscale)[:, 1].tolist() #LINE# #TAB# #TAB# for k, hexcode in enumerate(plotly_colors): #LINE# #TAB# #TAB# #TAB# hexcode = hexcode.lstrip(""#"") #LINE# #TAB# #TAB# #TAB# hex_len = len(hexcode) #LINE# #TAB# #TAB# #TAB# step = hex_len // 3 #LINE# #TAB# #TAB# #TAB# colorscale[k][1] = ""rgb"" + str( #LINE# #TAB# #TAB# #TAB# #TAB# tuple(int(hexcode[j : j + step], 16) for j in range(0, hex_len, step)) #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# return colorscale"
Returns a dictionary encoding the information about this version of pyslim  <code> def make_pyslim_provenance_dict(): ,"#LINE# #TAB# document = {'schema_version': '1.0.0', 'software': {'name': 'pyslim', #LINE# #TAB# #TAB# 'version': __version__}, 'parameters': {'command': {}}, #LINE# #TAB# #TAB# 'environment': get_environment()} #LINE# #TAB# return document"
expressions : expression expressions <code> def p_expressions_expression_expressions(p): ,#LINE# #TAB# p[2].appendleft(p[1]) #LINE# #TAB# p[0] = p[2]
"Loops over incoming data looking for base64 encoded data and converts them to a readable format  <code> def sanitize_resources(cls, resources): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# for resource in cls._loop_raw(resources): #LINE# #TAB# #TAB# #TAB# cls._sanitize_resource(resource) #LINE# #TAB# except (KeyError, TypeError): #LINE# #TAB# #TAB# _LOGGER.debug('no shade data available') #LINE# #TAB# #TAB# return None"
Return project and benchmark from instancemethod for reporter  <code> def project_and_benchmark(instancemethod): ,"#LINE# #TAB# class_name = class_from_instancemethod(instancemethod).__name__.replace( #LINE# #TAB# #TAB# 'BenchmarkCase', '') #LINE# #TAB# benchmark = instancemethod.__name__.replace('benchmark_', '') #LINE# #TAB# project = _get_metainfo(instancemethod, 'project') #LINE# #TAB# if project: #LINE# #TAB# #TAB# benchmark = class_name.lower() + '_' + benchmark #LINE# #TAB# else: #LINE# #TAB# #TAB# project = class_name #LINE# #TAB# return {'project': project, 'benchmark': benchmark}"
Check for use of LOG . * ( _ ( V319 <code> def no_translate_logs(logical_line): ,"#LINE# #TAB# if translated_logs.match(logical_line): #LINE# #TAB# #TAB# yield 0, ""V319: Don't translate logs"""
"Add app to local registry by name <code> def add_usable_app(name, app): ",#LINE# #TAB# 'Add app to local registry by name' #LINE# #TAB# name = slugify(name) #LINE# #TAB# global usable_apps #LINE# #TAB# usable_apps[name] = app #LINE# #TAB# return name
"Get customized logger . Args : name : Name of the logger . level : Level to log . Returns : A logger  <code> def get_logger(name=None, level=WARNING): ","#LINE# #TAB# logger = logging.getLogger(name) #LINE# #TAB# if name and not getattr(logger, '_init_done', None): #LINE# #TAB# #TAB# logger._init_done = True #LINE# #TAB# #TAB# logger.addHandler(_handler) #LINE# #TAB# #TAB# logger.setLevel(level) #LINE# #TAB# return logger"
"Creates the root directory for a backup and assigns the provided gid . Permissions are set as 770  <code> def create_backup_root(backupHome, backup, gid): ","#LINE# #TAB# root = join(backupHome, backup) #LINE# #TAB# os.makedirs(root) #LINE# #TAB# os.chmod(root, 504) #LINE# #TAB# fn.tryToSetOwnership(root, os.getuid(), gid) #LINE# #TAB# return root"
Returns the client of the contact of a Plone user <code> def get_user_client(user_or_contact): ,"#LINE# #TAB# if not user_or_contact or ILabContact.providedBy(user_or_contact): #LINE# #TAB# #TAB# return None #LINE# #TAB# if not IContact.providedBy(user_or_contact): #LINE# #TAB# #TAB# contact = get_user_contact(user_or_contact, contact_types=['Contact']) #LINE# #TAB# #TAB# if IContact.providedBy(contact): #LINE# #TAB# #TAB# #TAB# return get_user_client(contact) #LINE# #TAB# #TAB# return None #LINE# #TAB# client = get_parent(user_or_contact) #LINE# #TAB# if client and IClient.providedBy(client): #LINE# #TAB# #TAB# return client #LINE# #TAB# return None"
Used in converting public key to p2wpkh script <code> def mk_p2wpkh_script(pubkey): ,#LINE# #TAB# script = mk_p2wpkh_redeemscript(pubkey)[2:] #LINE# #TAB# return 'a914' + hex_to_hash160(script) + '87'
"Check if a folder exists , make it if it does n't , set it to home if None . : param folder : Folder to check . : type folder : str <code> def generate_workfolder(folder=None): ","#LINE# #TAB# folder = utilities.dirhandler(folder, os.getcwd()) #LINE# #TAB# if folder is not None and not os.path.exists(folder): #LINE# #TAB# #TAB# os.makedirs(folder) #LINE# #TAB# return folder"
"CLI selection given options  <code> def select_option(options, message='', val=None): ",#LINE# #TAB# while val not in options: #LINE# #TAB# #TAB# val = input(message) #LINE# #TAB# #TAB# if val not in options: #LINE# #TAB# #TAB# #TAB# logger.error('Invalid choice.') #LINE# #TAB# return val
"Ensure VDI is a snapshot , and not cached image  <code> def is_vdi_a_snapshot(vdi_rec): ",#LINE# #TAB# is_a_snapshot = vdi_rec['is_a_snapshot'] #LINE# #TAB# image_id = vdi_rec['other_config'].get('image-id') #LINE# #TAB# return is_a_snapshot and not image_id
"Accroding the paper , we only need to do power iteration one time  <code> def power_iteration(W, u, rounds=1): ","#LINE# #TAB# _u = u #LINE# #TAB# for i in range(rounds): #LINE# #TAB# #TAB# _v = _l2normalizer(K.dot(_u, W)) #LINE# #TAB# #TAB# _u = _l2normalizer(K.dot(_v, K.transpose(W))) #LINE# #TAB# W_sn = K.sum(K.dot(_u, W) * _v) #LINE# #TAB# return W_sn, _u, _v"
Return a mostly obfuscated version of the API Key : param API_key : input string : return : str <code> def obfuscate_api_key(API_key): ,#LINE# #TAB# if API_key is not None: #LINE# #TAB# #TAB# return (len(API_key) - 8) * '*' + API_key[-8:]
Calculates an ilm order based on the shape of an image . This is based on something that works for our particular images . Your mileage will vary  <code> def calc_ilm_order(imshape): ,"#LINE# #TAB# zorder = int(imshape[0] / 6.25) + 1 #LINE# #TAB# l_npts = int(imshape[1] / 42.5)+1 #LINE# #TAB# npts = () #LINE# #TAB# for a in range(l_npts): #LINE# #TAB# #TAB# if a < 5: #LINE# #TAB# #TAB# #TAB# npts += (int(imshape[2] * [59, 39, 29, 19, 14][a]/512.) + 1,) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# npts += (int(imshape[2] * 11/512.) + 1,) #LINE# #TAB# return npts, zorder"
"Generator that produces sequence of keypairs for nested dictionaries  <code> def recursive_keypairs(d, separator=':'): ","#LINE# #TAB# for name, value in sorted(six.iteritems(d)): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# for subname, subvalue in recursive_keypairs(value, separator): #LINE# #TAB# #TAB# #TAB# #TAB# yield '%s%s%s' % (name, separator, subname), subvalue #LINE# #TAB# #TAB# elif isinstance(value, (tuple, list)): #LINE# #TAB# #TAB# #TAB# yield name, decode_unicode(value) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# yield name, value"
"Save a new configuration by name  <code> def save_config(config, logdir=None): ","#LINE# if logdir: #LINE# #TAB# with config.unlocked: #LINE# #TAB# config.logdir = logdir #LINE# #TAB# message = 'Start a new run and write summaries and checkpoints to {}.' #LINE# #TAB# tf.logging.info(message.format(config.logdir)) #LINE# #TAB# tf.gfile.MakeDirs(config.logdir) #LINE# #TAB# config_path = os.path.join(config.logdir, 'config.yaml') #LINE# #TAB# with tf.gfile.FastGFile(config_path, 'w') as file_: #LINE# #TAB# yaml.dump(config, file_, default_flow_style=False) #LINE# else: #LINE# #TAB# message = ( #LINE# #TAB# #TAB# 'Start a new run without storing summaries and checkpoints since no ' #LINE# #TAB# #TAB# 'logging directory was specified.') #LINE# #TAB# tf.logging.info(message) #LINE# return config"
"Return element only if zoom condition matches with config string  <code> def filter_by_zoom(element=None, conf_string=None, zoom=None): ","#LINE# #TAB# for op_str, op_func in [('=', operator.eq), ('<=', operator.le), ('>=', #LINE# #TAB# #TAB# operator.ge), ('<', operator.lt), ('>', operator.gt)]: #LINE# #TAB# #TAB# if conf_string.startswith(op_str): #LINE# #TAB# #TAB# #TAB# return element if op_func(zoom, _strip_zoom(conf_string, op_str) #LINE# #TAB# #TAB# #TAB# #TAB# ) else None"
"Look up for an address in memory . Return an Address object if found , None otherwise  <code> def process_lookup_address(address): ",#LINE# #TAB# if not is_alive(): #LINE# #TAB# #TAB# err('Process is not running') #LINE# #TAB# #TAB# return None #LINE# #TAB# if is_x86(): #LINE# #TAB# #TAB# if is_in_x86_kernel(address): #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# for sect in get_process_maps(): #LINE# #TAB# #TAB# if sect.page_start <= address < sect.page_end: #LINE# #TAB# #TAB# #TAB# return sect #LINE# #TAB# return None
Convert lists back to numpy arrays <code> def core_to_numpy(obj_to_encode): ,"#LINE# #TAB# if isinstance(obj_to_encode, list): #LINE# #TAB# #TAB# return np.array(obj_to_encode) #LINE# #TAB# return obj_to_encode"
Splits a long title around the middle comma <code> def split_long_title(title: str) ->str: ,"#LINE# #TAB# if len(title) <= 60: #LINE# #TAB# #TAB# return title #LINE# #TAB# comma_indices = np.where(np.array([c for c in title]) == ',')[0] #LINE# #TAB# if not comma_indices.size: #LINE# #TAB# #TAB# return title #LINE# #TAB# best_index = comma_indices[np.argmin(abs(comma_indices - len(title) // 2))] #LINE# #TAB# title = title[:best_index + 1] + '\n' + title[best_index + 1:] #LINE# #TAB# return title"
"login into Scantist backend and get an scantist token for source code scan : param org_id : Integer , scantist organization i d : return : <code> def create_scantist_token(apikey, baseurl, org_id): ","#LINE# #TAB# endpoint = '/v1/orgs/%s/integration-tokens/' % org_id #LINE# #TAB# url = parseurl.urljoin(baseurl, endpoint) #LINE# #TAB# headers = {'Content-type': 'application/json', 'Authorization': #LINE# #TAB# #TAB# 'Token ' + apikey} #LINE# #TAB# payload = {'name': 'SCANTISTTOKEN%s' % int(time.time())} #LINE# #TAB# r = requests.post(url=url, json=payload, headers=headers) #LINE# #TAB# if r.status_code != 201: #LINE# #TAB# #TAB# error_exit('failed to create org token,err=%s' % r.text) #LINE# #TAB# set_config('SCANTIST', 'scantist_token', r.json()['token']) #LINE# #TAB# return r.json()['token']"
"@param weeks_ago : specify how many weeks ago to give count for ( 0 = this week so far ) <code> def get_week_dates(cls, weeks_ago): ","#LINE# #TAB# package_revision = table('package_revision') #LINE# #TAB# revision = table('revision') #LINE# #TAB# today = datetime.date.today() #LINE# #TAB# date_from = datetime.datetime(today.year, today.month, today.day #LINE# #TAB# #TAB# ) - datetime.timedelta(days=datetime.date.weekday(today) + 7 * #LINE# #TAB# #TAB# weeks_ago) #LINE# #TAB# date_to = date_from + datetime.timedelta(days=7) #LINE# #TAB# return date_from, date_to"
"Validate that the source directory exists and it contains the user script <code> def validate_source_dir(script, directory): ","#LINE# #TAB# if directory: #LINE# #TAB# #TAB# if not os.path.isfile(os.path.join(directory, script)): #LINE# #TAB# #TAB# #TAB# raise ValueError('No file named ""{}"" was found in directory ""{}"".'.format(script, directory)) #LINE# #TAB# return True"
Get default package values for all third party libraries <code> def get_defaults(): ,"#LINE# #TAB# defaults = {pkg_name: {'version': pkg['version'], 'path': pkg[ #LINE# #TAB# #TAB# 'versions'][pkg['version']]['path']} for pkg_name, pkg in packages. #LINE# #TAB# #TAB# items()} #LINE# #TAB# return defaults"
create a module cache in $ HOME/.sumo.modulecache if needed . returns the cache data  <code> def build_cache(): ,#LINE# #TAB# if build_cache_callback is None: #LINE# #TAB# #TAB# raise AssertionError('Error build_cache_callback is not initialized') #LINE# #TAB# if CACHING_ENABLED and os.path.exists(BUILDCACHE): #LINE# #TAB# #TAB# age = time.time() - os.path.getmtime(BUILDCACHE) #LINE# #TAB# #TAB# if age < MAX_AGE: #LINE# #TAB# #TAB# #TAB# touch(BUILDCACHE) #LINE# #TAB# #TAB# #TAB# db = sumolib.Builds.DB_overlay.from_pickle_file(BUILDCACHE) #LINE# #TAB# #TAB# #TAB# return db #LINE# #TAB# builddb = build_cache_callback() #LINE# #TAB# if CACHING_ENABLED: #LINE# #TAB# #TAB# builddb.pickle_save(BUILDCACHE) #LINE# #TAB# return builddb
"send out KOBE products via sms ( twilio package needed ) <code> def send_sms(_account, _token, _from, _to, _txt): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# from twilio.rest import Client #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# account_sid = _account #LINE# #TAB# auth_token = _token #LINE# #TAB# client = Client(account_sid, auth_token) #LINE# #TAB# message = client.messages.create(to=_to, from_=_from, body=_txt) #LINE# #TAB# print(message.sid) #LINE# #TAB# return True"
Check if input is csr or SparseLR and raise an error otherwise  <code> def check_csr_or_slr(adjacency): ,"#LINE# #TAB# if type(adjacency) not in [sparse.csr_matrix, SparseLR]: #LINE# #TAB# #TAB# raise TypeError( #LINE# #TAB# #TAB# #TAB# 'Input must be a scipy CSR matrix or a SparseLR object.') #LINE# #TAB# else: #LINE# #TAB# #TAB# return"
"Get Git SHA , if it 's a Git repo  <code> def get_from_repo(): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# repo = git.Repo(search_parent_directories=True) #LINE# #TAB# #TAB# return repo.head.object.hexsha #LINE# #TAB# except git.InvalidGitRepositoryError: #LINE# #TAB# #TAB# pass
Return the location of an export by name : param export_name : Target export : return : Location of target export or None <code> def get_export_addr(export_name): ,"#LINE# #TAB# for ea, name in iter_exports(): #LINE# #TAB# #TAB# if name == export_name: #LINE# #TAB# #TAB# #TAB# return ea"
"Returns True if the experiment URI has been set , False otherwise  <code> def is_experiment_uri_set(): ",#LINE# #TAB# if _experiment_uri or env.get_env(_EXPERIMENT_URI_ENV_VAR): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
Helper function returning the library directory of libgfortran . Useful on OSX where the C compiler oftentimes has no knowledge of the library directories of the Fortran compiler . I don t think it can do any harm on Linux  <code> def get_libgfortran_dir(): ,"#LINE# #TAB# for ending in ["".3.dylib"", "".dylib"", "".3.so"", "".so""]: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# p = Popen(['gfortran', ""-print-file-name=libgfortran"" + ending], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# stdout=PIPE, stderr=PIPE) #LINE# #TAB# #TAB# #TAB# p.stderr.close() #LINE# #TAB# #TAB# #TAB# line = p.stdout.readline().decode().strip() #LINE# #TAB# #TAB# #TAB# p.stdout.close() #LINE# #TAB# #TAB# #TAB# if os.path.exists(line): #LINE# #TAB# #TAB# #TAB# #TAB# return [os.path.dirname(line)] #LINE# #TAB# #TAB# except: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# return []"
IDENTITY REQUEST Section 9 . 2 . 10 <code> def identity_request(): ,#LINE# #TAB# a = TpPd(pd=0x5) #LINE# #TAB# b = MessageType(mesType=0x8) #LINE# #TAB# c = IdentityTypeAndSpareHalfOctets() #LINE# #TAB# packet = a / b / c #LINE# #TAB# return packet
Normalize a global config key . Does not check validity of the key . : param str key : User - supplied global config key : return : The normalized key formatted as an attribute of : class:` . LDAP ` : rtype : str <code> def normalize_global_config_param(key): ,#LINE# #TAB# key = key.upper() #LINE# #TAB# if not key.startswith('DEFAULT_'): #LINE# #TAB# #TAB# key = 'DEFAULT_' + key #LINE# #TAB# return key
Theoretical overturned concentration  <code> def c_overturned(step): ,"#LINE# #TAB# rbot, rtop = misc.get_rbounds(step) #LINE# #TAB# cinit, rad = init_c_overturn(step) #LINE# #TAB# radf = (rtop**3 + rbot**3 - rad**3)**(1 / 3) #LINE# #TAB# return cinit, radf"
"Probability that a value from pmf1 is less than a value from pmf2 . Args : pmf1 : Pmf object pmf2 : Pmf object Returns : float probability <code> def pmf_prob_less(pmf1, pmf2): ","#LINE# #TAB# total = 0 #LINE# #TAB# for v1, p1 in pmf1.Items(): #LINE# #TAB# #TAB# for v2, p2 in pmf2.Items(): #LINE# #TAB# #TAB# #TAB# if v1 < v2: #LINE# #TAB# #TAB# #TAB# #TAB# total += p1 * p2 #LINE# #TAB# return total"
Creates the Determinant header <code> def get_determinants_header(): ,"#LINE# #TAB# str = '' #LINE# #TAB# str += """"""--------- ----- ------ ---------------------#TAB# --------------#TAB# --------------#TAB# -------------- #LINE# """""" #LINE# #TAB# str += """"""#TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# DESOLVATION EFFECTS#TAB# SIDECHAIN#TAB# #TAB# BACKBONE#TAB# #TAB# COULOMBIC#TAB# #LINE# """""" #LINE# #TAB# str += """""" RESIDUE#TAB# pKa#TAB# BURIED#TAB# REGULAR#TAB# RE#TAB# #TAB# HYDROGEN BOND#TAB# HYDROGEN BOND#TAB# INTERACTION #LINE# """""" #LINE# #TAB# str += """"""--------- ----- ------ --------- ---------#TAB# --------------#TAB# --------------#TAB# -------------- #LINE# """""" #LINE# #TAB# return str"
"Try to refresh the TLD list from a remote source , if unable to then use the hardcoded values  <code> def refresh_tld_list(cls): ",#LINE# #TAB# try: #LINE# #TAB# #TAB# data = urlopen(Request(tld_iana_url)).read() #LINE# #TAB# #TAB# ACTIVE['tld']['map'] = [] #LINE# #TAB# #TAB# for line in data.split('\n'): #LINE# #TAB# #TAB# #TAB# line = line.strip() #LINE# #TAB# #TAB# #TAB# if not line.startswith('#'): #LINE# #TAB# #TAB# #TAB# #TAB# ACTIVE['tld']['map'].append(line) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass
Read content from file path <code> def read_file(file_path): ,"#LINE# #TAB# full_file_path = os.path.join(user_dir, file_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# with open(full_file_path) as infile: #LINE# #TAB# #TAB# #TAB# content = json.load(infile) #LINE# #TAB# #TAB# #TAB# return content #LINE# #TAB# except OSError: #LINE# #TAB# #TAB# return None"
"Parses out the head and link properties based on the HTTP Request from the client and the Protobuf response from the validator  <code> def _get_metadata(cls, request, response, head=None): ","#LINE# #TAB# #TAB# head = response.get('head_id', head) #LINE# #TAB# #TAB# metadata = {'link': cls._build_url(request, head=head)} #LINE# #TAB# #TAB# if head is not None: #LINE# #TAB# #TAB# #TAB# metadata['head'] = head #LINE# #TAB# #TAB# return metadata"
The the entity id by the path  <code> def get_id_by_impath(path): ,"#LINE# #TAB# #TAB# logger.info('Get Entiry, Path: {0}'.format(path)) #LINE# #TAB# #TAB# entity_list = TabEntity.select().where(TabEntity.path == path) #LINE# #TAB# #TAB# out_val = None #LINE# #TAB# #TAB# if entity_list.count() == 1: #LINE# #TAB# #TAB# #TAB# out_val = entity_list.get() #LINE# #TAB# #TAB# elif entity_list.count() > 1: #LINE# #TAB# #TAB# #TAB# for rec in entity_list: #LINE# #TAB# #TAB# #TAB# #TAB# MEntity.delete(rec.uid) #LINE# #TAB# #TAB# #TAB# out_val = None #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# #TAB# return out_val"
"Convert the Google Maps tile coordinate to a Bing Maps quadkey  <code> def tile_to_quadkey(cls, x=0, y=0, z=0): ","#LINE# #TAB# quadkey = '' #LINE# #TAB# for i in xrange(z, 0, -1): #LINE# #TAB# #TAB# digit = 0 #LINE# #TAB# #TAB# mask = 1 << i - 1 #LINE# #TAB# #TAB# if x & mask != 0: #LINE# #TAB# #TAB# #TAB# digit += 1 #LINE# #TAB# #TAB# if y & mask != 0: #LINE# #TAB# #TAB# #TAB# digit += 2 #LINE# #TAB# #TAB# quadkey += str(digit) #LINE# #TAB# return quadkey"
Class decorator to add a logger to a class  <code> def with_logger(cls): ,"#LINE# #TAB# attr_name = '_logger' #LINE# #TAB# cls_name = cls.__qualname__ #LINE# #TAB# module = cls.__module__ #LINE# #TAB# if module is not None: #LINE# #TAB# #TAB# cls_name = module + '.' + cls_name #LINE# #TAB# else: #LINE# #TAB# #TAB# raise AssertionError #LINE# #TAB# setattr(cls, attr_name, logging.getLogger(cls_name)) #LINE# #TAB# return cls"
Returned this particular hazard classification  <code> def layer_hazard_classification(layer): ,#LINE# #TAB# if not layer.keywords.get('hazard'): #LINE# #TAB# #TAB# return None #LINE# #TAB# hazard_classification = None #LINE# #TAB# for classification in hazard_classes_all: #LINE# #TAB# #TAB# classification_name = layer.keywords['classification'] #LINE# #TAB# #TAB# if classification_name == classification['key']: #LINE# #TAB# #TAB# #TAB# hazard_classification = classification #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return hazard_classification
convert a dataframe 's week numbers to period objects <code> def convert_week_numbers(dataframe): ,"#LINE# #TAB# weeks = [key for key, group in dataframe.groupby(['year', 'week'])] #LINE# #TAB# periods = [(week[0], week[1], _period_for_week(*week)) for week in weeks] #LINE# #TAB# period_dataframe = pandas.DataFrame(periods, columns=['year', 'week', #LINE# #TAB# #TAB# 'period']) #LINE# #TAB# merged = pandas.merge(dataframe, period_dataframe, left_on=['year', #LINE# #TAB# #TAB# 'week'], right_on=['year', 'week']) #LINE# #TAB# column_names = dataframe.columns.tolist() #LINE# #TAB# column_names.remove('week') #LINE# #TAB# column_names.remove('year') #LINE# #TAB# column_names.insert(2, 'period') #LINE# #TAB# return merged[column_names]"
defines the way to parse the magic command ` ` % blob_up ` ` <code> def blob_up_parser(): ,"#LINE# #TAB# parser = MagicCommandParser(prog='blob_up', description= #LINE# #TAB# #TAB# 'upload a file on a blob storage, we assume the container is the first element to the remote path' #LINE# #TAB# #TAB# ) #LINE# #TAB# parser.add_argument('localfile', type=str, help='local file to upload') #LINE# #TAB# parser.add_argument('remotepath', type=str, help= #LINE# #TAB# #TAB# 'remote path of the uploaded file') #LINE# #TAB# return parser"
"Populate puzzle based on a dictionary  <code> def load_puzzle_from_dict(cls, puzzle_dict): ","#LINE# #TAB# puzzle = Puzzle(puzzle_dict[cls.PUZZLE_ID_FIELD], puzzle_dict[cls. #LINE# #TAB# #TAB# PUZZLE_MOVES_FIELD]) #LINE# #TAB# for region in puzzle_dict[cls.PUZZLE_REGIONS_FIELD]: #LINE# #TAB# #TAB# region_id = region[cls.REGION_ID_FIELD] #LINE# #TAB# #TAB# region_color = Color.get(region[cls.REGION_COLOR_FIELD]) #LINE# #TAB# #TAB# puzzle.add_region(region_id, region_color) #LINE# #TAB# #TAB# for neighbor_id in region[cls.REGION_NEIGHBORS_FIELD]: #LINE# #TAB# #TAB# #TAB# puzzle.assign_neighbor(region_id, neighbor_id) #LINE# #TAB# return puzzle"
"Determine if directory or not : param client : SSHClient : param file_path : absolute path of file <code> def is_dir(client, file_path): ","#LINE# #TAB# command = ""[ -d '{}' ] && echo True || echo False"".format(file_path) #LINE# #TAB# _, stdout, _ = ssh_execute(client, command) #LINE# #TAB# return stdout.strip() == 'True'"
"Locate all files matching supplied filename pattern in and below supplied root directory . Parameters ---------- pattern : file pattern Pattern used in fnmatch.filter root : directory , default is os.curdir Directory where to start Returns ------- File list <code> def locate_files(pattern, root=os.curdir): ","#LINE# #TAB# matchfiles = [] #LINE# #TAB# for path, dirs, files in os.walk(os.path.abspath(root)): #LINE# #TAB# #TAB# for filename in fnmatch.filter(files, pattern): #LINE# #TAB# #TAB# #TAB# matchfiles.append(os.path.join(path, filename)) #LINE# #TAB# return matchfiles"
"Compares nm with the supplied patterns and returns True if it matches at least one  <code> def match_pattern(nm, patterns): ","#LINE# #TAB# patterns = coerce_to_list(patterns) #LINE# #TAB# for pat in patterns: #LINE# #TAB# #TAB# if fnmatch.fnmatch(nm, pat): #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# return False"
"Collect stacktraces for the given events  <code> def record_stacktrace(runner, params=None): ","#LINE# #TAB# STACKTRACE_EVENTS.clear() #LINE# #TAB# if params: #LINE# #TAB# #TAB# LOGGER.debug('Collect stacktraces for events %r', params) #LINE# #TAB# #TAB# STACKTRACE_EVENTS.update(params) #LINE# #TAB# else: #LINE# #TAB# #TAB# LOGGER.debug(""Don't collect any stacktrace event"") #LINE# #TAB# return {'status': True}"
This takes care of python2/3 differences <code> def to_string(s): ,"#LINE# #TAB# if isinstance(s, str): #LINE# #TAB# #TAB# return s #LINE# #TAB# if isinstance(s, bytes): #LINE# #TAB# #TAB# if sys.version_info[0] == 2: #LINE# #TAB# #TAB# #TAB# return str(s) #LINE# #TAB# #TAB# return s.decode('ascii') #LINE# #TAB# if isinstance(s, list): #LINE# #TAB# #TAB# return [to_string(x) for x in s] #LINE# #TAB# if isinstance(s, np.ndarray): #LINE# #TAB# #TAB# return s.astype(str) #LINE# #TAB# return s"
"Splits the header into lines , putting multi - line headers together  <code> def get_header_lines(header): ","#LINE# #TAB# r = [] #LINE# #TAB# lines = header.split(b'\n') #LINE# #TAB# for line in lines: #LINE# #TAB# #TAB# if line.startswith((b' ', b'\t')): #LINE# #TAB# #TAB# #TAB# r[-1] = r[-1] + line[1:] #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# r.append(line) #LINE# #TAB# return r"
Find the fn tags included in author - notes <code> def author_notes(soup): ,#LINE# #TAB# author_notes = [] #LINE# #TAB# author_notes_section = raw_parser.author_notes(soup) #LINE# #TAB# if author_notes_section: #LINE# #TAB# #TAB# fn_nodes = raw_parser.fn(author_notes_section) #LINE# #TAB# #TAB# for tag in fn_nodes: #LINE# #TAB# #TAB# #TAB# if 'fn-type' in tag.attrs: #LINE# #TAB# #TAB# #TAB# #TAB# if tag['fn-type'] != 'present-address': #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# author_notes.append(node_text(tag)) #LINE# #TAB# return author_notes
"Pylint method to fix no_value_for_parameter error <code> def no_value_for_parameter(editor, item): ","#LINE# #TAB# line_no = item.line_no #LINE# #TAB# return line_no, 0"
test that the spec_info has all the required subfields spec_info : - name : Check that spec_info has all required subfields - level : error - function : openschemas.main.validate.criteria.structure.spec_info <code> def spec_info(spec): ,"#LINE# #TAB# if 'spec_info' not in spec: #LINE# #TAB# #TAB# bot.exit('""spec_info"" key is missing from specification upper level!') #LINE# #TAB# required_fields = [('description', str, False, True), ('full_example', #LINE# #TAB# #TAB# str, True, True), ('version', str, False, True), ('version_date', #LINE# #TAB# #TAB# str, False, True)] #LINE# #TAB# _test_fields(spec['spec_info'], required_fields) #LINE# #TAB# if not re.search('[0-9]{8}T[0-9]{6}', spec['spec_info']['version_date']): #LINE# #TAB# #TAB# bot.exit('spec_info > version_date is malformed: ""YYYYMMDDTHHMMSS""') #LINE# #TAB# return True"
"Get the text from an XML property  <code> def read_property_from_xml(root, path): ","#LINE# #TAB# element = root.find(path, XML_NS) #LINE# #TAB# try: #LINE# #TAB# #TAB# return element.text.strip(' \t\n\r') #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return None"
To load an indexed bam file <code> def load_samfile(sam_file): ,"#LINE# #TAB# ftype = sam_file.split('.')[-1] #LINE# #TAB# if ftype != 'bam' and ftype != 'sam' and ftype != 'cram': #LINE# #TAB# #TAB# print('Error: file type need suffix of bam, sam or cram.') #LINE# #TAB# #TAB# sys.exit(1) #LINE# #TAB# if ftype == 'cram': #LINE# #TAB# #TAB# samfile = pysam.AlignmentFile(sam_file, 'rc') #LINE# #TAB# elif ftype == 'bam': #LINE# #TAB# #TAB# samfile = pysam.AlignmentFile(sam_file, 'rb') #LINE# #TAB# else: #LINE# #TAB# #TAB# samfile = pysam.AlignmentFile(sam_file, 'r') #LINE# #TAB# return samfile"
"Gets a regex that should match the column <code> def get_regex(cls, column: 'Column'): ","#LINE# #TAB# regex = column.value.regex #LINE# #TAB# if isinstance(regex, str): #LINE# #TAB# #TAB# return re.compile('^\\s*' + '\\s*'.join(regex.split()) + '\\s*$', #LINE# #TAB# #TAB# #TAB# re.IGNORECASE) #LINE# #TAB# return regex"
Return the name of file or a description  <code> def describe_file_handle(fthing): ,#LINE# #TAB# if is_block(fthing): #LINE# #TAB# #TAB# return 'block device' #LINE# #TAB# else: #LINE# #TAB# #TAB# return fthing.name
Gets the config for a jig initialized Git repo  <code> def get_jigconfig(gitrepo): ,"#LINE# #TAB# jig_dir = join(gitrepo, JIG_DIR_NAME) #LINE# #TAB# if not repo_jiginitialized(gitrepo): #LINE# #TAB# #TAB# raise GitRepoNotInitialized('This repository has not been initialized.' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# with open(join(jig_dir, JIG_PLUGIN_CONFIG_FILENAME), 'r') as fh: #LINE# #TAB# #TAB# plugins = SafeConfigParser() #LINE# #TAB# #TAB# plugins.readfp(fh) #LINE# #TAB# #TAB# return plugins"
"get Element class by its number <code> def from_atomic_number(cls, number: int) ->Type['Element']: ","#LINE# #TAB# try: #LINE# #TAB# #TAB# element = next(x for x in Element.__subclasses__() if x. #LINE# #TAB# #TAB# #TAB# atomic_number.fget(None) == number) #LINE# #TAB# except StopIteration: #LINE# #TAB# #TAB# raise ValueError(f'Element with number ""{number}"" not found') #LINE# #TAB# return element"
"Return a modified version of a schema . The source schema * must * have a ' version ' attribute and a ' django ' section . The resulting schema is almost a copy of the original one , except for excluded options in the ' django ' section  <code> def derivate_django_schema(schema, exclude=None): ","#LINE# #TAB# if not exclude: #LINE# #TAB# #TAB# return schema #LINE# #TAB# cls = type(schema.__name__, (schema,), {'version': schema.version}) #LINE# #TAB# options = {} #LINE# #TAB# for option in schema().django.options(): #LINE# #TAB# #TAB# if option.name in exclude: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# options[option.name] = option #LINE# #TAB# django_section = type('django', (Section,), options) #LINE# #TAB# setattr(cls, 'django', django_section) #LINE# #TAB# return cls"
Convert a String - based line into a list with input and target data  <code> def line_to_list(line): ,"#LINE# #TAB# elements = line.split(',') #LINE# #TAB# if len(elements) > 1: #LINE# #TAB# #TAB# target = target_string_to_int(elements[4]) #LINE# #TAB# #TAB# full_sample = [float(i) for i in elements[0:4]] #LINE# #TAB# #TAB# full_sample.append(target) #LINE# #TAB# #TAB# return tuple(full_sample) #LINE# #TAB# else: #LINE# #TAB# #TAB# return None"
Describe this timedelta in human - readable terms  <code> def describe_delta(delta): ,"#LINE# #TAB# s = delta.total_seconds() #LINE# #TAB# s = abs(s) #LINE# #TAB# hours, remainder = divmod(s, 3600) #LINE# #TAB# minutes, seconds = divmod(remainder, 60) #LINE# #TAB# if hours: #LINE# #TAB# #TAB# return '%d hr %d min' % (hours, minutes) #LINE# #TAB# if minutes: #LINE# #TAB# #TAB# return '%d min %d secs' % (minutes, seconds) #LINE# #TAB# return '%d secs' % seconds"
Returns the : py : class:`PlannedWorkTime ` of the holder if it does not yet exist one is created <code> def planned_worktime_of_holder(holder): ,#LINE# #TAB# if not 'planned_worktime' in holder: #LINE# #TAB# #TAB# holder['planned_worktime'] = PlannedWorkTime() #LINE# #TAB# return holder['planned_worktime']
Set the value of zero_step_mode  <code> def set_zero_step_mode(): ,"#LINE# #TAB# function = LegacyFunctionSpecification() #LINE# #TAB# function.addParameter('zero_step_mode', dtype='int32', direction= #LINE# #TAB# #TAB# function.IN) #LINE# #TAB# function.result_type = 'int32' #LINE# #TAB# return function"
"Parses the given expression string and returns a query object . Requires Python > = 2.6  <code> def parse_query(expr, optimize_query=True): ",#LINE# #TAB# if not ast_support: #LINE# #TAB# #TAB# raise NotImplementedError('Parsing of CQEs requires Python >= 2.6') #LINE# #TAB# query = _AstParser(expr).parse() #LINE# #TAB# if optimize_query: #LINE# #TAB# #TAB# query = optimize(query) #LINE# #TAB# return query
"Switch the status of a page  <code> def change_status(request, page_id): ",#LINE# #TAB# perm = request.user.has_perm('pages.change_page') #LINE# #TAB# if perm and request.method == 'POST': #LINE# #TAB# #TAB# page = Page.objects.get(pk=page_id) #LINE# #TAB# #TAB# page.status = int(request.POST['status']) #LINE# #TAB# #TAB# page.invalidate() #LINE# #TAB# #TAB# page.save() #LINE# #TAB# #TAB# return HttpResponse(str(page.status)) #LINE# #TAB# raise Http404
Assigns new ids to every cleft segment <code> def assign_unique_ids_serial(cleft_info_arr): ,"#LINE# #TAB# chunk_id_maps = empty_obj_array(cleft_info_arr.shape) #LINE# #TAB# df_parts = [] #LINE# #TAB# next_id = 1 #LINE# #TAB# for x, y, z in np.ndindex(cleft_info_arr.shape): #LINE# #TAB# #TAB# new_df = cleft_info_arr[x, y, z] #LINE# #TAB# #TAB# chunk_id_maps[x, y, z], next_id = new_id_map(new_df, next_id) #LINE# #TAB# #TAB# df_parts.append(remap_ids(new_df, chunk_id_maps[x, y, z])) #LINE# #TAB# full_df = pd.concat(df_parts, copy=False) #LINE# #TAB# return full_df, chunk_id_maps"
return test counts that are set via pyt environment variables when pyt runs the test <code> def get_counts(): ,"#LINE# #TAB# counts = {} #LINE# #TAB# ks = [ #LINE# #TAB# #TAB# ('PYT_TEST_CLASS_COUNT', ""classes""), #LINE# #TAB# #TAB# ('PYT_TEST_COUNT', ""tests""), #LINE# #TAB# #TAB# ('PYT_TEST_MODULE_COUNT', ""modules""), #LINE# #TAB# ] #LINE# #TAB# for ek, cn in ks: #LINE# #TAB# #TAB# counts[cn] = int(os.environ.get(ek, 0)) #LINE# #TAB# return counts"
"Enable Server in haproxy <code> def enable_server(name, backend, socket=DEFAULT_SOCKET_URL): ","#LINE# #TAB# if backend == '*': #LINE# #TAB# #TAB# backends = show_backends(socket=socket).split('\n') #LINE# #TAB# else: #LINE# #TAB# #TAB# backends = [backend] #LINE# #TAB# results = {} #LINE# #TAB# for backend in backends: #LINE# #TAB# #TAB# ha_conn = _get_conn(socket) #LINE# #TAB# #TAB# ha_cmd = haproxy.cmds.enableServer(server=name, backend=backend) #LINE# #TAB# #TAB# ha_conn.sendCmd(ha_cmd) #LINE# #TAB# #TAB# results[backend] = list_servers(backend, socket=socket) #LINE# #TAB# return results"
"Copy docstring from a plotly.io function to a Figure method , removing the fig argument docstring in the process <code> def copy_doc_without_fig(from_fn, to_method): ","#LINE# #TAB# docstr = _re.sub(' {4}fig:(?:.*?\\n)*? {4}(\\w+)', '#TAB# \\1', from_fn. #LINE# #TAB# #TAB# __doc__) #LINE# #TAB# if sys.version_info[0] < 3: #LINE# #TAB# #TAB# to_method.__func__.__doc__ = docstr #LINE# #TAB# else: #LINE# #TAB# #TAB# to_method.__doc__ = docstr"
Calculate RPS for all requests Args : data_frame ( DataFrame ) : data Returns : int : Requests per second <code> def get_rps(data_frame): ,"#LINE# #TAB# from_date = data_frame.iloc[0].time #LINE# #TAB# to_date = data_frame.iloc[-1].time #LINE# #TAB# duration = to_date - from_date #LINE# #TAB# if duration < 0: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Incorrect time values from_date > to_data (%f > %f)' % ( #LINE# #TAB# #TAB# #TAB# from_date, to_date)) #LINE# #TAB# if duration == 0: #LINE# #TAB# #TAB# duration = 1 #LINE# #TAB# requests_count = data_frame.shape[0] #LINE# #TAB# return requests_count / duration"
Returns parent repo or input path if none found  <code> def get_repo_parent(path): ,"#LINE# #TAB# if is_repo(path): #LINE# #TAB# #TAB# return Local(path) #LINE# #TAB# elif not os.path.isdir(path): #LINE# #TAB# #TAB# _rel = '' #LINE# #TAB# #TAB# while path and path != '/': #LINE# #TAB# #TAB# #TAB# if is_repo(path): #LINE# #TAB# #TAB# #TAB# #TAB# return Local(path) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# _rel = os.path.join(os.path.basename(path), _rel) #LINE# #TAB# #TAB# #TAB# #TAB# path = os.path.dirname(path) #LINE# #TAB# #TAB# return path"
"Get the nearest token which clearly identifies the next statement <code> def nearest_token(dit: str, tokens: List[str]) ->str: ","#LINE# #TAB# occurs = [] #LINE# #TAB# for token in tokens: #LINE# #TAB# #TAB# occur = dit.find(token) #LINE# #TAB# #TAB# if occur != -1: #LINE# #TAB# #TAB# #TAB# occurs.append((occur, token)) #LINE# #TAB# if len(occurs) == 0: #LINE# #TAB# #TAB# raise ParseError(f'Found no tokens: {tokens}') #LINE# #TAB# index, token = min(occurs) #LINE# #TAB# return dit[index:index + len(token)]"
"Price has a set of requirements : - period - > [ minute , hour , day ] - exchange - open - close - high - low - volumefrom - volumeto Ensure that we have all of them for inserts . Will <code> def price_insert_filter(price_dict): ","#LINE# #TAB# if not isinstance(price_dict, dict): #LINE# #TAB# #TAB# raise TypeError('dict_list is not a list. Please try again') #LINE# #TAB# required = ['period', 'exchange', 'open', 'close', 'high', 'low', #LINE# #TAB# #TAB# 'volumefrom', 'volumeto'] #LINE# #TAB# pkeys = price_dict.keys() #LINE# #TAB# for r in required: #LINE# #TAB# #TAB# if r not in pkeys: #LINE# #TAB# #TAB# #TAB# raise KeyError('An important key is not available') #LINE# #TAB# return True"
"Get partial form based on original Form and fields set . : param Form : django.forms . ModelForm : param list fields : list of field names  <code> def get_partial_form(cls, Form, fields): ","#LINE# #TAB# if not fields: #LINE# #TAB# #TAB# return Form #LINE# #TAB# if not set(fields) <= set(Form.base_fields.keys()): #LINE# #TAB# #TAB# fields = set(fields) & set(Form.base_fields.keys()) #LINE# #TAB# meta_attributes = dict(fields=list(fields)) #LINE# #TAB# meta_bases = Form.Meta, #LINE# #TAB# if not issubclass(Form.Meta, object): #LINE# #TAB# #TAB# meta_bases += object, #LINE# #TAB# PartialForm = type('PartialForm', (Form,), {'Meta': type('Meta', #LINE# #TAB# #TAB# meta_bases, meta_attributes)}) #LINE# #TAB# return PartialForm"
"Convert class labels from scalars to one - hot vectors  <code> def dense_to_one_hot(labels_dense, num_classes): ","#LINE# #TAB# num_labels = labels_dense.shape[0] #LINE# #TAB# index_offset = np.arange(num_labels) * num_classes #LINE# #TAB# labels_one_hot = np.zeros((num_labels, num_classes)) #LINE# #TAB# labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1 #LINE# #TAB# return labels_one_hot"
"This function gets as arguments an array of dicts through the dicts_objects parameter then it ll return the dicts that have a value value_of_filter of the key field_to_filter  <code> def dicts_filter(dicts_object, field_to_filter, value_of_filter): ","#LINE# #TAB# lambda_query = lambda value: value[field_to_filter] == value_of_filter #LINE# #TAB# filtered_coin = filter(lambda_query, dicts_object) #LINE# #TAB# selected_coins = list(filtered_coin) #LINE# #TAB# return selected_coins"
"Determine if a zone file hash was sent by a name . Return True if so false if not <code> def namedb_is_name_zonefile_hash(cur, name, zonefile_hash): ","#LINE# #TAB# select_query = 'SELECT COUNT(value_hash) FROM history WHERE history_id = ? AND value_hash = ?' #LINE# #TAB# select_args = (name,zonefile_hash) #LINE# #TAB# rows = namedb_query_execute(cur, select_query, select_args) #LINE# #TAB# count = None #LINE# #TAB# for r in rows: #LINE# #TAB# #TAB# count = r['COUNT(value_hash)'] #LINE# #TAB# #TAB# break #LINE# #TAB# return count > 0"
"Deletes a server from RightScale : param server_href : URL representing the server to delete : param nickname : ( optional ) String representing the nickname of the server : return : Boolean of operation success / failure <code> def delete_server(server_href, nickname=None): ","#LINE# #TAB# return _request(_lookup_server(server_href, nickname), method='DELETE', #LINE# #TAB# #TAB# prepend_api_base=False).status_code == 200"
> > > nice_hex(0x1 ) ' $ 01 ' > > > nice_hex(0x123 ) ' $ 0123 ' <code> def nice_hex(v): ,#LINE# #TAB# if v < 256: #LINE# #TAB# #TAB# return '$%02x' % v #LINE# #TAB# if v < 65536: #LINE# #TAB# #TAB# return '$%04x' % v #LINE# #TAB# return '$%x' % v
Helper function : return True if ' value ' is ' list - like '  <code> def is_multi_value(value): ,"#LINE# #TAB# if isString(value) or isinstance(value, bytes): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# iter(value) #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Given odds ratio , calculate effect size  <code> def convert_or_to_beta(data): ",#LINE# #TAB# if EFFECT_SIZE in data.columns: #LINE# #TAB# #TAB# logging.debug(f' {EFFECT_SIZE} is included.') #LINE# #TAB# elif ODDS_RATIO in data.columns and EFFECT_SIZE not in data.columns: #LINE# #TAB# #TAB# data[EFFECT_SIZE] = _or_to_beta(data[ODDS_RATIO]) #LINE# #TAB# else: #LINE# #TAB# #TAB# logging.warning( #LINE# #TAB# #TAB# #TAB# f' {EFFECT_SIZE} could not be estimated from available data.') #LINE# #TAB# return data
Get OS hardening apt audits  <code> def get_audits(): ,"#LINE# #TAB# audits = [AptConfig([{'key': 'APT::Get::AllowUnauthenticated', #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# 'expected': 'false'}])] #LINE# #TAB# settings = get_settings('os') #LINE# #TAB# clean_packages = settings['security']['packages_clean'] #LINE# #TAB# if clean_packages: #LINE# #TAB# #TAB# security_packages = settings['security']['packages_list'] #LINE# #TAB# #TAB# if security_packages: #LINE# #TAB# #TAB# #TAB# audits.append(RestrictedPackages(security_packages)) #LINE# #TAB# return audits"
"Gets a Tensor of type dtype 0 if tol is None validation optional  <code> def get_tol(tol, dtype, validate_args): ","#LINE# if tol is None: #LINE# #TAB# return tf.convert_to_tensor(value=0, dtype=dtype) #LINE# tol = tf.convert_to_tensor(value=tol, dtype=dtype) #LINE# if validate_args: #LINE# #TAB# tol = distribution_util.with_dependencies([ #LINE# #TAB# #TAB# assert_util.assert_non_negative( #LINE# #TAB# #TAB# #TAB# tol, message=""Argument 'tol' must be non-negative"") #LINE# #TAB# ], tol) #LINE# return tol"
Download a file pointed to by url to a temp file on local disk <code> def download_file(url): ,"#LINE# try: #LINE# #TAB# (local_file, headers) = urllib.urlretrieve(url) #LINE# except: #LINE# #TAB# sys.exit(""ERROR: Problem downloading config file. Please check the URL ("" + url + ""). Exiting..."") #LINE# return local_file"
"Create a skewed threshold graph with a given number of vertices ( n ) and a given number of edges ( m ) . The routine returns an unlabeled creation sequence for the threshold graph . FIXME : describe algorithm <code> def left_d_threshold_sequence(n, m): ",#LINE# #TAB# cs = ['d'] + ['i'] * (n - 1) #LINE# #TAB# if m < n: #LINE# #TAB# #TAB# cs[m] = 'd' #LINE# #TAB# #TAB# return cs #LINE# #TAB# if m > n * (n - 1) / 2: #LINE# #TAB# #TAB# raise ValueError('Too many edges for this many nodes.') #LINE# #TAB# cs[n - 1] = 'd' #LINE# #TAB# sum = n - 1 #LINE# #TAB# ind = 1 #LINE# #TAB# while sum < m: #LINE# #TAB# #TAB# cs[ind] = 'd' #LINE# #TAB# #TAB# sum += ind #LINE# #TAB# #TAB# ind += 1 #LINE# #TAB# if sum > m: #LINE# #TAB# #TAB# cs[sum - m] = 'i' #LINE# #TAB# return cs
Project a chromagram on the tonnetz  <code> def to_tonnetz(chromagram): ,"#LINE# #TAB# if np.sum(np.abs(chromagram)) == 0.: #LINE# #TAB# #TAB# return np.zeros(6) #LINE# #TAB# _tonnetz = np.dot(__TONNETZ_MATRIX, chromagram) #LINE# #TAB# one_norm = np.sum(np.abs(_tonnetz)) #LINE# #TAB# _tonnetz = _tonnetz / float(one_norm) #LINE# #TAB# return _tonnetz"
Wait until the VM is powered down and stopped . : param qmp : QMP handle : type qmp : QEMUMonitorProtocol <code> def wait_shutdown(qmp): ,#LINE# #TAB# logger = logging.getLogger(__name__) #LINE# #TAB# context_logger = alpy.utils.make_context_logger(logger) #LINE# #TAB# with context_logger('Wait until the VM is powered down'): #LINE# #TAB# #TAB# while qmp.pull_event(wait=True)['event'] != 'SHUTDOWN': #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# with context_logger('Wait until the VM is stopped'): #LINE# #TAB# #TAB# while qmp.pull_event(wait=True)['event'] != 'STOP': #LINE# #TAB# #TAB# #TAB# pass
is this requirement cyclic? <code> def req_cycle(req): ,"#LINE# #TAB# cls = req.__class__ #LINE# #TAB# seen = {req.name} #LINE# #TAB# while isinstance(req.comes_from, cls): #LINE# #TAB# #TAB# req = req.comes_from #LINE# #TAB# #TAB# if req.name in seen: #LINE# #TAB# #TAB# #TAB# return True #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# seen.add(req.name) #LINE# #TAB# return False"
"Get randomly oriented unit vector . : param rstate : : py : class:`numpy.random . RandomState ` object , can be used to create reproducible pseudo - random sequences <code> def random_axis(rstate=None): ",#LINE# #TAB# rstate = rstate or num.random #LINE# #TAB# while True: #LINE# #TAB# #TAB# axis = rstate.uniform(size=3) * 2.0 - 1.0 #LINE# #TAB# #TAB# uabs = math.sqrt(num.sum(axis ** 2)) #LINE# #TAB# #TAB# if 0.001 < uabs < 1.0: #LINE# #TAB# #TAB# #TAB# return axis / uabs
"Konversi url sebagai gambar ( PNG atau JPEG )  <code> def convert_to_image(url, format, datauri=False, absolute_link=True): ","#LINE# #TAB# retVal = html_converter(url, 'image', format, absolute_link) #LINE# #TAB# if retVal and datauri: #LINE# #TAB# #TAB# mimetype = 'image/' + format.lower() #LINE# #TAB# #TAB# retVal = convert_to_data_uri(retVal, mimetype) #LINE# #TAB# return retVal"
Checks if path is in a git repository . Args : path ( str ) : The path to check . Returns : bool : Whether the path is a git repository  <code> def is_git_repository(path): ,"#LINE# #TAB# if os.path.isfile(path): #LINE# #TAB# #TAB# path = os.path.dirname(path) #LINE# #TAB# with open(os.devnull, 'wb') as devnull: #LINE# #TAB# #TAB# proc = subprocess.Popen(['git', 'rev-parse', #LINE# #TAB# #TAB# #TAB# '--is-inside-work-tree'], cwd=path, stdout=devnull, stderr=devnull) #LINE# #TAB# #TAB# proc.wait() #LINE# #TAB# return proc.returncode == 0"
Transform a value in preparation for serializing as json <code> def safe_for_serialization(value): ,"#LINE# #TAB# if isinstance(value, six.string_types): #LINE# #TAB# #TAB# return value #LINE# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# return { #LINE# #TAB# #TAB# #TAB# safe_for_serialization(key): safe_for_serialization(val) #LINE# #TAB# #TAB# #TAB# for key, val in six.iteritems(value) #LINE# #TAB# #TAB# } #LINE# #TAB# if isinstance(value, collections.Iterable): #LINE# #TAB# #TAB# return list(map(safe_for_serialization, value)) #LINE# #TAB# try: #LINE# #TAB# #TAB# return six.text_type(value) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return '[__unicode__ failed]'"
Checks if the given type is a string type  <code> def is_string_type(type_): ,#LINE# #TAB# string_types = _get_types(Types.STRING) #LINE# #TAB# if is_typing_type(type_): #LINE# #TAB# #TAB# return type_ in string_types or is_regex_type(type_) #LINE# #TAB# return type_ in string_types
Remove ISA - Tab specific information from Header[real name ] headers  <code> def clean_header(header): ,"#LINE# #TAB# if header.find('[') >= 0: #LINE# #TAB# #TAB# header = header.replace(']', '').split('[')[-1] #LINE# #TAB# try: #LINE# #TAB# #TAB# int(header[0]) #LINE# #TAB# #TAB# header = 'isa_' + header #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return header"
Build message to transfer over the socket from a request  <code> def build_request(request): ,"#LINE# #TAB# msg = bytes([request['cmd']]) #LINE# #TAB# if 'dest' in request: #LINE# #TAB# #TAB# msg += bytes([request['dest']]) #LINE# #TAB# else: #LINE# #TAB# #TAB# msg += b'\0' #LINE# #TAB# if 'sha' in request: #LINE# #TAB# #TAB# msg += request['sha'] #LINE# #TAB# else: #LINE# #TAB# #TAB# for dummy in range(64): #LINE# #TAB# #TAB# #TAB# msg += b'0' #LINE# #TAB# logging.debug(""Request (%d): %s"", len(msg), msg) #LINE# #TAB# return msg"
Return the filename of the javascript file  <code> def get_webasset_js_file(cls): ,#LINE# #TAB# if cls.webasset_js_file: #LINE# #TAB# #TAB# return cls.webasset_js_file
Yield a patient ID and path for each patient folder in data_folder  <code> def generate_patient_paths(data_folder): ,"#LINE# #TAB# for filename in os.listdir(data_folder): #LINE# #TAB# #TAB# filepath = os.path.join(data_folder, filename) #LINE# #TAB# #TAB# if not os.path.isdir(filepath): #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# match = PATIENT_FOLDER_RE.match(filename) #LINE# #TAB# #TAB# if match is None: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# yield match.group(1), filepath"
Decode the callbacks to an executable form  <code> def decode_callbacks(encoded_callbacks): ,"#LINE# #TAB# from furious.async import Async #LINE# #TAB# callbacks = {} #LINE# #TAB# for event, callback in encoded_callbacks.iteritems(): #LINE# #TAB# #TAB# if isinstance(callback, dict): #LINE# #TAB# #TAB# #TAB# async_type = Async #LINE# #TAB# #TAB# #TAB# if '_type' in callback: #LINE# #TAB# #TAB# #TAB# #TAB# async_type = path_to_reference(callback['_type']) #LINE# #TAB# #TAB# #TAB# callback = async_type.from_dict(callback) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# callback = path_to_reference(callback) #LINE# #TAB# #TAB# callbacks[event] = callback #LINE# #TAB# return callbacks"
"Saves the attributes of an object to the specified json file Parameters ---------- obj : flightline_project . FlightlineProject ( ) json_file : str - file location <code> def dump_to_projectconfig(obj, json_file): ","#LINE# #TAB# _json_file_ = open(json_file, 'w') #LINE# #TAB# json.dump(obj.__dict__, _json_file_) #LINE# #TAB# _json_file_.close() #LINE# #TAB# return"
"Checks an exception for given keywords and raises a new ActionError with the desired message if the keywords are found . This allows selective control over API error messages  <code> def check_message(keywords, message): ","#LINE# #TAB# exc_type, exc_value, exc_traceback = sys.exc_info() #LINE# #TAB# if set(str(exc_value).split("" "")).issuperset(set(keywords)): #LINE# #TAB# #TAB# exc_value.message = message #LINE# #TAB# #TAB# raise"
"Creates an extractor from each token in a sentence  <code> def make_response_extractor(resp_dict, key, lang): ",#LINE# #TAB# for sentence in resp_dict['sentences']: #LINE# #TAB# #TAB# for token in sentence['tokens']: #LINE# #TAB# #TAB# #TAB# key = ('word' if lang is SupportedLang.ENGLISH else #LINE# #TAB# #TAB# #TAB# #TAB# 'word_translation') #LINE# #TAB# #TAB# #TAB# yield token[key]
"Find the index where youy value appears in a list of percentiles : param value : : param percentiles : : return : example : <code> def find_percentile(value, percentiles): ",#LINE# #TAB# diffs = abs(value - percentiles) #LINE# #TAB# ii = np.argmin(diffs) #LINE# #TAB# percentile = ii #LINE# #TAB# return percentile
"Get SNMP Credential that matches name and protocol  <code> def get_snmp_cred_by_name_and_protocol(context, name, proto_type): ","#LINE# #TAB# try: #LINE# #TAB# #TAB# query = context.session.query(models.BNPSNMPCredential) #LINE# #TAB# #TAB# snmp_creds = query.filter_by(name=name, protocol_type=proto_type).all() #LINE# #TAB# except exc.NoResultFound: #LINE# #TAB# #TAB# LOG.info(_LI( #LINE# #TAB# #TAB# #TAB# 'no snmp credential found with name: %(name)s and protocol: %(proto_type)s' #LINE# #TAB# #TAB# #TAB# ), {'name': name, 'proto_type': proto_type}) #LINE# #TAB# #TAB# return #LINE# #TAB# return snmp_creds"
Get the full path to gdcmconv . If not found raise error <code> def get_gdcmconv(): ,#LINE# #TAB# gdcmconv_executable = settings.gdcmconv_path #LINE# #TAB# if gdcmconv_executable is None: #LINE# #TAB# #TAB# gdcmconv_executable = _which('gdcmconv') #LINE# #TAB# if gdcmconv_executable is None: #LINE# #TAB# #TAB# gdcmconv_executable = _which('gdcmconv.exe') #LINE# #TAB# if gdcmconv_executable is None: #LINE# #TAB# #TAB# raise ConversionError('GDCMCONV_NOT_FOUND') #LINE# #TAB# return gdcmconv_executable
This method is used to calculate the area of a polygon  <code> def calculate_polygon_area(polygon: Polygon) ->float: ,"#LINE# #TAB# polygon_aea = transform(partial(pyproj.transform, pyproj.Proj(init= #LINE# #TAB# #TAB# 'EPSG:4326'), pyproj.Proj(proj='aea', lat_1=polygon.bounds[1], #LINE# #TAB# #TAB# lat_2=polygon.bounds[3])), polygon) #LINE# #TAB# return polygon_aea.area"
"Checks that a basis set exists and if not raises a helpful exception <code> def cli_check_basis(name, data_dir): ","#LINE# #TAB# if name is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# name = misc.transform_basis_name(name) #LINE# #TAB# metadata = api.get_metadata(data_dir) #LINE# #TAB# if not name in metadata: #LINE# #TAB# #TAB# errstr = ""Basis set '"" + name + ""' does not exist.\n"" #LINE# #TAB# #TAB# errstr += ""For a complete list of basis sets, use the 'bse list-basis-sets' command"" #LINE# #TAB# #TAB# raise RuntimeError(errstr) #LINE# #TAB# return name"
"Convert a value into a table cell . Args : entry : The value to decorate width : Final size of the table cell in characters prepend : Prepend table border character Returns : Decorated table cell <code> def decorate_entry(entry, width=None, prepend=False): ","#LINE# #TAB# if width: #LINE# #TAB# #TAB# md_entry = f"" {entry}{' ' * (width - len(entry))} |"" #LINE# #TAB# else: #LINE# #TAB# #TAB# md_entry = f' {entry} |' #LINE# #TAB# if prepend: #LINE# #TAB# #TAB# md_entry = f'|{md_entry}' #LINE# #TAB# return md_entry"
"Merge the exitcases of two Links . Args : exit1 : The exitcase of a Link object . exit2 : Another exitcase to merge with exit1 . Returns : The merged exitcases  <code> def merge_exitcases(exit1, exit2): ","#LINE# #TAB# if exit1: #LINE# #TAB# #TAB# if exit2: #LINE# #TAB# #TAB# #TAB# return ast.BoolOp(ast.And(), values=[exit1, exit2]) #LINE# #TAB# #TAB# return exit1 #LINE# #TAB# return exit2"
"Helper function for merging config dictionaries <code> def dict_merge(src, dest): ","#LINE# #TAB# for key, value in src.items(): #LINE# #TAB# #TAB# if isinstance(value, dict): #LINE# #TAB# #TAB# #TAB# dict_merge(value, dest.setdefault(key, {})) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# dest[key] = value #LINE# #TAB# return dest"
Create a formatted time stamp of current time @return : Time stamp of the current time ( Day Month Date HH : MM : SS Year ) @rtype : string <code> def time_stamp(): ,#LINE# #TAB# now = time.localtime(time.time()) #LINE# #TAB# now = time.asctime(now) #LINE# #TAB# return now
Check if a column is DateTime ( or implements DateTime ) : param Column col : the column object to be checked : rtype : bool <code> def is_datetime_column(col): ,"#LINE# #TAB# from sqlalchemy import DateTime #LINE# #TAB# if not isinstance(col, Column): #LINE# #TAB# #TAB# return False #LINE# #TAB# if hasattr(col.type, 'impl'): #LINE# #TAB# #TAB# return type(col.type.impl) is DateTime #LINE# #TAB# else: #LINE# #TAB# #TAB# return type(col.type) is DateTime"
"create_definition_list : create_definition_list ' , ' create_definition <code> def p_create_definition_list_many(p): ",#LINE# #TAB# create_definition_list = p[1] #LINE# #TAB# create_definition_list[p[3][0]] = p[3][1] #LINE# #TAB# p[0] = create_definition_list
"If user does n't provide any padding around the mutation we need to at least include enough of the surrounding non - mutated esidues to construct candidate epitopes of the specified lengths  <code> def check_padding_around_mutation(given_padding, epitope_lengths): ","#LINE# #TAB# min_required_padding = max(epitope_lengths) - 1 #LINE# #TAB# if not given_padding: #LINE# #TAB# #TAB# return min_required_padding #LINE# #TAB# else: #LINE# #TAB# #TAB# require_integer(given_padding, 'Padding around mutation') #LINE# #TAB# #TAB# if given_padding < min_required_padding: #LINE# #TAB# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# #TAB# 'Padding around mutation %d cannot be less than %d for epitope lengths %s' #LINE# #TAB# #TAB# #TAB# #TAB# % (given_padding, min_required_padding, epitope_lengths)) #LINE# #TAB# #TAB# return given_padding"
unloads username from default POST request <code> def username_from_request(request): ,#LINE# #TAB# if config.USERNAME_FORM_FIELD in request.POST: #LINE# #TAB# #TAB# return request.POST[config.USERNAME_FORM_FIELD][:255] #LINE# #TAB# return None
"Utility function to obtain a smoothed random checker board  <code> def random_checkerboard_smooth(cell_width, sigma): ","#LINE# #TAB# texture = ['vehicles.library.textures.RandomCheckerboard', dict( #LINE# #TAB# #TAB# cell_width=cell_width, seed=np.random.randint(100000))] #LINE# #TAB# return ['vehicles.library.textures.Smoothed', dict(sigma=sigma, texture #LINE# #TAB# #TAB# =texture)]"
Returns the plural verbose name for the given formset <code> def formset_verbose_name_plural(formset): ,"#LINE# #TAB# if getattr(formset, 'verbose_name_plural'): #LINE# #TAB# #TAB# return formset.verbose_name_plural #LINE# #TAB# if getattr(formset, 'model'): #LINE# #TAB# #TAB# return formset.model._meta.verbose_name_plural #LINE# #TAB# return formset_verbose_name(formset) + 's'"
Initializes all values that might be needed in order to avoid exceptions . : param repo_details : details associated with the analyzed repository . : return : <code> def initialize_values(repo_details: dict) ->None: ,#LINE# #TAB# if 'issue_durations' not in repo_details: #LINE# #TAB# #TAB# repo_details['issue_durations'] = list() #LINE# #TAB# if 'open_pull_requests' not in repo_details: #LINE# #TAB# #TAB# repo_details['open_pull_requests'] = 0 #LINE# #TAB# if 'open_issues' not in repo_details: #LINE# #TAB# #TAB# repo_details['open_issues'] = 0
"Sort a stream based on a lambda expression Parameters ------------ stream Stream sort_function Sort function reverse Boolean ( sort by reverse order ) Returns ------------ stream Sorted stream <code> def sort_lambda_stream(stream, sort_function, reverse=False): ","#LINE# #TAB# new_event_log = deepcopy(stream) #LINE# #TAB# new_event_log._list.sort(key=sort_function, reverse=reverse) #LINE# #TAB# return new_event_log"
Import a dotted module path and return the attribute / class designated by the last name in the path . Raise ImportError if the import failed . Inspired by Django <code> def import_string(dotted_path): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# module_path, class_name = dotted_path.rsplit('.', 1) #LINE# #TAB# except ValueError as err: #LINE# #TAB# #TAB# raise ImportError(""%s doesn't look like a module path"" % dotted_path #LINE# #TAB# #TAB# #TAB# ) from err #LINE# #TAB# module = import_module(module_path) #LINE# #TAB# try: #LINE# #TAB# #TAB# return getattr(module, class_name) #LINE# #TAB# except AttributeError as err: #LINE# #TAB# #TAB# raise ImportError( #LINE# #TAB# #TAB# #TAB# 'Module ""{0}"" does not define a ""{1}"" attribute/class'.format( #LINE# #TAB# #TAB# #TAB# module_path, class_name)) from err"
"if_match_return_true = True means find duplicates if_match_return_true = False means find if there is only one copy of it  <code> def get_duplicate_files(size_dict, topdir): ","#LINE# #TAB# files = [] #LINE# #TAB# if_match_return_true = True #LINE# #TAB# files = [os.path.abspath(os.path.join(dirpath, filename)) for dirpath, #LINE# #TAB# #TAB# dirnames, files in os.walk(topdir) for filename in files if os.path #LINE# #TAB# #TAB# .getsize(os.path.join(dirpath, filename)) in size_dict and is_match #LINE# #TAB# #TAB# (os.path.getsize(os.path.join(dirpath, filename)), os.path.abspath( #LINE# #TAB# #TAB# os.path.join(dirpath, filename)), size_dict[os.path.getsize(os.path #LINE# #TAB# #TAB# .join(dirpath, filename))], if_match_return_true)] #LINE# #TAB# return files"
Convert a few common variations of true and false to boolean <code> def convert_to_bool(text): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return bool(int(text)) #LINE# #TAB# except: #LINE# #TAB# #TAB# pass #LINE# #TAB# text = str(text).lower() #LINE# #TAB# if text == ""true"": #LINE# #TAB# #TAB# return True #LINE# #TAB# if text == ""yes"": #LINE# #TAB# #TAB# return True #LINE# #TAB# if text == ""false"": #LINE# #TAB# #TAB# return False #LINE# #TAB# if text == ""no"": #LINE# #TAB# #TAB# return False #LINE# #TAB# raise ValueError"
Input a list of labeled tuples and return a dictionary of sequentially labeled regions  <code> def label_sequential_regions(inlist): ,"#LINE# #TAB# import more_itertools as mit #LINE# #TAB# df = pd.DataFrame(inlist).set_index(0) #LINE# #TAB# labeled = {} #LINE# #TAB# for label in df[1].unique(): #LINE# #TAB# #TAB# iterable = df[df[1] == label].index.tolist() #LINE# #TAB# #TAB# labeled.update({'{}{}'.format(label, i + 1): items for i, items in #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# enumerate([list(group) for group in mit.consecutive_groups(iterable)])}) #LINE# #TAB# return labeled"
Formats timestamp to human readable format <code> def format_time(timestamp): ,#LINE# #TAB# format_string = '%Y_%m_%d_%Hh%Mm%Ss' #LINE# #TAB# formatted_time = datetime.datetime.fromtimestamp(timestamp).strftime(format_string) #LINE# #TAB# return formatted_time
"Ca n't compare relativedeltas directly , so as a hack , add it to a datetime and compare those  <code> def is_positive_relativedelta(rd: relativedelta) ->bool: ",#LINE# #TAB# now = datetime.now() #LINE# #TAB# now_delta = now + rd #LINE# #TAB# return now_delta >= now
Return a new list of roots sorted first on the real part then the imag  <code> def sort_roots(roots): ,"#LINE# #TAB# lst = [] #LINE# #TAB# for i, r in enumerate(roots): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# lst.append((r.real, r.imag, i, r)) #LINE# #TAB# #TAB# except AttributeError: #LINE# #TAB# #TAB# #TAB# lst.append((r, 0, i, r)) #LINE# #TAB# lst.sort() #LINE# #TAB# return [r[-1] for r in lst]"
"Inferes an annotation node . This means that it inferes the part of ` int ` here : foo : int = 3 Also checks for forward references ( strings ) <code> def infer_annotation(context, annotation): ","#LINE# #TAB# value_set = context.infer_node(annotation) #LINE# #TAB# if len(value_set) != 1: #LINE# #TAB# #TAB# debug.warning( #LINE# #TAB# #TAB# #TAB# 'Inferred typing index %s should lead to 1 object, not %s' % ( #LINE# #TAB# #TAB# #TAB# annotation, value_set)) #LINE# #TAB# #TAB# return value_set #LINE# #TAB# inferred_value = list(value_set)[0] #LINE# #TAB# if is_string(inferred_value): #LINE# #TAB# #TAB# result = _get_forward_reference_node(context, inferred_value. #LINE# #TAB# #TAB# #TAB# get_safe_value()) #LINE# #TAB# #TAB# if result is not None: #LINE# #TAB# #TAB# #TAB# return context.infer_node(result) #LINE# #TAB# return value_set"
Gets the version of the IgBLAST executable Arguments : exec ( str ) : the name or path to the igblastn executable . Returns : str : version number  <code> def get_ig_blastversion(exec=default_igblastn_exec): ,"#LINE# #TAB# cmd = [exec, '-version'] #LINE# #TAB# try: #LINE# #TAB# #TAB# stdout_str = check_output(cmd, stderr=STDOUT, shell=False, #LINE# #TAB# #TAB# #TAB# universal_newlines=True) #LINE# #TAB# except CalledProcessError as e: #LINE# #TAB# #TAB# printError('Running command: %s\n%s' % (' '.join(cmd), e.output)) #LINE# #TAB# match = re.search('(?<=Package: igblast )(\\d+\\.\\d+\\.\\d+)', stdout_str) #LINE# #TAB# version = match.group(0) #LINE# #TAB# return version"
Display a search form for searching the list  <code> def search_form(cl): ,"#LINE# #TAB# return {'cl': cl, 'show_result_count': cl.result_count != cl. #LINE# #TAB# #TAB# full_result_count, 'search_var': SEARCH_VAR}"
"Checks if a string is an "" A "" command . Args : command : a string . Examples : "" "" @2 "" , "" "" D = A "" . Returns : A boolean . True , or False <code> def is_a_command(command: str) ->bool: ",#LINE# #TAB# first_char = command[0] #LINE# #TAB# if first_char == '@': #LINE# #TAB# #TAB# return True #LINE# #TAB# return False
"Initializes a pygit Repository instance from a remote repo at ' url ' <code> def repo_from_url(url, remote_name='origin'): ","#LINE# #TAB# tempdir = tempfile.mkdtemp() #LINE# #TAB# repo = pygit2.init_repository(tempdir, False) #LINE# #TAB# remote = repo.create_remote(remote_name, url) #LINE# #TAB# remote.fetch() #LINE# #TAB# return repo, remote"
Generates an i d for a state It simply uses a global counter that is increased each time . It is intended for the name of a new state . : return : a new state machine i d <code> def generate_state_name_id(): ,#LINE# #TAB# global state_name_counter #LINE# #TAB# state_name_counter += 1 #LINE# #TAB# return state_name_counter
Check if the given location represents a valid cellular component  <code> def get_valid_location(location): ,#LINE# #TAB# if location is not None and cellular_components.get(location) is None: #LINE# #TAB# #TAB# loc = cellular_components_reverse.get(location) #LINE# #TAB# #TAB# if loc is None: #LINE# #TAB# #TAB# #TAB# raise InvalidLocationError(location) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return loc #LINE# #TAB# return location
Load user - supplied value as a JSON dict . : param str user_value : User - supplied value to load as a JSON dict <code> def process_json(user_value): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# user_dict = json.loads(user_value, object_pairs_hook=OrderedDict) #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# raise click.UsageError('Unable to decode to JSON.') #LINE# #TAB# if not isinstance(user_dict, dict): #LINE# #TAB# #TAB# raise click.UsageError('Requires JSON dict.') #LINE# #TAB# return user_dict"
"Must be called by frontend when host is ready . The call will launch all the callbacks , then remove the listeners list . @param host(QuickApp ) : the instancied QuickApp subclass <code> def call_listeners(host): ",#LINE# #TAB# global listeners #LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# cb = listeners.pop(0) #LINE# #TAB# #TAB# #TAB# cb(host) #LINE# #TAB# #TAB# except IndexError: #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# del listeners
"Returns a list of all tfvc branches for the supplied project within the supplied collection <code> def get_tfvc_repos(url, token, collection, project): ","#LINE# #TAB# branch_list = [] #LINE# #TAB# tfvc_client = create_tfs_tfvc_client('{url}/{collection_name}'.format(url=url, collection_name=collection.name), token) #LINE# #TAB# logger.debug('Retrieving Tfvc Branches for Project: {project_name}'.format(project_name=project.name)) #LINE# #TAB# branches = tfvc_client.get_branches(project.id, True, True, False, True) #LINE# #TAB# if branches: #LINE# #TAB# #TAB# branch_list.extend(branches) #LINE# #TAB# else: #LINE# #TAB# #TAB# logger.debug('No Tfvcc Branches in Project: {project_name}'.format(project_name=project.name)) #LINE# #TAB# return branch_list"
"Split in_value according to the ratios specified in ratios <code> def ratio_split(amount, ratios): ","#LINE# #TAB# ratio_total = sum(ratios) #LINE# #TAB# divided_value = amount / ratio_total #LINE# #TAB# values = [] #LINE# #TAB# for ratio in ratios: #LINE# #TAB# #TAB# value = divided_value * ratio #LINE# #TAB# #TAB# values.append(value) #LINE# #TAB# rounded = [v.quantize(Decimal(""0.01"")) for v in values] #LINE# #TAB# remainders = [v - rounded[i] for i, v in enumerate(values)] #LINE# #TAB# remainder = sum(remainders) #LINE# #TAB# rounded[-1] = (rounded[-1] + remainder).quantize(Decimal(""0.01"")) #LINE# #TAB# assert sum(rounded) == amount #LINE# #TAB# return rounded"
"Clean a dictionary to make sure it contains only valid , non - null keys and values  <code> def safe_dict(data): ","#LINE# #TAB# if data is None: #LINE# #TAB# #TAB# return #LINE# #TAB# safe = {} #LINE# #TAB# for key, value in data.items(): #LINE# #TAB# #TAB# key = safe_string(key) #LINE# #TAB# #TAB# value = safe_string(value) #LINE# #TAB# #TAB# if key is not None and value is not None: #LINE# #TAB# #TAB# #TAB# safe[key] = value #LINE# #TAB# if len(safe): #LINE# #TAB# #TAB# return safe"
Receives a whitelist_path containing 1 IP / CIDR per line and returns a list <code> def parse_whitelist_file(whitelist_path: str) ->list: ,"#LINE# #TAB# with open(whitelist_path, 'r') as whitelist_file: #LINE# #TAB# #TAB# whitelist = whitelist_file.read().splitlines() #LINE# #TAB# return whitelist"
"Takes an MSA and a description as an input and returns the sequence length of that description  <code> def sequence_len(msa, description): ",#LINE# #TAB# sequence = msa.get_sequence(description) #LINE# #TAB# if sequence: #LINE# #TAB# #TAB# return len(sequence.ungapped()) #LINE# #TAB# return 0
"List by their IDs all hosted zones in the bound account  <code> def list_all_zones_by_id(region=None, key=None, keyid=None, profile=None): ","#LINE# #TAB# ret = describe_hosted_zones(region=region, key=key, keyid=keyid, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# profile=profile) #LINE# #TAB# return [r['Id'].replace('/hostedzone/', '') for r in ret]"
"This method assumes valid JSON input . The input will be json encoded , gzip compressed , and base64 encoded  <code> def serverless_payload_encode(payload): ",#LINE# #TAB# json_encode_data = json_encode(payload) #LINE# #TAB# compressed_data = gzip_compress(json_encode_data) #LINE# #TAB# encoded_data = base64.b64encode(compressed_data) #LINE# #TAB# return encoded_data
"Apply a migration to the SQL server <code> def run_migration(connection, queries, engine): ","#LINE# #TAB# with connection.cursor() as cursorMig: #LINE# #TAB# #TAB# queries = parse_statements(queries, engine) #LINE# #TAB# #TAB# for query in queries: #LINE# #TAB# #TAB# #TAB# cursorMig.execute(query) #LINE# #TAB# #TAB# connection.commit() #LINE# #TAB# return True"
"JSON save value encoding <code> def to_json_value(obj, fieldname, value=_marker, default=None): ","#LINE# #TAB# if value is _marker: #LINE# #TAB# #TAB# value = IDataManager(obj).json_data(fieldname) #LINE# #TAB# if isinstance(value, ImplicitAcquisitionWrapper): #LINE# #TAB# #TAB# return get_url_info(value) #LINE# #TAB# if callable(value): #LINE# #TAB# #TAB# value = value() #LINE# #TAB# if is_date(value): #LINE# #TAB# #TAB# return to_iso_date(value) #LINE# #TAB# if not is_json_serializable(value): #LINE# #TAB# #TAB# logger.warn(""Output {} is not JSON serializable"".format(repr(value))) #LINE# #TAB# #TAB# return default #LINE# #TAB# return value"
"Return the coefficient of variation ( CV ) of a sequence of numbers . : param data_mean : Precomputed mean of the sequence . : param data_stdevp : Precomputed stdevp of the sequence  <code> def coefficient_of_variation(data, data_mean=None, data_stdev=None): ","#LINE# #TAB# data_mean = data_mean or mean(data) #LINE# #TAB# data_stdev = data_stdev or stdevp(data, data_mean) #LINE# #TAB# if data_mean == 0: #LINE# #TAB# #TAB# return float('inf') if data_stdev != 0 else 0 #LINE# #TAB# else: #LINE# #TAB# #TAB# return data_stdev / data_mean"
"Adds SBG namespace for CWL application ( tool , workflow ) . : param app : an instance of cwl . App : return : app with added SBG namespace <code> def add_sbg_namespace(app): ","#LINE# #TAB# namespaces = app.get('$namespaces', dict()) #LINE# #TAB# namespaces['sbg'] = SBG_NAMESPACE #LINE# #TAB# app['$namespaces'] = namespaces #LINE# #TAB# return app"
Return stack objects with names that match the given prefix  <code> def get_cluster_substacks(cluster_name): ,#LINE# #TAB# resources = get_stack_resources(get_stack_name(cluster_name)) #LINE# #TAB# return [get_stack(r.get('PhysicalResourceId')) for r in resources if r. #LINE# #TAB# #TAB# get('ResourceType') == STACK_TYPE]
"Read a variable value from the status response page  <code> def get_variable(page, name): ","#LINE# #TAB# regex = f""var\\s+{name!s}\\s+=\\s+'[0-9a-f-]+';"" #LINE# #TAB# match = re.search(regex, page) #LINE# #TAB# if match is not None: #LINE# #TAB# #TAB# line = match.group() #LINE# #TAB# #TAB# index = line.find(""'"") #LINE# #TAB# #TAB# return line[index + 1:-2] #LINE# #TAB# return None"
"Reads a csv file and returns a dictionary with the respective keys specified in the first row of the csv file  <code> def read_csv_to_dictionary(csvfile, delimiter=';'): ","#LINE# #TAB# data = [] #LINE# #TAB# with open(csvfile, mode='r') as infile: #LINE# #TAB# #TAB# reader = csv.reader(infile, delimiter=delimiter) #LINE# #TAB# #TAB# for i, rows in enumerate(reader): #LINE# #TAB# #TAB# #TAB# data.append(rows) #LINE# #TAB# #TAB# infile.close() #LINE# #TAB# data = zip(*data) #LINE# #TAB# data_dict = {} #LINE# #TAB# for l in data: #LINE# #TAB# #TAB# key = l[0] #LINE# #TAB# #TAB# values = list(l[1:]) #LINE# #TAB# #TAB# data_dict.update({key: values}) #LINE# #TAB# return data_dict"
"Write metadata to a HDF5 file Parameters ---------- source : ` ctapipe.io.event_source ` output_filename : path <code> def write_metadata(metadata, output_filename): ","#LINE# #TAB# with open_file(output_filename, mode='a') as file: #LINE# #TAB# #TAB# for k, item in metadata.as_dict().items(): #LINE# #TAB# #TAB# #TAB# if k in file.root._v_attrs and type(item) is list: #LINE# #TAB# #TAB# #TAB# #TAB# attribute = file.root._v_attrs[k].extend(metadata[k]) #LINE# #TAB# #TAB# #TAB# #TAB# file.root._v_attrs[k] = attribute #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# file.root._v_attrs[k] = metadata[k]"
Sanity check FASTA files  <code> def sanity_check_fasta(handle): ,#LINE# #TAB# header_found = False #LINE# #TAB# for line in handle: #LINE# #TAB# #TAB# if line.startswith('>'): #LINE# #TAB# #TAB# #TAB# header_found = True #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# handle.seek(0) #LINE# #TAB# if header_found: #LINE# #TAB# #TAB# return handle #LINE# #TAB# fake_header_line = '>DUMMY' #LINE# #TAB# new_handle = StringIO() #LINE# #TAB# new_handle.write('%s\n' % fake_header_line) #LINE# #TAB# new_handle.write(handle.read()) #LINE# #TAB# new_handle.seek(0) #LINE# #TAB# return new_handle
Get a JSON Schema by filename  <code> def get_json_schema(filename): ,"#LINE# #TAB# file_path = os.path.join(""schemas"", filename) #LINE# #TAB# with open(file_path) as f: #LINE# #TAB# #TAB# schema = yaml.load(f) #LINE# #TAB# return schema"
Return a list names of the card sets in the loop  <code> def get_loop(oCardSet): ,#LINE# #TAB# aLoop = [oCardSet] #LINE# #TAB# oParent = oCardSet.parent #LINE# #TAB# while oParent not in aLoop and oParent: #LINE# #TAB# #TAB# aLoop.append(oParent) #LINE# #TAB# #TAB# oParent = oParent.parent #LINE# #TAB# if not oParent: #LINE# #TAB# #TAB# return [] #LINE# #TAB# if oParent != oCardSet: #LINE# #TAB# #TAB# return get_loop(oParent) #LINE# #TAB# return aLoop
"Return a dict containing the arguments and default arguments to the function  <code> def arg_lookup(fun, aspec=None): ","#LINE# #TAB# ret = {'kwargs': {}} #LINE# #TAB# if aspec is None: #LINE# #TAB# #TAB# aspec = get_function_argspec(fun) #LINE# #TAB# if aspec.defaults: #LINE# #TAB# #TAB# ret['kwargs'] = dict(zip(aspec.args[::-1], aspec.defaults[::-1])) #LINE# #TAB# ret['args'] = [arg for arg in aspec.args if arg not in ret['kwargs']] #LINE# #TAB# return ret"
Formats a transaction DataFrame  <code> def make_transaction_frame(transactions): ,"#LINE# #TAB# transaction_list = [] #LINE# #TAB# for dt in transactions.index: #LINE# #TAB# #TAB# txns = transactions.loc[dt] #LINE# #TAB# #TAB# if len(txns) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# for txn in txns: #LINE# #TAB# #TAB# #TAB# txn = map_transaction(txn) #LINE# #TAB# #TAB# #TAB# transaction_list.append(txn) #LINE# #TAB# df = pd.DataFrame(sorted(transaction_list, key=lambda x: x['dt'])) #LINE# #TAB# df['txn_dollars'] = -df['amount'] * df['price'] #LINE# #TAB# df.index = list(map(pd.Timestamp, df.dt.values)) #LINE# #TAB# return df"
Parse a tag - length - value style beacon packet  <code> def parse_ltv_packet(packet): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# frame = LTVFrame.parse(packet) #LINE# #TAB# #TAB# for ltv in frame: #LINE# #TAB# #TAB# #TAB# if ltv['type'] == SERVICE_DATA_TYPE: #LINE# #TAB# #TAB# #TAB# #TAB# data = ltv['value'] #LINE# #TAB# #TAB# #TAB# #TAB# if data['service_identifier'] == EDDYSTONE_UUID: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return parse_eddystone_service_data(data) #LINE# #TAB# #TAB# #TAB# #TAB# elif data['service_identifier'] == ESTIMOTE_UUID: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# return parse_estimote_service_data(data) #LINE# #TAB# except ConstructError: #LINE# #TAB# #TAB# return None #LINE# #TAB# return None
"Helper function for calc_center_centroid <code> def sine_function_derivative(t, params, eval_idx): ","#LINE# #TAB# offset, amplitude, phase = params #LINE# #TAB# n_angles = t.shape[0] #LINE# #TAB# w = 2.0 * pi * (1.0 / (2.0 * n_angles)) * t + phase #LINE# #TAB# grad = 1.0, np.sin(w), amplitude * np.cos(w) #LINE# #TAB# return grad[eval_idx]"
"Given a DatabaseObject in code.node , return relationships of derived nodes  <code> def get_relationships_lte(schemaclass): ","#LINE# #TAB# rels_all = set() #LINE# #TAB# schs_all = set() #LINE# #TAB# seen = set() #LINE# #TAB# _get_relationships_lte(schemaclass, seen, rels_all, schs_all) #LINE# #TAB# return rels_all"
"Identify whether the input is an iterable  <code> def is_iterable(seq, include_strings=False): ",#LINE# #TAB# if not include_strings and is_string(seq): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# iter(seq) #LINE# #TAB# #TAB# return True #LINE# #TAB# except TypeError: #LINE# #TAB# #TAB# return False
"Draw distribution of the Portmanteu whiteness test  <code> def plot_whiteness(var, h, repeats=1000, axis=None): ","#LINE# #TAB# pr, q0, q = var.test_whiteness(h, repeats, True) #LINE# #TAB# if axis is None: #LINE# #TAB# #TAB# axis = current_axis() #LINE# #TAB# pdf, _, _ = axis.hist(q0, 30, normed=True, label='surrogate distribution') #LINE# #TAB# axis.plot([q,q], [0,np.max(pdf)], 'r-', label='fitted model') #LINE# #TAB# axis.set_title('significance: p = %f'%pr) #LINE# #TAB# axis.set_xlabel('Li-McLeod statistic (Q)') #LINE# #TAB# axis.set_ylabel('probability') #LINE# #TAB# axis.legend() #LINE# #TAB# return pr"
"Given a list of dask vals , return a single graph and a list of keys such that ` ` get(dsk , keys ) ` ` is equivalent to ` ` [ v.compute ( ) for v in vals ] ` `  <code> def extract_graph_and_keys(vals): ","#LINE# #TAB# from .highlevelgraph import HighLevelGraph #LINE# #TAB# graphs = [v.__dask_graph__() for v in vals] #LINE# #TAB# keys = [v.__dask_keys__() for v in vals] #LINE# #TAB# if any(isinstance(graph, HighLevelGraph) for graph in graphs): #LINE# #TAB# #TAB# graph = HighLevelGraph.merge(*graphs) #LINE# #TAB# else: #LINE# #TAB# #TAB# graph = merge(*map(ensure_dict, graphs)) #LINE# #TAB# return graph, keys"
"Returns the begin and end offsets for the entire phrase containing the head token and its dependents  <code> def phrase_extent_for_head(tokens, head_index): ","#LINE# #TAB# begin = tokens[head_index]['text']['beginOffset'] #LINE# #TAB# end = begin + len(tokens[head_index]['text']['content']) #LINE# #TAB# for child in dependents(tokens, head_index): #LINE# #TAB# #TAB# child_begin, child_end = phrase_extent_for_head(tokens, child) #LINE# #TAB# #TAB# begin = min(begin, child_begin) #LINE# #TAB# #TAB# end = max(end, child_end) #LINE# #TAB# return begin, end"
"Pings an address at a specific port using nmap and checks whether or not the address responds : param address : The address to ping : param port : The port to ping : return : Whether or not the device responds to the ping <code> def port_ping(address: str, port: int) ->bool: ","#LINE# #TAB# try: #LINE# #TAB# #TAB# response = check_output(['nmap', '-p', str(port), address, '-Pn']) #LINE# #TAB# #TAB# return 'Host is up' in response.decode('utf-8') #LINE# #TAB# except CalledProcessError: #LINE# #TAB# #TAB# return False"
Join overlapping intervals of Pandas DataFrame . Uses ` join_overlapping ` to join overlapping intervals of : class:`pandas . DataFrame ` ` d `  <code> def join_overlapping_frame(d): ,"#LINE# #TAB# d = d.sort_values(['chromo', 'start', 'end']) #LINE# #TAB# e = [] #LINE# #TAB# for chromo in d.chromo.unique(): #LINE# #TAB# #TAB# dc = d.loc[d.chromo == chromo] #LINE# #TAB# #TAB# start, end = join_overlapping(dc.start.values, dc.end.values) #LINE# #TAB# #TAB# ec = pd.DataFrame(dict(chromo=chromo, start=start, end=end)) #LINE# #TAB# #TAB# e.append(ec) #LINE# #TAB# e = pd.concat(e) #LINE# #TAB# e = e.loc[:, (['chromo', 'start', 'end'])] #LINE# #TAB# return e"
Construct a cell from an egi . Parameters ---------- egi A mesh object representing the egi . Returns ------- cell A mesh object representing the unit polyhedron  <code> def cell_from_egi(egi): ,"#LINE# #TAB# cell = Cell() #LINE# #TAB# cell.name = 'cell' #LINE# #TAB# for fkey in egi.face: #LINE# #TAB# #TAB# x, y, z = egi.face_center(fkey) #LINE# #TAB# #TAB# cell.add_vertex(key=fkey, x=x, y=y, z=z) #LINE# #TAB# for vkey in egi.vertex: #LINE# #TAB# #TAB# cell_face = egi.vertex_faces(vkey, ordered=True) #LINE# #TAB# #TAB# cell.add_face(cell_face[::-1], fkey=vkey) #LINE# #TAB# #TAB# cell.facedata[vkey]['type'] = egi.vertex[vkey]['type'] #LINE# #TAB# return cell"
"Selection function either for getting a shuffled list of selected sources or a single source filtered by its network location . : param sources : reference list of sources : param netloc : network location of the source : return : list of selected sources <code> def filter_sources(sources_list, netloc=None): ",#LINE# #TAB# if netloc is not None: #LINE# #TAB# #TAB# sources = [] #LINE# #TAB# #TAB# for s in sources_list: #LINE# #TAB# #TAB# #TAB# if netloc == urlparse(s[1]).netloc: #LINE# #TAB# #TAB# #TAB# #TAB# sources.append(s) #LINE# #TAB# #TAB# #TAB# #TAB# break #LINE# #TAB# else: #LINE# #TAB# #TAB# sources = [s for s in sources_list] #LINE# #TAB# #TAB# random.shuffle(sources) #LINE# #TAB# return sources
Make dictionary to map location name to IBGE code  <code> def name_to_code(geolevel): ,#LINE# #TAB# url = build_url(geolevel) #LINE# #TAB# locations = get_geojson(url) #LINE# #TAB# return {location['nome']: location['id'] for location in locations}
Check if file is ocad cours - setting file <code> def file_is_cs(path_file): ,"#LINE# #TAB# file, file_close = _input_2_file_object(path_file) #LINE# #TAB# is_cs = _file_typ(file) == 'cs' #LINE# #TAB# if file_close: #LINE# #TAB# #TAB# file.close() #LINE# #TAB# return is_cs"
"Split a mailbox name If a separator is found in ` ` fullname ` ` , this function returns the corresponding name and parent mailbox name  <code> def separate_mailbox(fullname, sep='.'): ","#LINE# #TAB# if sep in fullname: #LINE# #TAB# #TAB# parts = fullname.split(sep) #LINE# #TAB# #TAB# name = parts[-1] #LINE# #TAB# #TAB# parent = sep.join(parts[0:len(parts) - 1]) #LINE# #TAB# #TAB# return name, parent #LINE# #TAB# return fullname, None"
Get current status of application in context <code> def get_app_state(): ,"#LINE# #TAB# if not hasattr(g, 'app_state'): #LINE# #TAB# #TAB# model = get_model() #LINE# #TAB# #TAB# g.app_state = { #LINE# #TAB# #TAB# #TAB# 'app_title': APP_TITLE, #LINE# #TAB# #TAB# #TAB# 'model_name': type(model).__name__, #LINE# #TAB# #TAB# #TAB# 'latest_ckpt_name': model.latest_ckpt_name, #LINE# #TAB# #TAB# #TAB# 'latest_ckpt_time': model.latest_ckpt_time #LINE# #TAB# #TAB# } #LINE# #TAB# return g.app_state"
Render server alive count max option  <code> def format_keep_alive_packages(keep_alive_packages): ,#LINE# #TAB# format_str = '-o ServerAliveCountMax={}'.format #LINE# #TAB# return keep_alive_packages and format_str(keep_alive_packages) or ''
Create Bool object from a compatible object . : param obj : - Boolean value . : return : - Converted value as a Bool object  <code> def create_from(obj: bool) ->'Bool': ,#LINE# #TAB# if obj: #LINE# #TAB# #TAB# if Bool._TRUE is None: #LINE# #TAB# #TAB# #TAB# Bool._TRUE = Bool(True) #LINE# #TAB# #TAB# return Bool._TRUE #LINE# #TAB# else: #LINE# #TAB# #TAB# if Bool._FALSE is None: #LINE# #TAB# #TAB# #TAB# Bool._FALSE = Bool(False) #LINE# #TAB# #TAB# return Bool._FALSE
"For each variable , list each of its markov blankets in graphs  <code> def markov_blankets(graphs, boolean=True, tol=1e-08, unique=False): ","#LINE# #TAB# m_blankets = [np.array([G[i] for G in graphs]) for i in range(graphs[0] #LINE# #TAB# #TAB# .shape[0])] #LINE# #TAB# for i, mb in enumerate(m_blankets): #LINE# #TAB# #TAB# mb[:, (i)] = 0 #LINE# #TAB# #TAB# mb[np.abs(mb) < tol] = 0 #LINE# #TAB# if boolean: #LINE# #TAB# #TAB# m_blankets = [(mb != 0) for mb in m_blankets] #LINE# #TAB# if unique: #LINE# #TAB# #TAB# m_blankets = np.unique(np.array(m_blankets), axis=1) #LINE# #TAB# return m_blankets"
"Launches an ssh shell <code> def launch_shell(username, hostname, password, port=22): ","#LINE# #TAB# if not username or not hostname or not password: #LINE# #TAB# #TAB# return False #LINE# #TAB# with tempfile.NamedTemporaryFile() as tmpFile: #LINE# #TAB# #TAB# os.system(sshCmdLine.format(password, tmpFile.name, username, hostname, #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# port)) #LINE# #TAB# return True"
"get list of ` models . OtherGeneName ` objects from XML node entry : param entry : XML node entry : return : list of : class:`pyuniprot.manager.models.models . OtherGeneName ` objects <code> def get_other_gene_names(cls, entry): ","#LINE# #TAB# alternative_gene_names = [] #LINE# #TAB# for alternative_gene_name in entry.iterfind('./gene/name'): #LINE# #TAB# #TAB# if alternative_gene_name.attrib['type'] != 'primary': #LINE# #TAB# #TAB# #TAB# alternative_gene_name_dict = {'type_': alternative_gene_name. #LINE# #TAB# #TAB# #TAB# #TAB# attrib['type'], 'name': alternative_gene_name.text} #LINE# #TAB# #TAB# #TAB# alternative_gene_names.append(models.OtherGeneName(** #LINE# #TAB# #TAB# #TAB# #TAB# alternative_gene_name_dict)) #LINE# #TAB# return alternative_gene_names"
Extend the selenium WebElement using the ExtendedRemoteWebElement or ExtendedFirefoxWebElement class <code> def extend_webelement(web_element) ->ExtendedRemoteWebElement: ,"#LINE# #TAB# if isinstance(web_element.parent, FirefoxDriver): #LINE# #TAB# #TAB# web_element.__class__ = ExtendedFirefoxWebElement #LINE# #TAB# else: #LINE# #TAB# #TAB# web_element.__class__ = ExtendedRemoteWebElement #LINE# #TAB# return web_element"
"Formats an object and returns output as a string . For test purposes only  <code> def formatter_test_output(Formatter, obj): ","#LINE# #TAB# from tempfile import mkstemp #LINE# #TAB# from os import remove #LINE# #TAB# _, name = mkstemp() #LINE# #TAB# fmt = Formatter() #LINE# #TAB# fmt.outfile = name #LINE# #TAB# fmt.format(obj) #LINE# #TAB# fmt.flush() #LINE# #TAB# with open(name) as f: #LINE# #TAB# #TAB# res = f.read() #LINE# #TAB# remove(name) #LINE# #TAB# return res"
"Find all the configurations in this folder <code> def find_configurations(folder, filename_match): ","#LINE# #TAB# found = [] #LINE# #TAB# for root, dirs, files in os.walk(folder): #LINE# #TAB# #TAB# for filename in files: #LINE# #TAB# #TAB# #TAB# location = os.path.join(root, filename) #LINE# #TAB# #TAB# #TAB# relative_location = os.path.relpath(location, start=folder) #LINE# #TAB# #TAB# #TAB# if fnmatch.fnmatch(relative_location, filename_match): #LINE# #TAB# #TAB# #TAB# #TAB# found.append(location) #LINE# #TAB# if not found: #LINE# #TAB# #TAB# raise NoConfiguration(folder=folder) #LINE# #TAB# return found"
Reads an XML with untangle which is in a ZipFile inside another ZipFile  <code> def read_xml_inside_nested_zip(xml_path): ,"#LINE# #TAB# zip_filepath = list(xml_path.keys())[0] #LINE# #TAB# inner_zip_info = list(xml_path.values())[0] #LINE# #TAB# inner_zip_name = list(inner_zip_info.keys())[0] #LINE# #TAB# xml_name = list(inner_zip_info.values())[0] #LINE# #TAB# z = ZipFile(zip_filepath) #LINE# #TAB# inner_zip = ZipFile(io.BytesIO(z.read(inner_zip_name))) #LINE# #TAB# file_size = inner_zip.getinfo(xml_name).file_size #LINE# #TAB# parsed_xml = untangle.parse(io.TextIOWrapper(io.BytesIO(inner_zip.read( #LINE# #TAB# #TAB# xml_name)))) #LINE# #TAB# return parsed_xml, file_size, xml_name"
Read credentials from environment variables <code> def read_creds_from_environment_variables(): ,#LINE# #TAB# creds = init_creds() #LINE# #TAB# if 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ: #LINE# #TAB# #TAB# creds['AccessKeyId'] = os.environ['AWS_ACCESS_KEY_ID'] #LINE# #TAB# #TAB# creds['SecretAccessKey'] = os.environ['AWS_SECRET_ACCESS_KEY'] #LINE# #TAB# #TAB# if 'AWS_SESSION_TOKEN' in os.environ: #LINE# #TAB# #TAB# #TAB# creds['SessionToken'] = os.environ['AWS_SESSION_TOKEN'] #LINE# #TAB# return creds
Converts a dictionary to an XML ElementTree Element <code> def dict_to_xml(xml_dict): ,"#LINE# #TAB# import lxml.etree as etree #LINE# #TAB# root_tag = list(xml_dict.keys())[0] #LINE# #TAB# root = etree.Element(root_tag) #LINE# #TAB# _dict_to_xml_recurse(root, xml_dict[root_tag]) #LINE# #TAB# return root"
Replace NaN and Inf ( there should not be any ! ) : param array : : return : <code> def sanitize_array(array): ,#LINE# #TAB# a = np.ravel(array) #LINE# #TAB# maxi = np.nanmax(a[np.isfinite(a)]) #LINE# #TAB# mini = np.nanmin(a[np.isfinite(a)]) #LINE# #TAB# array[array == float('inf')] = maxi #LINE# #TAB# array[array == float('-inf')] = mini #LINE# #TAB# mid = (maxi + mini) / 2 #LINE# #TAB# array[np.isnan(array)] = mid #LINE# #TAB# return array
"Takes a client and returns the set of keys associated with it . Returns a list of keys  <code> def get_client_alg_keys(cls, client): ","#LINE# #TAB# if client.jwt_alg == 'RS256': #LINE# #TAB# #TAB# keys = [] #LINE# #TAB# #TAB# for rsa_key in get_oidc_rsa_key_model().objects.all(): #LINE# #TAB# #TAB# #TAB# keys.append(RSAKey(key=importKey(rsa_key.key), kid=rsa_key.kid)) #LINE# #TAB# #TAB# if not keys: #LINE# #TAB# #TAB# #TAB# raise Exception('You must add at least one RSA Key.') #LINE# #TAB# elif client.jwt_alg == 'HS256': #LINE# #TAB# #TAB# keys = [SYMKey(key=client.client_secret, alg=client.jwt_alg)] #LINE# #TAB# else: #LINE# #TAB# #TAB# raise Exception('Unsupported key algorithm.') #LINE# #TAB# return keys"
Allows to deactivate encrypted communication  <code> def set_encrypted_communication(value: bool): ,#LINE# #TAB# global ENCRYPTED_COMMUNICATION #LINE# #TAB# ENCRYPTED_COMMUNICATION = value
"Legacy method . Can be ignored  <code> def new_vector(r, v, dim=4): ","#LINE# #TAB# translate = np.zeros(dim) #LINE# #TAB# translate[0] = 1000 #LINE# #TAB# translate[1] = 1000 #LINE# #TAB# v = v - translate #LINE# #TAB# v = v / scale #LINE# #TAB# v = np.dot(r, v) #LINE# #TAB# v = v * scale #LINE# #TAB# v = v + translate #LINE# #TAB# return v"
"Takes an array removes all values that are the same as the previous value . : param values : array of floats : return : cleaned array , indices of clean values in original array <code> def clean_out_non_changing(values): ","#LINE# #TAB# diff_values = np.ediff1d(values, to_begin=values[0]) #LINE# #TAB# non_zero_indices = np.where(diff_values != 0)[0] #LINE# #TAB# non_zero_indices = np.insert(non_zero_indices, 0, 0) #LINE# #TAB# cleaned_values = np.take(values, non_zero_indices) #LINE# #TAB# return cleaned_values, non_zero_indices"
"Ensures a path is parsed  <code> def normalise_path(path: Union[str, pathlib.Path]) -> pathlib.Path: ","#LINE# #TAB# if isinstance(path, str): #LINE# #TAB# #TAB# return pathlib.Path(path) #LINE# #TAB# return path"
Returns the labels of table columns to display results : param dict facets : The facet dictionary : returns : The title labels : rtype : * list * <code> def get_labels(facets): ,#LINE# #TAB# labels = list() #LINE# #TAB# for i in sorted(facets.values()): #LINE# #TAB# #TAB# facet = facets.keys()[facets.values().index(i)] #LINE# #TAB# #TAB# if facet not in IGNORED_FACETS: #LINE# #TAB# #TAB# #TAB# labels.append(facet) #LINE# #TAB# return labels
checks to make sure that the card passes a luhn mod-10 checksum <code> def luhn_check(card_number): ,"#LINE# #TAB# sum = 0 #LINE# #TAB# num_digits = len(card_number) #LINE# #TAB# oddeven = num_digits & 1 #LINE# #TAB# for count in range(0, num_digits): #LINE# #TAB# #TAB# digit = int(card_number[count]) #LINE# #TAB# #TAB# if not count & 1 ^ oddeven: #LINE# #TAB# #TAB# #TAB# digit *= 2 #LINE# #TAB# #TAB# if digit > 9: #LINE# #TAB# #TAB# #TAB# digit -= 9 #LINE# #TAB# #TAB# sum += digit #LINE# #TAB# return sum % 10 == 0"
"Retrieve attributes from raw Neo4j property dict  <code> def properties_to_attributes(result, var_name): ",#LINE# #TAB# attrs = {} #LINE# #TAB# for record in result: #LINE# #TAB# #TAB# if var_name in record.keys(): #LINE# #TAB# #TAB# #TAB# raw_dict = record[var_name] #LINE# #TAB# #TAB# #TAB# if raw_dict: #LINE# #TAB# #TAB# #TAB# #TAB# attrs = convert_props_to_attrs(raw_dict) #LINE# #TAB# return attrs
Read BTi PDF channel  <code> def read_channel(fid): ,"#LINE# #TAB# out = {'chan_label': read_str(fid, 16), 'chan_no': read_int16(fid), #LINE# #TAB# #TAB# 'attributes': read_int16(fid), 'scale': read_float(fid), #LINE# #TAB# #TAB# 'yaxis_label': read_str(fid, 16), 'valid_min_max': read_int16(fid)} #LINE# #TAB# fid.seek(6, 1) #LINE# #TAB# out.update({'ymin': read_double(fid), 'ymax': read_double(fid), 'index': #LINE# #TAB# #TAB# read_int32(fid), 'checksum': read_int32(fid), 'off_flag': read_str( #LINE# #TAB# #TAB# fid, 4), 'offset': read_float(fid)}) #LINE# #TAB# fid.seek(24, 1) #LINE# #TAB# return out"
"Turn the string copied from the Inspect->Network window into a Dict  <code> def make_json(raw_json, prettyprint=False): ","#LINE# #TAB# json = eval(raw_json.replace('null', 'None').replace('false', 'False'). #LINE# #TAB# #TAB# replace('true', 'True')) #LINE# #TAB# if prettyprint: #LINE# #TAB# #TAB# import pprint #LINE# #TAB# #TAB# pprint.pprint(json) #LINE# #TAB# return json"
"Return a Profile object , as the result of merging a potentially existing Profile file and the args command - line arguments <code> def profile_from_args(profiles, settings, options, env, cwd, cache): ","#LINE# #TAB# default_profile = cache.default_profile #LINE# #TAB# if profiles is None: #LINE# #TAB# #TAB# result = default_profile #LINE# #TAB# else: #LINE# #TAB# #TAB# result = Profile() #LINE# #TAB# #TAB# for p in profiles: #LINE# #TAB# #TAB# #TAB# tmp, _ = read_profile(p, cwd, cache.profiles_path) #LINE# #TAB# #TAB# #TAB# result.update(tmp) #LINE# #TAB# args_profile = _profile_parse_args(settings, options, env) #LINE# #TAB# if result: #LINE# #TAB# #TAB# result.update(args_profile) #LINE# #TAB# else: #LINE# #TAB# #TAB# result = args_profile #LINE# #TAB# return result"
Extract night from sonobat filename . Consider anything prior to noon as part of the previous night  <code> def extract_sonobat_night(pass_filename): ,#LINE# #TAB# match = FILENAME_RE.search(pass_filename) #LINE# #TAB# if match: #LINE# #TAB# #TAB# values = [int(v) for v in match.groups()] #LINE# #TAB# #TAB# values[-1] = values[-1] * 1000 #LINE# #TAB# #TAB# d = datetime.datetime(*values) #LINE# #TAB# #TAB# if d.hour < 12: #LINE# #TAB# #TAB# #TAB# d = d - datetime.timedelta(days=1) #LINE# #TAB# #TAB# return d.strftime('%m/%d/%Y') #LINE# #TAB# return None
Train a single overall model using the entire dataset using the Docker image . : param label_type : label type provided by user . : return : None <code> def train_all(label_type): ,"#LINE# #TAB# level = 'all' #LINE# #TAB# job_config = MorfJobConfig(CONFIG_FILENAME) #LINE# #TAB# job_config.update_mode('train') #LINE# #TAB# check_label_type(label_type) #LINE# #TAB# clear_s3_subdirectory(job_config) #LINE# #TAB# run_image(job_config, raw_data_bucket=job_config.raw_data_buckets, #LINE# #TAB# #TAB# level=level, label_type=label_type) #LINE# #TAB# send_email_alert(job_config) #LINE# #TAB# return"
Build a dict mapping names to their variations <code> def names_to_abbreviations(reporters): ,"#LINE# #TAB# names = {} #LINE# #TAB# for reporter_key, data_list in reporters.items(): #LINE# #TAB# #TAB# for data in data_list: #LINE# #TAB# #TAB# #TAB# abbrevs = data['editions'].keys() #LINE# #TAB# #TAB# #TAB# sort_func = lambda x: str(data['editions'][x]['start']) + x #LINE# #TAB# #TAB# #TAB# abbrevs = sorted(abbrevs, key=sort_func) #LINE# #TAB# #TAB# #TAB# names[data['name']] = abbrevs #LINE# #TAB# sorted_names = OrderedDict(sorted(names.items(), key=lambda t: t[0])) #LINE# #TAB# return sorted_names"
Fix for legacy appenlight clients <code> def rewrite_type(input_data): ,#LINE# #TAB# if input_data == 'remote_call': #LINE# #TAB# #TAB# return 'remote' #LINE# #TAB# return input_data
"A tiny function used by our SDK to sign and verify requests  <code> def get_signature(url, body, nonce, api_secret): ","#LINE# #TAB# message = str(nonce) + url + body #LINE# #TAB# h = hmac.new(api_secret, message, hashlib.sha256) #LINE# #TAB# signature = h.hexdigest() #LINE# #TAB# return signature"
Finds the center of the three reference clusters  <code> def find_ref_centers(mds): ,"#LINE# #TAB# ceu_mds = mds[mds[""pop""] == ""CEU""] #LINE# #TAB# yri_mds = mds[mds[""pop""] == ""YRI""] #LINE# #TAB# asn_mds = mds[mds[""pop""] == ""JPT-CHB""] #LINE# #TAB# centers = [[np.mean(ceu_mds[""c1""]), np.mean(ceu_mds[""c2""])], #LINE# #TAB# #TAB# #TAB# [np.mean(yri_mds[""c1""]), np.mean(yri_mds[""c2""])], #LINE# #TAB# #TAB# #TAB# [np.mean(asn_mds[""c1""]), np.mean(asn_mds[""c2""])]] #LINE# #TAB# return np.array(centers), {""CEU"": 0, ""YRI"": 1, ""JPT-CHB"": 2}"
"Based on recorded data and a policy , compute and return the unused events , states , and transitions <code> def unused_events_states_transitions(policy, recorded_requests_responses): ","#LINE# #TAB# all_events, all_states, all_transitions = events_states_transitions(policy) #LINE# #TAB# used_events, used_states, used_transitions = ( #LINE# #TAB# #TAB# used_events_states_transitions(recorded_requests_responses)) #LINE# #TAB# unused_states = all_states - used_states #LINE# #TAB# unused_events = all_events - used_events #LINE# #TAB# unused_transitions = all_transitions - used_transitions #LINE# #TAB# return unused_events, unused_states, unused_transitions"
"Detach a data disk from a VMSS VM model <code> def detach_model(vmssvm_model, lun): ",#LINE# #TAB# data_disks = vmssvm_model['properties']['storageProfile']['dataDisks'] #LINE# #TAB# data_disks[:] = [disk for disk in data_disks if disk.get('lun') != lun] #LINE# #TAB# vmssvm_model['properties']['storageProfile']['dataDisks'] = data_disks #LINE# #TAB# return vmssvm_model
"Yield file contents by block then close the file  <code> def iter_and_close(file_obj, block_size, charset): ","#LINE# #TAB# while True: #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# block = file_obj.read(block_size) #LINE# #TAB# #TAB# #TAB# if block: #LINE# #TAB# #TAB# #TAB# #TAB# if isinstance(block, bytes): #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield block #LINE# #TAB# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# yield block.encode(charset) #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# raise StopIteration #LINE# #TAB# #TAB# except StopIteration: #LINE# #TAB# #TAB# #TAB# file_obj.close() #LINE# #TAB# #TAB# #TAB# break"
"Add the chat to the log . Return an auto response , if the chat is directed to the system . Otherwise , return an updated chat list of the sender  <code> def post_chat_box(): ","#LINE# #TAB# recipient_user_id = request.POST.get('recipientUserId', None) #LINE# #TAB# message = request.POST.get('message', None) #LINE# #TAB# worksheet_uuid = request.POST.get('worksheetId', -1) #LINE# #TAB# bundle_uuid = request.POST.get('bundleId', -1) #LINE# #TAB# info = {'sender_user_id': request.user.user_id, 'recipient_user_id': #LINE# #TAB# #TAB# recipient_user_id, 'message': message, 'worksheet_uuid': #LINE# #TAB# #TAB# worksheet_uuid, 'bundle_uuid': bundle_uuid} #LINE# #TAB# chats = add_chat_log_info(info) #LINE# #TAB# return {'chats': chats}"
Returns the jinja2 templating environment updated with the most recent cauldron environment configurations <code> def get_environment() -> Environment: ,"#LINE# #TAB# env = JINJA_ENVIRONMENT #LINE# #TAB# loader = env.loader #LINE# #TAB# resource_path = environ.configs.make_path( #LINE# #TAB# #TAB# 'resources', 'templates', #LINE# #TAB# #TAB# override_key='template_path' #LINE# #TAB# ) #LINE# #TAB# if not loader: #LINE# #TAB# #TAB# env.filters['id'] = get_id #LINE# #TAB# #TAB# env.filters['latex'] = get_latex #LINE# #TAB# if not loader or resource_path not in loader.searchpath: #LINE# #TAB# #TAB# env.loader = FileSystemLoader(resource_path) #LINE# #TAB# return env"
Check if the user is authenticated . This is a compatibility function needed to support both Django 1.x and 2.x ; Django 2.x turns the function into a proper boolean so function calls will fail  <code> def get_user_is_authenticated(user): ,#LINE# #TAB# if callable(user.is_authenticated): #LINE# #TAB# #TAB# return user.is_authenticated() #LINE# #TAB# else: #LINE# #TAB# #TAB# return user.is_authenticated
"Gaussian log - likelihood without constant term in time <code> def log_likelihood_t(emp_cov, precision): ","#LINE# #TAB# score = 0 #LINE# #TAB# for e, p in zip(emp_cov, precision): #LINE# #TAB# #TAB# score += fast_logdet(p) - np.sum(e * p) #LINE# #TAB# return score"
"Return the catalog which indexes objects of instance 's type . If an object is indexed by more than one catalog , the first match will be returned  <code> def get_catalog(instance, field='UID'): ","#LINE# #TAB# uid = instance.UID() #LINE# #TAB# if 'workflow_skiplist' in instance.REQUEST and [x for x in instance. #LINE# #TAB# #TAB# REQUEST['workflow_skiplist'] if x.find(uid) > -1]: #LINE# #TAB# #TAB# return None #LINE# #TAB# else: #LINE# #TAB# #TAB# at = getToolByName(instance, 'archetype_tool') #LINE# #TAB# #TAB# plone = instance.portal_url.getPortalObject() #LINE# #TAB# #TAB# catalog_name = (instance.portal_type in at.catalog_map and at. #LINE# #TAB# #TAB# #TAB# catalog_map[instance.portal_type][0] or 'portal_catalog') #LINE# #TAB# #TAB# catalog = getToolByName(plone, catalog_name) #LINE# #TAB# #TAB# return catalog"
"Calculate the absolute vorticity of the horizontal wind  <code> def absolute_vorticity(u, v, dx, dy, lats, dim_order='yx'): ","#LINE# #TAB# f = coriolis_parameter(lats) #LINE# #TAB# relative_vorticity = vorticity(u, v, dx, dy, dim_order=dim_order) #LINE# #TAB# return relative_vorticity + f"
"Raises a matrix with two opposing eigenvalues to a power  <code> def reflection_matrix_pow(reflection_matrix: np.ndarray, exponent: float): ","#LINE# #TAB# squared_phase = np.dot(reflection_matrix[:, 0], #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# #TAB# reflection_matrix[0, :]) #LINE# #TAB# phase = complex(np.sqrt(squared_phase)) #LINE# #TAB# i = np.eye(reflection_matrix.shape[0]) * phase #LINE# #TAB# pos_part = (i + reflection_matrix) * 0.5 #LINE# #TAB# neg_part = (i - reflection_matrix) * 0.5 #LINE# #TAB# pos_factor = phase**(exponent - 1) #LINE# #TAB# neg_factor = pos_factor * complex(-1)**exponent #LINE# #TAB# pos_part_raised = pos_factor * pos_part #LINE# #TAB# neg_part_raised = neg_part * neg_factor #LINE# #TAB# return pos_part_raised + neg_part_raised"
Get operating system ; only works for unix - like machines <code> def get_os(): ,"#LINE# #TAB# OS = platform.uname()[0] #LINE# #TAB# if OS == 'Linux': #LINE# #TAB# #TAB# OS = 'linux' #LINE# #TAB# elif OS == 'Darwin': #LINE# #TAB# #TAB# OS = 'mac' #LINE# #TAB# else: #LINE# #TAB# #TAB# sys.stderr.write('OS: ""{}"" not supported\n'.format(OS)) #LINE# #TAB# return OS"
Find a model reference by its table name <code> def find_model_by_table_name(name): ,"#LINE# #TAB# for model in ModelBase._decl_class_registry.values(): #LINE# #TAB# #TAB# if hasattr(model, '__table__') and model.__table__.fullname == name: #LINE# #TAB# #TAB# #TAB# return model #LINE# #TAB# return None"
"Returns LSTM residual block <code> def block_bilstm(filters, drop_rate, inpR): ","#LINE# #TAB# prev = inpR #LINE# #TAB# x_rnn = Bidirectional(LSTM(filters, return_sequences=True, dropout= #LINE# #TAB# #TAB# drop_rate, recurrent_dropout=drop_rate))(prev) #LINE# #TAB# NiN = Conv1D(filters, 1, padding='same')(x_rnn) #LINE# #TAB# res_out = BatchNormalization()(NiN) #LINE# #TAB# return res_out"
Determine the target type for a ` ` str ` ` value . Returns ` ` None ` ` if a suitable target can not be determined  <code> def get_cast_type_for_str(field_type: type) ->Optional[Callable]: ,#LINE# #TAB# if field_type is datetime or _is_typing_type(field_type #LINE# #TAB# #TAB# ) and _is_nested_type(field_type #LINE# #TAB# #TAB# ) and datetime in field_type._subs_tree(): #LINE# #TAB# #TAB# return dateutil.parser.parse #LINE# #TAB# return None
"Returns the verbose name of an object s field  <code> def get_verbose(obj, field_name=""""): ","#LINE# #TAB# if hasattr(obj, ""_meta"") and hasattr(obj._meta, ""get_field_by_name""): #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# return obj._meta.get_field(field_name).verbose_name #LINE# #TAB# #TAB# except FieldDoesNotExist: #LINE# #TAB# #TAB# #TAB# pass #LINE# #TAB# return """""
"Returns n evenly spaced indexes . Returns as many as possible if trunc is true <code> def spaced_indexes(len_, n, trunc=False): ","#LINE# #TAB# if n is None: #LINE# #TAB# #TAB# return np.arange(len_) #LINE# #TAB# all_indexes = np.arange(len_) #LINE# #TAB# if trunc: #LINE# #TAB# #TAB# n = min(len_, n) #LINE# #TAB# if n == 0: #LINE# #TAB# #TAB# return np.empty(0) #LINE# #TAB# stride = len_ // n #LINE# #TAB# try: #LINE# #TAB# #TAB# indexes = all_indexes[0:-1:stride] #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# raise ValueError('cannot slice list of len_=%r into n=%r parts' % (len_, n)) #LINE# #TAB# return indexes"
"Takes a string and makes it suitable for use as an identifier <code> def string_to_identifier(s, white_space_becomes='_'): ","#LINE# #TAB# import re #LINE# #TAB# s = s.lower() #LINE# #TAB# s = re.sub(r""\s+"", white_space_becomes, s) #LINE# #TAB# s = re.sub(r""-"", ""_"", s) #LINE# #TAB# s = re.sub(r""[^A-Za-z0-9_]"", """", s) #LINE# #TAB# return s"
Check if the dataset is imbalanced  <code> def imbalance_check(P): ,#LINE# #TAB# p_list = list(P.values()) #LINE# #TAB# max_value = max(p_list) #LINE# #TAB# min_value = min(p_list) #LINE# #TAB# if min_value > 0: #LINE# #TAB# #TAB# balance_ratio = max_value / min_value #LINE# #TAB# else: #LINE# #TAB# #TAB# balance_ratio = max_value #LINE# #TAB# is_imbalanced = False #LINE# #TAB# if balance_ratio > BALANCE_RATIO_THRESHOLD: #LINE# #TAB# #TAB# is_imbalanced = True #LINE# #TAB# return is_imbalanced
"Create a valid token and put into authentication payload similar to OpenID connect payloads  <code> def get_auth_payload(user_name, client_id='api-key-client', roles=None): ","#LINE# #TAB# access_token = create_development_jwt_token(user_name, client_id, #LINE# #TAB# #TAB# 'Bearer', 5 * 60, roles) #LINE# #TAB# refresh_token = create_development_jwt_token(None, client_id, 'Offline', #LINE# #TAB# #TAB# 0, roles) #LINE# #TAB# auth_payload = {'access_token': access_token, 'refresh_token': #LINE# #TAB# #TAB# refresh_token} #LINE# #TAB# return auth_payload"
Return the coordinates of the center of each areas in the segmented image <code> def get_center_areas(labels): ,"#LINE# #TAB# pts = [r.centroid for r in regionprops(labels + 1, coordinates='rc')] #LINE# #TAB# rcpts = np.asarray(pts, dtype=np.int) #LINE# #TAB# return rcpts"
"Generate symmetric reflections given symmetry operations symhkl = gen_sym_ref(sym_ops , h , k , l ) <code> def gen_sym_ref(sym_ops, hkl): ","#LINE# #TAB# hkl = np.asarray(hkl, dtype=np.float) #LINE# #TAB# sym_mat = gen_sym_mat(sym_ops) #LINE# #TAB# nops = len(sym_ops) #LINE# #TAB# symhkl = np.zeros([nops, 4]) #LINE# #TAB# for n, sym in enumerate(sym_mat): #LINE# #TAB# #TAB# symhkl[(n), :] = np.dot(hkl, sym) #LINE# #TAB# return symhkl[:, :3]"
retrieve the data dictionary kaggle_data_dictionary.csv as a data frame <code> def get_data_dictionary(data_dictionary_path: pathlib.Path) ->pd.DataFrame: ,"#LINE# #TAB# data_dict_df = pd.read_csv(data_dictionary_path, header=0) #LINE# #TAB# columns = [name.replace(' ', '_') for name in data_dict_df.columns] #LINE# #TAB# data_dict_df.columns = columns #LINE# #TAB# return data_dict_df"
Get a registered table . Decorated functions will be converted to ` DataFrameWrapper ` . Parameters ---------- table_name : str Returns ------- table : ` DataFrameWrapper ` <code> def get_table(table_name): ,"#LINE# #TAB# table = get_raw_table(table_name) #LINE# #TAB# if isinstance(table, TableFuncWrapper): #LINE# #TAB# #TAB# table = table() #LINE# #TAB# return table"
Do all of the gruntwork associated with creating a new module  <code> def new_module(name): ,"#LINE# #TAB# parent = None #LINE# #TAB# if '.' in name: #LINE# #TAB# #TAB# parent_name = name.rsplit('.', 1)[0] #LINE# #TAB# #TAB# parent = __import__(parent_name, fromlist=['']) #LINE# #TAB# module = imp.new_module(name) #LINE# #TAB# sys.modules[name] = module #LINE# #TAB# if parent: #LINE# #TAB# #TAB# setattr(parent, name.rsplit('.', 1)[1], module) #LINE# #TAB# return module"
"get file size , in megabytes : param file : : return : size of file <code> def get_size(file): ",#LINE# #TAB# size_in_bytes = os.path.getsize(file) #LINE# #TAB# size_in_megabytes = size_in_bytes / 1000000 #LINE# #TAB# return size_in_megabytes
"Find the next Site if we 're not "" on "" a site already  <code> def get_site(): ",#LINE# #TAB# site = context = getSite() #LINE# #TAB# if ISite.providedBy(context): #LINE# #TAB# #TAB# return context #LINE# #TAB# return site
"Return btn bs3 class depending on status : param boolean : Boolean : return : "" btn - success "" and "" btn - danger "" depending on boolean <code> def f_btn(boolean): ",#LINE# #TAB# if boolean: #LINE# #TAB# #TAB# return 'btn-success' #LINE# #TAB# return 'btn-danger'
"Some boilerplate code for obtaining configuration dict from ctx . : return : config dict <code> def load_config(ctx, network): ","#LINE# #TAB# config = ctx.obj['config'] #LINE# #TAB# setting = config[network] #LINE# #TAB# if network not in CONFIG: #LINE# #TAB# #TAB# if click.confirm( #LINE# #TAB# #TAB# #TAB# 'Unfamiliar with that network. Do you wish to set it now?', #LINE# #TAB# #TAB# #TAB# abort=True): #LINE# #TAB# #TAB# #TAB# set_config(ctx) #LINE# #TAB# printer = ctx.obj['printer'] #LINE# #TAB# return setting, printer"
Ca n't get this to work ... <code> def set_terminal_size(size): ,"#LINE# #TAB# rows, cols = size #LINE# #TAB# return"
Parses a HTTP - style headers <code> def parse_headers(str): ,"#LINE# #TAB# headers = {} #LINE# #TAB# for line in str.split('\n'): #LINE# #TAB# #TAB# if len(line) == 0: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# key, value = line.split(': ', 1) #LINE# #TAB# #TAB# headers[key] = value #LINE# #TAB# return headers"
"Proxy of pop where we know we re popping a stack off of a stack  <code> def pop_stack(stack, op_id): ","#LINE# if __debug__: #LINE# #TAB# pushed_stack, pushed_op_id = stack.pop() #LINE# #TAB# assert pushed_op_id == op_id, 'Wanted %s, got %s' % (op_id, pushed_op_id) #LINE# else: #LINE# #TAB# pushed_stack = stack.pop() #LINE# return pushed_stack"
Given a string representing a variable return a new string that is safe for C ++ codegen . If string is already safe will leave it alone  <code> def safe_cpp_var(s): ,"#LINE# #TAB# s = str(s) #LINE# #TAB# s = re.sub(r""[^\w\s]"", '', s) #LINE# #TAB# s = re.sub(r""\s+"", '_', s) #LINE# #TAB# s = ""_"" + s if s in CPP_KEYWORDS or s[0].isdigit() else s #LINE# #TAB# return s"
Iterate indefinitely over a Sequence . # Arguments seq : Sequence object # Returns Generator yielding batches  <code> def iter_sequence_infinite(seq): ,#LINE# #TAB# while True: #LINE# #TAB# #TAB# for item in seq: #LINE# #TAB# #TAB# #TAB# yield item
Set the default prefix for where to find assets  <code> def set_default_asset_prefix(asset_prefix): ,#LINE# #TAB# global _DEFAULT_ASSET_PREFIX #LINE# #TAB# _DEFAULT_ASSET_PREFIX = asset_prefix
Fetch and parse crypto data <code> def fetch_crypto_data(): ,"#LINE# #TAB# global crypto_data #LINE# #TAB# if not crypto_data: #LINE# #TAB# #TAB# response = requests.get(CRYPTO_LIST).json() #LINE# #TAB# #TAB# crypto_data = {symbol.lower(): data for symbol, data in response[ #LINE# #TAB# #TAB# #TAB# 'Data'].items()} #LINE# #TAB# return crypto_data"
Check if string can be converted to float  <code> def is_float(string): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# float(string) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# else: #LINE# #TAB# #TAB# return True
Hook : for checking commit message  <code> def post_commit_hook(argv=None): ,"#LINE# #TAB# _, stdout, _ = run('git log -1 --format=%B HEAD') #LINE# #TAB# message = '\n'.join(stdout) #LINE# #TAB# options = {'allow_empty': True} #LINE# #TAB# if not _check_message(message, options): #LINE# #TAB# #TAB# print(""Commit message errors (fix with 'git commit --amend')."", #LINE# #TAB# #TAB# #TAB# file=sys.stderr) #LINE# #TAB# #TAB# return 1 #LINE# #TAB# return 0"
"Returns True if the parameter contract is a custom contract , False otherwise <code> def get_custom_contract(param_contract): ","#LINE# #TAB# if not isinstance(param_contract, str): #LINE# #TAB# #TAB# return None #LINE# #TAB# for custom_contract in _CUSTOM_CONTRACTS: #LINE# #TAB# #TAB# if re.search('\\b{0}\\b'.format(custom_contract), param_contract): #LINE# #TAB# #TAB# #TAB# return custom_contract #LINE# #TAB# return None"
"Generate a new Message instance from this Descriptor and a byte string . Args : descriptor : Protobuf Descriptor object byte_str : Serialized protocol buffer byte string Returns : Newly created protobuf Message object  <code> def parse_message(descriptor, byte_str): ",#LINE# #TAB# result_class = MakeClass(descriptor) #LINE# #TAB# new_msg = result_class() #LINE# #TAB# new_msg.ParseFromString(byte_str) #LINE# #TAB# return new_msg
"Parse type and ID from an URI . Parameters ---------- uri URI to parse Returns ------- tuple ( type , ID ) parsed from the URI <code> def from_uri(uri: str) ->tuple: ","#LINE# #TAB# spotify, type_, id_ = uri.split(':') #LINE# #TAB# if spotify != 'spotify': #LINE# #TAB# #TAB# raise ConversionError(f'Invalid URI prefix ""{spotify}""!') #LINE# #TAB# check_type(type_) #LINE# #TAB# check_id(id_) #LINE# #TAB# return type_, id_"
Remember current system time and set ` ` t ` ` marker to that . Useful to count execution time of some parts of the code  <code> def time_push(t): ,#LINE# #TAB# global _TimeTotalDict #LINE# #TAB# global _TimeDeltaDict #LINE# #TAB# global _TimeCountsDict #LINE# #TAB# tm = time.time() #LINE# #TAB# if t not in _TimeTotalDict: #LINE# #TAB# #TAB# _TimeTotalDict[t] = 0.0 #LINE# #TAB# #TAB# _TimeCountsDict[t] = 0 #LINE# #TAB# _TimeDeltaDict[t] = tm
Returns a selection for content_type  <code> def content_type_selection(cls): ,"#LINE# #TAB# default_types = [('html', 'HTML'), ('plain', 'Plain Text')] #LINE# #TAB# if markdown: #LINE# #TAB# #TAB# default_types.append(('markdown', 'Markdown')) #LINE# #TAB# if publish_parts: #LINE# #TAB# #TAB# default_types.append(('rst', 'reStructured TeXT')) #LINE# #TAB# return default_types"
"Checks whether it is valid shapefile or not . input "" path "" : type string , path to file which shall be extracted output true if file is valid , false if not <code> def is_valid(path): ","#LINE# #TAB# pathWithoutEnding = path[:len(path) - 4] #LINE# #TAB# if not (hf.exists(pathWithoutEnding + '.dbf') and hf.exists( #LINE# #TAB# #TAB# pathWithoutEnding + '.shp') and hf.exists(pathWithoutEnding + '.shx')): #LINE# #TAB# #TAB# return False #LINE# #TAB# try: #LINE# #TAB# #TAB# mydbf = open(pathWithoutEnding + '.dbf', 'rb') #LINE# #TAB# #TAB# myshp = open(pathWithoutEnding + '.shp', 'rb') #LINE# #TAB# #TAB# myshx = open(pathWithoutEnding + '.shx', 'rb') #LINE# #TAB# #TAB# r = shapefile.Reader(shp=myshp, dbf=mydbf, shx=myshx) #LINE# #TAB# except: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Save window crop as image file to be read by PIL . Filename should match the image_name + window index <code> def save_crop(base_dir, image_name, index, crop): ","#LINE# #TAB# if not os.path.exists(base_dir): #LINE# #TAB# #TAB# os.makedirs(base_dir) #LINE# #TAB# im = Image.fromarray(crop) #LINE# #TAB# image_basename = os.path.splitext(image_name)[0] #LINE# #TAB# filename = '{}/{}_{}.png'.format(base_dir, image_basename, index) #LINE# #TAB# im.save(filename) #LINE# #TAB# return filename"
Return the value of the Earth Rotation Angle ( theta ) for a UT1 date . Uses the expression from the note to IAU Resolution B1.8 of 2000 . Returns a fraction between 0.0 and 1.0 whole rotations  <code> def earth_rotation_angle(jd_ut1): ,#LINE# #TAB# thet1 = 0.779057273264 + 0.00273781191135448 * (jd_ut1 - T0) #LINE# #TAB# thet3 = jd_ut1 % 1.0 #LINE# #TAB# return (thet1 + thet3) % 1.0
Write RPCAP block data  <code> def write_rpcap(parameters): ,#LINE# #TAB# data = deepcopy(default) #LINE# #TAB# data.update(parameters['default']) #LINE# #TAB# out = [] #LINE# #TAB# out += add_record(data['relative_permeability'] #LINE# #TAB# #TAB# ) if 'relative_permeability' in data.keys() else write_record([]) #LINE# #TAB# out += add_record(data['capillarity']) if 'capillarity' in data.keys( #LINE# #TAB# #TAB# ) else write_record([]) #LINE# #TAB# return out
"Given an integer bitmap and an enumerated type , build a set that includes zero or more enumerated type values corresponding to the bitmap  <code> def unpack_bitmap(bitmap, xenum): ",#LINE# #TAB# unpacked = set() #LINE# #TAB# for enval in xenum: #LINE# #TAB# #TAB# if enval.value & bitmap == enval.value: #LINE# #TAB# #TAB# #TAB# unpacked.add(enval) #LINE# #TAB# return unpacked
Check if the object is a file - like object  <code> def is_file_like(obj): ,"#LINE# #TAB# if not (hasattr(obj, 'read') or hasattr(obj, 'write')): #LINE# #TAB# #TAB# return False #LINE# #TAB# if not hasattr(obj, ""__iter__""): #LINE# #TAB# #TAB# return False #LINE# #TAB# return True"
"Create a WebhookMetadata from a comment added to an issue  <code> def build_from_issue_comment(gh_token, body): ","#LINE# #TAB# if body['action'] in ['created', 'edited']: #LINE# #TAB# #TAB# github_con = Github(gh_token) #LINE# #TAB# #TAB# repo = github_con.get_repo(body['repository']['full_name']) #LINE# #TAB# #TAB# issue = repo.get_issue(body['issue']['number']) #LINE# #TAB# #TAB# text = body['comment']['body'] #LINE# #TAB# #TAB# try: #LINE# #TAB# #TAB# #TAB# comment = issue.get_comment(body['comment']['id']) #LINE# #TAB# #TAB# except UnknownObjectException: #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# #TAB# return WebhookMetadata(repo, issue, text, comment) #LINE# #TAB# return None"
"The command generator when state is merged : rtype : A list : returns : the commands necessary to merge the provided into the current configuration <code> def state_merged(want, have): ","#LINE# #TAB# commands = [] #LINE# #TAB# if not have: #LINE# #TAB# #TAB# have = {'name': want['name']} #LINE# #TAB# for key, value in iteritems(flatten_dict(remove_empties(dict_diff(have, #LINE# #TAB# #TAB# want)))): #LINE# #TAB# #TAB# commands.append(Lacp_interfaces._compute_commands(key, value)) #LINE# #TAB# if commands: #LINE# #TAB# #TAB# pad_commands(commands, want['name']) #LINE# #TAB# return commands"
List all the existing screens and build a Screen instance for each  <code> def list_screens(): ,#LINE# #TAB# list_cmd = 'screen -ls' #LINE# #TAB# return [Screen('.'.join(l.split('.')[1:]).split('\t')[0]) for l in #LINE# #TAB# #TAB# getoutput(list_cmd).split('\n') if '\t' in l and '.'.join(l.split( #LINE# #TAB# #TAB# '.')[1:]).split('\t')[0]]
"Get the current alternate setting of the interface . dev is the Device object to which the request will be sent to  <code> def get_interface(dev, bInterfaceNumber): ","#LINE# #TAB# bmRequestType = util.build_request_type(util.CTRL_IN, util. #LINE# #TAB# #TAB# CTRL_TYPE_STANDARD, util.CTRL_RECIPIENT_INTERFACE) #LINE# #TAB# return dev.ctrl_transfer(bmRequestType=bmRequestType, bRequest=10, #LINE# #TAB# #TAB# wIndex=bInterfaceNumber, data_or_wLength=1)[0]"
"Returns the index of the last element of the list which matches the predicate , or -1 if no element matches . Acts as a transducer if a transformer is given in list position <code> def find_last_index(p, xs): ","#LINE# #TAB# for i in (i for i, x in enumerate(reversed(xs)) if p(x)): #LINE# #TAB# #TAB# return len(xs) - i"
Loads the MNIST dataset : return : <code> def load_mnist(dataset_path): ,"#LINE# #TAB# mnist = torchvision.datasets.MNIST(root=dataset_path, download=True, #LINE# #TAB# #TAB# train=True) #LINE# #TAB# x_train, y_train = mnist.train_data.numpy(), mnist.train_labels.numpy() #LINE# #TAB# mnist = torchvision.datasets.MNIST(root=dataset_path, download=True, #LINE# #TAB# #TAB# train=False) #LINE# #TAB# x_test, y_test = mnist.test_data.numpy(), mnist.test_labels.numpy() #LINE# #TAB# x_train = x_train.reshape((x_train.shape[0], -1)) / 255.0 #LINE# #TAB# x_test = x_test.reshape((x_test.shape[0], -1)) / 255.0 #LINE# #TAB# return np.float32(x_train), y_train, np.float32(x_test), y_test"
"Wrapper for source.read_segment that secedes from worker thread pool <code> def read_segment(st, segment, cfile, vys_timeout): ","#LINE# #TAB# logger.info('Reading datasetId {0}, segment {1} locally.'.format(st. #LINE# #TAB# #TAB# metadata.scanId, segment)) #LINE# #TAB# with distributed.worker_client() as cl_loc: #LINE# #TAB# #TAB# fut = cl_loc.submit(source.read_segment, st, segment, cfile, #LINE# #TAB# #TAB# #TAB# vys_timeout) #LINE# #TAB# #TAB# data = fut.result() #LINE# #TAB# logger.info('Finished reading datasetId {0}, segment {1} locally.'. #LINE# #TAB# #TAB# format(st.metadata.scanId, segment)) #LINE# #TAB# return data"
this method will convert string none to the python none type <code> def convert_string_none(value): ,#LINE# #TAB# if type(value) is str: #LINE# #TAB# #TAB# val = value.lower() #LINE# #TAB# #TAB# if val == 'none' or val == 'null': #LINE# #TAB# #TAB# #TAB# return None #LINE# #TAB# return value
"Create a UNTL XML subelement  <code> def create_untl_xml_subelement(parent, element, prefix=''): ","#LINE# #TAB# subelement = SubElement(parent, prefix + element.tag) #LINE# #TAB# if element.content is not None: #LINE# #TAB# #TAB# subelement.text = element.content #LINE# #TAB# if element.qualifier is not None: #LINE# #TAB# #TAB# subelement.attrib[""qualifier""] = element.qualifier #LINE# #TAB# if element.children > 0: #LINE# #TAB# #TAB# for child in element.children: #LINE# #TAB# #TAB# #TAB# SubElement(subelement, prefix + child.tag).text = child.content #LINE# #TAB# else: #LINE# #TAB# #TAB# subelement.text = element.content #LINE# #TAB# return subelement"
"A view response shortcut which finds the required template by some concatenating and then process the return object with a RequestContext and some optional context as specified in the ` ` extra_vars ` ` parameter <code> def response_shortcut(request, template=False, extra_vars=False): ","#LINE# #TAB# template = skeleton(template) if template else skeleton(request.GET['v' #LINE# #TAB# #TAB# ], request.GET['sctn']) #LINE# #TAB# context = RequestContext(request, {}, [site_proc]) #LINE# #TAB# if extra_vars: #LINE# #TAB# #TAB# context.update(extra_vars) #LINE# #TAB# h = HttpResponse(template.render(context)) #LINE# #TAB# if template == 'form_errors': #LINE# #TAB# #TAB# h.set_cookie('tt_formContainsErrors', 'true') #LINE# #TAB# return h"
"Write a CSV / JSON file at location given a list of About objects . Return a list of Error objects  <code> def write_output(abouts, location, format): ","#LINE# #TAB# about_dicts = about_object_to_list_of_dictionary(abouts) #LINE# #TAB# location = add_unc(location) #LINE# #TAB# if format == 'csv': #LINE# #TAB# #TAB# errors = save_as_csv(location, about_dicts, get_field_names(abouts)) #LINE# #TAB# else: #LINE# #TAB# #TAB# errors = save_as_json(location, about_dicts) #LINE# #TAB# return errors"
"Check if an API key is valid <code> def check_api_key(request, key, hproPk): ","#LINE# #TAB# if settings.PIAPI_STANDALONE: #LINE# #TAB# #TAB# return True #LINE# #TAB# (_, _, hproject) = getPlugItObject(hproPk) #LINE# #TAB# if not hproject: #LINE# #TAB# #TAB# return False #LINE# #TAB# if hproject.plugItApiKey is None or hproject.plugItApiKey == '': #LINE# #TAB# #TAB# return False #LINE# #TAB# return hproject.plugItApiKey == key"
Return a list of file paths to each data file in the specified corpus  <code> def list_corpus_files(dotted_path): ,"#LINE# #TAB# CORPUS_EXTENSION = 'yml' #LINE# #TAB# corpus_path = get_file_path(dotted_path, extension=CORPUS_EXTENSION) #LINE# #TAB# paths = [] #LINE# #TAB# if os.path.isdir(corpus_path): #LINE# #TAB# #TAB# paths = glob2.glob(corpus_path + '/**/*.' + CORPUS_EXTENSION) #LINE# #TAB# else: #LINE# #TAB# #TAB# paths.append(corpus_path) #LINE# #TAB# paths.sort() #LINE# #TAB# return paths"
"Writes ` key`-`value ` pair to ` module ` 's state hook  <code> def set_state(module, key, value): ","#LINE# #TAB# if hasattr(module, '_state_hooks'): #LINE# #TAB# #TAB# state_hooks = getattr(module, '_state_hooks') #LINE# #TAB# #TAB# assert isinstance(state_hooks, dict #LINE# #TAB# #TAB# #TAB# ), 'State hook (i.e. module._state_hooks) is not a dictionary.' #LINE# #TAB# #TAB# state_hooks.update({key: value}) #LINE# #TAB# else: #LINE# #TAB# #TAB# setattr(module, '_state_hooks', {key: value}) #LINE# #TAB# return module"
"Check whether file exists : param isamAppliance : : param instance_id : : param i d : : return : True / False <code> def check_file(isamAppliance, instance_id, id): ","#LINE# #TAB# ret_obj = get(isamAppliance, instance_id, id) #LINE# #TAB# logger.info(ret_obj['data']) #LINE# #TAB# if ret_obj['rc'] == 0: #LINE# #TAB# #TAB# return True #LINE# #TAB# else: #LINE# #TAB# #TAB# return False"
"URL decode a single string with a given encoding . If the charset is set to None no unicode decoding is performed and raw bytes are returned  <code> def url_unquote(string, charset='utf-8', errors='replace', unsafe=''): ","#LINE# #TAB# rv = _unquote_to_bytes(string, unsafe) #LINE# #TAB# if charset is not None: #LINE# #TAB# #TAB# rv = rv.decode(charset, errors) #LINE# #TAB# return rv"
"Takes an existing ` ` context ` ` object ( as constructed in ` ` get_context_data ( ) ` ` and populates any session messages we may have  <code> def populate_session_msg_context(request, context): ","#LINE# #TAB# for varname in ['success', 'fail']: #LINE# #TAB# #TAB# session_var = 'exordium_msg_%s' % varname #LINE# #TAB# #TAB# ctx_var = 'messages_%s' % varname #LINE# #TAB# #TAB# if session_var in request.session: #LINE# #TAB# #TAB# #TAB# if len(request.session[session_var]) > 0: #LINE# #TAB# #TAB# #TAB# #TAB# context[ctx_var] = request.session[session_var] #LINE# #TAB# #TAB# #TAB# #TAB# del request.session[session_var]"
"Converts an AST node into source code . Args : node : Any AST node derived from ast . AST . Returns : str : A string containing the source code corresponding to the AST  <code> def to_source(node, parent_op=None, descend=0): ","#LINE# #TAB# if node is None: #LINE# #TAB# #TAB# return None #LINE# #TAB# func = mapping[node.__class__] #LINE# #TAB# if func in (_src_UnaryOp, _src_BinOp, _src_BoolOp, _src_Compare): #LINE# #TAB# #TAB# rtn = func(node, parent_op, descend) #LINE# #TAB# else: #LINE# #TAB# #TAB# rtn = func(node) #LINE# #TAB# return rtn"
"A content file that converts lf to crlf  <code> def to_crlf_converter(chunks, context=None): ","#LINE# #TAB# content = b''.join(chunks) #LINE# #TAB# if b'\x00' in content: #LINE# #TAB# #TAB# return [content] #LINE# #TAB# else: #LINE# #TAB# #TAB# return [_UNIX_NL_RE.sub(b'\r\n', content)]"
"Returns the names of all nova active compute servers : param nova : the Nova client : param zone_name : the Nova client : return : a list of compute server names <code> def get_availability_zone_hosts(nova, zone_name='nova'): ","#LINE# #TAB# out = list() #LINE# #TAB# zones = nova.availability_zones.list() #LINE# #TAB# for zone in zones: #LINE# #TAB# #TAB# if zone.zoneName == zone_name and zone.hosts: #LINE# #TAB# #TAB# #TAB# for key, host in zone.hosts.items(): #LINE# #TAB# #TAB# #TAB# #TAB# if host['nova-compute']['available']: #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# out.append(zone.zoneName + ':' + key) #LINE# #TAB# return out"
"Calculate the component black hole masses from the total mass of the system and the mass ratio  <code> def components_from_total(total_mass, mass_ratio): ","#LINE# #TAB# m1 = total_mass / (mass_ratio + 1) #LINE# #TAB# m2 = total_mass - m1 #LINE# #TAB# return m1, m2"
Return True if s operates like a string  <code> def string_like(s): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# s + '' #LINE# #TAB# except Exception: #LINE# #TAB# #TAB# return False #LINE# #TAB# return True
Makes sure domain is valid with proper format  <code> def is_domain(domain: str) ->bool: ,"#LINE# #TAB# if not domain or type(domain) is not str: #LINE# #TAB# #TAB# return False #LINE# #TAB# domain_regex = '[a-z0-9]+([a-z0-9\\-]+\\.)+([a-z]+[a-z0-9]?)+$' #LINE# #TAB# if not re.match('^' + domain_regex + '$', domain.lower()) is None: #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
Return programming language used for repository <code> def get_programming_language(repo_info): ,"#LINE# #TAB# try: #LINE# #TAB# #TAB# return repo_info.find('span', {'itemprop': 'programmingLanguage'} #LINE# #TAB# #TAB# #TAB# ).text.strip() #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return"
"Ignore tzinfo unless convertToUTC . Output string  <code> def date_time_to_string(dateTime, convertToUTC=False): ","#LINE# #TAB# if dateTime.tzinfo and convertToUTC: #LINE# #TAB# #TAB# dateTime = dateTime.astimezone(utc) #LINE# #TAB# datestr = '{}{}{}T{}{}{}'.format(numToDigits(dateTime.year, 4), #LINE# #TAB# #TAB# numToDigits(dateTime.month, 2), numToDigits(dateTime.day, 2), #LINE# #TAB# #TAB# numToDigits(dateTime.hour, 2), numToDigits(dateTime.minute, 2), #LINE# #TAB# #TAB# numToDigits(dateTime.second, 2)) #LINE# #TAB# if tzinfo_eq(dateTime.tzinfo, utc): #LINE# #TAB# #TAB# datestr += 'Z' #LINE# #TAB# return datestr"
"Get all user_ids or group_ids depending on ` ` user_type ` ` . See : py : mod:`toyz.utils.db ` for more info  <code> def get_all_ids(db_settings, user_type): ","#LINE# #TAB# db = sqlite3.connect(db_settings.path) #LINE# #TAB# users = db.execute('select user_id from users where user_type=?;', ( #LINE# #TAB# #TAB# user_type,)) #LINE# #TAB# users = users.fetchall() #LINE# #TAB# db.close() #LINE# #TAB# return [u[0] for u in users]"
List all auto updating feeds  <code> def list_all_auto_updating() ->typing.List[views.Feed]: ,"#LINE# #TAB# return [views.Feed.from_model(feed, add_system=True) for feed in #LINE# #TAB# #TAB# feeddam.list_all_auto_updating()]"
"selective postorder iteration - walk is postorder ; - if select is specified , return the node if select(node ) is True - if ignore is specified , skip completely the subtree rooted in node <code> def filtered_postorder_iterator(node, select=None, ignore=None): ","#LINE# #TAB# if ignore and ignore(node): #LINE# #TAB# #TAB# return #LINE# #TAB# for child in node.children: #LINE# #TAB# #TAB# yield from filtered_postorder_iterator(child, select, ignore) #LINE# #TAB# if select is None or select(node): #LINE# #TAB# #TAB# yield node"
Retrieve original items in a batch handling CWL and standard cases  <code> def get_orig_items(data): ,"#LINE# #TAB# if isinstance(data, dict): #LINE# #TAB# #TAB# if dd.get_align_bam(data) and tz.get_in([""metadata"", ""batch""], data) and ""group_orig"" in data: #LINE# #TAB# #TAB# #TAB# return vmulti.get_orig_items(data) #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# return [data] #LINE# #TAB# else: #LINE# #TAB# #TAB# return data"
Returns the list of the driver 's Module(s ) name(s ) ( classes Module_XXX ) <code> def get_module_names(driver_lib): ,"#LINE# #TAB# return [name.split('_')[1] for name, obj in inspect.getmembers( #LINE# #TAB# #TAB# driver_lib, inspect.isclass) if obj.__module__ is driver_lib. #LINE# #TAB# #TAB# __name__ and name.startswith('Module_')]"
"Convert numpy points to a vtkPoints object <code> def vtk_points(points, deep=True): ","#LINE# #TAB# if not points.flags['C_CONTIGUOUS']: #LINE# #TAB# #TAB# points = np.ascontiguousarray(points) #LINE# #TAB# vtkpts = vtk.vtkPoints() #LINE# #TAB# vtkpts.SetData(numpy_to_vtk(points, deep=deep)) #LINE# #TAB# return vtkpts"
"Returns the website with name as given host If not silent a website not found error is raised  <code> def get_from_host(cls, host, silent=False): ","#LINE# #TAB# websites = cls.search([]) #LINE# #TAB# if len(websites) == 1: #LINE# #TAB# #TAB# return websites[0] #LINE# #TAB# try: #LINE# #TAB# #TAB# website, = cls.search([('name', '=', host)]) #LINE# #TAB# except ValueError: #LINE# #TAB# #TAB# if not silent: #LINE# #TAB# #TAB# #TAB# raise WebsiteNotFound() #LINE# #TAB# else: #LINE# #TAB# #TAB# return website"
"Lookup occurrence issue definitions and short codes <code> def occ_issues_lookup(issue=None, code=None): ","#LINE# #TAB# if code is None: #LINE# #TAB# #TAB# bb = [trymatch(issue, x) for x in gbifissues['issue'] ] #LINE# #TAB# #TAB# tmp = filter(None, bb) #LINE# #TAB# else: #LINE# #TAB# #TAB# bb = [trymatch(code, x) for x in gbifissues['code'] ] #LINE# #TAB# #TAB# tmp = filter(None, bb) #LINE# #TAB# return tmp"
"Return usage for specified hour  <code> def get_usage_for_date(usage_dict, start): ","#LINE# #TAB# usage_amount = None #LINE# #TAB# if usage_dict: #LINE# #TAB# #TAB# for date, usage in usage_dict.items(): #LINE# #TAB# #TAB# #TAB# if date == 'full_period': #LINE# #TAB# #TAB# #TAB# #TAB# usage_amount = usage #LINE# #TAB# #TAB# #TAB# elif parser.parse(date).date() == start.date(): #LINE# #TAB# #TAB# #TAB# #TAB# usage_amount = usage #LINE# #TAB# return usage_amount"
defines the way to parse the magic command ` ` % HIVE_azure ` ` <code> def hive_azure_parser(): ,"#LINE# #TAB# parser = MagicCommandParser(prog='HIVE_azure', description= #LINE# #TAB# #TAB# 'The command store the content of the cell as a local file.') #LINE# #TAB# parser.add_argument('file', type=str, help='file name') #LINE# #TAB# return parser"
returns the greatest - length common portion of the path in the given collection of file - names <code> def common_root_path(file_names): ,"#LINE# #TAB# common = file_names[0] #LINE# #TAB# for fname in file_names[1:]: #LINE# #TAB# #TAB# common = ''.join([x[0] for x in zip(common, fname) if x[0] == x[1]]) #LINE# #TAB# return common"
"Return the default value for given pattern . Parameters ---------- pat : string Returns ------- RegisteredOption ( namedtuple ) if key is deprecated , None otherwise <code> def get_default_val(pat): ","#LINE# #TAB# key = _get_single_key(pat, silent=True) #LINE# #TAB# return _get_registered_option(key).defval"
Helper to check if func is wraped by our deprecated decorator <code> def is_deprecated(func): ,"#LINE# #TAB# if sys.version_info < (3, 5): #LINE# #TAB# #TAB# raise NotImplementedError( #LINE# #TAB# #TAB# #TAB# 'This is only available for python3.5 or above') #LINE# #TAB# closures = getattr(func, '__closure__', []) #LINE# #TAB# if closures is None: #LINE# #TAB# #TAB# closures = [] #LINE# #TAB# is_deprecated = 'deprecated' in ''.join([c.cell_contents for c in #LINE# #TAB# #TAB# closures if isinstance(c.cell_contents, str)]) #LINE# #TAB# return is_deprecated"
Return given sequence as a dictionary with extra data like project name  <code> def get_full_sequence(sequence_id): ,#LINE# #TAB# sequence = get_sequence(sequence_id) #LINE# #TAB# project = projects_service.get_project(sequence['project_id']) #LINE# #TAB# sequence['project_name'] = project['name'] #LINE# #TAB# if sequence['parent_id'] is not None: #LINE# #TAB# #TAB# episode = get_episode(sequence['parent_id']) #LINE# #TAB# #TAB# sequence['episode_id'] = episode['id'] #LINE# #TAB# #TAB# sequence['episode_name'] = episode['name'] #LINE# #TAB# return sequence
"Splits off the species component of the allele name from the rest of it . Given "" HLA - A*02:01 "" , returns ( "" HLA "" , "" A*02:01 "" )  <code> def split_species_prefix(name, seps='-:_ '): ","#LINE# #TAB# species = None #LINE# #TAB# name_upper = name.upper() #LINE# #TAB# name_len = len(name) #LINE# #TAB# for curr_prefix in _all_prefixes: #LINE# #TAB# #TAB# n = len(curr_prefix) #LINE# #TAB# #TAB# if name_len <= n: #LINE# #TAB# #TAB# #TAB# continue #LINE# #TAB# #TAB# if name_upper.startswith(curr_prefix.upper()): #LINE# #TAB# #TAB# #TAB# species = curr_prefix #LINE# #TAB# #TAB# #TAB# name = name[n:].strip(seps) #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return species, name"
get the last ' return < object > ' statement <code> def line_last_return_subject(line): ,#LINE# #TAB# lista = list(reversed(line.split())) #LINE# #TAB# if len(lista) > 1: #LINE# #TAB# #TAB# for el in lista: #LINE# #TAB# #TAB# #TAB# if 'return' in el: #LINE# #TAB# #TAB# #TAB# #TAB# s = lista[lista.index('return') - 1] #LINE# #TAB# #TAB# #TAB# #TAB# return s.split('[')[0]
"Setup a jinja env based on the tool options <code> def env_create(options, default_template=None): ","#LINE# #TAB# LOG.info('Template dirs:') #LINE# #TAB# for t in options.templatedirs: #LINE# #TAB# #TAB# LOG.info("" - '%s'"", t) #LINE# #TAB# loaders = [UTF8FileSystemLoader(t) for t in options.templatedirs] #LINE# #TAB# if default_template is not None: #LINE# #TAB# #TAB# loaders.append(DefaultTemplateLoader('default', default_template)) #LINE# #TAB# _loader = ChoiceLoader(loaders) #LINE# #TAB# env = Environment(loader=_loader, block_start_string='/*{%', #LINE# #TAB# #TAB# block_end_string='%}*/', variable_start_string='/*{{', #LINE# #TAB# #TAB# variable_end_string='}}*/', comment_start_string='/*{#', #LINE# #TAB# #TAB# comment_end_string='#}*/') #LINE# #TAB# return env"
Increments a string that ends in a number <code> def increment_title(title): ,"#LINE# #TAB# count = re.search('\d+$', title).group(0) #LINE# #TAB# new_title = title[:-(len(count))] + str(int(count)+1) #LINE# #TAB# return new_title"
"Fill the model with alias since V1 <code> def model_alias(vk, model): ","#LINE# #TAB# model['alias'] = {} #LINE# #TAB# for s in vk['registry']['types']['type']: #LINE# #TAB# #TAB# if s.get('@category', None) == 'handle' and s.get('@alias'): #LINE# #TAB# #TAB# #TAB# model['alias'][s['@alias']] = s['@name'] #LINE# #TAB# for c in vk['registry']['commands']['command']: #LINE# #TAB# #TAB# if c.get('@alias'): #LINE# #TAB# #TAB# #TAB# model['alias'][c['@alias']] = c['@name']"
"Converts a firestore document snapshot to FirestoreObject : param snapshot : firestore document snapshot : param super_cls : subclass of FirestoreObject : return : <code> def snapshot_to_obj(snapshot: DocumentSnapshot, super_cls: T=None) ->T: ","#LINE# #TAB# if not snapshot.exists: #LINE# #TAB# #TAB# return None #LINE# #TAB# d = snapshot.to_dict() #LINE# #TAB# obj_type = d['obj_type'] #LINE# #TAB# obj_cls = ModelRegistry.get_cls_from_name(obj_type) #LINE# #TAB# if obj_cls is None: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# 'Cannot read obj_type: {}. Make sure that obj_type is a subclass of {}. ' #LINE# #TAB# #TAB# #TAB# .format(obj_type, super_cls)) #LINE# #TAB# if super_cls is not None: #LINE# #TAB# #TAB# assert issubclass(obj_cls, super_cls) #LINE# #TAB# obj = obj_cls.from_dict(d=d, doc_ref=snapshot.reference) #LINE# #TAB# return obj"
"Update a method object  <code> def update_method(oldmeth, newmeth): ","#LINE# #TAB# _update_function(six.get_unbound_function(oldmeth), six. #LINE# #TAB# #TAB# get_unbound_function(newmeth)) #LINE# #TAB# return oldmeth"
` ` float SDL_cosf(float ) ` ` <code> def sdl_cosf(x): ,#LINE# #TAB# x_c = x #LINE# #TAB# rc = lib.sdl_cosf(x_c) #LINE# #TAB# return rc
Convert the short form of an argument to its long form  <code> def short_to_long(arg: str) ->str: ,#LINE# #TAB# if arg == '-h': #LINE# #TAB# #TAB# return '--help' #LINE# #TAB# elif arg == '-v': #LINE# #TAB# #TAB# return '--version' #LINE# #TAB# elif arg == '-q': #LINE# #TAB# #TAB# return '--quiet' #LINE# #TAB# else: #LINE# #TAB# #TAB# return arg
Identify the best loader based on connection  <code> def get_loader(conn) ->Callable: ,#LINE# #TAB# content_type = _get_content_type(conn) #LINE# #TAB# if 'json' in content_type: #LINE# #TAB# #TAB# return json.load #LINE# #TAB# return CONTENT_LOADER
Convert a constraint to SI if possible  <code> def constraint_to_si(expr): ,"#LINE# #TAB# satisfiable = True #LINE# #TAB# replace_list = [ ] #LINE# #TAB# satisfiable, replace_list = backends.vsa.constraint_to_si(expr) #LINE# #TAB# for i in xrange(len(replace_list)): #LINE# #TAB# #TAB# ori, new = replace_list[i] #LINE# #TAB# #TAB# if not isinstance(new, Base): #LINE# #TAB# #TAB# #TAB# new = BVS(new.name, new._bits, min=new._lower_bound, max=new._upper_bound, stride=new._stride, explicit_name=True) #LINE# #TAB# #TAB# #TAB# replace_list[i] = (ori, new) #LINE# #TAB# return satisfiable, replace_list"
Issue A - RELEASE confirmation primitive and close transport connection  <code> def ar_3(provider): ,#LINE# #TAB# provider.to_service_user.put(provider.primitive) #LINE# #TAB# provider.dul_socket.close() #LINE# #TAB# provider.dul_socket = None #LINE# #TAB# return 'Sta1'
Try to get the local path for a tracked object <code> def try_get_local_path(target): ,#LINE# #TAB# try: #LINE# #TAB# #TAB# return target.local #LINE# #TAB# except AttributeError: #LINE# #TAB# #TAB# return target
"Return an compare attribute , supporting AT , DX and IEventAccessor objects  <code> def get_compare_attr(obj, attr): ","#LINE# #TAB# val = getattr(obj, attr, None) #LINE# #TAB# if safe_callable(val): #LINE# #TAB# #TAB# val = val() #LINE# #TAB# if isinstance(val, DateTime): #LINE# #TAB# #TAB# val = pydt(val) #LINE# #TAB# return val"
"Generate tree width suitable for ` ` merge_sorted ` ` given N inputs The larger N is , the more tasks are reduced in a single task . In theory , this is designed so all tasks are of comparable effort  <code> def tree_width(N, to_binary=False): ","#LINE# #TAB# if N < 32: #LINE# #TAB# #TAB# group_size = 2 #LINE# #TAB# else: #LINE# #TAB# #TAB# group_size = int(math.log(N)) #LINE# #TAB# num_groups = N // group_size #LINE# #TAB# if to_binary or num_groups < 16: #LINE# #TAB# #TAB# return 2 ** int(math.log(N / group_size, 2)) #LINE# #TAB# else: #LINE# #TAB# #TAB# return num_groups"
"Helper function . Checks whether found in the system libcapn version can be used by pypusher . : return : Tuple ( version_satisfied_bool , found_version_number ) <code> def lib_capn_version_satisfied(): ","#LINE# #TAB# if LIB_CAPN is None: #LINE# #TAB# #TAB# raise PusherException( #LINE# #TAB# #TAB# #TAB# 'Unable to find libcapn in the system. Verify it exists and registered (ldconfig).' #LINE# #TAB# #TAB# #TAB# ) #LINE# #TAB# version = LIB_CAPN.apn_version_string #LINE# #TAB# version.restype = ctypes.c_char_p #LINE# #TAB# version = version() #LINE# #TAB# if version.startswith(LIB_CAPN_VERSION_EXPECTED): #LINE# #TAB# #TAB# return True, version #LINE# #TAB# return False, None"
Prune the swagger ( in place ) of its empty paths ( ie paths with no verb ) <code> def prune_empty_paths(swagger): ,"#LINE# #TAB# actions = [] #LINE# #TAB# for endpoint_name, endpoint, path in get_elements(swagger, JSPATH_ENDPOINTS #LINE# #TAB# #TAB# ): #LINE# #TAB# #TAB# if not endpoint or len(endpoint) == 1 and 'parameters' in endpoint: #LINE# #TAB# #TAB# #TAB# del swagger['paths'][endpoint_name] #LINE# #TAB# #TAB# #TAB# actions.append(PathsEmptyFilterError(path=path, reason= #LINE# #TAB# #TAB# #TAB# #TAB# f""path '{endpoint_name}' has no operations defined"")) #LINE# #TAB# return swagger, actions"
Load single image file of common medical image types Parameters ---------- filename : TYPE DESCRIPTION . Raises ------ ValueError DESCRIPTION . Returns ------- img : TYPE DESCRIPTION  <code> def safe_load_medical_img(filename): ,"#LINE# #TAB# img = sitk.GetArrayFromImage(sitk.ReadImage(filename)) #LINE# #TAB# if np.ndim(img) != 3: #LINE# #TAB# #TAB# raise ValueError( #LINE# #TAB# #TAB# #TAB# f'Unsupported image dim {np.ndim(img)} (with shape {img.shape})') #LINE# #TAB# img = np.moveaxis(img, 0, 2) #LINE# #TAB# return img"
Serialize an array of objects  <code> def serialize_array(array: t.Iterable) ->bytes: ,"#LINE# #TAB# return b''.join(serialize_object(item, terminate=False) for item in array #LINE# #TAB# #TAB# ) + ARRAY_END_MARKER"
Load signal groups  <code> def load_signal_groups(tokens): ,"#LINE# #TAB# signal_groups = defaultdict(list) #LINE# #TAB# for signal_group in tokens.get('SIG_GROUP_', []): #LINE# #TAB# #TAB# frame_id = int(signal_group[1]) #LINE# #TAB# #TAB# signal_groups[frame_id].append(SignalGroup(name=signal_group[2], #LINE# #TAB# #TAB# #TAB# repetitions=int(signal_group[3]), signal_names=signal_group[5])) #LINE# #TAB# return signal_groups"
Checks whether a given deprecation name is a valid deprecation  <code> def is_deprecation(deprecation_name): ,"#LINE# #TAB# assert isinstance(deprecation_name, str #LINE# #TAB# #TAB# ), 'Deprecation name should be a string.' #LINE# #TAB# if hasattr(DeprecationCodes, deprecation_name): #LINE# #TAB# #TAB# return True #LINE# #TAB# return False"
"Decodes a floating point number stored in a single byte  <code> def byte_to_float(b, mantissabits=5, zeroexp=2): ","#LINE# #TAB# if type(b) is not int: #LINE# #TAB# #TAB# b = ord(b) #LINE# #TAB# if b == 0: #LINE# #TAB# #TAB# return 0.0 #LINE# #TAB# bits = (b & 255) << 24 - mantissabits #LINE# #TAB# bits += 63 - zeroexp << 24 #LINE# #TAB# return unpack('f', pack('i', bits))[0]"
"Ensures the child process dies if this process dies ( fate - sharing ) . Windows - only . Must be called by the parent , after spawning the child . Args : child_proc : The subprocess . Popen or subprocess . Handle object  <code> def set_kill_child_on_death_win32(child_proc): ","#LINE# #TAB# if isinstance(child_proc, subprocess.Popen): #LINE# #TAB# #TAB# child_proc = child_proc._handle #LINE# #TAB# assert isinstance(child_proc, subprocess.Handle) #LINE# #TAB# if detect_fate_sharing_support_win32(): #LINE# #TAB# #TAB# if not win32_AssignProcessToJobObject(win32_job, int(child_proc)): #LINE# #TAB# #TAB# #TAB# import ctypes #LINE# #TAB# #TAB# #TAB# raise OSError(ctypes.get_last_error(), #LINE# #TAB# #TAB# #TAB# #TAB# 'AssignProcessToJobObject() failed') #LINE# #TAB# else: #LINE# #TAB# #TAB# assert False, 'AssignProcessToJobObject used despite being unavailable'"
Format output of daemon to a dictionnary <code> def whos_to_dic(string): ,"#LINE# #TAB# variables = {} #LINE# #TAB# for item in string.split('\n')[2:]: #LINE# #TAB# #TAB# tmp = [j for j in item.split(' ') if j is not ''] #LINE# #TAB# #TAB# if tmp: #LINE# #TAB# #TAB# #TAB# var_name = tmp[0] #LINE# #TAB# #TAB# #TAB# var_typ = tmp[1] #LINE# #TAB# #TAB# #TAB# var_val = ' '.join(tmp[2:]) #LINE# #TAB# #TAB# #TAB# variables[var_name] = {'value': var_val, 'type': var_typ} #LINE# #TAB# return variables"
Get current IP value Returns : ---------- result : string current IP value <code> def see_ip(): ,"#LINE# #TAB# import socket #LINE# #TAB# s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) #LINE# #TAB# s.connect(('8.8.8.8', 80)) #LINE# #TAB# cw_ip = s.getsockname()[0] #LINE# #TAB# s.close() #LINE# #TAB# result = {'ip': str(cw_ip)} #LINE# #TAB# return result"
"Check that component name starts with an alphanumeric character , and disalllow all non - alphanumeric characters except underscore , hyphen , colon , and period in component names  <code> def validate_component_name(component_name, allow_slashes=False): ","#LINE# #TAB# allowed = '_', '-', ':', '.' #LINE# #TAB# if allow_slashes: #LINE# #TAB# #TAB# allowed += '/', #LINE# #TAB# if len(component_name) == 0: #LINE# #TAB# #TAB# return False #LINE# #TAB# if not component_name[0].isalnum(): #LINE# #TAB# #TAB# return False #LINE# #TAB# for c in component_name: #LINE# #TAB# #TAB# if c not in allowed and not c.isalnum(): #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True"
"Return ( width , height ) of console terminal on POSIX system . ( 0 , 0 ) on IOError , i.e. when no console is allocated  <code> def posix_get_window_size(): ","#LINE# #TAB# from fcntl import ioctl #LINE# #TAB# from termios import TIOCGWINSZ #LINE# #TAB# from array import array #LINE# #TAB# winsize = array('H', [0] * 4) #LINE# #TAB# try: #LINE# #TAB# #TAB# ioctl(sys.stdout.fileno(), TIOCGWINSZ, winsize) #LINE# #TAB# except IOError: #LINE# #TAB# #TAB# pass #LINE# #TAB# return winsize[1], winsize[0]"
"apply func to the locals at each stack frame till func returns a non false value <code> def find_locals(func, depth=0): ",#LINE# #TAB# while 1: #LINE# #TAB# #TAB# _ = func(sys._getframe(depth).f_locals) #LINE# #TAB# #TAB# if _: #LINE# #TAB# #TAB# #TAB# return _ #LINE# #TAB# #TAB# depth += 1
Check is text made of small hiragana <code> def is_small_hiragana(text: str) ->bool: ,#LINE# #TAB# small_hiragana = HIRAGANA[46:55] #LINE# #TAB# for character in text: #LINE# #TAB# #TAB# if character not in small_hiragana: #LINE# #TAB# #TAB# #TAB# return False #LINE# #TAB# return True
"Similar to parse_args ( ) , but stripping leading colons from the first argument of a line ( usually the sender field )  <code> def parse_prefixed_args(cls, args): ","#LINE# #TAB# args = cls.parse_args(args) #LINE# #TAB# args[0] = args[0].split(':', 1)[1] #LINE# #TAB# return args"
"Given profile data and the name of a social media service , format it for the zone file . @serviceName : name of the service @data : legacy profile verification Returns the formatted account on success , as a dict  <code> def format_account(service_name, data): ","#LINE# #TAB# assert 'username' in data, 'Missing username' #LINE# #TAB# account = {'@type': 'Account', 'service': service_name, 'identifier': #LINE# #TAB# #TAB# data['username'], 'proofType': 'http'} #LINE# #TAB# if data.has_key(service_name) and data[service_name].has_key('proof'): #LINE# #TAB# #TAB# account['proofUrl'] = data[service_name]['proof'] #LINE# #TAB# return account"
"Remove the rows denoted by indices form the CSR sparse matrix mat  <code> def delete_rows_csr(mat, indices): ","#LINE# #TAB# if not isinstance(mat, scipy.sparse.csr_matrix): #LINE# #TAB# #TAB# raise ValueError(""works only for CSR format -- use .tocsr() first"") #LINE# #TAB# indices = list(indices) #LINE# #TAB# mask = np.ones(mat.shape[0], dtype=bool) #LINE# #TAB# mask[indices] = False #LINE# #TAB# return mat[mask]"
Return information about table s fields in dictionary type  <code> def get_type_for_inputs(table): ,"#LINE# #TAB# #TAB# return [ #LINE# #TAB# #TAB# #TAB# dict( #LINE# #TAB# #TAB# #TAB# #TAB# type=INPUT_TYPES.get( #LINE# #TAB# #TAB# #TAB# #TAB# #TAB# type(field_type.type), rc.TEXT_INPUT.value #LINE# #TAB# #TAB# #TAB# #TAB# ), #LINE# #TAB# #TAB# #TAB# #TAB# name=name, #LINE# #TAB# #TAB# #TAB# #TAB# isPrimaryKey=(name in table.primary_key), #LINE# #TAB# #TAB# #TAB# #TAB# props=None, #LINE# #TAB# #TAB# #TAB# ) for name, field_type in table.c.items() #LINE# #TAB# #TAB# ]"
"Adapter for fields of date / datetime / time <code> def datetime_auth_adapter(field, ctx, source_dict, target_obj): ","#LINE# #TAB# name = field._compute_property(ctx) #LINE# #TAB# source = field.to_python_value(ctx, source_dict[name]) #LINE# #TAB# target = field._get(target_obj) #LINE# #TAB# return name, source, target"
fills the create filename template dialog <code> def dialog_create_filename_template(request): ,"#LINE# #TAB# entity_types = ['Asset', 'Shot', 'Sequence', 'Project', 'Task'] #LINE# #TAB# return {'mode': 'CREATE', 'has_permission': PermissionChecker(request), #LINE# #TAB# #TAB# 'entity_types': entity_types}"
"Merge the ' second ' multiple - dictionary into the ' first ' one  <code> def merge_dicts(first, second): ","#LINE# #TAB# new = deepcopy(first) #LINE# #TAB# for k, v in second.items(): #LINE# #TAB# #TAB# if isinstance(v, dict) and v: #LINE# #TAB# #TAB# #TAB# ret = merge_dicts(new.get(k, dict()), v) #LINE# #TAB# #TAB# #TAB# new[k] = ret #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# new[k] = second[k] #LINE# #TAB# return new"
"Yield an warning message ` msg ` if ` expr ` is False <code> def warn_if(expr, msg=''): ",#LINE# #TAB# if expr: #LINE# #TAB# #TAB# env.logger.warning(msg) #LINE# #TAB# return 0
"returns the minimum platform version that can load the given class version indicated by major . minor or None if no known platforms match the given version <code> def platform_from_version(major, minor): ","#LINE# #TAB# v = (major, minor) #LINE# #TAB# for low, high, name in _platforms: #LINE# #TAB# #TAB# if low <= v <= high: #LINE# #TAB# #TAB# #TAB# return name #LINE# #TAB# return None"
"Search the given device for the specified string property @param device_type Type of Device @param property String to search for @return Python string containing the value , or None if not found  <code> def get_string_property(device_type, property): ","#LINE# #TAB# key = cf.CFStringCreateWithCString(kCFAllocatorDefault, property.encode #LINE# #TAB# #TAB# ('mac_roman'), kCFStringEncodingMacRoman) #LINE# #TAB# CFContainer = iokit.IORegistryEntryCreateCFProperty(device_type, key, #LINE# #TAB# #TAB# kCFAllocatorDefault, 0) #LINE# #TAB# output = None #LINE# #TAB# if CFContainer: #LINE# #TAB# #TAB# output = cf.CFStringGetCStringPtr(CFContainer, 0) #LINE# #TAB# #TAB# if output is not None: #LINE# #TAB# #TAB# #TAB# output = output.decode('mac_roman') #LINE# #TAB# #TAB# cf.CFRelease(CFContainer) #LINE# #TAB# return output"
"Returns symmetric vertex or -1 if not symmetric vertex found : param vertex_index : int <code> def get_symmetric_vertex(vertex_index, sym_table_list): ",#LINE# #TAB# for i in range(len(sym_table_list)): #LINE# #TAB# #TAB# if int(vertex_index) == int(sym_table_list[i]): #LINE# #TAB# #TAB# #TAB# if i % 2 == 0: #LINE# #TAB# #TAB# #TAB# #TAB# sym_vtx = sym_table_list[i + 1] #LINE# #TAB# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# #TAB# sym_vtx = sym_table_list[i - 1] #LINE# #TAB# #TAB# #TAB# break #LINE# #TAB# return sym_vtx
"Normalizes length of strings in ` ` string_list ` ` . All strings are modified to be as long as the longest one in list . Missing characters in string are appended using ` ` fill_chr ` `  <code> def normalize_width(string_list, fill_chr=' '): ",#LINE# #TAB# width = calculate_width(string_list) #LINE# #TAB# return [(l + fill_chr * (width - len(l))) for l in string_list]
"Will get the children of the specified dn <code> def get_dn_children(session, dn): ",#LINE# #TAB# mo_query_url = '/api/mo/' + dn + '.json?query-target=children' #LINE# #TAB# ret = session.get(mo_query_url) #LINE# #TAB# node_data = ret.json()['imdata'] #LINE# #TAB# return node_data
"Given a ZimbraId value , returns its corresponding UID <code> def zimbraid_to_uid(ldap_data, target_zimbraid): ",#LINE# #TAB# attrs = [i[1] for i in ldap_data] #LINE# #TAB# for attr in attrs: #LINE# #TAB# #TAB# decoded_oc = [a.decode() for a in attr.get('objectClass')] #LINE# #TAB# #TAB# if 'inetOrgPerson' in decoded_oc and attr.get('uid'): #LINE# #TAB# #TAB# #TAB# zimbraid = attr.get('zimbraId')[0].decode() #LINE# #TAB# #TAB# #TAB# uid = attr.get('uid')[0].decode() #LINE# #TAB# #TAB# #TAB# if target_zimbraid == zimbraid: #LINE# #TAB# #TAB# #TAB# #TAB# return uid
"Convert a string from one keyspace to another  <code> def change_keyspace(string, original_keyspace, target_keyspace): ","#LINE# #TAB# assert isinstance(string, str) #LINE# #TAB# intermediate_integer = string_to_int(string, original_keyspace) #LINE# #TAB# output_string = int_to_string(intermediate_integer, target_keyspace) #LINE# #TAB# return output_string"
"Generator that yields tracebacks found in a file object <code> def tracebacks_from_file(fileobj, reverse=False): ",#LINE# #TAB# if reverse: #LINE# #TAB# #TAB# lines = deque() #LINE# #TAB# #TAB# for line in BackwardsReader(fileobj): #LINE# #TAB# #TAB# #TAB# lines.appendleft(line) #LINE# #TAB# #TAB# #TAB# if tb_head in line: #LINE# #TAB# #TAB# #TAB# #TAB# yield next(tracebacks_from_lines(lines)) #LINE# #TAB# #TAB# #TAB# #TAB# lines.clear() #LINE# #TAB# else: #LINE# #TAB# #TAB# for traceback in tracebacks_from_lines(fileobj): #LINE# #TAB# #TAB# #TAB# yield traceback
"Chop up and word - wrap a long string of text  <code> def word_wrap(text_to_wrap, text_width): ",#LINE# #TAB# this_row = '' #LINE# #TAB# previous_row = '' #LINE# #TAB# rows = [] #LINE# #TAB# input_words = text_to_wrap.split(' ') #LINE# #TAB# for word in input_words: #LINE# #TAB# #TAB# this_row += word + ' ' #LINE# #TAB# #TAB# if len(this_row) > text_width: #LINE# #TAB# #TAB# #TAB# rows.append(previous_row[:-1]) #LINE# #TAB# #TAB# #TAB# this_row = '' #LINE# #TAB# #TAB# #TAB# this_row += word + ' ' #LINE# #TAB# #TAB# else: #LINE# #TAB# #TAB# #TAB# previous_row = this_row #LINE# #TAB# if not this_row == '': #LINE# #TAB# #TAB# rows.append(this_row) #LINE# #TAB# return rows
Return a sparse row sub - matrix of the identity matrix for ` ` var ` `  <code> def identity_sub(var): ,"#LINE# #TAB# n = var.dim #LINE# #TAB# if var.parent is not None: #LINE# #TAB# #TAB# n = var.parent.dim #LINE# #TAB# if var.dim == 1: #LINE# #TAB# #TAB# m = lil_matrix((1, n)) #LINE# #TAB# #TAB# m[0, var.indices[0]] = 1 #LINE# #TAB# #TAB# return m #LINE# #TAB# return I(n)[(var.indices), :]"
Recursively converts payload with mixed ABI objects and dicts to dict <code> def mixed_to_dict(payload: Any): ,"#LINE# #TAB# if isinstance(payload, dict): #LINE# #TAB# #TAB# return {k: mixed_to_dict(v) for k, v in payload.items()} #LINE# #TAB# if is_abi_object(type(payload)): #LINE# #TAB# #TAB# return asdict(payload) #LINE# #TAB# return payload"
"Instantiate DeploymentDefinitions from a list  <code> def from_list(cls, deployments): ","#LINE# #TAB# results = [] #LINE# #TAB# if not deployments: #LINE# #TAB# #TAB# return [] #LINE# #TAB# for i, deployment in enumerate(deployments): #LINE# #TAB# #TAB# if not deployment.get('name'): #LINE# #TAB# #TAB# #TAB# deployment['name'] = 'deployment_{}'.format(str(i + 1)) #LINE# #TAB# #TAB# results.append(cls(deployment)) #LINE# #TAB# return results"
"Extract the first image for the given URL which is a sibling of an "" h1 "" <code> def get_first_image(url, soup=None): ",#LINE# #TAB# if soup is None: #LINE# #TAB# #TAB# soup = _get_soup_from_url(url) #LINE# #TAB# h1_tag = soup.find('h1') #LINE# #TAB# if h1_tag: #LINE# #TAB# #TAB# first_image = h1_tag.find_next_sibling('img') #LINE# #TAB# #TAB# if first_image and first_image['src'] != '': #LINE# #TAB# #TAB# #TAB# return first_image['src'] #LINE# #TAB# return None
